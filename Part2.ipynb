{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "doFa4mfZWDq9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "dDYh-uO7b7r9",
    "outputId": "b7a54755-333d-4772-dd74-d2787d42cc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 766 entries, 0 to 765\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       766 non-null    float64\n",
      " 1   1       766 non-null    float64\n",
      " 2   2       766 non-null    float64\n",
      " 3   3       766 non-null    float64\n",
      " 4   4       766 non-null    float64\n",
      " 5   5       766 non-null    float64\n",
      " 6   6       766 non-null    float64\n",
      " 7   target  766 non-null    int64  \n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 48.0 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dataset_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.6596861445779,\n        \"min\": -1.1425774921219225,\n        \"max\": 766.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          9.276014827155354e-17,\n          -0.2521817894326822,\n          766.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.8556349732026,\n        \"min\": -3.7895382741712553,\n        \"max\": 766.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.588798933361102e-16,\n          -0.1225405312450935,\n          766.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.8007775122975,\n        \"min\": -3.569402517404558,\n        \"max\": 766.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.933539689087881e-16,\n          0.1487897130212839,\n          766.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.615850580935,\n        \"min\": -1.288765040534549,\n        \"max\": 766.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          766.0,\n          1.2986420758017498e-16,\n          0.7188053185036796\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.5150553894902,\n        \"min\": -0.6950959541679449,\n        \"max\": 766.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          766.0,\n          -1.913178058100792e-17,\n          0.41361726161287016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.76202958914314,\n        \"min\": -4.05658522390442,\n        \"max\": 766.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.287328451478552e-16,\n          0.0002320075214056,\n          766.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.56961504294566,\n        \"min\": -1.1903187128105774,\n        \"max\": 766.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.3682121870054148e-16,\n          -0.2948497161787228,\n          766.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270.67942547044333,\n        \"min\": 0.0,\n        \"max\": 766.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.34986945169712796,\n          1.0,\n          0.4772401417206076\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b709cdd7-f13c-42bd-9aff-617bc8156770\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>7.660000e+02</td>\n",
       "      <td>766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.276015e-17</td>\n",
       "      <td>5.588799e-16</td>\n",
       "      <td>2.933540e-16</td>\n",
       "      <td>1.298642e-16</td>\n",
       "      <td>-1.913178e-17</td>\n",
       "      <td>5.287328e-16</td>\n",
       "      <td>1.368212e-16</td>\n",
       "      <td>0.349869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>1.000653e+00</td>\n",
       "      <td>0.477240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.142577e+00</td>\n",
       "      <td>-3.789538e+00</td>\n",
       "      <td>-3.569403e+00</td>\n",
       "      <td>-1.288765e+00</td>\n",
       "      <td>-6.950960e-01</td>\n",
       "      <td>-4.056585e+00</td>\n",
       "      <td>-1.190319e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.457789e-01</td>\n",
       "      <td>-6.866940e-01</td>\n",
       "      <td>-3.418051e-01</td>\n",
       "      <td>-1.288765e+00</td>\n",
       "      <td>-6.950960e-01</td>\n",
       "      <td>-5.956130e-01</td>\n",
       "      <td>-6.889769e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.521818e-01</td>\n",
       "      <td>-1.225405e-01</td>\n",
       "      <td>1.487897e-01</td>\n",
       "      <td>1.541762e-01</td>\n",
       "      <td>-3.826601e-01</td>\n",
       "      <td>2.320075e-04</td>\n",
       "      <td>-2.948497e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.382139e-01</td>\n",
       "      <td>5.983223e-01</td>\n",
       "      <td>5.619222e-01</td>\n",
       "      <td>7.188053e-01</td>\n",
       "      <td>4.136173e-01</td>\n",
       "      <td>5.833995e-01</td>\n",
       "      <td>4.632033e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.902998e+00</td>\n",
       "      <td>2.447492e+00</td>\n",
       "      <td>2.730868e+00</td>\n",
       "      <td>4.922156e+00</td>\n",
       "      <td>6.647146e+00</td>\n",
       "      <td>4.450053e+00</td>\n",
       "      <td>5.882829e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b709cdd7-f13c-42bd-9aff-617bc8156770')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b709cdd7-f13c-42bd-9aff-617bc8156770 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b709cdd7-f13c-42bd-9aff-617bc8156770');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-95eb3a53-35f4-413c-ae4b-269b4df70171\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-95eb3a53-35f4-413c-ae4b-269b4df70171')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-95eb3a53-35f4-413c-ae4b-269b4df70171 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  7.660000e+02  7.660000e+02  7.660000e+02  7.660000e+02  7.660000e+02   \n",
       "mean   9.276015e-17  5.588799e-16  2.933540e-16  1.298642e-16 -1.913178e-17   \n",
       "std    1.000653e+00  1.000653e+00  1.000653e+00  1.000653e+00  1.000653e+00   \n",
       "min   -1.142577e+00 -3.789538e+00 -3.569403e+00 -1.288765e+00 -6.950960e-01   \n",
       "25%   -8.457789e-01 -6.866940e-01 -3.418051e-01 -1.288765e+00 -6.950960e-01   \n",
       "50%   -2.521818e-01 -1.225405e-01  1.487897e-01  1.541762e-01 -3.826601e-01   \n",
       "75%    6.382139e-01  5.983223e-01  5.619222e-01  7.188053e-01  4.136173e-01   \n",
       "max    3.902998e+00  2.447492e+00  2.730868e+00  4.922156e+00  6.647146e+00   \n",
       "\n",
       "                  5             6      target  \n",
       "count  7.660000e+02  7.660000e+02  766.000000  \n",
       "mean   5.287328e-16  1.368212e-16    0.349869  \n",
       "std    1.000653e+00  1.000653e+00    0.477240  \n",
       "min   -4.056585e+00 -1.190319e+00    0.000000  \n",
       "25%   -5.956130e-01 -6.889769e-01    0.000000  \n",
       "50%    2.320075e-04 -2.948497e-01    0.000000  \n",
       "75%    5.833995e-01  4.632033e-01    1.000000  \n",
       "max    4.450053e+00  5.882829e+00    1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv('/content/preprocessed_data.csv')\n",
    "dataset_df.info()\n",
    "dataset_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXx3ZbzscA8M",
    "outputId": "6f4adc63-db3f-41f6-da18-a369abe98225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (602, 7) (602,)\n",
      "Validation set shape: (87, 7) (87,)\n",
      "Test set shape: (77, 7) (77,)\n",
      "X_train_tensor shape: torch.Size([602, 7])\n",
      "y_train_tensor shape: torch.Size([602])\n",
      "X_train_validate_tensor shape: torch.Size([87, 7])\n",
      "y_train_validate_tensor shape: torch.Size([87])\n",
      "X_test_validate_tensor shape: torch.Size([689, 7])\n",
      "y_test_validate_tensor shape: torch.Size([689])\n",
      "X_test_tensor shape: torch.Size([77, 7])\n",
      "y_test_tensor shape: torch.Size([77])\n"
     ]
    }
   ],
   "source": [
    "X = dataset_df.drop(columns='target')\n",
    "y = dataset_df['target']\n",
    "\n",
    "X_test_validate, X_test, y_test_validate, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "X_train, X_train_validate, y_train, y_train_validate = train_test_split(X_test_validate, y_test_validate, test_size=0.125, random_state=42, stratify=y_test_validate)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_train_validate.shape, y_train_validate.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "X_test_validate_tensor = torch.tensor(X_test_validate.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_validate_tensor = torch.tensor(y_test_validate.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_train_validate_tensor = torch.tensor(X_train_validate.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_train_validate_tensor = torch.tensor(y_train_validate.values, dtype=torch.float32)\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"X_train_validate_tensor shape:\", X_train_validate_tensor.shape)\n",
    "print(\"y_train_validate_tensor shape:\", y_train_validate_tensor.shape)\n",
    "print(\"X_test_validate_tensor shape:\", X_test_validate_tensor.shape)\n",
    "print(\"y_test_validate_tensor shape:\", y_test_validate_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Et6kmrGddtDd",
    "outputId": "2d5ea7f1-a3e0-4d58-cd28-7b63ea1007cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lk1_Hq0vf8dM"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_train_validate_tensor = X_train_validate_tensor.to(device)\n",
    "y_train_validate_tensor = y_train_validate_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Npmdaqrjeh_y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVExVLmAgKMm"
   },
   "source": [
    "Taking Optimizer as the Hyperparameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_SqruoDgVhH"
   },
   "source": [
    "Optimizer=ASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgmScscpfv0n",
    "outputId": "11860313-eba4-4d90-9c25-a5c70de92183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 41.80739, Validation Loss: 6.17083\n",
      "Epoch 2/1000, Training Loss: 41.78589, Validation Loss: 6.16714\n",
      "Epoch 3/1000, Training Loss: 41.76453, Validation Loss: 6.16347\n",
      "Epoch 4/1000, Training Loss: 41.74330, Validation Loss: 6.15982\n",
      "Epoch 5/1000, Training Loss: 41.72220, Validation Loss: 6.15620\n",
      "Epoch 6/1000, Training Loss: 41.70124, Validation Loss: 6.15259\n",
      "Epoch 7/1000, Training Loss: 41.68040, Validation Loss: 6.14901\n",
      "Epoch 8/1000, Training Loss: 41.65969, Validation Loss: 6.14546\n",
      "Epoch 9/1000, Training Loss: 41.63911, Validation Loss: 6.14192\n",
      "Epoch 10/1000, Training Loss: 41.61865, Validation Loss: 6.13840\n",
      "Epoch 11/1000, Training Loss: 41.59833, Validation Loss: 6.13490\n",
      "Epoch 12/1000, Training Loss: 41.57813, Validation Loss: 6.13142\n",
      "Epoch 13/1000, Training Loss: 41.55804, Validation Loss: 6.12797\n",
      "Epoch 14/1000, Training Loss: 41.53806, Validation Loss: 6.12453\n",
      "Epoch 15/1000, Training Loss: 41.51820, Validation Loss: 6.12110\n",
      "Epoch 16/1000, Training Loss: 41.49845, Validation Loss: 6.11770\n",
      "Epoch 17/1000, Training Loss: 41.47881, Validation Loss: 6.11433\n",
      "Epoch 18/1000, Training Loss: 41.45928, Validation Loss: 6.11097\n",
      "Epoch 19/1000, Training Loss: 41.43985, Validation Loss: 6.10763\n",
      "Epoch 20/1000, Training Loss: 41.42053, Validation Loss: 6.10430\n",
      "Epoch 21/1000, Training Loss: 41.40132, Validation Loss: 6.10099\n",
      "Epoch 22/1000, Training Loss: 41.38220, Validation Loss: 6.09770\n",
      "Epoch 23/1000, Training Loss: 41.36319, Validation Loss: 6.09442\n",
      "Epoch 24/1000, Training Loss: 41.34428, Validation Loss: 6.09115\n",
      "Epoch 25/1000, Training Loss: 41.32547, Validation Loss: 6.08791\n",
      "Epoch 26/1000, Training Loss: 41.30677, Validation Loss: 6.08468\n",
      "Epoch 27/1000, Training Loss: 41.28818, Validation Loss: 6.08148\n",
      "Epoch 28/1000, Training Loss: 41.26970, Validation Loss: 6.07829\n",
      "Epoch 29/1000, Training Loss: 41.25133, Validation Loss: 6.07512\n",
      "Epoch 30/1000, Training Loss: 41.23307, Validation Loss: 6.07196\n",
      "Epoch 31/1000, Training Loss: 41.21490, Validation Loss: 6.06882\n",
      "Epoch 32/1000, Training Loss: 41.19685, Validation Loss: 6.06570\n",
      "Epoch 33/1000, Training Loss: 41.17890, Validation Loss: 6.06259\n",
      "Epoch 34/1000, Training Loss: 41.16106, Validation Loss: 6.05950\n",
      "Epoch 35/1000, Training Loss: 41.14333, Validation Loss: 6.05642\n",
      "Epoch 36/1000, Training Loss: 41.12569, Validation Loss: 6.05336\n",
      "Epoch 37/1000, Training Loss: 41.10815, Validation Loss: 6.05032\n",
      "Epoch 38/1000, Training Loss: 41.09070, Validation Loss: 6.04729\n",
      "Epoch 39/1000, Training Loss: 41.07335, Validation Loss: 6.04427\n",
      "Epoch 40/1000, Training Loss: 41.05609, Validation Loss: 6.04127\n",
      "Epoch 41/1000, Training Loss: 41.03892, Validation Loss: 6.03829\n",
      "Epoch 42/1000, Training Loss: 41.02184, Validation Loss: 6.03532\n",
      "Epoch 43/1000, Training Loss: 41.00485, Validation Loss: 6.03236\n",
      "Epoch 44/1000, Training Loss: 40.98795, Validation Loss: 6.02941\n",
      "Epoch 45/1000, Training Loss: 40.97112, Validation Loss: 6.02649\n",
      "Epoch 46/1000, Training Loss: 40.95438, Validation Loss: 6.02357\n",
      "Epoch 47/1000, Training Loss: 40.93773, Validation Loss: 6.02067\n",
      "Epoch 48/1000, Training Loss: 40.92115, Validation Loss: 6.01778\n",
      "Epoch 49/1000, Training Loss: 40.90465, Validation Loss: 6.01491\n",
      "Epoch 50/1000, Training Loss: 40.88824, Validation Loss: 6.01205\n",
      "Epoch 51/1000, Training Loss: 40.87191, Validation Loss: 6.00921\n",
      "Epoch 52/1000, Training Loss: 40.85567, Validation Loss: 6.00638\n",
      "Epoch 53/1000, Training Loss: 40.83951, Validation Loss: 6.00357\n",
      "Epoch 54/1000, Training Loss: 40.82343, Validation Loss: 6.00077\n",
      "Epoch 55/1000, Training Loss: 40.80743, Validation Loss: 5.99798\n",
      "Epoch 56/1000, Training Loss: 40.79151, Validation Loss: 5.99521\n",
      "Epoch 57/1000, Training Loss: 40.77565, Validation Loss: 5.99245\n",
      "Epoch 58/1000, Training Loss: 40.75987, Validation Loss: 5.98970\n",
      "Epoch 59/1000, Training Loss: 40.74417, Validation Loss: 5.98696\n",
      "Epoch 60/1000, Training Loss: 40.72853, Validation Loss: 5.98423\n",
      "Epoch 61/1000, Training Loss: 40.71297, Validation Loss: 5.98152\n",
      "Epoch 62/1000, Training Loss: 40.69748, Validation Loss: 5.97882\n",
      "Epoch 63/1000, Training Loss: 40.68205, Validation Loss: 5.97612\n",
      "Epoch 64/1000, Training Loss: 40.66670, Validation Loss: 5.97344\n",
      "Epoch 65/1000, Training Loss: 40.65142, Validation Loss: 5.97078\n",
      "Epoch 66/1000, Training Loss: 40.63621, Validation Loss: 5.96812\n",
      "Epoch 67/1000, Training Loss: 40.62106, Validation Loss: 5.96547\n",
      "Epoch 68/1000, Training Loss: 40.60598, Validation Loss: 5.96284\n",
      "Epoch 69/1000, Training Loss: 40.59095, Validation Loss: 5.96022\n",
      "Epoch 70/1000, Training Loss: 40.57600, Validation Loss: 5.95760\n",
      "Epoch 71/1000, Training Loss: 40.56111, Validation Loss: 5.95500\n",
      "Epoch 72/1000, Training Loss: 40.54629, Validation Loss: 5.95241\n",
      "Epoch 73/1000, Training Loss: 40.53154, Validation Loss: 5.94983\n",
      "Epoch 74/1000, Training Loss: 40.51685, Validation Loss: 5.94725\n",
      "Epoch 75/1000, Training Loss: 40.50222, Validation Loss: 5.94469\n",
      "Epoch 76/1000, Training Loss: 40.48765, Validation Loss: 5.94214\n",
      "Epoch 77/1000, Training Loss: 40.47314, Validation Loss: 5.93960\n",
      "Epoch 78/1000, Training Loss: 40.45871, Validation Loss: 5.93707\n",
      "Epoch 79/1000, Training Loss: 40.44433, Validation Loss: 5.93454\n",
      "Epoch 80/1000, Training Loss: 40.43000, Validation Loss: 5.93203\n",
      "Epoch 81/1000, Training Loss: 40.41574, Validation Loss: 5.92953\n",
      "Epoch 82/1000, Training Loss: 40.40153, Validation Loss: 5.92703\n",
      "Epoch 83/1000, Training Loss: 40.38738, Validation Loss: 5.92455\n",
      "Epoch 84/1000, Training Loss: 40.37329, Validation Loss: 5.92208\n",
      "Epoch 85/1000, Training Loss: 40.35926, Validation Loss: 5.91961\n",
      "Epoch 86/1000, Training Loss: 40.34529, Validation Loss: 5.91716\n",
      "Epoch 87/1000, Training Loss: 40.33137, Validation Loss: 5.91471\n",
      "Epoch 88/1000, Training Loss: 40.31750, Validation Loss: 5.91227\n",
      "Epoch 89/1000, Training Loss: 40.30368, Validation Loss: 5.90984\n",
      "Epoch 90/1000, Training Loss: 40.28992, Validation Loss: 5.90742\n",
      "Epoch 91/1000, Training Loss: 40.27622, Validation Loss: 5.90501\n",
      "Epoch 92/1000, Training Loss: 40.26257, Validation Loss: 5.90260\n",
      "Epoch 93/1000, Training Loss: 40.24898, Validation Loss: 5.90020\n",
      "Epoch 94/1000, Training Loss: 40.23543, Validation Loss: 5.89781\n",
      "Epoch 95/1000, Training Loss: 40.22194, Validation Loss: 5.89544\n",
      "Epoch 96/1000, Training Loss: 40.20851, Validation Loss: 5.89307\n",
      "Epoch 97/1000, Training Loss: 40.19513, Validation Loss: 5.89071\n",
      "Epoch 98/1000, Training Loss: 40.18181, Validation Loss: 5.88836\n",
      "Epoch 99/1000, Training Loss: 40.16854, Validation Loss: 5.88602\n",
      "Epoch 100/1000, Training Loss: 40.15533, Validation Loss: 5.88369\n",
      "Epoch 101/1000, Training Loss: 40.14218, Validation Loss: 5.88136\n",
      "Epoch 102/1000, Training Loss: 40.12907, Validation Loss: 5.87905\n",
      "Epoch 103/1000, Training Loss: 40.11601, Validation Loss: 5.87674\n",
      "Epoch 104/1000, Training Loss: 40.10300, Validation Loss: 5.87444\n",
      "Epoch 105/1000, Training Loss: 40.09004, Validation Loss: 5.87215\n",
      "Epoch 106/1000, Training Loss: 40.07713, Validation Loss: 5.86986\n",
      "Epoch 107/1000, Training Loss: 40.06426, Validation Loss: 5.86759\n",
      "Epoch 108/1000, Training Loss: 40.05144, Validation Loss: 5.86532\n",
      "Epoch 109/1000, Training Loss: 40.03867, Validation Loss: 5.86307\n",
      "Epoch 110/1000, Training Loss: 40.02595, Validation Loss: 5.86082\n",
      "Epoch 111/1000, Training Loss: 40.01327, Validation Loss: 5.85857\n",
      "Epoch 112/1000, Training Loss: 40.00063, Validation Loss: 5.85634\n",
      "Epoch 113/1000, Training Loss: 39.98803, Validation Loss: 5.85410\n",
      "Epoch 114/1000, Training Loss: 39.97547, Validation Loss: 5.85188\n",
      "Epoch 115/1000, Training Loss: 39.96295, Validation Loss: 5.84966\n",
      "Epoch 116/1000, Training Loss: 39.95048, Validation Loss: 5.84744\n",
      "Epoch 117/1000, Training Loss: 39.93805, Validation Loss: 5.84524\n",
      "Epoch 118/1000, Training Loss: 39.92566, Validation Loss: 5.84304\n",
      "Epoch 119/1000, Training Loss: 39.91330, Validation Loss: 5.84084\n",
      "Epoch 120/1000, Training Loss: 39.90099, Validation Loss: 5.83866\n",
      "Epoch 121/1000, Training Loss: 39.88872, Validation Loss: 5.83648\n",
      "Epoch 122/1000, Training Loss: 39.87649, Validation Loss: 5.83431\n",
      "Epoch 123/1000, Training Loss: 39.86430, Validation Loss: 5.83214\n",
      "Epoch 124/1000, Training Loss: 39.85217, Validation Loss: 5.82998\n",
      "Epoch 125/1000, Training Loss: 39.84007, Validation Loss: 5.82783\n",
      "Epoch 126/1000, Training Loss: 39.82801, Validation Loss: 5.82568\n",
      "Epoch 127/1000, Training Loss: 39.81599, Validation Loss: 5.82354\n",
      "Epoch 128/1000, Training Loss: 39.80402, Validation Loss: 5.82140\n",
      "Epoch 129/1000, Training Loss: 39.79208, Validation Loss: 5.81928\n",
      "Epoch 130/1000, Training Loss: 39.78019, Validation Loss: 5.81716\n",
      "Epoch 131/1000, Training Loss: 39.76833, Validation Loss: 5.81504\n",
      "Epoch 132/1000, Training Loss: 39.75652, Validation Loss: 5.81293\n",
      "Epoch 133/1000, Training Loss: 39.74474, Validation Loss: 5.81083\n",
      "Epoch 134/1000, Training Loss: 39.73300, Validation Loss: 5.80873\n",
      "Epoch 135/1000, Training Loss: 39.72129, Validation Loss: 5.80664\n",
      "Epoch 136/1000, Training Loss: 39.70963, Validation Loss: 5.80456\n",
      "Epoch 137/1000, Training Loss: 39.69799, Validation Loss: 5.80248\n",
      "Epoch 138/1000, Training Loss: 39.68639, Validation Loss: 5.80041\n",
      "Epoch 139/1000, Training Loss: 39.67485, Validation Loss: 5.79835\n",
      "Epoch 140/1000, Training Loss: 39.66334, Validation Loss: 5.79629\n",
      "Epoch 141/1000, Training Loss: 39.65187, Validation Loss: 5.79424\n",
      "Epoch 142/1000, Training Loss: 39.64046, Validation Loss: 5.79219\n",
      "Epoch 143/1000, Training Loss: 39.62908, Validation Loss: 5.79016\n",
      "Epoch 144/1000, Training Loss: 39.61773, Validation Loss: 5.78812\n",
      "Epoch 145/1000, Training Loss: 39.60640, Validation Loss: 5.78610\n",
      "Epoch 146/1000, Training Loss: 39.59512, Validation Loss: 5.78408\n",
      "Epoch 147/1000, Training Loss: 39.58387, Validation Loss: 5.78206\n",
      "Epoch 148/1000, Training Loss: 39.57264, Validation Loss: 5.78005\n",
      "Epoch 149/1000, Training Loss: 39.56144, Validation Loss: 5.77804\n",
      "Epoch 150/1000, Training Loss: 39.55027, Validation Loss: 5.77604\n",
      "Epoch 151/1000, Training Loss: 39.53914, Validation Loss: 5.77404\n",
      "Epoch 152/1000, Training Loss: 39.52803, Validation Loss: 5.77205\n",
      "Epoch 153/1000, Training Loss: 39.51696, Validation Loss: 5.77006\n",
      "Epoch 154/1000, Training Loss: 39.50591, Validation Loss: 5.76807\n",
      "Epoch 155/1000, Training Loss: 39.49489, Validation Loss: 5.76609\n",
      "Epoch 156/1000, Training Loss: 39.48390, Validation Loss: 5.76411\n",
      "Epoch 157/1000, Training Loss: 39.47293, Validation Loss: 5.76214\n",
      "Epoch 158/1000, Training Loss: 39.46198, Validation Loss: 5.76017\n",
      "Epoch 159/1000, Training Loss: 39.45107, Validation Loss: 5.75821\n",
      "Epoch 160/1000, Training Loss: 39.44018, Validation Loss: 5.75626\n",
      "Epoch 161/1000, Training Loss: 39.42932, Validation Loss: 5.75431\n",
      "Epoch 162/1000, Training Loss: 39.41848, Validation Loss: 5.75236\n",
      "Epoch 163/1000, Training Loss: 39.40768, Validation Loss: 5.75042\n",
      "Epoch 164/1000, Training Loss: 39.39690, Validation Loss: 5.74848\n",
      "Epoch 165/1000, Training Loss: 39.38616, Validation Loss: 5.74655\n",
      "Epoch 166/1000, Training Loss: 39.37543, Validation Loss: 5.74462\n",
      "Epoch 167/1000, Training Loss: 39.36474, Validation Loss: 5.74270\n",
      "Epoch 168/1000, Training Loss: 39.35407, Validation Loss: 5.74078\n",
      "Epoch 169/1000, Training Loss: 39.34343, Validation Loss: 5.73887\n",
      "Epoch 170/1000, Training Loss: 39.33281, Validation Loss: 5.73696\n",
      "Epoch 171/1000, Training Loss: 39.32223, Validation Loss: 5.73505\n",
      "Epoch 172/1000, Training Loss: 39.31167, Validation Loss: 5.73315\n",
      "Epoch 173/1000, Training Loss: 39.30115, Validation Loss: 5.73125\n",
      "Epoch 174/1000, Training Loss: 39.29065, Validation Loss: 5.72936\n",
      "Epoch 175/1000, Training Loss: 39.28018, Validation Loss: 5.72747\n",
      "Epoch 176/1000, Training Loss: 39.26975, Validation Loss: 5.72559\n",
      "Epoch 177/1000, Training Loss: 39.25934, Validation Loss: 5.72371\n",
      "Epoch 178/1000, Training Loss: 39.24894, Validation Loss: 5.72184\n",
      "Epoch 179/1000, Training Loss: 39.23858, Validation Loss: 5.71997\n",
      "Epoch 180/1000, Training Loss: 39.22824, Validation Loss: 5.71810\n",
      "Epoch 181/1000, Training Loss: 39.21792, Validation Loss: 5.71624\n",
      "Epoch 182/1000, Training Loss: 39.20762, Validation Loss: 5.71438\n",
      "Epoch 183/1000, Training Loss: 39.19734, Validation Loss: 5.71253\n",
      "Epoch 184/1000, Training Loss: 39.18709, Validation Loss: 5.71068\n",
      "Epoch 185/1000, Training Loss: 39.17687, Validation Loss: 5.70884\n",
      "Epoch 186/1000, Training Loss: 39.16666, Validation Loss: 5.70699\n",
      "Epoch 187/1000, Training Loss: 39.15647, Validation Loss: 5.70515\n",
      "Epoch 188/1000, Training Loss: 39.14631, Validation Loss: 5.70332\n",
      "Epoch 189/1000, Training Loss: 39.13617, Validation Loss: 5.70149\n",
      "Epoch 190/1000, Training Loss: 39.12605, Validation Loss: 5.69966\n",
      "Epoch 191/1000, Training Loss: 39.11595, Validation Loss: 5.69783\n",
      "Epoch 192/1000, Training Loss: 39.10586, Validation Loss: 5.69601\n",
      "Epoch 193/1000, Training Loss: 39.09578, Validation Loss: 5.69419\n",
      "Epoch 194/1000, Training Loss: 39.08574, Validation Loss: 5.69237\n",
      "Epoch 195/1000, Training Loss: 39.07572, Validation Loss: 5.69056\n",
      "Epoch 196/1000, Training Loss: 39.06571, Validation Loss: 5.68875\n",
      "Epoch 197/1000, Training Loss: 39.05572, Validation Loss: 5.68695\n",
      "Epoch 198/1000, Training Loss: 39.04576, Validation Loss: 5.68515\n",
      "Epoch 199/1000, Training Loss: 39.03581, Validation Loss: 5.68335\n",
      "Epoch 200/1000, Training Loss: 39.02588, Validation Loss: 5.68156\n",
      "Epoch 201/1000, Training Loss: 39.01597, Validation Loss: 5.67977\n",
      "Epoch 202/1000, Training Loss: 39.00609, Validation Loss: 5.67798\n",
      "Epoch 203/1000, Training Loss: 38.99622, Validation Loss: 5.67620\n",
      "Epoch 204/1000, Training Loss: 38.98637, Validation Loss: 5.67441\n",
      "Epoch 205/1000, Training Loss: 38.97654, Validation Loss: 5.67264\n",
      "Epoch 206/1000, Training Loss: 38.96673, Validation Loss: 5.67086\n",
      "Epoch 207/1000, Training Loss: 38.95693, Validation Loss: 5.66909\n",
      "Epoch 208/1000, Training Loss: 38.94716, Validation Loss: 5.66732\n",
      "Epoch 209/1000, Training Loss: 38.93740, Validation Loss: 5.66555\n",
      "Epoch 210/1000, Training Loss: 38.92767, Validation Loss: 5.66379\n",
      "Epoch 211/1000, Training Loss: 38.91795, Validation Loss: 5.66203\n",
      "Epoch 212/1000, Training Loss: 38.90825, Validation Loss: 5.66027\n",
      "Epoch 213/1000, Training Loss: 38.89856, Validation Loss: 5.65851\n",
      "Epoch 214/1000, Training Loss: 38.88888, Validation Loss: 5.65676\n",
      "Epoch 215/1000, Training Loss: 38.87923, Validation Loss: 5.65501\n",
      "Epoch 216/1000, Training Loss: 38.86959, Validation Loss: 5.65327\n",
      "Epoch 217/1000, Training Loss: 38.85997, Validation Loss: 5.65152\n",
      "Epoch 218/1000, Training Loss: 38.85036, Validation Loss: 5.64978\n",
      "Epoch 219/1000, Training Loss: 38.84077, Validation Loss: 5.64804\n",
      "Epoch 220/1000, Training Loss: 38.83120, Validation Loss: 5.64631\n",
      "Epoch 221/1000, Training Loss: 38.82166, Validation Loss: 5.64458\n",
      "Epoch 222/1000, Training Loss: 38.81214, Validation Loss: 5.64285\n",
      "Epoch 223/1000, Training Loss: 38.80263, Validation Loss: 5.64112\n",
      "Epoch 224/1000, Training Loss: 38.79314, Validation Loss: 5.63940\n",
      "Epoch 225/1000, Training Loss: 38.78367, Validation Loss: 5.63768\n",
      "Epoch 226/1000, Training Loss: 38.77421, Validation Loss: 5.63595\n",
      "Epoch 227/1000, Training Loss: 38.76477, Validation Loss: 5.63423\n",
      "Epoch 228/1000, Training Loss: 38.75533, Validation Loss: 5.63251\n",
      "Epoch 229/1000, Training Loss: 38.74590, Validation Loss: 5.63080\n",
      "Epoch 230/1000, Training Loss: 38.73648, Validation Loss: 5.62908\n",
      "Epoch 231/1000, Training Loss: 38.72708, Validation Loss: 5.62737\n",
      "Epoch 232/1000, Training Loss: 38.71769, Validation Loss: 5.62566\n",
      "Epoch 233/1000, Training Loss: 38.70832, Validation Loss: 5.62395\n",
      "Epoch 234/1000, Training Loss: 38.69895, Validation Loss: 5.62224\n",
      "Epoch 235/1000, Training Loss: 38.68960, Validation Loss: 5.62054\n",
      "Epoch 236/1000, Training Loss: 38.68026, Validation Loss: 5.61883\n",
      "Epoch 237/1000, Training Loss: 38.67094, Validation Loss: 5.61713\n",
      "Epoch 238/1000, Training Loss: 38.66164, Validation Loss: 5.61543\n",
      "Epoch 239/1000, Training Loss: 38.65234, Validation Loss: 5.61374\n",
      "Epoch 240/1000, Training Loss: 38.64305, Validation Loss: 5.61204\n",
      "Epoch 241/1000, Training Loss: 38.63378, Validation Loss: 5.61035\n",
      "Epoch 242/1000, Training Loss: 38.62452, Validation Loss: 5.60866\n",
      "Epoch 243/1000, Training Loss: 38.61528, Validation Loss: 5.60697\n",
      "Epoch 244/1000, Training Loss: 38.60605, Validation Loss: 5.60529\n",
      "Epoch 245/1000, Training Loss: 38.59682, Validation Loss: 5.60361\n",
      "Epoch 246/1000, Training Loss: 38.58760, Validation Loss: 5.60193\n",
      "Epoch 247/1000, Training Loss: 38.57839, Validation Loss: 5.60025\n",
      "Epoch 248/1000, Training Loss: 38.56921, Validation Loss: 5.59857\n",
      "Epoch 249/1000, Training Loss: 38.56003, Validation Loss: 5.59690\n",
      "Epoch 250/1000, Training Loss: 38.55087, Validation Loss: 5.59524\n",
      "Epoch 251/1000, Training Loss: 38.54171, Validation Loss: 5.59357\n",
      "Epoch 252/1000, Training Loss: 38.53257, Validation Loss: 5.59191\n",
      "Epoch 253/1000, Training Loss: 38.52344, Validation Loss: 5.59025\n",
      "Epoch 254/1000, Training Loss: 38.51431, Validation Loss: 5.58859\n",
      "Epoch 255/1000, Training Loss: 38.50520, Validation Loss: 5.58694\n",
      "Epoch 256/1000, Training Loss: 38.49610, Validation Loss: 5.58528\n",
      "Epoch 257/1000, Training Loss: 38.48700, Validation Loss: 5.58363\n",
      "Epoch 258/1000, Training Loss: 38.47791, Validation Loss: 5.58199\n",
      "Epoch 259/1000, Training Loss: 38.46882, Validation Loss: 5.58034\n",
      "Epoch 260/1000, Training Loss: 38.45975, Validation Loss: 5.57869\n",
      "Epoch 261/1000, Training Loss: 38.45068, Validation Loss: 5.57705\n",
      "Epoch 262/1000, Training Loss: 38.44162, Validation Loss: 5.57540\n",
      "Epoch 263/1000, Training Loss: 38.43258, Validation Loss: 5.57376\n",
      "Epoch 264/1000, Training Loss: 38.42354, Validation Loss: 5.57212\n",
      "Epoch 265/1000, Training Loss: 38.41451, Validation Loss: 5.57048\n",
      "Epoch 266/1000, Training Loss: 38.40548, Validation Loss: 5.56884\n",
      "Epoch 267/1000, Training Loss: 38.39646, Validation Loss: 5.56721\n",
      "Epoch 268/1000, Training Loss: 38.38746, Validation Loss: 5.56558\n",
      "Epoch 269/1000, Training Loss: 38.37846, Validation Loss: 5.56394\n",
      "Epoch 270/1000, Training Loss: 38.36947, Validation Loss: 5.56231\n",
      "Epoch 271/1000, Training Loss: 38.36048, Validation Loss: 5.56068\n",
      "Epoch 272/1000, Training Loss: 38.35151, Validation Loss: 5.55905\n",
      "Epoch 273/1000, Training Loss: 38.34254, Validation Loss: 5.55742\n",
      "Epoch 274/1000, Training Loss: 38.33359, Validation Loss: 5.55580\n",
      "Epoch 275/1000, Training Loss: 38.32464, Validation Loss: 5.55417\n",
      "Epoch 276/1000, Training Loss: 38.31570, Validation Loss: 5.55255\n",
      "Epoch 277/1000, Training Loss: 38.30676, Validation Loss: 5.55092\n",
      "Epoch 278/1000, Training Loss: 38.29783, Validation Loss: 5.54930\n",
      "Epoch 279/1000, Training Loss: 38.28891, Validation Loss: 5.54767\n",
      "Epoch 280/1000, Training Loss: 38.27999, Validation Loss: 5.54605\n",
      "Epoch 281/1000, Training Loss: 38.27108, Validation Loss: 5.54443\n",
      "Epoch 282/1000, Training Loss: 38.26219, Validation Loss: 5.54281\n",
      "Epoch 283/1000, Training Loss: 38.25330, Validation Loss: 5.54119\n",
      "Epoch 284/1000, Training Loss: 38.24442, Validation Loss: 5.53957\n",
      "Epoch 285/1000, Training Loss: 38.23555, Validation Loss: 5.53796\n",
      "Epoch 286/1000, Training Loss: 38.22669, Validation Loss: 5.53635\n",
      "Epoch 287/1000, Training Loss: 38.21783, Validation Loss: 5.53473\n",
      "Epoch 288/1000, Training Loss: 38.20899, Validation Loss: 5.53312\n",
      "Epoch 289/1000, Training Loss: 38.20016, Validation Loss: 5.53152\n",
      "Epoch 290/1000, Training Loss: 38.19133, Validation Loss: 5.52991\n",
      "Epoch 291/1000, Training Loss: 38.18251, Validation Loss: 5.52830\n",
      "Epoch 292/1000, Training Loss: 38.17369, Validation Loss: 5.52669\n",
      "Epoch 293/1000, Training Loss: 38.16487, Validation Loss: 5.52508\n",
      "Epoch 294/1000, Training Loss: 38.15606, Validation Loss: 5.52347\n",
      "Epoch 295/1000, Training Loss: 38.14726, Validation Loss: 5.52187\n",
      "Epoch 296/1000, Training Loss: 38.13847, Validation Loss: 5.52026\n",
      "Epoch 297/1000, Training Loss: 38.12968, Validation Loss: 5.51865\n",
      "Epoch 298/1000, Training Loss: 38.12090, Validation Loss: 5.51705\n",
      "Epoch 299/1000, Training Loss: 38.11213, Validation Loss: 5.51545\n",
      "Epoch 300/1000, Training Loss: 38.10336, Validation Loss: 5.51384\n",
      "Epoch 301/1000, Training Loss: 38.09459, Validation Loss: 5.51224\n",
      "Epoch 302/1000, Training Loss: 38.08582, Validation Loss: 5.51064\n",
      "Epoch 303/1000, Training Loss: 38.07706, Validation Loss: 5.50904\n",
      "Epoch 304/1000, Training Loss: 38.06830, Validation Loss: 5.50744\n",
      "Epoch 305/1000, Training Loss: 38.05954, Validation Loss: 5.50584\n",
      "Epoch 306/1000, Training Loss: 38.05079, Validation Loss: 5.50424\n",
      "Epoch 307/1000, Training Loss: 38.04204, Validation Loss: 5.50264\n",
      "Epoch 308/1000, Training Loss: 38.03330, Validation Loss: 5.50105\n",
      "Epoch 309/1000, Training Loss: 38.02456, Validation Loss: 5.49945\n",
      "Epoch 310/1000, Training Loss: 38.01583, Validation Loss: 5.49786\n",
      "Epoch 311/1000, Training Loss: 38.00711, Validation Loss: 5.49626\n",
      "Epoch 312/1000, Training Loss: 37.99838, Validation Loss: 5.49467\n",
      "Epoch 313/1000, Training Loss: 37.98967, Validation Loss: 5.49307\n",
      "Epoch 314/1000, Training Loss: 37.98096, Validation Loss: 5.49148\n",
      "Epoch 315/1000, Training Loss: 37.97226, Validation Loss: 5.48989\n",
      "Epoch 316/1000, Training Loss: 37.96356, Validation Loss: 5.48829\n",
      "Epoch 317/1000, Training Loss: 37.95489, Validation Loss: 5.48670\n",
      "Epoch 318/1000, Training Loss: 37.94622, Validation Loss: 5.48511\n",
      "Epoch 319/1000, Training Loss: 37.93755, Validation Loss: 5.48353\n",
      "Epoch 320/1000, Training Loss: 37.92889, Validation Loss: 5.48194\n",
      "Epoch 321/1000, Training Loss: 37.92024, Validation Loss: 5.48035\n",
      "Epoch 322/1000, Training Loss: 37.91161, Validation Loss: 5.47877\n",
      "Epoch 323/1000, Training Loss: 37.90298, Validation Loss: 5.47719\n",
      "Epoch 324/1000, Training Loss: 37.89435, Validation Loss: 5.47561\n",
      "Epoch 325/1000, Training Loss: 37.88572, Validation Loss: 5.47402\n",
      "Epoch 326/1000, Training Loss: 37.87709, Validation Loss: 5.47244\n",
      "Epoch 327/1000, Training Loss: 37.86846, Validation Loss: 5.47086\n",
      "Epoch 328/1000, Training Loss: 37.85983, Validation Loss: 5.46928\n",
      "Epoch 329/1000, Training Loss: 37.85121, Validation Loss: 5.46770\n",
      "Epoch 330/1000, Training Loss: 37.84259, Validation Loss: 5.46613\n",
      "Epoch 331/1000, Training Loss: 37.83397, Validation Loss: 5.46455\n",
      "Epoch 332/1000, Training Loss: 37.82536, Validation Loss: 5.46298\n",
      "Epoch 333/1000, Training Loss: 37.81675, Validation Loss: 5.46140\n",
      "Epoch 334/1000, Training Loss: 37.80814, Validation Loss: 5.45983\n",
      "Epoch 335/1000, Training Loss: 37.79954, Validation Loss: 5.45826\n",
      "Epoch 336/1000, Training Loss: 37.79094, Validation Loss: 5.45668\n",
      "Epoch 337/1000, Training Loss: 37.78234, Validation Loss: 5.45511\n",
      "Epoch 338/1000, Training Loss: 37.77373, Validation Loss: 5.45354\n",
      "Epoch 339/1000, Training Loss: 37.76513, Validation Loss: 5.45197\n",
      "Epoch 340/1000, Training Loss: 37.75653, Validation Loss: 5.45039\n",
      "Epoch 341/1000, Training Loss: 37.74792, Validation Loss: 5.44882\n",
      "Epoch 342/1000, Training Loss: 37.73930, Validation Loss: 5.44725\n",
      "Epoch 343/1000, Training Loss: 37.73070, Validation Loss: 5.44567\n",
      "Epoch 344/1000, Training Loss: 37.72210, Validation Loss: 5.44410\n",
      "Epoch 345/1000, Training Loss: 37.71350, Validation Loss: 5.44253\n",
      "Epoch 346/1000, Training Loss: 37.70490, Validation Loss: 5.44096\n",
      "Epoch 347/1000, Training Loss: 37.69630, Validation Loss: 5.43939\n",
      "Epoch 348/1000, Training Loss: 37.68770, Validation Loss: 5.43782\n",
      "Epoch 349/1000, Training Loss: 37.67912, Validation Loss: 5.43625\n",
      "Epoch 350/1000, Training Loss: 37.67055, Validation Loss: 5.43468\n",
      "Epoch 351/1000, Training Loss: 37.66197, Validation Loss: 5.43311\n",
      "Epoch 352/1000, Training Loss: 37.65341, Validation Loss: 5.43155\n",
      "Epoch 353/1000, Training Loss: 37.64484, Validation Loss: 5.42998\n",
      "Epoch 354/1000, Training Loss: 37.63628, Validation Loss: 5.42841\n",
      "Epoch 355/1000, Training Loss: 37.62774, Validation Loss: 5.42684\n",
      "Epoch 356/1000, Training Loss: 37.61920, Validation Loss: 5.42528\n",
      "Epoch 357/1000, Training Loss: 37.61066, Validation Loss: 5.42371\n",
      "Epoch 358/1000, Training Loss: 37.60212, Validation Loss: 5.42214\n",
      "Epoch 359/1000, Training Loss: 37.59358, Validation Loss: 5.42057\n",
      "Epoch 360/1000, Training Loss: 37.58504, Validation Loss: 5.41900\n",
      "Epoch 361/1000, Training Loss: 37.57650, Validation Loss: 5.41743\n",
      "Epoch 362/1000, Training Loss: 37.56798, Validation Loss: 5.41586\n",
      "Epoch 363/1000, Training Loss: 37.55946, Validation Loss: 5.41430\n",
      "Epoch 364/1000, Training Loss: 37.55094, Validation Loss: 5.41273\n",
      "Epoch 365/1000, Training Loss: 37.54242, Validation Loss: 5.41116\n",
      "Epoch 366/1000, Training Loss: 37.53391, Validation Loss: 5.40959\n",
      "Epoch 367/1000, Training Loss: 37.52539, Validation Loss: 5.40802\n",
      "Epoch 368/1000, Training Loss: 37.51687, Validation Loss: 5.40645\n",
      "Epoch 369/1000, Training Loss: 37.50836, Validation Loss: 5.40489\n",
      "Epoch 370/1000, Training Loss: 37.49984, Validation Loss: 5.40332\n",
      "Epoch 371/1000, Training Loss: 37.49132, Validation Loss: 5.40176\n",
      "Epoch 372/1000, Training Loss: 37.48281, Validation Loss: 5.40019\n",
      "Epoch 373/1000, Training Loss: 37.47429, Validation Loss: 5.39863\n",
      "Epoch 374/1000, Training Loss: 37.46577, Validation Loss: 5.39706\n",
      "Epoch 375/1000, Training Loss: 37.45726, Validation Loss: 5.39550\n",
      "Epoch 376/1000, Training Loss: 37.44875, Validation Loss: 5.39393\n",
      "Epoch 377/1000, Training Loss: 37.44024, Validation Loss: 5.39236\n",
      "Epoch 378/1000, Training Loss: 37.43172, Validation Loss: 5.39079\n",
      "Epoch 379/1000, Training Loss: 37.42320, Validation Loss: 5.38922\n",
      "Epoch 380/1000, Training Loss: 37.41469, Validation Loss: 5.38766\n",
      "Epoch 381/1000, Training Loss: 37.40619, Validation Loss: 5.38609\n",
      "Epoch 382/1000, Training Loss: 37.39769, Validation Loss: 5.38453\n",
      "Epoch 383/1000, Training Loss: 37.38919, Validation Loss: 5.38297\n",
      "Epoch 384/1000, Training Loss: 37.38069, Validation Loss: 5.38140\n",
      "Epoch 385/1000, Training Loss: 37.37219, Validation Loss: 5.37984\n",
      "Epoch 386/1000, Training Loss: 37.36370, Validation Loss: 5.37827\n",
      "Epoch 387/1000, Training Loss: 37.35521, Validation Loss: 5.37671\n",
      "Epoch 388/1000, Training Loss: 37.34673, Validation Loss: 5.37515\n",
      "Epoch 389/1000, Training Loss: 37.33823, Validation Loss: 5.37359\n",
      "Epoch 390/1000, Training Loss: 37.32974, Validation Loss: 5.37203\n",
      "Epoch 391/1000, Training Loss: 37.32125, Validation Loss: 5.37046\n",
      "Epoch 392/1000, Training Loss: 37.31275, Validation Loss: 5.36890\n",
      "Epoch 393/1000, Training Loss: 37.30426, Validation Loss: 5.36734\n",
      "Epoch 394/1000, Training Loss: 37.29576, Validation Loss: 5.36578\n",
      "Epoch 395/1000, Training Loss: 37.28727, Validation Loss: 5.36421\n",
      "Epoch 396/1000, Training Loss: 37.27877, Validation Loss: 5.36265\n",
      "Epoch 397/1000, Training Loss: 37.27027, Validation Loss: 5.36108\n",
      "Epoch 398/1000, Training Loss: 37.26178, Validation Loss: 5.35952\n",
      "Epoch 399/1000, Training Loss: 37.25329, Validation Loss: 5.35795\n",
      "Epoch 400/1000, Training Loss: 37.24481, Validation Loss: 5.35639\n",
      "Epoch 401/1000, Training Loss: 37.23632, Validation Loss: 5.35482\n",
      "Epoch 402/1000, Training Loss: 37.22782, Validation Loss: 5.35326\n",
      "Epoch 403/1000, Training Loss: 37.21932, Validation Loss: 5.35169\n",
      "Epoch 404/1000, Training Loss: 37.21083, Validation Loss: 5.35013\n",
      "Epoch 405/1000, Training Loss: 37.20233, Validation Loss: 5.34856\n",
      "Epoch 406/1000, Training Loss: 37.19384, Validation Loss: 5.34700\n",
      "Epoch 407/1000, Training Loss: 37.18535, Validation Loss: 5.34544\n",
      "Epoch 408/1000, Training Loss: 37.17686, Validation Loss: 5.34387\n",
      "Epoch 409/1000, Training Loss: 37.16838, Validation Loss: 5.34231\n",
      "Epoch 410/1000, Training Loss: 37.15989, Validation Loss: 5.34075\n",
      "Epoch 411/1000, Training Loss: 37.15140, Validation Loss: 5.33919\n",
      "Epoch 412/1000, Training Loss: 37.14292, Validation Loss: 5.33762\n",
      "Epoch 413/1000, Training Loss: 37.13444, Validation Loss: 5.33606\n",
      "Epoch 414/1000, Training Loss: 37.12597, Validation Loss: 5.33450\n",
      "Epoch 415/1000, Training Loss: 37.11749, Validation Loss: 5.33294\n",
      "Epoch 416/1000, Training Loss: 37.10900, Validation Loss: 5.33137\n",
      "Epoch 417/1000, Training Loss: 37.10052, Validation Loss: 5.32981\n",
      "Epoch 418/1000, Training Loss: 37.09204, Validation Loss: 5.32824\n",
      "Epoch 419/1000, Training Loss: 37.08356, Validation Loss: 5.32668\n",
      "Epoch 420/1000, Training Loss: 37.07509, Validation Loss: 5.32511\n",
      "Epoch 421/1000, Training Loss: 37.06662, Validation Loss: 5.32355\n",
      "Epoch 422/1000, Training Loss: 37.05814, Validation Loss: 5.32198\n",
      "Epoch 423/1000, Training Loss: 37.04967, Validation Loss: 5.32042\n",
      "Epoch 424/1000, Training Loss: 37.04120, Validation Loss: 5.31886\n",
      "Epoch 425/1000, Training Loss: 37.03272, Validation Loss: 5.31729\n",
      "Epoch 426/1000, Training Loss: 37.02425, Validation Loss: 5.31573\n",
      "Epoch 427/1000, Training Loss: 37.01578, Validation Loss: 5.31416\n",
      "Epoch 428/1000, Training Loss: 37.00731, Validation Loss: 5.31260\n",
      "Epoch 429/1000, Training Loss: 36.99885, Validation Loss: 5.31104\n",
      "Epoch 430/1000, Training Loss: 36.99038, Validation Loss: 5.30948\n",
      "Epoch 431/1000, Training Loss: 36.98192, Validation Loss: 5.30791\n",
      "Epoch 432/1000, Training Loss: 36.97346, Validation Loss: 5.30635\n",
      "Epoch 433/1000, Training Loss: 36.96499, Validation Loss: 5.30479\n",
      "Epoch 434/1000, Training Loss: 36.95653, Validation Loss: 5.30323\n",
      "Epoch 435/1000, Training Loss: 36.94808, Validation Loss: 5.30167\n",
      "Epoch 436/1000, Training Loss: 36.93962, Validation Loss: 5.30011\n",
      "Epoch 437/1000, Training Loss: 36.93117, Validation Loss: 5.29855\n",
      "Epoch 438/1000, Training Loss: 36.92273, Validation Loss: 5.29699\n",
      "Epoch 439/1000, Training Loss: 36.91429, Validation Loss: 5.29543\n",
      "Epoch 440/1000, Training Loss: 36.90585, Validation Loss: 5.29387\n",
      "Epoch 441/1000, Training Loss: 36.89740, Validation Loss: 5.29231\n",
      "Epoch 442/1000, Training Loss: 36.88895, Validation Loss: 5.29075\n",
      "Epoch 443/1000, Training Loss: 36.88051, Validation Loss: 5.28919\n",
      "Epoch 444/1000, Training Loss: 36.87205, Validation Loss: 5.28763\n",
      "Epoch 445/1000, Training Loss: 36.86359, Validation Loss: 5.28606\n",
      "Epoch 446/1000, Training Loss: 36.85513, Validation Loss: 5.28450\n",
      "Epoch 447/1000, Training Loss: 36.84667, Validation Loss: 5.28293\n",
      "Epoch 448/1000, Training Loss: 36.83821, Validation Loss: 5.28137\n",
      "Epoch 449/1000, Training Loss: 36.82976, Validation Loss: 5.27980\n",
      "Epoch 450/1000, Training Loss: 36.82132, Validation Loss: 5.27824\n",
      "Epoch 451/1000, Training Loss: 36.81286, Validation Loss: 5.27667\n",
      "Epoch 452/1000, Training Loss: 36.80439, Validation Loss: 5.27510\n",
      "Epoch 453/1000, Training Loss: 36.79592, Validation Loss: 5.27353\n",
      "Epoch 454/1000, Training Loss: 36.78745, Validation Loss: 5.27196\n",
      "Epoch 455/1000, Training Loss: 36.77898, Validation Loss: 5.27039\n",
      "Epoch 456/1000, Training Loss: 36.77051, Validation Loss: 5.26882\n",
      "Epoch 457/1000, Training Loss: 36.76204, Validation Loss: 5.26726\n",
      "Epoch 458/1000, Training Loss: 36.75356, Validation Loss: 5.26569\n",
      "Epoch 459/1000, Training Loss: 36.74507, Validation Loss: 5.26412\n",
      "Epoch 460/1000, Training Loss: 36.73660, Validation Loss: 5.26254\n",
      "Epoch 461/1000, Training Loss: 36.72812, Validation Loss: 5.26097\n",
      "Epoch 462/1000, Training Loss: 36.71966, Validation Loss: 5.25940\n",
      "Epoch 463/1000, Training Loss: 36.71119, Validation Loss: 5.25782\n",
      "Epoch 464/1000, Training Loss: 36.70272, Validation Loss: 5.25625\n",
      "Epoch 465/1000, Training Loss: 36.69426, Validation Loss: 5.25468\n",
      "Epoch 466/1000, Training Loss: 36.68581, Validation Loss: 5.25311\n",
      "Epoch 467/1000, Training Loss: 36.67737, Validation Loss: 5.25154\n",
      "Epoch 468/1000, Training Loss: 36.66892, Validation Loss: 5.24996\n",
      "Epoch 469/1000, Training Loss: 36.66047, Validation Loss: 5.24839\n",
      "Epoch 470/1000, Training Loss: 36.65202, Validation Loss: 5.24682\n",
      "Epoch 471/1000, Training Loss: 36.64356, Validation Loss: 5.24525\n",
      "Epoch 472/1000, Training Loss: 36.63510, Validation Loss: 5.24368\n",
      "Epoch 473/1000, Training Loss: 36.62664, Validation Loss: 5.24211\n",
      "Epoch 474/1000, Training Loss: 36.61818, Validation Loss: 5.24054\n",
      "Epoch 475/1000, Training Loss: 36.60972, Validation Loss: 5.23897\n",
      "Epoch 476/1000, Training Loss: 36.60127, Validation Loss: 5.23740\n",
      "Epoch 477/1000, Training Loss: 36.59281, Validation Loss: 5.23583\n",
      "Epoch 478/1000, Training Loss: 36.58436, Validation Loss: 5.23425\n",
      "Epoch 479/1000, Training Loss: 36.57590, Validation Loss: 5.23268\n",
      "Epoch 480/1000, Training Loss: 36.56745, Validation Loss: 5.23112\n",
      "Epoch 481/1000, Training Loss: 36.55900, Validation Loss: 5.22955\n",
      "Epoch 482/1000, Training Loss: 36.55055, Validation Loss: 5.22799\n",
      "Epoch 483/1000, Training Loss: 36.54210, Validation Loss: 5.22643\n",
      "Epoch 484/1000, Training Loss: 36.53365, Validation Loss: 5.22487\n",
      "Epoch 485/1000, Training Loss: 36.52520, Validation Loss: 5.22331\n",
      "Epoch 486/1000, Training Loss: 36.51674, Validation Loss: 5.22175\n",
      "Epoch 487/1000, Training Loss: 36.50829, Validation Loss: 5.22019\n",
      "Epoch 488/1000, Training Loss: 36.49983, Validation Loss: 5.21863\n",
      "Epoch 489/1000, Training Loss: 36.49136, Validation Loss: 5.21706\n",
      "Epoch 490/1000, Training Loss: 36.48289, Validation Loss: 5.21550\n",
      "Epoch 491/1000, Training Loss: 36.47442, Validation Loss: 5.21394\n",
      "Epoch 492/1000, Training Loss: 36.46595, Validation Loss: 5.21238\n",
      "Epoch 493/1000, Training Loss: 36.45748, Validation Loss: 5.21082\n",
      "Epoch 494/1000, Training Loss: 36.44901, Validation Loss: 5.20926\n",
      "Epoch 495/1000, Training Loss: 36.44054, Validation Loss: 5.20769\n",
      "Epoch 496/1000, Training Loss: 36.43207, Validation Loss: 5.20613\n",
      "Epoch 497/1000, Training Loss: 36.42360, Validation Loss: 5.20457\n",
      "Epoch 498/1000, Training Loss: 36.41512, Validation Loss: 5.20300\n",
      "Epoch 499/1000, Training Loss: 36.40664, Validation Loss: 5.20144\n",
      "Epoch 500/1000, Training Loss: 36.39816, Validation Loss: 5.19987\n",
      "Epoch 501/1000, Training Loss: 36.38967, Validation Loss: 5.19831\n",
      "Epoch 502/1000, Training Loss: 36.38117, Validation Loss: 5.19674\n",
      "Epoch 503/1000, Training Loss: 36.37266, Validation Loss: 5.19518\n",
      "Epoch 504/1000, Training Loss: 36.36415, Validation Loss: 5.19361\n",
      "Epoch 505/1000, Training Loss: 36.35564, Validation Loss: 5.19204\n",
      "Epoch 506/1000, Training Loss: 36.34713, Validation Loss: 5.19047\n",
      "Epoch 507/1000, Training Loss: 36.33862, Validation Loss: 5.18891\n",
      "Epoch 508/1000, Training Loss: 36.33010, Validation Loss: 5.18735\n",
      "Epoch 509/1000, Training Loss: 36.32159, Validation Loss: 5.18578\n",
      "Epoch 510/1000, Training Loss: 36.31309, Validation Loss: 5.18422\n",
      "Epoch 511/1000, Training Loss: 36.30459, Validation Loss: 5.18266\n",
      "Epoch 512/1000, Training Loss: 36.29608, Validation Loss: 5.18109\n",
      "Epoch 513/1000, Training Loss: 36.28757, Validation Loss: 5.17953\n",
      "Epoch 514/1000, Training Loss: 36.27905, Validation Loss: 5.17796\n",
      "Epoch 515/1000, Training Loss: 36.27054, Validation Loss: 5.17639\n",
      "Epoch 516/1000, Training Loss: 36.26202, Validation Loss: 5.17483\n",
      "Epoch 517/1000, Training Loss: 36.25351, Validation Loss: 5.17326\n",
      "Epoch 518/1000, Training Loss: 36.24499, Validation Loss: 5.17170\n",
      "Epoch 519/1000, Training Loss: 36.23647, Validation Loss: 5.17013\n",
      "Epoch 520/1000, Training Loss: 36.22794, Validation Loss: 5.16857\n",
      "Epoch 521/1000, Training Loss: 36.21942, Validation Loss: 5.16700\n",
      "Epoch 522/1000, Training Loss: 36.21089, Validation Loss: 5.16544\n",
      "Epoch 523/1000, Training Loss: 36.20236, Validation Loss: 5.16387\n",
      "Epoch 524/1000, Training Loss: 36.19382, Validation Loss: 5.16230\n",
      "Epoch 525/1000, Training Loss: 36.18528, Validation Loss: 5.16073\n",
      "Epoch 526/1000, Training Loss: 36.17673, Validation Loss: 5.15916\n",
      "Epoch 527/1000, Training Loss: 36.16819, Validation Loss: 5.15759\n",
      "Epoch 528/1000, Training Loss: 36.15963, Validation Loss: 5.15603\n",
      "Epoch 529/1000, Training Loss: 36.15108, Validation Loss: 5.15446\n",
      "Epoch 530/1000, Training Loss: 36.14253, Validation Loss: 5.15289\n",
      "Epoch 531/1000, Training Loss: 36.13398, Validation Loss: 5.15132\n",
      "Epoch 532/1000, Training Loss: 36.12543, Validation Loss: 5.14976\n",
      "Epoch 533/1000, Training Loss: 36.11689, Validation Loss: 5.14819\n",
      "Epoch 534/1000, Training Loss: 36.10835, Validation Loss: 5.14662\n",
      "Epoch 535/1000, Training Loss: 36.09981, Validation Loss: 5.14505\n",
      "Epoch 536/1000, Training Loss: 36.09126, Validation Loss: 5.14349\n",
      "Epoch 537/1000, Training Loss: 36.08272, Validation Loss: 5.14192\n",
      "Epoch 538/1000, Training Loss: 36.07417, Validation Loss: 5.14035\n",
      "Epoch 539/1000, Training Loss: 36.06561, Validation Loss: 5.13878\n",
      "Epoch 540/1000, Training Loss: 36.05706, Validation Loss: 5.13721\n",
      "Epoch 541/1000, Training Loss: 36.04848, Validation Loss: 5.13564\n",
      "Epoch 542/1000, Training Loss: 36.03990, Validation Loss: 5.13407\n",
      "Epoch 543/1000, Training Loss: 36.03131, Validation Loss: 5.13250\n",
      "Epoch 544/1000, Training Loss: 36.02272, Validation Loss: 5.13093\n",
      "Epoch 545/1000, Training Loss: 36.01413, Validation Loss: 5.12936\n",
      "Epoch 546/1000, Training Loss: 36.00553, Validation Loss: 5.12779\n",
      "Epoch 547/1000, Training Loss: 35.99693, Validation Loss: 5.12622\n",
      "Epoch 548/1000, Training Loss: 35.98832, Validation Loss: 5.12465\n",
      "Epoch 549/1000, Training Loss: 35.97972, Validation Loss: 5.12309\n",
      "Epoch 550/1000, Training Loss: 35.97111, Validation Loss: 5.12152\n",
      "Epoch 551/1000, Training Loss: 35.96249, Validation Loss: 5.11995\n",
      "Epoch 552/1000, Training Loss: 35.95388, Validation Loss: 5.11838\n",
      "Epoch 553/1000, Training Loss: 35.94526, Validation Loss: 5.11682\n",
      "Epoch 554/1000, Training Loss: 35.93664, Validation Loss: 5.11525\n",
      "Epoch 555/1000, Training Loss: 35.92802, Validation Loss: 5.11368\n",
      "Epoch 556/1000, Training Loss: 35.91940, Validation Loss: 5.11211\n",
      "Epoch 557/1000, Training Loss: 35.91079, Validation Loss: 5.11054\n",
      "Epoch 558/1000, Training Loss: 35.90219, Validation Loss: 5.10897\n",
      "Epoch 559/1000, Training Loss: 35.89358, Validation Loss: 5.10740\n",
      "Epoch 560/1000, Training Loss: 35.88499, Validation Loss: 5.10583\n",
      "Epoch 561/1000, Training Loss: 35.87639, Validation Loss: 5.10426\n",
      "Epoch 562/1000, Training Loss: 35.86778, Validation Loss: 5.10269\n",
      "Epoch 563/1000, Training Loss: 35.85918, Validation Loss: 5.10112\n",
      "Epoch 564/1000, Training Loss: 35.85057, Validation Loss: 5.09955\n",
      "Epoch 565/1000, Training Loss: 35.84196, Validation Loss: 5.09798\n",
      "Epoch 566/1000, Training Loss: 35.83335, Validation Loss: 5.09641\n",
      "Epoch 567/1000, Training Loss: 35.82474, Validation Loss: 5.09484\n",
      "Epoch 568/1000, Training Loss: 35.81612, Validation Loss: 5.09327\n",
      "Epoch 569/1000, Training Loss: 35.80750, Validation Loss: 5.09169\n",
      "Epoch 570/1000, Training Loss: 35.79886, Validation Loss: 5.09012\n",
      "Epoch 571/1000, Training Loss: 35.79022, Validation Loss: 5.08854\n",
      "Epoch 572/1000, Training Loss: 35.78158, Validation Loss: 5.08697\n",
      "Epoch 573/1000, Training Loss: 35.77293, Validation Loss: 5.08539\n",
      "Epoch 574/1000, Training Loss: 35.76428, Validation Loss: 5.08382\n",
      "Epoch 575/1000, Training Loss: 35.75562, Validation Loss: 5.08224\n",
      "Epoch 576/1000, Training Loss: 35.74696, Validation Loss: 5.08066\n",
      "Epoch 577/1000, Training Loss: 35.73830, Validation Loss: 5.07909\n",
      "Epoch 578/1000, Training Loss: 35.72963, Validation Loss: 5.07751\n",
      "Epoch 579/1000, Training Loss: 35.72097, Validation Loss: 5.07593\n",
      "Epoch 580/1000, Training Loss: 35.71233, Validation Loss: 5.07435\n",
      "Epoch 581/1000, Training Loss: 35.70370, Validation Loss: 5.07277\n",
      "Epoch 582/1000, Training Loss: 35.69506, Validation Loss: 5.07120\n",
      "Epoch 583/1000, Training Loss: 35.68643, Validation Loss: 5.06962\n",
      "Epoch 584/1000, Training Loss: 35.67780, Validation Loss: 5.06804\n",
      "Epoch 585/1000, Training Loss: 35.66916, Validation Loss: 5.06647\n",
      "Epoch 586/1000, Training Loss: 35.66053, Validation Loss: 5.06490\n",
      "Epoch 587/1000, Training Loss: 35.65190, Validation Loss: 5.06333\n",
      "Epoch 588/1000, Training Loss: 35.64327, Validation Loss: 5.06176\n",
      "Epoch 589/1000, Training Loss: 35.63464, Validation Loss: 5.06018\n",
      "Epoch 590/1000, Training Loss: 35.62600, Validation Loss: 5.05861\n",
      "Epoch 591/1000, Training Loss: 35.61736, Validation Loss: 5.05704\n",
      "Epoch 592/1000, Training Loss: 35.60872, Validation Loss: 5.05547\n",
      "Epoch 593/1000, Training Loss: 35.60008, Validation Loss: 5.05390\n",
      "Epoch 594/1000, Training Loss: 35.59145, Validation Loss: 5.05233\n",
      "Epoch 595/1000, Training Loss: 35.58281, Validation Loss: 5.05076\n",
      "Epoch 596/1000, Training Loss: 35.57417, Validation Loss: 5.04919\n",
      "Epoch 597/1000, Training Loss: 35.56553, Validation Loss: 5.04762\n",
      "Epoch 598/1000, Training Loss: 35.55690, Validation Loss: 5.04604\n",
      "Epoch 599/1000, Training Loss: 35.54826, Validation Loss: 5.04447\n",
      "Epoch 600/1000, Training Loss: 35.53961, Validation Loss: 5.04290\n",
      "Epoch 601/1000, Training Loss: 35.53097, Validation Loss: 5.04132\n",
      "Epoch 602/1000, Training Loss: 35.52232, Validation Loss: 5.03975\n",
      "Epoch 603/1000, Training Loss: 35.51366, Validation Loss: 5.03817\n",
      "Epoch 604/1000, Training Loss: 35.50500, Validation Loss: 5.03660\n",
      "Epoch 605/1000, Training Loss: 35.49634, Validation Loss: 5.03502\n",
      "Epoch 606/1000, Training Loss: 35.48767, Validation Loss: 5.03344\n",
      "Epoch 607/1000, Training Loss: 35.47899, Validation Loss: 5.03186\n",
      "Epoch 608/1000, Training Loss: 35.47033, Validation Loss: 5.03029\n",
      "Epoch 609/1000, Training Loss: 35.46167, Validation Loss: 5.02871\n",
      "Epoch 610/1000, Training Loss: 35.45303, Validation Loss: 5.02713\n",
      "Epoch 611/1000, Training Loss: 35.44439, Validation Loss: 5.02555\n",
      "Epoch 612/1000, Training Loss: 35.43576, Validation Loss: 5.02397\n",
      "Epoch 613/1000, Training Loss: 35.42713, Validation Loss: 5.02239\n",
      "Epoch 614/1000, Training Loss: 35.41852, Validation Loss: 5.02082\n",
      "Epoch 615/1000, Training Loss: 35.40992, Validation Loss: 5.01925\n",
      "Epoch 616/1000, Training Loss: 35.40132, Validation Loss: 5.01768\n",
      "Epoch 617/1000, Training Loss: 35.39272, Validation Loss: 5.01611\n",
      "Epoch 618/1000, Training Loss: 35.38412, Validation Loss: 5.01454\n",
      "Epoch 619/1000, Training Loss: 35.37553, Validation Loss: 5.01297\n",
      "Epoch 620/1000, Training Loss: 35.36692, Validation Loss: 5.01139\n",
      "Epoch 621/1000, Training Loss: 35.35832, Validation Loss: 5.00982\n",
      "Epoch 622/1000, Training Loss: 35.34971, Validation Loss: 5.00825\n",
      "Epoch 623/1000, Training Loss: 35.34110, Validation Loss: 5.00667\n",
      "Epoch 624/1000, Training Loss: 35.33249, Validation Loss: 5.00510\n",
      "Epoch 625/1000, Training Loss: 35.32388, Validation Loss: 5.00352\n",
      "Epoch 626/1000, Training Loss: 35.31527, Validation Loss: 5.00195\n",
      "Epoch 627/1000, Training Loss: 35.30666, Validation Loss: 5.00038\n",
      "Epoch 628/1000, Training Loss: 35.29805, Validation Loss: 4.99881\n",
      "Epoch 629/1000, Training Loss: 35.28943, Validation Loss: 4.99725\n",
      "Epoch 630/1000, Training Loss: 35.28082, Validation Loss: 4.99568\n",
      "Epoch 631/1000, Training Loss: 35.27220, Validation Loss: 4.99411\n",
      "Epoch 632/1000, Training Loss: 35.26358, Validation Loss: 4.99254\n",
      "Epoch 633/1000, Training Loss: 35.25496, Validation Loss: 4.99097\n",
      "Epoch 634/1000, Training Loss: 35.24633, Validation Loss: 4.98940\n",
      "Epoch 635/1000, Training Loss: 35.23771, Validation Loss: 4.98783\n",
      "Epoch 636/1000, Training Loss: 35.22908, Validation Loss: 4.98626\n",
      "Epoch 637/1000, Training Loss: 35.22044, Validation Loss: 4.98468\n",
      "Epoch 638/1000, Training Loss: 35.21181, Validation Loss: 4.98311\n",
      "Epoch 639/1000, Training Loss: 35.20317, Validation Loss: 4.98154\n",
      "Epoch 640/1000, Training Loss: 35.19453, Validation Loss: 4.97996\n",
      "Epoch 641/1000, Training Loss: 35.18589, Validation Loss: 4.97839\n",
      "Epoch 642/1000, Training Loss: 35.17725, Validation Loss: 4.97681\n",
      "Epoch 643/1000, Training Loss: 35.16861, Validation Loss: 4.97524\n",
      "Epoch 644/1000, Training Loss: 35.15998, Validation Loss: 4.97367\n",
      "Epoch 645/1000, Training Loss: 35.15135, Validation Loss: 4.97210\n",
      "Epoch 646/1000, Training Loss: 35.14271, Validation Loss: 4.97053\n",
      "Epoch 647/1000, Training Loss: 35.13407, Validation Loss: 4.96897\n",
      "Epoch 648/1000, Training Loss: 35.12544, Validation Loss: 4.96740\n",
      "Epoch 649/1000, Training Loss: 35.11683, Validation Loss: 4.96583\n",
      "Epoch 650/1000, Training Loss: 35.10821, Validation Loss: 4.96427\n",
      "Epoch 651/1000, Training Loss: 35.09960, Validation Loss: 4.96270\n",
      "Epoch 652/1000, Training Loss: 35.09099, Validation Loss: 4.96113\n",
      "Epoch 653/1000, Training Loss: 35.08239, Validation Loss: 4.95957\n",
      "Epoch 654/1000, Training Loss: 35.07380, Validation Loss: 4.95800\n",
      "Epoch 655/1000, Training Loss: 35.06520, Validation Loss: 4.95644\n",
      "Epoch 656/1000, Training Loss: 35.05661, Validation Loss: 4.95487\n",
      "Epoch 657/1000, Training Loss: 35.04802, Validation Loss: 4.95331\n",
      "Epoch 658/1000, Training Loss: 35.03943, Validation Loss: 4.95174\n",
      "Epoch 659/1000, Training Loss: 35.03086, Validation Loss: 4.95018\n",
      "Epoch 660/1000, Training Loss: 35.02229, Validation Loss: 4.94861\n",
      "Epoch 661/1000, Training Loss: 35.01372, Validation Loss: 4.94705\n",
      "Epoch 662/1000, Training Loss: 35.00517, Validation Loss: 4.94549\n",
      "Epoch 663/1000, Training Loss: 34.99661, Validation Loss: 4.94392\n",
      "Epoch 664/1000, Training Loss: 34.98805, Validation Loss: 4.94236\n",
      "Epoch 665/1000, Training Loss: 34.97949, Validation Loss: 4.94080\n",
      "Epoch 666/1000, Training Loss: 34.97093, Validation Loss: 4.93923\n",
      "Epoch 667/1000, Training Loss: 34.96238, Validation Loss: 4.93767\n",
      "Epoch 668/1000, Training Loss: 34.95381, Validation Loss: 4.93610\n",
      "Epoch 669/1000, Training Loss: 34.94524, Validation Loss: 4.93454\n",
      "Epoch 670/1000, Training Loss: 34.93668, Validation Loss: 4.93297\n",
      "Epoch 671/1000, Training Loss: 34.92811, Validation Loss: 4.93141\n",
      "Epoch 672/1000, Training Loss: 34.91954, Validation Loss: 4.92984\n",
      "Epoch 673/1000, Training Loss: 34.91097, Validation Loss: 4.92828\n",
      "Epoch 674/1000, Training Loss: 34.90241, Validation Loss: 4.92672\n",
      "Epoch 675/1000, Training Loss: 34.89384, Validation Loss: 4.92516\n",
      "Epoch 676/1000, Training Loss: 34.88528, Validation Loss: 4.92359\n",
      "Epoch 677/1000, Training Loss: 34.87672, Validation Loss: 4.92203\n",
      "Epoch 678/1000, Training Loss: 34.86815, Validation Loss: 4.92047\n",
      "Epoch 679/1000, Training Loss: 34.85958, Validation Loss: 4.91891\n",
      "Epoch 680/1000, Training Loss: 34.85103, Validation Loss: 4.91735\n",
      "Epoch 681/1000, Training Loss: 34.84248, Validation Loss: 4.91579\n",
      "Epoch 682/1000, Training Loss: 34.83393, Validation Loss: 4.91423\n",
      "Epoch 683/1000, Training Loss: 34.82537, Validation Loss: 4.91267\n",
      "Epoch 684/1000, Training Loss: 34.81682, Validation Loss: 4.91111\n",
      "Epoch 685/1000, Training Loss: 34.80826, Validation Loss: 4.90956\n",
      "Epoch 686/1000, Training Loss: 34.79971, Validation Loss: 4.90800\n",
      "Epoch 687/1000, Training Loss: 34.79115, Validation Loss: 4.90644\n",
      "Epoch 688/1000, Training Loss: 34.78259, Validation Loss: 4.90489\n",
      "Epoch 689/1000, Training Loss: 34.77403, Validation Loss: 4.90333\n",
      "Epoch 690/1000, Training Loss: 34.76548, Validation Loss: 4.90177\n",
      "Epoch 691/1000, Training Loss: 34.75695, Validation Loss: 4.90022\n",
      "Epoch 692/1000, Training Loss: 34.74841, Validation Loss: 4.89866\n",
      "Epoch 693/1000, Training Loss: 34.73987, Validation Loss: 4.89710\n",
      "Epoch 694/1000, Training Loss: 34.73133, Validation Loss: 4.89554\n",
      "Epoch 695/1000, Training Loss: 34.72279, Validation Loss: 4.89398\n",
      "Epoch 696/1000, Training Loss: 34.71426, Validation Loss: 4.89242\n",
      "Epoch 697/1000, Training Loss: 34.70572, Validation Loss: 4.89086\n",
      "Epoch 698/1000, Training Loss: 34.69718, Validation Loss: 4.88931\n",
      "Epoch 699/1000, Training Loss: 34.68863, Validation Loss: 4.88775\n",
      "Epoch 700/1000, Training Loss: 34.68010, Validation Loss: 4.88619\n",
      "Epoch 701/1000, Training Loss: 34.67158, Validation Loss: 4.88463\n",
      "Epoch 702/1000, Training Loss: 34.66306, Validation Loss: 4.88308\n",
      "Epoch 703/1000, Training Loss: 34.65454, Validation Loss: 4.88152\n",
      "Epoch 704/1000, Training Loss: 34.64603, Validation Loss: 4.87996\n",
      "Epoch 705/1000, Training Loss: 34.63751, Validation Loss: 4.87841\n",
      "Epoch 706/1000, Training Loss: 34.62900, Validation Loss: 4.87685\n",
      "Epoch 707/1000, Training Loss: 34.62051, Validation Loss: 4.87529\n",
      "Epoch 708/1000, Training Loss: 34.61203, Validation Loss: 4.87374\n",
      "Epoch 709/1000, Training Loss: 34.60355, Validation Loss: 4.87218\n",
      "Epoch 710/1000, Training Loss: 34.59508, Validation Loss: 4.87062\n",
      "Epoch 711/1000, Training Loss: 34.58660, Validation Loss: 4.86907\n",
      "Epoch 712/1000, Training Loss: 34.57812, Validation Loss: 4.86752\n",
      "Epoch 713/1000, Training Loss: 34.56964, Validation Loss: 4.86597\n",
      "Epoch 714/1000, Training Loss: 34.56116, Validation Loss: 4.86442\n",
      "Epoch 715/1000, Training Loss: 34.55269, Validation Loss: 4.86287\n",
      "Epoch 716/1000, Training Loss: 34.54423, Validation Loss: 4.86132\n",
      "Epoch 717/1000, Training Loss: 34.53578, Validation Loss: 4.85977\n",
      "Epoch 718/1000, Training Loss: 34.52733, Validation Loss: 4.85822\n",
      "Epoch 719/1000, Training Loss: 34.51888, Validation Loss: 4.85667\n",
      "Epoch 720/1000, Training Loss: 34.51043, Validation Loss: 4.85512\n",
      "Epoch 721/1000, Training Loss: 34.50199, Validation Loss: 4.85356\n",
      "Epoch 722/1000, Training Loss: 34.49354, Validation Loss: 4.85201\n",
      "Epoch 723/1000, Training Loss: 34.48510, Validation Loss: 4.85046\n",
      "Epoch 724/1000, Training Loss: 34.47665, Validation Loss: 4.84891\n",
      "Epoch 725/1000, Training Loss: 34.46820, Validation Loss: 4.84735\n",
      "Epoch 726/1000, Training Loss: 34.45975, Validation Loss: 4.84580\n",
      "Epoch 727/1000, Training Loss: 34.45130, Validation Loss: 4.84425\n",
      "Epoch 728/1000, Training Loss: 34.44286, Validation Loss: 4.84270\n",
      "Epoch 729/1000, Training Loss: 34.43442, Validation Loss: 4.84115\n",
      "Epoch 730/1000, Training Loss: 34.42599, Validation Loss: 4.83960\n",
      "Epoch 731/1000, Training Loss: 34.41755, Validation Loss: 4.83806\n",
      "Epoch 732/1000, Training Loss: 34.40912, Validation Loss: 4.83651\n",
      "Epoch 733/1000, Training Loss: 34.40069, Validation Loss: 4.83496\n",
      "Epoch 734/1000, Training Loss: 34.39227, Validation Loss: 4.83341\n",
      "Epoch 735/1000, Training Loss: 34.38384, Validation Loss: 4.83187\n",
      "Epoch 736/1000, Training Loss: 34.37542, Validation Loss: 4.83032\n",
      "Epoch 737/1000, Training Loss: 34.36700, Validation Loss: 4.82877\n",
      "Epoch 738/1000, Training Loss: 34.35859, Validation Loss: 4.82722\n",
      "Epoch 739/1000, Training Loss: 34.35018, Validation Loss: 4.82567\n",
      "Epoch 740/1000, Training Loss: 34.34177, Validation Loss: 4.82412\n",
      "Epoch 741/1000, Training Loss: 34.33336, Validation Loss: 4.82257\n",
      "Epoch 742/1000, Training Loss: 34.32495, Validation Loss: 4.82103\n",
      "Epoch 743/1000, Training Loss: 34.31655, Validation Loss: 4.81948\n",
      "Epoch 744/1000, Training Loss: 34.30815, Validation Loss: 4.81793\n",
      "Epoch 745/1000, Training Loss: 34.29976, Validation Loss: 4.81638\n",
      "Epoch 746/1000, Training Loss: 34.29137, Validation Loss: 4.81484\n",
      "Epoch 747/1000, Training Loss: 34.28298, Validation Loss: 4.81329\n",
      "Epoch 748/1000, Training Loss: 34.27460, Validation Loss: 4.81174\n",
      "Epoch 749/1000, Training Loss: 34.26622, Validation Loss: 4.81020\n",
      "Epoch 750/1000, Training Loss: 34.25786, Validation Loss: 4.80865\n",
      "Epoch 751/1000, Training Loss: 34.24949, Validation Loss: 4.80711\n",
      "Epoch 752/1000, Training Loss: 34.24112, Validation Loss: 4.80556\n",
      "Epoch 753/1000, Training Loss: 34.23276, Validation Loss: 4.80402\n",
      "Epoch 754/1000, Training Loss: 34.22441, Validation Loss: 4.80247\n",
      "Epoch 755/1000, Training Loss: 34.21606, Validation Loss: 4.80093\n",
      "Epoch 756/1000, Training Loss: 34.20772, Validation Loss: 4.79939\n",
      "Epoch 757/1000, Training Loss: 34.19937, Validation Loss: 4.79784\n",
      "Epoch 758/1000, Training Loss: 34.19102, Validation Loss: 4.79629\n",
      "Epoch 759/1000, Training Loss: 34.18267, Validation Loss: 4.79475\n",
      "Epoch 760/1000, Training Loss: 34.17433, Validation Loss: 4.79321\n",
      "Epoch 761/1000, Training Loss: 34.16600, Validation Loss: 4.79166\n",
      "Epoch 762/1000, Training Loss: 34.15766, Validation Loss: 4.79012\n",
      "Epoch 763/1000, Training Loss: 34.14933, Validation Loss: 4.78858\n",
      "Epoch 764/1000, Training Loss: 34.14099, Validation Loss: 4.78703\n",
      "Epoch 765/1000, Training Loss: 34.13266, Validation Loss: 4.78549\n",
      "Epoch 766/1000, Training Loss: 34.12434, Validation Loss: 4.78395\n",
      "Epoch 767/1000, Training Loss: 34.11602, Validation Loss: 4.78241\n",
      "Epoch 768/1000, Training Loss: 34.10771, Validation Loss: 4.78087\n",
      "Epoch 769/1000, Training Loss: 34.09939, Validation Loss: 4.77933\n",
      "Epoch 770/1000, Training Loss: 34.09107, Validation Loss: 4.77780\n",
      "Epoch 771/1000, Training Loss: 34.08276, Validation Loss: 4.77626\n",
      "Epoch 772/1000, Training Loss: 34.07445, Validation Loss: 4.77473\n",
      "Epoch 773/1000, Training Loss: 34.06614, Validation Loss: 4.77320\n",
      "Epoch 774/1000, Training Loss: 34.05783, Validation Loss: 4.77166\n",
      "Epoch 775/1000, Training Loss: 34.04951, Validation Loss: 4.77013\n",
      "Epoch 776/1000, Training Loss: 34.04120, Validation Loss: 4.76860\n",
      "Epoch 777/1000, Training Loss: 34.03289, Validation Loss: 4.76707\n",
      "Epoch 778/1000, Training Loss: 34.02459, Validation Loss: 4.76553\n",
      "Epoch 779/1000, Training Loss: 34.01629, Validation Loss: 4.76400\n",
      "Epoch 780/1000, Training Loss: 34.00799, Validation Loss: 4.76247\n",
      "Epoch 781/1000, Training Loss: 33.99970, Validation Loss: 4.76095\n",
      "Epoch 782/1000, Training Loss: 33.99140, Validation Loss: 4.75942\n",
      "Epoch 783/1000, Training Loss: 33.98311, Validation Loss: 4.75788\n",
      "Epoch 784/1000, Training Loss: 33.97482, Validation Loss: 4.75636\n",
      "Epoch 785/1000, Training Loss: 33.96653, Validation Loss: 4.75483\n",
      "Epoch 786/1000, Training Loss: 33.95824, Validation Loss: 4.75330\n",
      "Epoch 787/1000, Training Loss: 33.94995, Validation Loss: 4.75178\n",
      "Epoch 788/1000, Training Loss: 33.94167, Validation Loss: 4.75025\n",
      "Epoch 789/1000, Training Loss: 33.93339, Validation Loss: 4.74873\n",
      "Epoch 790/1000, Training Loss: 33.92511, Validation Loss: 4.74720\n",
      "Epoch 791/1000, Training Loss: 33.91683, Validation Loss: 4.74568\n",
      "Epoch 792/1000, Training Loss: 33.90857, Validation Loss: 4.74415\n",
      "Epoch 793/1000, Training Loss: 33.90030, Validation Loss: 4.74263\n",
      "Epoch 794/1000, Training Loss: 33.89204, Validation Loss: 4.74110\n",
      "Epoch 795/1000, Training Loss: 33.88378, Validation Loss: 4.73958\n",
      "Epoch 796/1000, Training Loss: 33.87553, Validation Loss: 4.73805\n",
      "Epoch 797/1000, Training Loss: 33.86728, Validation Loss: 4.73652\n",
      "Epoch 798/1000, Training Loss: 33.85904, Validation Loss: 4.73500\n",
      "Epoch 799/1000, Training Loss: 33.85079, Validation Loss: 4.73347\n",
      "Epoch 800/1000, Training Loss: 33.84255, Validation Loss: 4.73195\n",
      "Epoch 801/1000, Training Loss: 33.83431, Validation Loss: 4.73042\n",
      "Epoch 802/1000, Training Loss: 33.82607, Validation Loss: 4.72890\n",
      "Epoch 803/1000, Training Loss: 33.81784, Validation Loss: 4.72738\n",
      "Epoch 804/1000, Training Loss: 33.80960, Validation Loss: 4.72585\n",
      "Epoch 805/1000, Training Loss: 33.80137, Validation Loss: 4.72433\n",
      "Epoch 806/1000, Training Loss: 33.79314, Validation Loss: 4.72281\n",
      "Epoch 807/1000, Training Loss: 33.78492, Validation Loss: 4.72129\n",
      "Epoch 808/1000, Training Loss: 33.77670, Validation Loss: 4.71977\n",
      "Epoch 809/1000, Training Loss: 33.76849, Validation Loss: 4.71825\n",
      "Epoch 810/1000, Training Loss: 33.76027, Validation Loss: 4.71674\n",
      "Epoch 811/1000, Training Loss: 33.75206, Validation Loss: 4.71522\n",
      "Epoch 812/1000, Training Loss: 33.74385, Validation Loss: 4.71371\n",
      "Epoch 813/1000, Training Loss: 33.73565, Validation Loss: 4.71219\n",
      "Epoch 814/1000, Training Loss: 33.72745, Validation Loss: 4.71067\n",
      "Epoch 815/1000, Training Loss: 33.71926, Validation Loss: 4.70916\n",
      "Epoch 816/1000, Training Loss: 33.71107, Validation Loss: 4.70765\n",
      "Epoch 817/1000, Training Loss: 33.70289, Validation Loss: 4.70614\n",
      "Epoch 818/1000, Training Loss: 33.69472, Validation Loss: 4.70462\n",
      "Epoch 819/1000, Training Loss: 33.68655, Validation Loss: 4.70311\n",
      "Epoch 820/1000, Training Loss: 33.67838, Validation Loss: 4.70161\n",
      "Epoch 821/1000, Training Loss: 33.67021, Validation Loss: 4.70010\n",
      "Epoch 822/1000, Training Loss: 33.66205, Validation Loss: 4.69859\n",
      "Epoch 823/1000, Training Loss: 33.65389, Validation Loss: 4.69708\n",
      "Epoch 824/1000, Training Loss: 33.64573, Validation Loss: 4.69557\n",
      "Epoch 825/1000, Training Loss: 33.63757, Validation Loss: 4.69407\n",
      "Epoch 826/1000, Training Loss: 33.62941, Validation Loss: 4.69256\n",
      "Epoch 827/1000, Training Loss: 33.62125, Validation Loss: 4.69106\n",
      "Epoch 828/1000, Training Loss: 33.61309, Validation Loss: 4.68955\n",
      "Epoch 829/1000, Training Loss: 33.60495, Validation Loss: 4.68805\n",
      "Epoch 830/1000, Training Loss: 33.59680, Validation Loss: 4.68654\n",
      "Epoch 831/1000, Training Loss: 33.58866, Validation Loss: 4.68504\n",
      "Epoch 832/1000, Training Loss: 33.58053, Validation Loss: 4.68354\n",
      "Epoch 833/1000, Training Loss: 33.57242, Validation Loss: 4.68204\n",
      "Epoch 834/1000, Training Loss: 33.56430, Validation Loss: 4.68054\n",
      "Epoch 835/1000, Training Loss: 33.55619, Validation Loss: 4.67904\n",
      "Epoch 836/1000, Training Loss: 33.54809, Validation Loss: 4.67755\n",
      "Epoch 837/1000, Training Loss: 33.53998, Validation Loss: 4.67605\n",
      "Epoch 838/1000, Training Loss: 33.53189, Validation Loss: 4.67455\n",
      "Epoch 839/1000, Training Loss: 33.52380, Validation Loss: 4.67306\n",
      "Epoch 840/1000, Training Loss: 33.51572, Validation Loss: 4.67156\n",
      "Epoch 841/1000, Training Loss: 33.50764, Validation Loss: 4.67007\n",
      "Epoch 842/1000, Training Loss: 33.49957, Validation Loss: 4.66857\n",
      "Epoch 843/1000, Training Loss: 33.49150, Validation Loss: 4.66708\n",
      "Epoch 844/1000, Training Loss: 33.48345, Validation Loss: 4.66559\n",
      "Epoch 845/1000, Training Loss: 33.47540, Validation Loss: 4.66410\n",
      "Epoch 846/1000, Training Loss: 33.46736, Validation Loss: 4.66262\n",
      "Epoch 847/1000, Training Loss: 33.45932, Validation Loss: 4.66113\n",
      "Epoch 848/1000, Training Loss: 33.45129, Validation Loss: 4.65965\n",
      "Epoch 849/1000, Training Loss: 33.44326, Validation Loss: 4.65816\n",
      "Epoch 850/1000, Training Loss: 33.43524, Validation Loss: 4.65668\n",
      "Epoch 851/1000, Training Loss: 33.42722, Validation Loss: 4.65520\n",
      "Epoch 852/1000, Training Loss: 33.41920, Validation Loss: 4.65372\n",
      "Epoch 853/1000, Training Loss: 33.41119, Validation Loss: 4.65224\n",
      "Epoch 854/1000, Training Loss: 33.40318, Validation Loss: 4.65076\n",
      "Epoch 855/1000, Training Loss: 33.39516, Validation Loss: 4.64928\n",
      "Epoch 856/1000, Training Loss: 33.38716, Validation Loss: 4.64780\n",
      "Epoch 857/1000, Training Loss: 33.37914, Validation Loss: 4.64633\n",
      "Epoch 858/1000, Training Loss: 33.37112, Validation Loss: 4.64485\n",
      "Epoch 859/1000, Training Loss: 33.36311, Validation Loss: 4.64337\n",
      "Epoch 860/1000, Training Loss: 33.35511, Validation Loss: 4.64190\n",
      "Epoch 861/1000, Training Loss: 33.34711, Validation Loss: 4.64043\n",
      "Epoch 862/1000, Training Loss: 33.33913, Validation Loss: 4.63896\n",
      "Epoch 863/1000, Training Loss: 33.33115, Validation Loss: 4.63748\n",
      "Epoch 864/1000, Training Loss: 33.32318, Validation Loss: 4.63601\n",
      "Epoch 865/1000, Training Loss: 33.31521, Validation Loss: 4.63454\n",
      "Epoch 866/1000, Training Loss: 33.30724, Validation Loss: 4.63308\n",
      "Epoch 867/1000, Training Loss: 33.29928, Validation Loss: 4.63162\n",
      "Epoch 868/1000, Training Loss: 33.29132, Validation Loss: 4.63016\n",
      "Epoch 869/1000, Training Loss: 33.28337, Validation Loss: 4.62870\n",
      "Epoch 870/1000, Training Loss: 33.27543, Validation Loss: 4.62724\n",
      "Epoch 871/1000, Training Loss: 33.26749, Validation Loss: 4.62578\n",
      "Epoch 872/1000, Training Loss: 33.25955, Validation Loss: 4.62432\n",
      "Epoch 873/1000, Training Loss: 33.25162, Validation Loss: 4.62287\n",
      "Epoch 874/1000, Training Loss: 33.24370, Validation Loss: 4.62142\n",
      "Epoch 875/1000, Training Loss: 33.23579, Validation Loss: 4.61996\n",
      "Epoch 876/1000, Training Loss: 33.22788, Validation Loss: 4.61851\n",
      "Epoch 877/1000, Training Loss: 33.21998, Validation Loss: 4.61706\n",
      "Epoch 878/1000, Training Loss: 33.21209, Validation Loss: 4.61562\n",
      "Epoch 879/1000, Training Loss: 33.20420, Validation Loss: 4.61417\n",
      "Epoch 880/1000, Training Loss: 33.19632, Validation Loss: 4.61273\n",
      "Epoch 881/1000, Training Loss: 33.18844, Validation Loss: 4.61129\n",
      "Epoch 882/1000, Training Loss: 33.18057, Validation Loss: 4.60985\n",
      "Epoch 883/1000, Training Loss: 33.17270, Validation Loss: 4.60841\n",
      "Epoch 884/1000, Training Loss: 33.16484, Validation Loss: 4.60697\n",
      "Epoch 885/1000, Training Loss: 33.15698, Validation Loss: 4.60553\n",
      "Epoch 886/1000, Training Loss: 33.14911, Validation Loss: 4.60410\n",
      "Epoch 887/1000, Training Loss: 33.14126, Validation Loss: 4.60266\n",
      "Epoch 888/1000, Training Loss: 33.13341, Validation Loss: 4.60123\n",
      "Epoch 889/1000, Training Loss: 33.12558, Validation Loss: 4.59980\n",
      "Epoch 890/1000, Training Loss: 33.11774, Validation Loss: 4.59836\n",
      "Epoch 891/1000, Training Loss: 33.10991, Validation Loss: 4.59693\n",
      "Epoch 892/1000, Training Loss: 33.10208, Validation Loss: 4.59550\n",
      "Epoch 893/1000, Training Loss: 33.09426, Validation Loss: 4.59407\n",
      "Epoch 894/1000, Training Loss: 33.08644, Validation Loss: 4.59265\n",
      "Epoch 895/1000, Training Loss: 33.07862, Validation Loss: 4.59122\n",
      "Epoch 896/1000, Training Loss: 33.07081, Validation Loss: 4.58979\n",
      "Epoch 897/1000, Training Loss: 33.06300, Validation Loss: 4.58836\n",
      "Epoch 898/1000, Training Loss: 33.05520, Validation Loss: 4.58694\n",
      "Epoch 899/1000, Training Loss: 33.04740, Validation Loss: 4.58551\n",
      "Epoch 900/1000, Training Loss: 33.03963, Validation Loss: 4.58409\n",
      "Epoch 901/1000, Training Loss: 33.03186, Validation Loss: 4.58267\n",
      "Epoch 902/1000, Training Loss: 33.02409, Validation Loss: 4.58125\n",
      "Epoch 903/1000, Training Loss: 33.01633, Validation Loss: 4.57983\n",
      "Epoch 904/1000, Training Loss: 33.00857, Validation Loss: 4.57841\n",
      "Epoch 905/1000, Training Loss: 33.00081, Validation Loss: 4.57699\n",
      "Epoch 906/1000, Training Loss: 32.99306, Validation Loss: 4.57557\n",
      "Epoch 907/1000, Training Loss: 32.98532, Validation Loss: 4.57416\n",
      "Epoch 908/1000, Training Loss: 32.97759, Validation Loss: 4.57275\n",
      "Epoch 909/1000, Training Loss: 32.96987, Validation Loss: 4.57133\n",
      "Epoch 910/1000, Training Loss: 32.96215, Validation Loss: 4.56992\n",
      "Epoch 911/1000, Training Loss: 32.95443, Validation Loss: 4.56851\n",
      "Epoch 912/1000, Training Loss: 32.94672, Validation Loss: 4.56710\n",
      "Epoch 913/1000, Training Loss: 32.93900, Validation Loss: 4.56569\n",
      "Epoch 914/1000, Training Loss: 32.93130, Validation Loss: 4.56428\n",
      "Epoch 915/1000, Training Loss: 32.92360, Validation Loss: 4.56288\n",
      "Epoch 916/1000, Training Loss: 32.91590, Validation Loss: 4.56147\n",
      "Epoch 917/1000, Training Loss: 32.90822, Validation Loss: 4.56007\n",
      "Epoch 918/1000, Training Loss: 32.90054, Validation Loss: 4.55866\n",
      "Epoch 919/1000, Training Loss: 32.89288, Validation Loss: 4.55726\n",
      "Epoch 920/1000, Training Loss: 32.88521, Validation Loss: 4.55586\n",
      "Epoch 921/1000, Training Loss: 32.87756, Validation Loss: 4.55445\n",
      "Epoch 922/1000, Training Loss: 32.86992, Validation Loss: 4.55305\n",
      "Epoch 923/1000, Training Loss: 32.86228, Validation Loss: 4.55165\n",
      "Epoch 924/1000, Training Loss: 32.85465, Validation Loss: 4.55025\n",
      "Epoch 925/1000, Training Loss: 32.84702, Validation Loss: 4.54885\n",
      "Epoch 926/1000, Training Loss: 32.83940, Validation Loss: 4.54746\n",
      "Epoch 927/1000, Training Loss: 32.83179, Validation Loss: 4.54606\n",
      "Epoch 928/1000, Training Loss: 32.82418, Validation Loss: 4.54467\n",
      "Epoch 929/1000, Training Loss: 32.81659, Validation Loss: 4.54328\n",
      "Epoch 930/1000, Training Loss: 32.80900, Validation Loss: 4.54188\n",
      "Epoch 931/1000, Training Loss: 32.80142, Validation Loss: 4.54049\n",
      "Epoch 932/1000, Training Loss: 32.79385, Validation Loss: 4.53910\n",
      "Epoch 933/1000, Training Loss: 32.78628, Validation Loss: 4.53772\n",
      "Epoch 934/1000, Training Loss: 32.77872, Validation Loss: 4.53633\n",
      "Epoch 935/1000, Training Loss: 32.77117, Validation Loss: 4.53495\n",
      "Epoch 936/1000, Training Loss: 32.76363, Validation Loss: 4.53356\n",
      "Epoch 937/1000, Training Loss: 32.75608, Validation Loss: 4.53219\n",
      "Epoch 938/1000, Training Loss: 32.74854, Validation Loss: 4.53081\n",
      "Epoch 939/1000, Training Loss: 32.74101, Validation Loss: 4.52943\n",
      "Epoch 940/1000, Training Loss: 32.73350, Validation Loss: 4.52806\n",
      "Epoch 941/1000, Training Loss: 32.72599, Validation Loss: 4.52668\n",
      "Epoch 942/1000, Training Loss: 32.71849, Validation Loss: 4.52531\n",
      "Epoch 943/1000, Training Loss: 32.71100, Validation Loss: 4.52394\n",
      "Epoch 944/1000, Training Loss: 32.70351, Validation Loss: 4.52257\n",
      "Epoch 945/1000, Training Loss: 32.69603, Validation Loss: 4.52120\n",
      "Epoch 946/1000, Training Loss: 32.68855, Validation Loss: 4.51983\n",
      "Epoch 947/1000, Training Loss: 32.68107, Validation Loss: 4.51846\n",
      "Epoch 948/1000, Training Loss: 32.67361, Validation Loss: 4.51709\n",
      "Epoch 949/1000, Training Loss: 32.66616, Validation Loss: 4.51572\n",
      "Epoch 950/1000, Training Loss: 32.65872, Validation Loss: 4.51435\n",
      "Epoch 951/1000, Training Loss: 32.65127, Validation Loss: 4.51298\n",
      "Epoch 952/1000, Training Loss: 32.64383, Validation Loss: 4.51162\n",
      "Epoch 953/1000, Training Loss: 32.63639, Validation Loss: 4.51025\n",
      "Epoch 954/1000, Training Loss: 32.62896, Validation Loss: 4.50889\n",
      "Epoch 955/1000, Training Loss: 32.62155, Validation Loss: 4.50753\n",
      "Epoch 956/1000, Training Loss: 32.61414, Validation Loss: 4.50618\n",
      "Epoch 957/1000, Training Loss: 32.60673, Validation Loss: 4.50482\n",
      "Epoch 958/1000, Training Loss: 32.59933, Validation Loss: 4.50346\n",
      "Epoch 959/1000, Training Loss: 32.59193, Validation Loss: 4.50211\n",
      "Epoch 960/1000, Training Loss: 32.58454, Validation Loss: 4.50075\n",
      "Epoch 961/1000, Training Loss: 32.57716, Validation Loss: 4.49940\n",
      "Epoch 962/1000, Training Loss: 32.56979, Validation Loss: 4.49805\n",
      "Epoch 963/1000, Training Loss: 32.56242, Validation Loss: 4.49670\n",
      "Epoch 964/1000, Training Loss: 32.55505, Validation Loss: 4.49535\n",
      "Epoch 965/1000, Training Loss: 32.54770, Validation Loss: 4.49400\n",
      "Epoch 966/1000, Training Loss: 32.54036, Validation Loss: 4.49265\n",
      "Epoch 967/1000, Training Loss: 32.53302, Validation Loss: 4.49130\n",
      "Epoch 968/1000, Training Loss: 32.52569, Validation Loss: 4.48995\n",
      "Epoch 969/1000, Training Loss: 32.51837, Validation Loss: 4.48861\n",
      "Epoch 970/1000, Training Loss: 32.51105, Validation Loss: 4.48727\n",
      "Epoch 971/1000, Training Loss: 32.50374, Validation Loss: 4.48592\n",
      "Epoch 972/1000, Training Loss: 32.49643, Validation Loss: 4.48458\n",
      "Epoch 973/1000, Training Loss: 32.48914, Validation Loss: 4.48324\n",
      "Epoch 974/1000, Training Loss: 32.48184, Validation Loss: 4.48190\n",
      "Epoch 975/1000, Training Loss: 32.47456, Validation Loss: 4.48057\n",
      "Epoch 976/1000, Training Loss: 32.46728, Validation Loss: 4.47923\n",
      "Epoch 977/1000, Training Loss: 32.46001, Validation Loss: 4.47789\n",
      "Epoch 978/1000, Training Loss: 32.45275, Validation Loss: 4.47656\n",
      "Epoch 979/1000, Training Loss: 32.44550, Validation Loss: 4.47523\n",
      "Epoch 980/1000, Training Loss: 32.43826, Validation Loss: 4.47390\n",
      "Epoch 981/1000, Training Loss: 32.43103, Validation Loss: 4.47256\n",
      "Epoch 982/1000, Training Loss: 32.42381, Validation Loss: 4.47123\n",
      "Epoch 983/1000, Training Loss: 32.41659, Validation Loss: 4.46990\n",
      "Epoch 984/1000, Training Loss: 32.40937, Validation Loss: 4.46857\n",
      "Epoch 985/1000, Training Loss: 32.40217, Validation Loss: 4.46724\n",
      "Epoch 986/1000, Training Loss: 32.39497, Validation Loss: 4.46592\n",
      "Epoch 987/1000, Training Loss: 32.38777, Validation Loss: 4.46459\n",
      "Epoch 988/1000, Training Loss: 32.38058, Validation Loss: 4.46327\n",
      "Epoch 989/1000, Training Loss: 32.37340, Validation Loss: 4.46195\n",
      "Epoch 990/1000, Training Loss: 32.36623, Validation Loss: 4.46062\n",
      "Epoch 991/1000, Training Loss: 32.35906, Validation Loss: 4.45930\n",
      "Epoch 992/1000, Training Loss: 32.35190, Validation Loss: 4.45798\n",
      "Epoch 993/1000, Training Loss: 32.34475, Validation Loss: 4.45666\n",
      "Epoch 994/1000, Training Loss: 32.33760, Validation Loss: 4.45535\n",
      "Epoch 995/1000, Training Loss: 32.33047, Validation Loss: 4.45403\n",
      "Epoch 996/1000, Training Loss: 32.32334, Validation Loss: 4.45272\n",
      "Epoch 997/1000, Training Loss: 32.31622, Validation Loss: 4.45141\n",
      "Epoch 998/1000, Training Loss: 32.30910, Validation Loss: 4.45010\n",
      "Epoch 999/1000, Training Loss: 32.30200, Validation Loss: 4.44879\n",
      "Epoch 1000/1000, Training Loss: 32.29490, Validation Loss: 4.44749\n",
      "Training took: 118.42 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_optimizer_asgd_1 = NeuralNetwork().to(device)\n",
    "summary(model_optimizer_asgd_1, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.ASGD(model_optimizer_asgd_1.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_optimizer_asgd_1=[]\n",
    "val_loss_list_optimizer_asgd_1=[]\n",
    "train_accuracy_list_optimizer_asgd_1=[]\n",
    "val_accuracy_list_optimizer_asgd_1=[]\n",
    "test_accuracy_list_optimizer_asgd_1=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_optimizer_asgd_1.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_optimizer_asgd_1(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_optimizer_asgd_1.append(train_accuracy)\n",
    "    train_loss_list_optimizer_asgd_1.append(train_loss)\n",
    "\n",
    "    model_optimizer_asgd_1.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_optimizer_asgd_1(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_optimizer_asgd_1.append(val_accuracy)\n",
    "    val_loss_list_optimizer_asgd_1.append(val_loss)\n",
    "\n",
    "    test_predictions_optimizer_asgd_1 = model_optimizer_asgd_1(X_test_tensor).view(-1)\n",
    "    test_predictions_rounded_optimizer_asgd_1 = torch.round(test_predictions_optimizer_asgd_1)\n",
    "    test_predictions_rounded_numpy_optimizer_asgd_1 = test_predictions_rounded_optimizer_asgd_1.cpu().detach().numpy()\n",
    "    y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "    accuracy_optimizer_asgd_1 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_optimizer_asgd_1)\n",
    "    test_accuracy_list_optimizer_asgd_1.append(accuracy_optimizer_asgd_1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW1i2nCJgvWj",
    "outputId": "63413434-8bc7-4ab6-a111-28ff42c4d6b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable optimizer with opmizer as asgd: 0.7662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_optimizer_asgd_1.eval()\n",
    "test_predictions_optimizer_asgd_1 = model_optimizer_asgd_1(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_optimizer_asgd_1 = torch.round(test_predictions_optimizer_asgd_1)\n",
    "\n",
    "test_predictions_rounded_numpy_optimizer_asgd_1 = test_predictions_rounded_optimizer_asgd_1.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_optimizer_asgd_1 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_optimizer_asgd_1)\n",
    "\n",
    "print(f\"Accuracy for variable optimizer with opmizer as asgd: {accuracy_optimizer_asgd_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJOaNFwyhxHm",
    "outputId": "616def9b-c582-41b0-ef18-f40b2b61cf73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable opmizer with opmizer as asgd: 0.50613\n"
     ]
    }
   ],
   "source": [
    "model_optimizer_asgd_1.eval()\n",
    "test_loss_optimizer_asgd_1=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_optimizer_asgd_1 = model_optimizer_asgd_1(X_test_tensor)\n",
    "    test_loss_optimizer_asgd_1 = loss_function(test_outputs_optimizer_asgd_1, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable opmizer with opmizer as asgd: {test_loss_optimizer_asgd_1.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSX83tvtliiA"
   },
   "source": [
    "Optimizer=Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSLEsj91kA6E",
    "outputId": "058c1e84-fdf0-44e2-8950-b66a212f1670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 41.84822, Validation Loss: 6.10785\n",
      "Epoch 2/1000, Training Loss: 41.68792, Validation Loss: 6.08813\n",
      "Epoch 3/1000, Training Loss: 41.58735, Validation Loss: 6.07315\n",
      "Epoch 4/1000, Training Loss: 41.50728, Validation Loss: 6.06063\n",
      "Epoch 5/1000, Training Loss: 41.43890, Validation Loss: 6.04966\n",
      "Epoch 6/1000, Training Loss: 41.37846, Validation Loss: 6.03981\n",
      "Epoch 7/1000, Training Loss: 41.32366, Validation Loss: 6.03082\n",
      "Epoch 8/1000, Training Loss: 41.27325, Validation Loss: 6.02249\n",
      "Epoch 9/1000, Training Loss: 41.22644, Validation Loss: 6.01472\n",
      "Epoch 10/1000, Training Loss: 41.18253, Validation Loss: 6.00740\n",
      "Epoch 11/1000, Training Loss: 41.14111, Validation Loss: 6.00048\n",
      "Epoch 12/1000, Training Loss: 41.10188, Validation Loss: 5.99389\n",
      "Epoch 13/1000, Training Loss: 41.06451, Validation Loss: 5.98758\n",
      "Epoch 14/1000, Training Loss: 41.02875, Validation Loss: 5.98154\n",
      "Epoch 15/1000, Training Loss: 40.99439, Validation Loss: 5.97571\n",
      "Epoch 16/1000, Training Loss: 40.96128, Validation Loss: 5.97009\n",
      "Epoch 17/1000, Training Loss: 40.92933, Validation Loss: 5.96468\n",
      "Epoch 18/1000, Training Loss: 40.89843, Validation Loss: 5.95942\n",
      "Epoch 19/1000, Training Loss: 40.86842, Validation Loss: 5.95432\n",
      "Epoch 20/1000, Training Loss: 40.83923, Validation Loss: 5.94937\n",
      "Epoch 21/1000, Training Loss: 40.81084, Validation Loss: 5.94456\n",
      "Epoch 22/1000, Training Loss: 40.78319, Validation Loss: 5.93987\n",
      "Epoch 23/1000, Training Loss: 40.75628, Validation Loss: 5.93531\n",
      "Epoch 24/1000, Training Loss: 40.73003, Validation Loss: 5.93086\n",
      "Epoch 25/1000, Training Loss: 40.70444, Validation Loss: 5.92651\n",
      "Epoch 26/1000, Training Loss: 40.67938, Validation Loss: 5.92227\n",
      "Epoch 27/1000, Training Loss: 40.65484, Validation Loss: 5.91812\n",
      "Epoch 28/1000, Training Loss: 40.63086, Validation Loss: 5.91406\n",
      "Epoch 29/1000, Training Loss: 40.60739, Validation Loss: 5.91009\n",
      "Epoch 30/1000, Training Loss: 40.58443, Validation Loss: 5.90619\n",
      "Epoch 31/1000, Training Loss: 40.56190, Validation Loss: 5.90236\n",
      "Epoch 32/1000, Training Loss: 40.53976, Validation Loss: 5.89860\n",
      "Epoch 33/1000, Training Loss: 40.51799, Validation Loss: 5.89488\n",
      "Epoch 34/1000, Training Loss: 40.49660, Validation Loss: 5.89123\n",
      "Epoch 35/1000, Training Loss: 40.47554, Validation Loss: 5.88763\n",
      "Epoch 36/1000, Training Loss: 40.45473, Validation Loss: 5.88408\n",
      "Epoch 37/1000, Training Loss: 40.43420, Validation Loss: 5.88058\n",
      "Epoch 38/1000, Training Loss: 40.41400, Validation Loss: 5.87714\n",
      "Epoch 39/1000, Training Loss: 40.39409, Validation Loss: 5.87376\n",
      "Epoch 40/1000, Training Loss: 40.37443, Validation Loss: 5.87041\n",
      "Epoch 41/1000, Training Loss: 40.35503, Validation Loss: 5.86710\n",
      "Epoch 42/1000, Training Loss: 40.33588, Validation Loss: 5.86383\n",
      "Epoch 43/1000, Training Loss: 40.31697, Validation Loss: 5.86059\n",
      "Epoch 44/1000, Training Loss: 40.29832, Validation Loss: 5.85739\n",
      "Epoch 45/1000, Training Loss: 40.27992, Validation Loss: 5.85424\n",
      "Epoch 46/1000, Training Loss: 40.26174, Validation Loss: 5.85112\n",
      "Epoch 47/1000, Training Loss: 40.24378, Validation Loss: 5.84804\n",
      "Epoch 48/1000, Training Loss: 40.22602, Validation Loss: 5.84500\n",
      "Epoch 49/1000, Training Loss: 40.20847, Validation Loss: 5.84199\n",
      "Epoch 50/1000, Training Loss: 40.19110, Validation Loss: 5.83902\n",
      "Epoch 51/1000, Training Loss: 40.17393, Validation Loss: 5.83608\n",
      "Epoch 52/1000, Training Loss: 40.15696, Validation Loss: 5.83318\n",
      "Epoch 53/1000, Training Loss: 40.14016, Validation Loss: 5.83030\n",
      "Epoch 54/1000, Training Loss: 40.12351, Validation Loss: 5.82745\n",
      "Epoch 55/1000, Training Loss: 40.10703, Validation Loss: 5.82463\n",
      "Epoch 56/1000, Training Loss: 40.09072, Validation Loss: 5.82184\n",
      "Epoch 57/1000, Training Loss: 40.07457, Validation Loss: 5.81907\n",
      "Epoch 58/1000, Training Loss: 40.05858, Validation Loss: 5.81633\n",
      "Epoch 59/1000, Training Loss: 40.04272, Validation Loss: 5.81361\n",
      "Epoch 60/1000, Training Loss: 40.02700, Validation Loss: 5.81092\n",
      "Epoch 61/1000, Training Loss: 40.01142, Validation Loss: 5.80826\n",
      "Epoch 62/1000, Training Loss: 39.99598, Validation Loss: 5.80562\n",
      "Epoch 63/1000, Training Loss: 39.98069, Validation Loss: 5.80300\n",
      "Epoch 64/1000, Training Loss: 39.96552, Validation Loss: 5.80041\n",
      "Epoch 65/1000, Training Loss: 39.95050, Validation Loss: 5.79783\n",
      "Epoch 66/1000, Training Loss: 39.93560, Validation Loss: 5.79528\n",
      "Epoch 67/1000, Training Loss: 39.92082, Validation Loss: 5.79274\n",
      "Epoch 68/1000, Training Loss: 39.90615, Validation Loss: 5.79022\n",
      "Epoch 69/1000, Training Loss: 39.89161, Validation Loss: 5.78773\n",
      "Epoch 70/1000, Training Loss: 39.87718, Validation Loss: 5.78525\n",
      "Epoch 71/1000, Training Loss: 39.86288, Validation Loss: 5.78280\n",
      "Epoch 72/1000, Training Loss: 39.84868, Validation Loss: 5.78036\n",
      "Epoch 73/1000, Training Loss: 39.83460, Validation Loss: 5.77795\n",
      "Epoch 74/1000, Training Loss: 39.82063, Validation Loss: 5.77555\n",
      "Epoch 75/1000, Training Loss: 39.80679, Validation Loss: 5.77318\n",
      "Epoch 76/1000, Training Loss: 39.79305, Validation Loss: 5.77082\n",
      "Epoch 77/1000, Training Loss: 39.77941, Validation Loss: 5.76847\n",
      "Epoch 78/1000, Training Loss: 39.76587, Validation Loss: 5.76615\n",
      "Epoch 79/1000, Training Loss: 39.75241, Validation Loss: 5.76384\n",
      "Epoch 80/1000, Training Loss: 39.73906, Validation Loss: 5.76155\n",
      "Epoch 81/1000, Training Loss: 39.72579, Validation Loss: 5.75927\n",
      "Epoch 82/1000, Training Loss: 39.71262, Validation Loss: 5.75702\n",
      "Epoch 83/1000, Training Loss: 39.69954, Validation Loss: 5.75478\n",
      "Epoch 84/1000, Training Loss: 39.68653, Validation Loss: 5.75255\n",
      "Epoch 85/1000, Training Loss: 39.67360, Validation Loss: 5.75034\n",
      "Epoch 86/1000, Training Loss: 39.66076, Validation Loss: 5.74814\n",
      "Epoch 87/1000, Training Loss: 39.64799, Validation Loss: 5.74595\n",
      "Epoch 88/1000, Training Loss: 39.63531, Validation Loss: 5.74378\n",
      "Epoch 89/1000, Training Loss: 39.62271, Validation Loss: 5.74163\n",
      "Epoch 90/1000, Training Loss: 39.61020, Validation Loss: 5.73949\n",
      "Epoch 91/1000, Training Loss: 39.59778, Validation Loss: 5.73736\n",
      "Epoch 92/1000, Training Loss: 39.58544, Validation Loss: 5.73525\n",
      "Epoch 93/1000, Training Loss: 39.57315, Validation Loss: 5.73315\n",
      "Epoch 94/1000, Training Loss: 39.56092, Validation Loss: 5.73105\n",
      "Epoch 95/1000, Training Loss: 39.54877, Validation Loss: 5.72897\n",
      "Epoch 96/1000, Training Loss: 39.53669, Validation Loss: 5.72690\n",
      "Epoch 97/1000, Training Loss: 39.52468, Validation Loss: 5.72484\n",
      "Epoch 98/1000, Training Loss: 39.51275, Validation Loss: 5.72279\n",
      "Epoch 99/1000, Training Loss: 39.50089, Validation Loss: 5.72075\n",
      "Epoch 100/1000, Training Loss: 39.48908, Validation Loss: 5.71872\n",
      "Epoch 101/1000, Training Loss: 39.47733, Validation Loss: 5.71670\n",
      "Epoch 102/1000, Training Loss: 39.46564, Validation Loss: 5.71469\n",
      "Epoch 103/1000, Training Loss: 39.45399, Validation Loss: 5.71269\n",
      "Epoch 104/1000, Training Loss: 39.44239, Validation Loss: 5.71069\n",
      "Epoch 105/1000, Training Loss: 39.43083, Validation Loss: 5.70870\n",
      "Epoch 106/1000, Training Loss: 39.41932, Validation Loss: 5.70671\n",
      "Epoch 107/1000, Training Loss: 39.40788, Validation Loss: 5.70474\n",
      "Epoch 108/1000, Training Loss: 39.39648, Validation Loss: 5.70277\n",
      "Epoch 109/1000, Training Loss: 39.38513, Validation Loss: 5.70081\n",
      "Epoch 110/1000, Training Loss: 39.37384, Validation Loss: 5.69886\n",
      "Epoch 111/1000, Training Loss: 39.36262, Validation Loss: 5.69692\n",
      "Epoch 112/1000, Training Loss: 39.35145, Validation Loss: 5.69498\n",
      "Epoch 113/1000, Training Loss: 39.34033, Validation Loss: 5.69304\n",
      "Epoch 114/1000, Training Loss: 39.32925, Validation Loss: 5.69111\n",
      "Epoch 115/1000, Training Loss: 39.31824, Validation Loss: 5.68920\n",
      "Epoch 116/1000, Training Loss: 39.30727, Validation Loss: 5.68729\n",
      "Epoch 117/1000, Training Loss: 39.29636, Validation Loss: 5.68539\n",
      "Epoch 118/1000, Training Loss: 39.28550, Validation Loss: 5.68349\n",
      "Epoch 119/1000, Training Loss: 39.27468, Validation Loss: 5.68161\n",
      "Epoch 120/1000, Training Loss: 39.26391, Validation Loss: 5.67973\n",
      "Epoch 121/1000, Training Loss: 39.25319, Validation Loss: 5.67786\n",
      "Epoch 122/1000, Training Loss: 39.24252, Validation Loss: 5.67599\n",
      "Epoch 123/1000, Training Loss: 39.23188, Validation Loss: 5.67413\n",
      "Epoch 124/1000, Training Loss: 39.22128, Validation Loss: 5.67228\n",
      "Epoch 125/1000, Training Loss: 39.21073, Validation Loss: 5.67044\n",
      "Epoch 126/1000, Training Loss: 39.20022, Validation Loss: 5.66860\n",
      "Epoch 127/1000, Training Loss: 39.18975, Validation Loss: 5.66678\n",
      "Epoch 128/1000, Training Loss: 39.17934, Validation Loss: 5.66496\n",
      "Epoch 129/1000, Training Loss: 39.16898, Validation Loss: 5.66315\n",
      "Epoch 130/1000, Training Loss: 39.15866, Validation Loss: 5.66134\n",
      "Epoch 131/1000, Training Loss: 39.14839, Validation Loss: 5.65955\n",
      "Epoch 132/1000, Training Loss: 39.13815, Validation Loss: 5.65776\n",
      "Epoch 133/1000, Training Loss: 39.12796, Validation Loss: 5.65598\n",
      "Epoch 134/1000, Training Loss: 39.11782, Validation Loss: 5.65421\n",
      "Epoch 135/1000, Training Loss: 39.10771, Validation Loss: 5.65244\n",
      "Epoch 136/1000, Training Loss: 39.09763, Validation Loss: 5.65068\n",
      "Epoch 137/1000, Training Loss: 39.08759, Validation Loss: 5.64892\n",
      "Epoch 138/1000, Training Loss: 39.07759, Validation Loss: 5.64718\n",
      "Epoch 139/1000, Training Loss: 39.06762, Validation Loss: 5.64544\n",
      "Epoch 140/1000, Training Loss: 39.05770, Validation Loss: 5.64370\n",
      "Epoch 141/1000, Training Loss: 39.04780, Validation Loss: 5.64197\n",
      "Epoch 142/1000, Training Loss: 39.03794, Validation Loss: 5.64024\n",
      "Epoch 143/1000, Training Loss: 39.02811, Validation Loss: 5.63852\n",
      "Epoch 144/1000, Training Loss: 39.01829, Validation Loss: 5.63681\n",
      "Epoch 145/1000, Training Loss: 39.00853, Validation Loss: 5.63510\n",
      "Epoch 146/1000, Training Loss: 38.99880, Validation Loss: 5.63340\n",
      "Epoch 147/1000, Training Loss: 38.98911, Validation Loss: 5.63170\n",
      "Epoch 148/1000, Training Loss: 38.97946, Validation Loss: 5.63001\n",
      "Epoch 149/1000, Training Loss: 38.96984, Validation Loss: 5.62833\n",
      "Epoch 150/1000, Training Loss: 38.96025, Validation Loss: 5.62665\n",
      "Epoch 151/1000, Training Loss: 38.95069, Validation Loss: 5.62498\n",
      "Epoch 152/1000, Training Loss: 38.94116, Validation Loss: 5.62331\n",
      "Epoch 153/1000, Training Loss: 38.93165, Validation Loss: 5.62165\n",
      "Epoch 154/1000, Training Loss: 38.92217, Validation Loss: 5.62000\n",
      "Epoch 155/1000, Training Loss: 38.91272, Validation Loss: 5.61835\n",
      "Epoch 156/1000, Training Loss: 38.90329, Validation Loss: 5.61670\n",
      "Epoch 157/1000, Training Loss: 38.89388, Validation Loss: 5.61506\n",
      "Epoch 158/1000, Training Loss: 38.88451, Validation Loss: 5.61342\n",
      "Epoch 159/1000, Training Loss: 38.87517, Validation Loss: 5.61179\n",
      "Epoch 160/1000, Training Loss: 38.86585, Validation Loss: 5.61017\n",
      "Epoch 161/1000, Training Loss: 38.85656, Validation Loss: 5.60854\n",
      "Epoch 162/1000, Training Loss: 38.84729, Validation Loss: 5.60693\n",
      "Epoch 163/1000, Training Loss: 38.83805, Validation Loss: 5.60532\n",
      "Epoch 164/1000, Training Loss: 38.82884, Validation Loss: 5.60371\n",
      "Epoch 165/1000, Training Loss: 38.81966, Validation Loss: 5.60211\n",
      "Epoch 166/1000, Training Loss: 38.81051, Validation Loss: 5.60051\n",
      "Epoch 167/1000, Training Loss: 38.80139, Validation Loss: 5.59892\n",
      "Epoch 168/1000, Training Loss: 38.79230, Validation Loss: 5.59733\n",
      "Epoch 169/1000, Training Loss: 38.78322, Validation Loss: 5.59574\n",
      "Epoch 170/1000, Training Loss: 38.77418, Validation Loss: 5.59417\n",
      "Epoch 171/1000, Training Loss: 38.76516, Validation Loss: 5.59259\n",
      "Epoch 172/1000, Training Loss: 38.75618, Validation Loss: 5.59102\n",
      "Epoch 173/1000, Training Loss: 38.74722, Validation Loss: 5.58945\n",
      "Epoch 174/1000, Training Loss: 38.73829, Validation Loss: 5.58788\n",
      "Epoch 175/1000, Training Loss: 38.72937, Validation Loss: 5.58632\n",
      "Epoch 176/1000, Training Loss: 38.72048, Validation Loss: 5.58477\n",
      "Epoch 177/1000, Training Loss: 38.71162, Validation Loss: 5.58322\n",
      "Epoch 178/1000, Training Loss: 38.70278, Validation Loss: 5.58167\n",
      "Epoch 179/1000, Training Loss: 38.69397, Validation Loss: 5.58013\n",
      "Epoch 180/1000, Training Loss: 38.68520, Validation Loss: 5.57859\n",
      "Epoch 181/1000, Training Loss: 38.67644, Validation Loss: 5.57705\n",
      "Epoch 182/1000, Training Loss: 38.66772, Validation Loss: 5.57553\n",
      "Epoch 183/1000, Training Loss: 38.65901, Validation Loss: 5.57400\n",
      "Epoch 184/1000, Training Loss: 38.65033, Validation Loss: 5.57248\n",
      "Epoch 185/1000, Training Loss: 38.64167, Validation Loss: 5.57096\n",
      "Epoch 186/1000, Training Loss: 38.63303, Validation Loss: 5.56945\n",
      "Epoch 187/1000, Training Loss: 38.62440, Validation Loss: 5.56794\n",
      "Epoch 188/1000, Training Loss: 38.61580, Validation Loss: 5.56643\n",
      "Epoch 189/1000, Training Loss: 38.60723, Validation Loss: 5.56493\n",
      "Epoch 190/1000, Training Loss: 38.59867, Validation Loss: 5.56343\n",
      "Epoch 191/1000, Training Loss: 38.59014, Validation Loss: 5.56194\n",
      "Epoch 192/1000, Training Loss: 38.58162, Validation Loss: 5.56045\n",
      "Epoch 193/1000, Training Loss: 38.57312, Validation Loss: 5.55897\n",
      "Epoch 194/1000, Training Loss: 38.56465, Validation Loss: 5.55748\n",
      "Epoch 195/1000, Training Loss: 38.55619, Validation Loss: 5.55601\n",
      "Epoch 196/1000, Training Loss: 38.54777, Validation Loss: 5.55453\n",
      "Epoch 197/1000, Training Loss: 38.53937, Validation Loss: 5.55306\n",
      "Epoch 198/1000, Training Loss: 38.53099, Validation Loss: 5.55160\n",
      "Epoch 199/1000, Training Loss: 38.52262, Validation Loss: 5.55013\n",
      "Epoch 200/1000, Training Loss: 38.51428, Validation Loss: 5.54867\n",
      "Epoch 201/1000, Training Loss: 38.50596, Validation Loss: 5.54721\n",
      "Epoch 202/1000, Training Loss: 38.49767, Validation Loss: 5.54576\n",
      "Epoch 203/1000, Training Loss: 38.48941, Validation Loss: 5.54431\n",
      "Epoch 204/1000, Training Loss: 38.48118, Validation Loss: 5.54287\n",
      "Epoch 205/1000, Training Loss: 38.47296, Validation Loss: 5.54143\n",
      "Epoch 206/1000, Training Loss: 38.46477, Validation Loss: 5.53999\n",
      "Epoch 207/1000, Training Loss: 38.45661, Validation Loss: 5.53856\n",
      "Epoch 208/1000, Training Loss: 38.44848, Validation Loss: 5.53713\n",
      "Epoch 209/1000, Training Loss: 38.44036, Validation Loss: 5.53571\n",
      "Epoch 210/1000, Training Loss: 38.43226, Validation Loss: 5.53428\n",
      "Epoch 211/1000, Training Loss: 38.42417, Validation Loss: 5.53286\n",
      "Epoch 212/1000, Training Loss: 38.41611, Validation Loss: 5.53144\n",
      "Epoch 213/1000, Training Loss: 38.40807, Validation Loss: 5.53003\n",
      "Epoch 214/1000, Training Loss: 38.40005, Validation Loss: 5.52862\n",
      "Epoch 215/1000, Training Loss: 38.39204, Validation Loss: 5.52721\n",
      "Epoch 216/1000, Training Loss: 38.38404, Validation Loss: 5.52581\n",
      "Epoch 217/1000, Training Loss: 38.37605, Validation Loss: 5.52440\n",
      "Epoch 218/1000, Training Loss: 38.36808, Validation Loss: 5.52300\n",
      "Epoch 219/1000, Training Loss: 38.36013, Validation Loss: 5.52161\n",
      "Epoch 220/1000, Training Loss: 38.35221, Validation Loss: 5.52022\n",
      "Epoch 221/1000, Training Loss: 38.34431, Validation Loss: 5.51883\n",
      "Epoch 222/1000, Training Loss: 38.33642, Validation Loss: 5.51744\n",
      "Epoch 223/1000, Training Loss: 38.32855, Validation Loss: 5.51606\n",
      "Epoch 224/1000, Training Loss: 38.32070, Validation Loss: 5.51469\n",
      "Epoch 225/1000, Training Loss: 38.31286, Validation Loss: 5.51331\n",
      "Epoch 226/1000, Training Loss: 38.30504, Validation Loss: 5.51195\n",
      "Epoch 227/1000, Training Loss: 38.29723, Validation Loss: 5.51058\n",
      "Epoch 228/1000, Training Loss: 38.28945, Validation Loss: 5.50922\n",
      "Epoch 229/1000, Training Loss: 38.28168, Validation Loss: 5.50786\n",
      "Epoch 230/1000, Training Loss: 38.27393, Validation Loss: 5.50650\n",
      "Epoch 231/1000, Training Loss: 38.26620, Validation Loss: 5.50515\n",
      "Epoch 232/1000, Training Loss: 38.25849, Validation Loss: 5.50380\n",
      "Epoch 233/1000, Training Loss: 38.25080, Validation Loss: 5.50246\n",
      "Epoch 234/1000, Training Loss: 38.24313, Validation Loss: 5.50112\n",
      "Epoch 235/1000, Training Loss: 38.23547, Validation Loss: 5.49978\n",
      "Epoch 236/1000, Training Loss: 38.22783, Validation Loss: 5.49844\n",
      "Epoch 237/1000, Training Loss: 38.22020, Validation Loss: 5.49710\n",
      "Epoch 238/1000, Training Loss: 38.21259, Validation Loss: 5.49577\n",
      "Epoch 239/1000, Training Loss: 38.20498, Validation Loss: 5.49445\n",
      "Epoch 240/1000, Training Loss: 38.19739, Validation Loss: 5.49312\n",
      "Epoch 241/1000, Training Loss: 38.18981, Validation Loss: 5.49180\n",
      "Epoch 242/1000, Training Loss: 38.18225, Validation Loss: 5.49049\n",
      "Epoch 243/1000, Training Loss: 38.17470, Validation Loss: 5.48917\n",
      "Epoch 244/1000, Training Loss: 38.16717, Validation Loss: 5.48786\n",
      "Epoch 245/1000, Training Loss: 38.15967, Validation Loss: 5.48656\n",
      "Epoch 246/1000, Training Loss: 38.15218, Validation Loss: 5.48525\n",
      "Epoch 247/1000, Training Loss: 38.14470, Validation Loss: 5.48395\n",
      "Epoch 248/1000, Training Loss: 38.13725, Validation Loss: 5.48265\n",
      "Epoch 249/1000, Training Loss: 38.12981, Validation Loss: 5.48135\n",
      "Epoch 250/1000, Training Loss: 38.12239, Validation Loss: 5.48006\n",
      "Epoch 251/1000, Training Loss: 38.11498, Validation Loss: 5.47876\n",
      "Epoch 252/1000, Training Loss: 38.10759, Validation Loss: 5.47747\n",
      "Epoch 253/1000, Training Loss: 38.10022, Validation Loss: 5.47618\n",
      "Epoch 254/1000, Training Loss: 38.09286, Validation Loss: 5.47489\n",
      "Epoch 255/1000, Training Loss: 38.08552, Validation Loss: 5.47360\n",
      "Epoch 256/1000, Training Loss: 38.07819, Validation Loss: 5.47232\n",
      "Epoch 257/1000, Training Loss: 38.07088, Validation Loss: 5.47104\n",
      "Epoch 258/1000, Training Loss: 38.06358, Validation Loss: 5.46976\n",
      "Epoch 259/1000, Training Loss: 38.05629, Validation Loss: 5.46849\n",
      "Epoch 260/1000, Training Loss: 38.04902, Validation Loss: 5.46721\n",
      "Epoch 261/1000, Training Loss: 38.04176, Validation Loss: 5.46595\n",
      "Epoch 262/1000, Training Loss: 38.03451, Validation Loss: 5.46468\n",
      "Epoch 263/1000, Training Loss: 38.02729, Validation Loss: 5.46341\n",
      "Epoch 264/1000, Training Loss: 38.02008, Validation Loss: 5.46215\n",
      "Epoch 265/1000, Training Loss: 38.01289, Validation Loss: 5.46088\n",
      "Epoch 266/1000, Training Loss: 38.00571, Validation Loss: 5.45962\n",
      "Epoch 267/1000, Training Loss: 37.99855, Validation Loss: 5.45836\n",
      "Epoch 268/1000, Training Loss: 37.99141, Validation Loss: 5.45711\n",
      "Epoch 269/1000, Training Loss: 37.98428, Validation Loss: 5.45585\n",
      "Epoch 270/1000, Training Loss: 37.97717, Validation Loss: 5.45460\n",
      "Epoch 271/1000, Training Loss: 37.97007, Validation Loss: 5.45335\n",
      "Epoch 272/1000, Training Loss: 37.96299, Validation Loss: 5.45211\n",
      "Epoch 273/1000, Training Loss: 37.95592, Validation Loss: 5.45086\n",
      "Epoch 274/1000, Training Loss: 37.94886, Validation Loss: 5.44962\n",
      "Epoch 275/1000, Training Loss: 37.94181, Validation Loss: 5.44838\n",
      "Epoch 276/1000, Training Loss: 37.93477, Validation Loss: 5.44715\n",
      "Epoch 277/1000, Training Loss: 37.92774, Validation Loss: 5.44591\n",
      "Epoch 278/1000, Training Loss: 37.92072, Validation Loss: 5.44468\n",
      "Epoch 279/1000, Training Loss: 37.91371, Validation Loss: 5.44345\n",
      "Epoch 280/1000, Training Loss: 37.90672, Validation Loss: 5.44223\n",
      "Epoch 281/1000, Training Loss: 37.89975, Validation Loss: 5.44100\n",
      "Epoch 282/1000, Training Loss: 37.89279, Validation Loss: 5.43978\n",
      "Epoch 283/1000, Training Loss: 37.88585, Validation Loss: 5.43856\n",
      "Epoch 284/1000, Training Loss: 37.87892, Validation Loss: 5.43734\n",
      "Epoch 285/1000, Training Loss: 37.87201, Validation Loss: 5.43613\n",
      "Epoch 286/1000, Training Loss: 37.86510, Validation Loss: 5.43491\n",
      "Epoch 287/1000, Training Loss: 37.85821, Validation Loss: 5.43370\n",
      "Epoch 288/1000, Training Loss: 37.85134, Validation Loss: 5.43249\n",
      "Epoch 289/1000, Training Loss: 37.84447, Validation Loss: 5.43128\n",
      "Epoch 290/1000, Training Loss: 37.83761, Validation Loss: 5.43007\n",
      "Epoch 291/1000, Training Loss: 37.83076, Validation Loss: 5.42886\n",
      "Epoch 292/1000, Training Loss: 37.82392, Validation Loss: 5.42766\n",
      "Epoch 293/1000, Training Loss: 37.81709, Validation Loss: 5.42646\n",
      "Epoch 294/1000, Training Loss: 37.81028, Validation Loss: 5.42526\n",
      "Epoch 295/1000, Training Loss: 37.80348, Validation Loss: 5.42406\n",
      "Epoch 296/1000, Training Loss: 37.79669, Validation Loss: 5.42286\n",
      "Epoch 297/1000, Training Loss: 37.78992, Validation Loss: 5.42166\n",
      "Epoch 298/1000, Training Loss: 37.78315, Validation Loss: 5.42047\n",
      "Epoch 299/1000, Training Loss: 37.77640, Validation Loss: 5.41928\n",
      "Epoch 300/1000, Training Loss: 37.76967, Validation Loss: 5.41809\n",
      "Epoch 301/1000, Training Loss: 37.76294, Validation Loss: 5.41690\n",
      "Epoch 302/1000, Training Loss: 37.75624, Validation Loss: 5.41572\n",
      "Epoch 303/1000, Training Loss: 37.74955, Validation Loss: 5.41453\n",
      "Epoch 304/1000, Training Loss: 37.74288, Validation Loss: 5.41335\n",
      "Epoch 305/1000, Training Loss: 37.73621, Validation Loss: 5.41218\n",
      "Epoch 306/1000, Training Loss: 37.72955, Validation Loss: 5.41100\n",
      "Epoch 307/1000, Training Loss: 37.72290, Validation Loss: 5.40983\n",
      "Epoch 308/1000, Training Loss: 37.71627, Validation Loss: 5.40866\n",
      "Epoch 309/1000, Training Loss: 37.70964, Validation Loss: 5.40749\n",
      "Epoch 310/1000, Training Loss: 37.70304, Validation Loss: 5.40632\n",
      "Epoch 311/1000, Training Loss: 37.69645, Validation Loss: 5.40516\n",
      "Epoch 312/1000, Training Loss: 37.68987, Validation Loss: 5.40399\n",
      "Epoch 313/1000, Training Loss: 37.68330, Validation Loss: 5.40283\n",
      "Epoch 314/1000, Training Loss: 37.67674, Validation Loss: 5.40168\n",
      "Epoch 315/1000, Training Loss: 37.67019, Validation Loss: 5.40052\n",
      "Epoch 316/1000, Training Loss: 37.66366, Validation Loss: 5.39937\n",
      "Epoch 317/1000, Training Loss: 37.65714, Validation Loss: 5.39821\n",
      "Epoch 318/1000, Training Loss: 37.65063, Validation Loss: 5.39706\n",
      "Epoch 319/1000, Training Loss: 37.64413, Validation Loss: 5.39591\n",
      "Epoch 320/1000, Training Loss: 37.63764, Validation Loss: 5.39477\n",
      "Epoch 321/1000, Training Loss: 37.63116, Validation Loss: 5.39362\n",
      "Epoch 322/1000, Training Loss: 37.62470, Validation Loss: 5.39248\n",
      "Epoch 323/1000, Training Loss: 37.61824, Validation Loss: 5.39134\n",
      "Epoch 324/1000, Training Loss: 37.61179, Validation Loss: 5.39020\n",
      "Epoch 325/1000, Training Loss: 37.60534, Validation Loss: 5.38906\n",
      "Epoch 326/1000, Training Loss: 37.59891, Validation Loss: 5.38792\n",
      "Epoch 327/1000, Training Loss: 37.59248, Validation Loss: 5.38678\n",
      "Epoch 328/1000, Training Loss: 37.58607, Validation Loss: 5.38565\n",
      "Epoch 329/1000, Training Loss: 37.57967, Validation Loss: 5.38452\n",
      "Epoch 330/1000, Training Loss: 37.57328, Validation Loss: 5.38339\n",
      "Epoch 331/1000, Training Loss: 37.56689, Validation Loss: 5.38226\n",
      "Epoch 332/1000, Training Loss: 37.56052, Validation Loss: 5.38113\n",
      "Epoch 333/1000, Training Loss: 37.55415, Validation Loss: 5.38001\n",
      "Epoch 334/1000, Training Loss: 37.54780, Validation Loss: 5.37889\n",
      "Epoch 335/1000, Training Loss: 37.54146, Validation Loss: 5.37777\n",
      "Epoch 336/1000, Training Loss: 37.53513, Validation Loss: 5.37666\n",
      "Epoch 337/1000, Training Loss: 37.52881, Validation Loss: 5.37554\n",
      "Epoch 338/1000, Training Loss: 37.52249, Validation Loss: 5.37443\n",
      "Epoch 339/1000, Training Loss: 37.51619, Validation Loss: 5.37332\n",
      "Epoch 340/1000, Training Loss: 37.50989, Validation Loss: 5.37221\n",
      "Epoch 341/1000, Training Loss: 37.50360, Validation Loss: 5.37110\n",
      "Epoch 342/1000, Training Loss: 37.49733, Validation Loss: 5.36999\n",
      "Epoch 343/1000, Training Loss: 37.49106, Validation Loss: 5.36888\n",
      "Epoch 344/1000, Training Loss: 37.48481, Validation Loss: 5.36778\n",
      "Epoch 345/1000, Training Loss: 37.47856, Validation Loss: 5.36668\n",
      "Epoch 346/1000, Training Loss: 37.47232, Validation Loss: 5.36557\n",
      "Epoch 347/1000, Training Loss: 37.46609, Validation Loss: 5.36447\n",
      "Epoch 348/1000, Training Loss: 37.45987, Validation Loss: 5.36337\n",
      "Epoch 349/1000, Training Loss: 37.45366, Validation Loss: 5.36227\n",
      "Epoch 350/1000, Training Loss: 37.44747, Validation Loss: 5.36118\n",
      "Epoch 351/1000, Training Loss: 37.44128, Validation Loss: 5.36008\n",
      "Epoch 352/1000, Training Loss: 37.43511, Validation Loss: 5.35899\n",
      "Epoch 353/1000, Training Loss: 37.42894, Validation Loss: 5.35790\n",
      "Epoch 354/1000, Training Loss: 37.42278, Validation Loss: 5.35681\n",
      "Epoch 355/1000, Training Loss: 37.41663, Validation Loss: 5.35572\n",
      "Epoch 356/1000, Training Loss: 37.41049, Validation Loss: 5.35463\n",
      "Epoch 357/1000, Training Loss: 37.40435, Validation Loss: 5.35354\n",
      "Epoch 358/1000, Training Loss: 37.39823, Validation Loss: 5.35246\n",
      "Epoch 359/1000, Training Loss: 37.39213, Validation Loss: 5.35137\n",
      "Epoch 360/1000, Training Loss: 37.38603, Validation Loss: 5.35029\n",
      "Epoch 361/1000, Training Loss: 37.37994, Validation Loss: 5.34922\n",
      "Epoch 362/1000, Training Loss: 37.37385, Validation Loss: 5.34814\n",
      "Epoch 363/1000, Training Loss: 37.36777, Validation Loss: 5.34706\n",
      "Epoch 364/1000, Training Loss: 37.36170, Validation Loss: 5.34599\n",
      "Epoch 365/1000, Training Loss: 37.35564, Validation Loss: 5.34492\n",
      "Epoch 366/1000, Training Loss: 37.34959, Validation Loss: 5.34385\n",
      "Epoch 367/1000, Training Loss: 37.34355, Validation Loss: 5.34278\n",
      "Epoch 368/1000, Training Loss: 37.33751, Validation Loss: 5.34171\n",
      "Epoch 369/1000, Training Loss: 37.33149, Validation Loss: 5.34065\n",
      "Epoch 370/1000, Training Loss: 37.32548, Validation Loss: 5.33958\n",
      "Epoch 371/1000, Training Loss: 37.31947, Validation Loss: 5.33852\n",
      "Epoch 372/1000, Training Loss: 37.31347, Validation Loss: 5.33746\n",
      "Epoch 373/1000, Training Loss: 37.30748, Validation Loss: 5.33639\n",
      "Epoch 374/1000, Training Loss: 37.30150, Validation Loss: 5.33533\n",
      "Epoch 375/1000, Training Loss: 37.29552, Validation Loss: 5.33427\n",
      "Epoch 376/1000, Training Loss: 37.28955, Validation Loss: 5.33321\n",
      "Epoch 377/1000, Training Loss: 37.28359, Validation Loss: 5.33216\n",
      "Epoch 378/1000, Training Loss: 37.27763, Validation Loss: 5.33110\n",
      "Epoch 379/1000, Training Loss: 37.27168, Validation Loss: 5.33004\n",
      "Epoch 380/1000, Training Loss: 37.26574, Validation Loss: 5.32899\n",
      "Epoch 381/1000, Training Loss: 37.25980, Validation Loss: 5.32794\n",
      "Epoch 382/1000, Training Loss: 37.25387, Validation Loss: 5.32689\n",
      "Epoch 383/1000, Training Loss: 37.24795, Validation Loss: 5.32584\n",
      "Epoch 384/1000, Training Loss: 37.24203, Validation Loss: 5.32479\n",
      "Epoch 385/1000, Training Loss: 37.23612, Validation Loss: 5.32374\n",
      "Epoch 386/1000, Training Loss: 37.23022, Validation Loss: 5.32270\n",
      "Epoch 387/1000, Training Loss: 37.22433, Validation Loss: 5.32166\n",
      "Epoch 388/1000, Training Loss: 37.21844, Validation Loss: 5.32061\n",
      "Epoch 389/1000, Training Loss: 37.21257, Validation Loss: 5.31957\n",
      "Epoch 390/1000, Training Loss: 37.20671, Validation Loss: 5.31853\n",
      "Epoch 391/1000, Training Loss: 37.20085, Validation Loss: 5.31749\n",
      "Epoch 392/1000, Training Loss: 37.19501, Validation Loss: 5.31645\n",
      "Epoch 393/1000, Training Loss: 37.18916, Validation Loss: 5.31542\n",
      "Epoch 394/1000, Training Loss: 37.18333, Validation Loss: 5.31438\n",
      "Epoch 395/1000, Training Loss: 37.17750, Validation Loss: 5.31335\n",
      "Epoch 396/1000, Training Loss: 37.17169, Validation Loss: 5.31232\n",
      "Epoch 397/1000, Training Loss: 37.16588, Validation Loss: 5.31129\n",
      "Epoch 398/1000, Training Loss: 37.16008, Validation Loss: 5.31026\n",
      "Epoch 399/1000, Training Loss: 37.15428, Validation Loss: 5.30923\n",
      "Epoch 400/1000, Training Loss: 37.14849, Validation Loss: 5.30820\n",
      "Epoch 401/1000, Training Loss: 37.14271, Validation Loss: 5.30718\n",
      "Epoch 402/1000, Training Loss: 37.13695, Validation Loss: 5.30615\n",
      "Epoch 403/1000, Training Loss: 37.13118, Validation Loss: 5.30513\n",
      "Epoch 404/1000, Training Loss: 37.12543, Validation Loss: 5.30410\n",
      "Epoch 405/1000, Training Loss: 37.11968, Validation Loss: 5.30308\n",
      "Epoch 406/1000, Training Loss: 37.11395, Validation Loss: 5.30206\n",
      "Epoch 407/1000, Training Loss: 37.10822, Validation Loss: 5.30104\n",
      "Epoch 408/1000, Training Loss: 37.10249, Validation Loss: 5.30002\n",
      "Epoch 409/1000, Training Loss: 37.09677, Validation Loss: 5.29900\n",
      "Epoch 410/1000, Training Loss: 37.09105, Validation Loss: 5.29799\n",
      "Epoch 411/1000, Training Loss: 37.08533, Validation Loss: 5.29697\n",
      "Epoch 412/1000, Training Loss: 37.07962, Validation Loss: 5.29596\n",
      "Epoch 413/1000, Training Loss: 37.07392, Validation Loss: 5.29495\n",
      "Epoch 414/1000, Training Loss: 37.06822, Validation Loss: 5.29394\n",
      "Epoch 415/1000, Training Loss: 37.06254, Validation Loss: 5.29293\n",
      "Epoch 416/1000, Training Loss: 37.05686, Validation Loss: 5.29192\n",
      "Epoch 417/1000, Training Loss: 37.05118, Validation Loss: 5.29091\n",
      "Epoch 418/1000, Training Loss: 37.04552, Validation Loss: 5.28991\n",
      "Epoch 419/1000, Training Loss: 37.03986, Validation Loss: 5.28890\n",
      "Epoch 420/1000, Training Loss: 37.03421, Validation Loss: 5.28790\n",
      "Epoch 421/1000, Training Loss: 37.02857, Validation Loss: 5.28690\n",
      "Epoch 422/1000, Training Loss: 37.02294, Validation Loss: 5.28590\n",
      "Epoch 423/1000, Training Loss: 37.01731, Validation Loss: 5.28490\n",
      "Epoch 424/1000, Training Loss: 37.01169, Validation Loss: 5.28390\n",
      "Epoch 425/1000, Training Loss: 37.00607, Validation Loss: 5.28291\n",
      "Epoch 426/1000, Training Loss: 37.00047, Validation Loss: 5.28191\n",
      "Epoch 427/1000, Training Loss: 36.99487, Validation Loss: 5.28092\n",
      "Epoch 428/1000, Training Loss: 36.98928, Validation Loss: 5.27992\n",
      "Epoch 429/1000, Training Loss: 36.98370, Validation Loss: 5.27893\n",
      "Epoch 430/1000, Training Loss: 36.97813, Validation Loss: 5.27794\n",
      "Epoch 431/1000, Training Loss: 36.97256, Validation Loss: 5.27695\n",
      "Epoch 432/1000, Training Loss: 36.96701, Validation Loss: 5.27596\n",
      "Epoch 433/1000, Training Loss: 36.96146, Validation Loss: 5.27497\n",
      "Epoch 434/1000, Training Loss: 36.95592, Validation Loss: 5.27398\n",
      "Epoch 435/1000, Training Loss: 36.95039, Validation Loss: 5.27300\n",
      "Epoch 436/1000, Training Loss: 36.94487, Validation Loss: 5.27201\n",
      "Epoch 437/1000, Training Loss: 36.93936, Validation Loss: 5.27103\n",
      "Epoch 438/1000, Training Loss: 36.93386, Validation Loss: 5.27005\n",
      "Epoch 439/1000, Training Loss: 36.92836, Validation Loss: 5.26907\n",
      "Epoch 440/1000, Training Loss: 36.92287, Validation Loss: 5.26809\n",
      "Epoch 441/1000, Training Loss: 36.91738, Validation Loss: 5.26712\n",
      "Epoch 442/1000, Training Loss: 36.91190, Validation Loss: 5.26614\n",
      "Epoch 443/1000, Training Loss: 36.90643, Validation Loss: 5.26517\n",
      "Epoch 444/1000, Training Loss: 36.90096, Validation Loss: 5.26419\n",
      "Epoch 445/1000, Training Loss: 36.89551, Validation Loss: 5.26322\n",
      "Epoch 446/1000, Training Loss: 36.89006, Validation Loss: 5.26225\n",
      "Epoch 447/1000, Training Loss: 36.88462, Validation Loss: 5.26128\n",
      "Epoch 448/1000, Training Loss: 36.87919, Validation Loss: 5.26032\n",
      "Epoch 449/1000, Training Loss: 36.87377, Validation Loss: 5.25935\n",
      "Epoch 450/1000, Training Loss: 36.86835, Validation Loss: 5.25839\n",
      "Epoch 451/1000, Training Loss: 36.86294, Validation Loss: 5.25742\n",
      "Epoch 452/1000, Training Loss: 36.85754, Validation Loss: 5.25646\n",
      "Epoch 453/1000, Training Loss: 36.85215, Validation Loss: 5.25550\n",
      "Epoch 454/1000, Training Loss: 36.84677, Validation Loss: 5.25454\n",
      "Epoch 455/1000, Training Loss: 36.84139, Validation Loss: 5.25357\n",
      "Epoch 456/1000, Training Loss: 36.83602, Validation Loss: 5.25261\n",
      "Epoch 457/1000, Training Loss: 36.83066, Validation Loss: 5.25166\n",
      "Epoch 458/1000, Training Loss: 36.82530, Validation Loss: 5.25070\n",
      "Epoch 459/1000, Training Loss: 36.81996, Validation Loss: 5.24974\n",
      "Epoch 460/1000, Training Loss: 36.81462, Validation Loss: 5.24879\n",
      "Epoch 461/1000, Training Loss: 36.80928, Validation Loss: 5.24783\n",
      "Epoch 462/1000, Training Loss: 36.80396, Validation Loss: 5.24688\n",
      "Epoch 463/1000, Training Loss: 36.79863, Validation Loss: 5.24593\n",
      "Epoch 464/1000, Training Loss: 36.79332, Validation Loss: 5.24498\n",
      "Epoch 465/1000, Training Loss: 36.78801, Validation Loss: 5.24403\n",
      "Epoch 466/1000, Training Loss: 36.78271, Validation Loss: 5.24308\n",
      "Epoch 467/1000, Training Loss: 36.77742, Validation Loss: 5.24213\n",
      "Epoch 468/1000, Training Loss: 36.77213, Validation Loss: 5.24118\n",
      "Epoch 469/1000, Training Loss: 36.76685, Validation Loss: 5.24024\n",
      "Epoch 470/1000, Training Loss: 36.76157, Validation Loss: 5.23929\n",
      "Epoch 471/1000, Training Loss: 36.75630, Validation Loss: 5.23835\n",
      "Epoch 472/1000, Training Loss: 36.75104, Validation Loss: 5.23741\n",
      "Epoch 473/1000, Training Loss: 36.74577, Validation Loss: 5.23647\n",
      "Epoch 474/1000, Training Loss: 36.74052, Validation Loss: 5.23553\n",
      "Epoch 475/1000, Training Loss: 36.73526, Validation Loss: 5.23459\n",
      "Epoch 476/1000, Training Loss: 36.73002, Validation Loss: 5.23365\n",
      "Epoch 477/1000, Training Loss: 36.72478, Validation Loss: 5.23271\n",
      "Epoch 478/1000, Training Loss: 36.71955, Validation Loss: 5.23177\n",
      "Epoch 479/1000, Training Loss: 36.71432, Validation Loss: 5.23084\n",
      "Epoch 480/1000, Training Loss: 36.70910, Validation Loss: 5.22990\n",
      "Epoch 481/1000, Training Loss: 36.70389, Validation Loss: 5.22897\n",
      "Epoch 482/1000, Training Loss: 36.69868, Validation Loss: 5.22803\n",
      "Epoch 483/1000, Training Loss: 36.69348, Validation Loss: 5.22710\n",
      "Epoch 484/1000, Training Loss: 36.68828, Validation Loss: 5.22617\n",
      "Epoch 485/1000, Training Loss: 36.68309, Validation Loss: 5.22524\n",
      "Epoch 486/1000, Training Loss: 36.67790, Validation Loss: 5.22431\n",
      "Epoch 487/1000, Training Loss: 36.67273, Validation Loss: 5.22338\n",
      "Epoch 488/1000, Training Loss: 36.66756, Validation Loss: 5.22246\n",
      "Epoch 489/1000, Training Loss: 36.66239, Validation Loss: 5.22153\n",
      "Epoch 490/1000, Training Loss: 36.65723, Validation Loss: 5.22061\n",
      "Epoch 491/1000, Training Loss: 36.65207, Validation Loss: 5.21968\n",
      "Epoch 492/1000, Training Loss: 36.64692, Validation Loss: 5.21876\n",
      "Epoch 493/1000, Training Loss: 36.64177, Validation Loss: 5.21783\n",
      "Epoch 494/1000, Training Loss: 36.63662, Validation Loss: 5.21691\n",
      "Epoch 495/1000, Training Loss: 36.63149, Validation Loss: 5.21599\n",
      "Epoch 496/1000, Training Loss: 36.62636, Validation Loss: 5.21507\n",
      "Epoch 497/1000, Training Loss: 36.62123, Validation Loss: 5.21415\n",
      "Epoch 498/1000, Training Loss: 36.61612, Validation Loss: 5.21323\n",
      "Epoch 499/1000, Training Loss: 36.61101, Validation Loss: 5.21231\n",
      "Epoch 500/1000, Training Loss: 36.60591, Validation Loss: 5.21139\n",
      "Epoch 501/1000, Training Loss: 36.60082, Validation Loss: 5.21048\n",
      "Epoch 502/1000, Training Loss: 36.59573, Validation Loss: 5.20956\n",
      "Epoch 503/1000, Training Loss: 36.59065, Validation Loss: 5.20865\n",
      "Epoch 504/1000, Training Loss: 36.58558, Validation Loss: 5.20773\n",
      "Epoch 505/1000, Training Loss: 36.58051, Validation Loss: 5.20682\n",
      "Epoch 506/1000, Training Loss: 36.57544, Validation Loss: 5.20591\n",
      "Epoch 507/1000, Training Loss: 36.57038, Validation Loss: 5.20500\n",
      "Epoch 508/1000, Training Loss: 36.56532, Validation Loss: 5.20409\n",
      "Epoch 509/1000, Training Loss: 36.56026, Validation Loss: 5.20318\n",
      "Epoch 510/1000, Training Loss: 36.55521, Validation Loss: 5.20227\n",
      "Epoch 511/1000, Training Loss: 36.55017, Validation Loss: 5.20136\n",
      "Epoch 512/1000, Training Loss: 36.54514, Validation Loss: 5.20046\n",
      "Epoch 513/1000, Training Loss: 36.54010, Validation Loss: 5.19955\n",
      "Epoch 514/1000, Training Loss: 36.53508, Validation Loss: 5.19865\n",
      "Epoch 515/1000, Training Loss: 36.53006, Validation Loss: 5.19774\n",
      "Epoch 516/1000, Training Loss: 36.52504, Validation Loss: 5.19684\n",
      "Epoch 517/1000, Training Loss: 36.52003, Validation Loss: 5.19593\n",
      "Epoch 518/1000, Training Loss: 36.51502, Validation Loss: 5.19503\n",
      "Epoch 519/1000, Training Loss: 36.51002, Validation Loss: 5.19413\n",
      "Epoch 520/1000, Training Loss: 36.50502, Validation Loss: 5.19323\n",
      "Epoch 521/1000, Training Loss: 36.50003, Validation Loss: 5.19233\n",
      "Epoch 522/1000, Training Loss: 36.49504, Validation Loss: 5.19143\n",
      "Epoch 523/1000, Training Loss: 36.49006, Validation Loss: 5.19054\n",
      "Epoch 524/1000, Training Loss: 36.48508, Validation Loss: 5.18964\n",
      "Epoch 525/1000, Training Loss: 36.48011, Validation Loss: 5.18874\n",
      "Epoch 526/1000, Training Loss: 36.47514, Validation Loss: 5.18785\n",
      "Epoch 527/1000, Training Loss: 36.47018, Validation Loss: 5.18695\n",
      "Epoch 528/1000, Training Loss: 36.46523, Validation Loss: 5.18606\n",
      "Epoch 529/1000, Training Loss: 36.46028, Validation Loss: 5.18517\n",
      "Epoch 530/1000, Training Loss: 36.45534, Validation Loss: 5.18428\n",
      "Epoch 531/1000, Training Loss: 36.45041, Validation Loss: 5.18339\n",
      "Epoch 532/1000, Training Loss: 36.44548, Validation Loss: 5.18250\n",
      "Epoch 533/1000, Training Loss: 36.44055, Validation Loss: 5.18161\n",
      "Epoch 534/1000, Training Loss: 36.43562, Validation Loss: 5.18072\n",
      "Epoch 535/1000, Training Loss: 36.43070, Validation Loss: 5.17984\n",
      "Epoch 536/1000, Training Loss: 36.42578, Validation Loss: 5.17895\n",
      "Epoch 537/1000, Training Loss: 36.42087, Validation Loss: 5.17807\n",
      "Epoch 538/1000, Training Loss: 36.41597, Validation Loss: 5.17718\n",
      "Epoch 539/1000, Training Loss: 36.41107, Validation Loss: 5.17630\n",
      "Epoch 540/1000, Training Loss: 36.40618, Validation Loss: 5.17542\n",
      "Epoch 541/1000, Training Loss: 36.40129, Validation Loss: 5.17454\n",
      "Epoch 542/1000, Training Loss: 36.39641, Validation Loss: 5.17366\n",
      "Epoch 543/1000, Training Loss: 36.39154, Validation Loss: 5.17278\n",
      "Epoch 544/1000, Training Loss: 36.38667, Validation Loss: 5.17190\n",
      "Epoch 545/1000, Training Loss: 36.38180, Validation Loss: 5.17102\n",
      "Epoch 546/1000, Training Loss: 36.37693, Validation Loss: 5.17015\n",
      "Epoch 547/1000, Training Loss: 36.37207, Validation Loss: 5.16927\n",
      "Epoch 548/1000, Training Loss: 36.36722, Validation Loss: 5.16839\n",
      "Epoch 549/1000, Training Loss: 36.36236, Validation Loss: 5.16752\n",
      "Epoch 550/1000, Training Loss: 36.35751, Validation Loss: 5.16664\n",
      "Epoch 551/1000, Training Loss: 36.35267, Validation Loss: 5.16577\n",
      "Epoch 552/1000, Training Loss: 36.34783, Validation Loss: 5.16489\n",
      "Epoch 553/1000, Training Loss: 36.34299, Validation Loss: 5.16402\n",
      "Epoch 554/1000, Training Loss: 36.33815, Validation Loss: 5.16314\n",
      "Epoch 555/1000, Training Loss: 36.33332, Validation Loss: 5.16227\n",
      "Epoch 556/1000, Training Loss: 36.32849, Validation Loss: 5.16140\n",
      "Epoch 557/1000, Training Loss: 36.32367, Validation Loss: 5.16053\n",
      "Epoch 558/1000, Training Loss: 36.31886, Validation Loss: 5.15966\n",
      "Epoch 559/1000, Training Loss: 36.31405, Validation Loss: 5.15879\n",
      "Epoch 560/1000, Training Loss: 36.30925, Validation Loss: 5.15793\n",
      "Epoch 561/1000, Training Loss: 36.30445, Validation Loss: 5.15706\n",
      "Epoch 562/1000, Training Loss: 36.29965, Validation Loss: 5.15620\n",
      "Epoch 563/1000, Training Loss: 36.29486, Validation Loss: 5.15533\n",
      "Epoch 564/1000, Training Loss: 36.29007, Validation Loss: 5.15447\n",
      "Epoch 565/1000, Training Loss: 36.28529, Validation Loss: 5.15361\n",
      "Epoch 566/1000, Training Loss: 36.28051, Validation Loss: 5.15275\n",
      "Epoch 567/1000, Training Loss: 36.27574, Validation Loss: 5.15189\n",
      "Epoch 568/1000, Training Loss: 36.27097, Validation Loss: 5.15103\n",
      "Epoch 569/1000, Training Loss: 36.26621, Validation Loss: 5.15017\n",
      "Epoch 570/1000, Training Loss: 36.26145, Validation Loss: 5.14931\n",
      "Epoch 571/1000, Training Loss: 36.25670, Validation Loss: 5.14845\n",
      "Epoch 572/1000, Training Loss: 36.25196, Validation Loss: 5.14760\n",
      "Epoch 573/1000, Training Loss: 36.24722, Validation Loss: 5.14674\n",
      "Epoch 574/1000, Training Loss: 36.24248, Validation Loss: 5.14589\n",
      "Epoch 575/1000, Training Loss: 36.23775, Validation Loss: 5.14503\n",
      "Epoch 576/1000, Training Loss: 36.23302, Validation Loss: 5.14418\n",
      "Epoch 577/1000, Training Loss: 36.22830, Validation Loss: 5.14333\n",
      "Epoch 578/1000, Training Loss: 36.22359, Validation Loss: 5.14248\n",
      "Epoch 579/1000, Training Loss: 36.21888, Validation Loss: 5.14163\n",
      "Epoch 580/1000, Training Loss: 36.21417, Validation Loss: 5.14078\n",
      "Epoch 581/1000, Training Loss: 36.20946, Validation Loss: 5.13993\n",
      "Epoch 582/1000, Training Loss: 36.20476, Validation Loss: 5.13908\n",
      "Epoch 583/1000, Training Loss: 36.20007, Validation Loss: 5.13823\n",
      "Epoch 584/1000, Training Loss: 36.19537, Validation Loss: 5.13738\n",
      "Epoch 585/1000, Training Loss: 36.19069, Validation Loss: 5.13654\n",
      "Epoch 586/1000, Training Loss: 36.18601, Validation Loss: 5.13570\n",
      "Epoch 587/1000, Training Loss: 36.18134, Validation Loss: 5.13485\n",
      "Epoch 588/1000, Training Loss: 36.17667, Validation Loss: 5.13401\n",
      "Epoch 589/1000, Training Loss: 36.17200, Validation Loss: 5.13317\n",
      "Epoch 590/1000, Training Loss: 36.16734, Validation Loss: 5.13233\n",
      "Epoch 591/1000, Training Loss: 36.16268, Validation Loss: 5.13149\n",
      "Epoch 592/1000, Training Loss: 36.15803, Validation Loss: 5.13065\n",
      "Epoch 593/1000, Training Loss: 36.15338, Validation Loss: 5.12981\n",
      "Epoch 594/1000, Training Loss: 36.14874, Validation Loss: 5.12897\n",
      "Epoch 595/1000, Training Loss: 36.14411, Validation Loss: 5.12813\n",
      "Epoch 596/1000, Training Loss: 36.13948, Validation Loss: 5.12730\n",
      "Epoch 597/1000, Training Loss: 36.13485, Validation Loss: 5.12646\n",
      "Epoch 598/1000, Training Loss: 36.13023, Validation Loss: 5.12563\n",
      "Epoch 599/1000, Training Loss: 36.12561, Validation Loss: 5.12480\n",
      "Epoch 600/1000, Training Loss: 36.12100, Validation Loss: 5.12396\n",
      "Epoch 601/1000, Training Loss: 36.11639, Validation Loss: 5.12313\n",
      "Epoch 602/1000, Training Loss: 36.11178, Validation Loss: 5.12230\n",
      "Epoch 603/1000, Training Loss: 36.10718, Validation Loss: 5.12147\n",
      "Epoch 604/1000, Training Loss: 36.10258, Validation Loss: 5.12064\n",
      "Epoch 605/1000, Training Loss: 36.09799, Validation Loss: 5.11982\n",
      "Epoch 606/1000, Training Loss: 36.09340, Validation Loss: 5.11899\n",
      "Epoch 607/1000, Training Loss: 36.08882, Validation Loss: 5.11816\n",
      "Epoch 608/1000, Training Loss: 36.08424, Validation Loss: 5.11734\n",
      "Epoch 609/1000, Training Loss: 36.07966, Validation Loss: 5.11651\n",
      "Epoch 610/1000, Training Loss: 36.07509, Validation Loss: 5.11569\n",
      "Epoch 611/1000, Training Loss: 36.07053, Validation Loss: 5.11487\n",
      "Epoch 612/1000, Training Loss: 36.06597, Validation Loss: 5.11404\n",
      "Epoch 613/1000, Training Loss: 36.06141, Validation Loss: 5.11322\n",
      "Epoch 614/1000, Training Loss: 36.05686, Validation Loss: 5.11240\n",
      "Epoch 615/1000, Training Loss: 36.05231, Validation Loss: 5.11158\n",
      "Epoch 616/1000, Training Loss: 36.04777, Validation Loss: 5.11077\n",
      "Epoch 617/1000, Training Loss: 36.04323, Validation Loss: 5.10995\n",
      "Epoch 618/1000, Training Loss: 36.03869, Validation Loss: 5.10913\n",
      "Epoch 619/1000, Training Loss: 36.03415, Validation Loss: 5.10832\n",
      "Epoch 620/1000, Training Loss: 36.02962, Validation Loss: 5.10750\n",
      "Epoch 621/1000, Training Loss: 36.02509, Validation Loss: 5.10669\n",
      "Epoch 622/1000, Training Loss: 36.02057, Validation Loss: 5.10587\n",
      "Epoch 623/1000, Training Loss: 36.01604, Validation Loss: 5.10506\n",
      "Epoch 624/1000, Training Loss: 36.01153, Validation Loss: 5.10425\n",
      "Epoch 625/1000, Training Loss: 36.00702, Validation Loss: 5.10344\n",
      "Epoch 626/1000, Training Loss: 36.00252, Validation Loss: 5.10263\n",
      "Epoch 627/1000, Training Loss: 35.99802, Validation Loss: 5.10183\n",
      "Epoch 628/1000, Training Loss: 35.99353, Validation Loss: 5.10102\n",
      "Epoch 629/1000, Training Loss: 35.98904, Validation Loss: 5.10021\n",
      "Epoch 630/1000, Training Loss: 35.98455, Validation Loss: 5.09941\n",
      "Epoch 631/1000, Training Loss: 35.98007, Validation Loss: 5.09861\n",
      "Epoch 632/1000, Training Loss: 35.97560, Validation Loss: 5.09780\n",
      "Epoch 633/1000, Training Loss: 35.97113, Validation Loss: 5.09700\n",
      "Epoch 634/1000, Training Loss: 35.96666, Validation Loss: 5.09620\n",
      "Epoch 635/1000, Training Loss: 35.96220, Validation Loss: 5.09540\n",
      "Epoch 636/1000, Training Loss: 35.95774, Validation Loss: 5.09460\n",
      "Epoch 637/1000, Training Loss: 35.95328, Validation Loss: 5.09380\n",
      "Epoch 638/1000, Training Loss: 35.94883, Validation Loss: 5.09300\n",
      "Epoch 639/1000, Training Loss: 35.94438, Validation Loss: 5.09220\n",
      "Epoch 640/1000, Training Loss: 35.93993, Validation Loss: 5.09141\n",
      "Epoch 641/1000, Training Loss: 35.93550, Validation Loss: 5.09061\n",
      "Epoch 642/1000, Training Loss: 35.93106, Validation Loss: 5.08982\n",
      "Epoch 643/1000, Training Loss: 35.92663, Validation Loss: 5.08902\n",
      "Epoch 644/1000, Training Loss: 35.92220, Validation Loss: 5.08823\n",
      "Epoch 645/1000, Training Loss: 35.91777, Validation Loss: 5.08743\n",
      "Epoch 646/1000, Training Loss: 35.91335, Validation Loss: 5.08664\n",
      "Epoch 647/1000, Training Loss: 35.90893, Validation Loss: 5.08585\n",
      "Epoch 648/1000, Training Loss: 35.90451, Validation Loss: 5.08505\n",
      "Epoch 649/1000, Training Loss: 35.90010, Validation Loss: 5.08426\n",
      "Epoch 650/1000, Training Loss: 35.89569, Validation Loss: 5.08347\n",
      "Epoch 651/1000, Training Loss: 35.89129, Validation Loss: 5.08268\n",
      "Epoch 652/1000, Training Loss: 35.88689, Validation Loss: 5.08189\n",
      "Epoch 653/1000, Training Loss: 35.88250, Validation Loss: 5.08110\n",
      "Epoch 654/1000, Training Loss: 35.87812, Validation Loss: 5.08032\n",
      "Epoch 655/1000, Training Loss: 35.87374, Validation Loss: 5.07953\n",
      "Epoch 656/1000, Training Loss: 35.86936, Validation Loss: 5.07874\n",
      "Epoch 657/1000, Training Loss: 35.86499, Validation Loss: 5.07796\n",
      "Epoch 658/1000, Training Loss: 35.86063, Validation Loss: 5.07717\n",
      "Epoch 659/1000, Training Loss: 35.85627, Validation Loss: 5.07639\n",
      "Epoch 660/1000, Training Loss: 35.85191, Validation Loss: 5.07561\n",
      "Epoch 661/1000, Training Loss: 35.84756, Validation Loss: 5.07483\n",
      "Epoch 662/1000, Training Loss: 35.84322, Validation Loss: 5.07404\n",
      "Epoch 663/1000, Training Loss: 35.83888, Validation Loss: 5.07326\n",
      "Epoch 664/1000, Training Loss: 35.83454, Validation Loss: 5.07248\n",
      "Epoch 665/1000, Training Loss: 35.83022, Validation Loss: 5.07170\n",
      "Epoch 666/1000, Training Loss: 35.82589, Validation Loss: 5.07092\n",
      "Epoch 667/1000, Training Loss: 35.82157, Validation Loss: 5.07014\n",
      "Epoch 668/1000, Training Loss: 35.81725, Validation Loss: 5.06936\n",
      "Epoch 669/1000, Training Loss: 35.81294, Validation Loss: 5.06858\n",
      "Epoch 670/1000, Training Loss: 35.80863, Validation Loss: 5.06781\n",
      "Epoch 671/1000, Training Loss: 35.80433, Validation Loss: 5.06703\n",
      "Epoch 672/1000, Training Loss: 35.80003, Validation Loss: 5.06625\n",
      "Epoch 673/1000, Training Loss: 35.79573, Validation Loss: 5.06548\n",
      "Epoch 674/1000, Training Loss: 35.79144, Validation Loss: 5.06471\n",
      "Epoch 675/1000, Training Loss: 35.78716, Validation Loss: 5.06393\n",
      "Epoch 676/1000, Training Loss: 35.78287, Validation Loss: 5.06316\n",
      "Epoch 677/1000, Training Loss: 35.77859, Validation Loss: 5.06239\n",
      "Epoch 678/1000, Training Loss: 35.77431, Validation Loss: 5.06162\n",
      "Epoch 679/1000, Training Loss: 35.77003, Validation Loss: 5.06085\n",
      "Epoch 680/1000, Training Loss: 35.76576, Validation Loss: 5.06008\n",
      "Epoch 681/1000, Training Loss: 35.76149, Validation Loss: 5.05931\n",
      "Epoch 682/1000, Training Loss: 35.75722, Validation Loss: 5.05854\n",
      "Epoch 683/1000, Training Loss: 35.75295, Validation Loss: 5.05777\n",
      "Epoch 684/1000, Training Loss: 35.74869, Validation Loss: 5.05701\n",
      "Epoch 685/1000, Training Loss: 35.74443, Validation Loss: 5.05624\n",
      "Epoch 686/1000, Training Loss: 35.74017, Validation Loss: 5.05547\n",
      "Epoch 687/1000, Training Loss: 35.73592, Validation Loss: 5.05471\n",
      "Epoch 688/1000, Training Loss: 35.73167, Validation Loss: 5.05394\n",
      "Epoch 689/1000, Training Loss: 35.72743, Validation Loss: 5.05318\n",
      "Epoch 690/1000, Training Loss: 35.72318, Validation Loss: 5.05241\n",
      "Epoch 691/1000, Training Loss: 35.71894, Validation Loss: 5.05165\n",
      "Epoch 692/1000, Training Loss: 35.71471, Validation Loss: 5.05089\n",
      "Epoch 693/1000, Training Loss: 35.71048, Validation Loss: 5.05012\n",
      "Epoch 694/1000, Training Loss: 35.70625, Validation Loss: 5.04936\n",
      "Epoch 695/1000, Training Loss: 35.70203, Validation Loss: 5.04860\n",
      "Epoch 696/1000, Training Loss: 35.69781, Validation Loss: 5.04784\n",
      "Epoch 697/1000, Training Loss: 35.69360, Validation Loss: 5.04708\n",
      "Epoch 698/1000, Training Loss: 35.68939, Validation Loss: 5.04632\n",
      "Epoch 699/1000, Training Loss: 35.68518, Validation Loss: 5.04556\n",
      "Epoch 700/1000, Training Loss: 35.68098, Validation Loss: 5.04481\n",
      "Epoch 701/1000, Training Loss: 35.67679, Validation Loss: 5.04405\n",
      "Epoch 702/1000, Training Loss: 35.67260, Validation Loss: 5.04329\n",
      "Epoch 703/1000, Training Loss: 35.66841, Validation Loss: 5.04254\n",
      "Epoch 704/1000, Training Loss: 35.66423, Validation Loss: 5.04178\n",
      "Epoch 705/1000, Training Loss: 35.66005, Validation Loss: 5.04103\n",
      "Epoch 706/1000, Training Loss: 35.65587, Validation Loss: 5.04028\n",
      "Epoch 707/1000, Training Loss: 35.65170, Validation Loss: 5.03952\n",
      "Epoch 708/1000, Training Loss: 35.64753, Validation Loss: 5.03877\n",
      "Epoch 709/1000, Training Loss: 35.64337, Validation Loss: 5.03802\n",
      "Epoch 710/1000, Training Loss: 35.63921, Validation Loss: 5.03727\n",
      "Epoch 711/1000, Training Loss: 35.63506, Validation Loss: 5.03652\n",
      "Epoch 712/1000, Training Loss: 35.63091, Validation Loss: 5.03577\n",
      "Epoch 713/1000, Training Loss: 35.62676, Validation Loss: 5.03502\n",
      "Epoch 714/1000, Training Loss: 35.62262, Validation Loss: 5.03427\n",
      "Epoch 715/1000, Training Loss: 35.61849, Validation Loss: 5.03353\n",
      "Epoch 716/1000, Training Loss: 35.61435, Validation Loss: 5.03278\n",
      "Epoch 717/1000, Training Loss: 35.61022, Validation Loss: 5.03204\n",
      "Epoch 718/1000, Training Loss: 35.60610, Validation Loss: 5.03129\n",
      "Epoch 719/1000, Training Loss: 35.60197, Validation Loss: 5.03055\n",
      "Epoch 720/1000, Training Loss: 35.59785, Validation Loss: 5.02980\n",
      "Epoch 721/1000, Training Loss: 35.59373, Validation Loss: 5.02906\n",
      "Epoch 722/1000, Training Loss: 35.58962, Validation Loss: 5.02832\n",
      "Epoch 723/1000, Training Loss: 35.58551, Validation Loss: 5.02758\n",
      "Epoch 724/1000, Training Loss: 35.58140, Validation Loss: 5.02683\n",
      "Epoch 725/1000, Training Loss: 35.57730, Validation Loss: 5.02609\n",
      "Epoch 726/1000, Training Loss: 35.57319, Validation Loss: 5.02535\n",
      "Epoch 727/1000, Training Loss: 35.56910, Validation Loss: 5.02461\n",
      "Epoch 728/1000, Training Loss: 35.56500, Validation Loss: 5.02387\n",
      "Epoch 729/1000, Training Loss: 35.56090, Validation Loss: 5.02313\n",
      "Epoch 730/1000, Training Loss: 35.55681, Validation Loss: 5.02240\n",
      "Epoch 731/1000, Training Loss: 35.55273, Validation Loss: 5.02166\n",
      "Epoch 732/1000, Training Loss: 35.54864, Validation Loss: 5.02092\n",
      "Epoch 733/1000, Training Loss: 35.54456, Validation Loss: 5.02019\n",
      "Epoch 734/1000, Training Loss: 35.54048, Validation Loss: 5.01945\n",
      "Epoch 735/1000, Training Loss: 35.53641, Validation Loss: 5.01872\n",
      "Epoch 736/1000, Training Loss: 35.53234, Validation Loss: 5.01798\n",
      "Epoch 737/1000, Training Loss: 35.52827, Validation Loss: 5.01725\n",
      "Epoch 738/1000, Training Loss: 35.52421, Validation Loss: 5.01652\n",
      "Epoch 739/1000, Training Loss: 35.52015, Validation Loss: 5.01578\n",
      "Epoch 740/1000, Training Loss: 35.51609, Validation Loss: 5.01505\n",
      "Epoch 741/1000, Training Loss: 35.51203, Validation Loss: 5.01432\n",
      "Epoch 742/1000, Training Loss: 35.50798, Validation Loss: 5.01359\n",
      "Epoch 743/1000, Training Loss: 35.50393, Validation Loss: 5.01286\n",
      "Epoch 744/1000, Training Loss: 35.49989, Validation Loss: 5.01213\n",
      "Epoch 745/1000, Training Loss: 35.49585, Validation Loss: 5.01140\n",
      "Epoch 746/1000, Training Loss: 35.49181, Validation Loss: 5.01067\n",
      "Epoch 747/1000, Training Loss: 35.48778, Validation Loss: 5.00995\n",
      "Epoch 748/1000, Training Loss: 35.48374, Validation Loss: 5.00922\n",
      "Epoch 749/1000, Training Loss: 35.47972, Validation Loss: 5.00849\n",
      "Epoch 750/1000, Training Loss: 35.47569, Validation Loss: 5.00777\n",
      "Epoch 751/1000, Training Loss: 35.47167, Validation Loss: 5.00704\n",
      "Epoch 752/1000, Training Loss: 35.46765, Validation Loss: 5.00632\n",
      "Epoch 753/1000, Training Loss: 35.46363, Validation Loss: 5.00559\n",
      "Epoch 754/1000, Training Loss: 35.45962, Validation Loss: 5.00487\n",
      "Epoch 755/1000, Training Loss: 35.45561, Validation Loss: 5.00414\n",
      "Epoch 756/1000, Training Loss: 35.45160, Validation Loss: 5.00342\n",
      "Epoch 757/1000, Training Loss: 35.44760, Validation Loss: 5.00270\n",
      "Epoch 758/1000, Training Loss: 35.44361, Validation Loss: 5.00198\n",
      "Epoch 759/1000, Training Loss: 35.43962, Validation Loss: 5.00125\n",
      "Epoch 760/1000, Training Loss: 35.43563, Validation Loss: 5.00053\n",
      "Epoch 761/1000, Training Loss: 35.43164, Validation Loss: 4.99981\n",
      "Epoch 762/1000, Training Loss: 35.42766, Validation Loss: 4.99909\n",
      "Epoch 763/1000, Training Loss: 35.42368, Validation Loss: 4.99837\n",
      "Epoch 764/1000, Training Loss: 35.41971, Validation Loss: 4.99765\n",
      "Epoch 765/1000, Training Loss: 35.41574, Validation Loss: 4.99694\n",
      "Epoch 766/1000, Training Loss: 35.41177, Validation Loss: 4.99622\n",
      "Epoch 767/1000, Training Loss: 35.40781, Validation Loss: 4.99550\n",
      "Epoch 768/1000, Training Loss: 35.40385, Validation Loss: 4.99479\n",
      "Epoch 769/1000, Training Loss: 35.39989, Validation Loss: 4.99407\n",
      "Epoch 770/1000, Training Loss: 35.39594, Validation Loss: 4.99336\n",
      "Epoch 771/1000, Training Loss: 35.39199, Validation Loss: 4.99264\n",
      "Epoch 772/1000, Training Loss: 35.38804, Validation Loss: 4.99193\n",
      "Epoch 773/1000, Training Loss: 35.38410, Validation Loss: 4.99122\n",
      "Epoch 774/1000, Training Loss: 35.38015, Validation Loss: 4.99050\n",
      "Epoch 775/1000, Training Loss: 35.37621, Validation Loss: 4.98979\n",
      "Epoch 776/1000, Training Loss: 35.37227, Validation Loss: 4.98908\n",
      "Epoch 777/1000, Training Loss: 35.36834, Validation Loss: 4.98837\n",
      "Epoch 778/1000, Training Loss: 35.36440, Validation Loss: 4.98766\n",
      "Epoch 779/1000, Training Loss: 35.36047, Validation Loss: 4.98695\n",
      "Epoch 780/1000, Training Loss: 35.35654, Validation Loss: 4.98624\n",
      "Epoch 781/1000, Training Loss: 35.35262, Validation Loss: 4.98553\n",
      "Epoch 782/1000, Training Loss: 35.34869, Validation Loss: 4.98482\n",
      "Epoch 783/1000, Training Loss: 35.34478, Validation Loss: 4.98412\n",
      "Epoch 784/1000, Training Loss: 35.34086, Validation Loss: 4.98341\n",
      "Epoch 785/1000, Training Loss: 35.33696, Validation Loss: 4.98270\n",
      "Epoch 786/1000, Training Loss: 35.33305, Validation Loss: 4.98200\n",
      "Epoch 787/1000, Training Loss: 35.32915, Validation Loss: 4.98129\n",
      "Epoch 788/1000, Training Loss: 35.32525, Validation Loss: 4.98059\n",
      "Epoch 789/1000, Training Loss: 35.32135, Validation Loss: 4.97989\n",
      "Epoch 790/1000, Training Loss: 35.31746, Validation Loss: 4.97918\n",
      "Epoch 791/1000, Training Loss: 35.31357, Validation Loss: 4.97848\n",
      "Epoch 792/1000, Training Loss: 35.30969, Validation Loss: 4.97778\n",
      "Epoch 793/1000, Training Loss: 35.30581, Validation Loss: 4.97708\n",
      "Epoch 794/1000, Training Loss: 35.30193, Validation Loss: 4.97638\n",
      "Epoch 795/1000, Training Loss: 35.29806, Validation Loss: 4.97568\n",
      "Epoch 796/1000, Training Loss: 35.29419, Validation Loss: 4.97498\n",
      "Epoch 797/1000, Training Loss: 35.29032, Validation Loss: 4.97428\n",
      "Epoch 798/1000, Training Loss: 35.28646, Validation Loss: 4.97358\n",
      "Epoch 799/1000, Training Loss: 35.28260, Validation Loss: 4.97288\n",
      "Epoch 800/1000, Training Loss: 35.27875, Validation Loss: 4.97219\n",
      "Epoch 801/1000, Training Loss: 35.27490, Validation Loss: 4.97149\n",
      "Epoch 802/1000, Training Loss: 35.27105, Validation Loss: 4.97080\n",
      "Epoch 803/1000, Training Loss: 35.26721, Validation Loss: 4.97010\n",
      "Epoch 804/1000, Training Loss: 35.26337, Validation Loss: 4.96941\n",
      "Epoch 805/1000, Training Loss: 35.25953, Validation Loss: 4.96872\n",
      "Epoch 806/1000, Training Loss: 35.25569, Validation Loss: 4.96803\n",
      "Epoch 807/1000, Training Loss: 35.25186, Validation Loss: 4.96734\n",
      "Epoch 808/1000, Training Loss: 35.24803, Validation Loss: 4.96665\n",
      "Epoch 809/1000, Training Loss: 35.24421, Validation Loss: 4.96596\n",
      "Epoch 810/1000, Training Loss: 35.24038, Validation Loss: 4.96527\n",
      "Epoch 811/1000, Training Loss: 35.23656, Validation Loss: 4.96458\n",
      "Epoch 812/1000, Training Loss: 35.23274, Validation Loss: 4.96389\n",
      "Epoch 813/1000, Training Loss: 35.22892, Validation Loss: 4.96320\n",
      "Epoch 814/1000, Training Loss: 35.22510, Validation Loss: 4.96252\n",
      "Epoch 815/1000, Training Loss: 35.22128, Validation Loss: 4.96183\n",
      "Epoch 816/1000, Training Loss: 35.21747, Validation Loss: 4.96114\n",
      "Epoch 817/1000, Training Loss: 35.21366, Validation Loss: 4.96046\n",
      "Epoch 818/1000, Training Loss: 35.20985, Validation Loss: 4.95978\n",
      "Epoch 819/1000, Training Loss: 35.20605, Validation Loss: 4.95909\n",
      "Epoch 820/1000, Training Loss: 35.20225, Validation Loss: 4.95841\n",
      "Epoch 821/1000, Training Loss: 35.19846, Validation Loss: 4.95773\n",
      "Epoch 822/1000, Training Loss: 35.19467, Validation Loss: 4.95705\n",
      "Epoch 823/1000, Training Loss: 35.19089, Validation Loss: 4.95637\n",
      "Epoch 824/1000, Training Loss: 35.18711, Validation Loss: 4.95569\n",
      "Epoch 825/1000, Training Loss: 35.18333, Validation Loss: 4.95501\n",
      "Epoch 826/1000, Training Loss: 35.17955, Validation Loss: 4.95433\n",
      "Epoch 827/1000, Training Loss: 35.17578, Validation Loss: 4.95365\n",
      "Epoch 828/1000, Training Loss: 35.17201, Validation Loss: 4.95298\n",
      "Epoch 829/1000, Training Loss: 35.16824, Validation Loss: 4.95230\n",
      "Epoch 830/1000, Training Loss: 35.16448, Validation Loss: 4.95162\n",
      "Epoch 831/1000, Training Loss: 35.16072, Validation Loss: 4.95095\n",
      "Epoch 832/1000, Training Loss: 35.15695, Validation Loss: 4.95027\n",
      "Epoch 833/1000, Training Loss: 35.15320, Validation Loss: 4.94960\n",
      "Epoch 834/1000, Training Loss: 35.14945, Validation Loss: 4.94892\n",
      "Epoch 835/1000, Training Loss: 35.14570, Validation Loss: 4.94825\n",
      "Epoch 836/1000, Training Loss: 35.14196, Validation Loss: 4.94758\n",
      "Epoch 837/1000, Training Loss: 35.13822, Validation Loss: 4.94690\n",
      "Epoch 838/1000, Training Loss: 35.13448, Validation Loss: 4.94623\n",
      "Epoch 839/1000, Training Loss: 35.13075, Validation Loss: 4.94556\n",
      "Epoch 840/1000, Training Loss: 35.12702, Validation Loss: 4.94489\n",
      "Epoch 841/1000, Training Loss: 35.12329, Validation Loss: 4.94421\n",
      "Epoch 842/1000, Training Loss: 35.11957, Validation Loss: 4.94354\n",
      "Epoch 843/1000, Training Loss: 35.11585, Validation Loss: 4.94287\n",
      "Epoch 844/1000, Training Loss: 35.11213, Validation Loss: 4.94220\n",
      "Epoch 845/1000, Training Loss: 35.10841, Validation Loss: 4.94153\n",
      "Epoch 846/1000, Training Loss: 35.10469, Validation Loss: 4.94086\n",
      "Epoch 847/1000, Training Loss: 35.10098, Validation Loss: 4.94019\n",
      "Epoch 848/1000, Training Loss: 35.09727, Validation Loss: 4.93952\n",
      "Epoch 849/1000, Training Loss: 35.09356, Validation Loss: 4.93885\n",
      "Epoch 850/1000, Training Loss: 35.08986, Validation Loss: 4.93818\n",
      "Epoch 851/1000, Training Loss: 35.08616, Validation Loss: 4.93751\n",
      "Epoch 852/1000, Training Loss: 35.08246, Validation Loss: 4.93684\n",
      "Epoch 853/1000, Training Loss: 35.07877, Validation Loss: 4.93617\n",
      "Epoch 854/1000, Training Loss: 35.07508, Validation Loss: 4.93551\n",
      "Epoch 855/1000, Training Loss: 35.07139, Validation Loss: 4.93484\n",
      "Epoch 856/1000, Training Loss: 35.06770, Validation Loss: 4.93417\n",
      "Epoch 857/1000, Training Loss: 35.06402, Validation Loss: 4.93351\n",
      "Epoch 858/1000, Training Loss: 35.06035, Validation Loss: 4.93284\n",
      "Epoch 859/1000, Training Loss: 35.05667, Validation Loss: 4.93218\n",
      "Epoch 860/1000, Training Loss: 35.05301, Validation Loss: 4.93151\n",
      "Epoch 861/1000, Training Loss: 35.04934, Validation Loss: 4.93085\n",
      "Epoch 862/1000, Training Loss: 35.04568, Validation Loss: 4.93019\n",
      "Epoch 863/1000, Training Loss: 35.04202, Validation Loss: 4.92953\n",
      "Epoch 864/1000, Training Loss: 35.03837, Validation Loss: 4.92886\n",
      "Epoch 865/1000, Training Loss: 35.03472, Validation Loss: 4.92820\n",
      "Epoch 866/1000, Training Loss: 35.03107, Validation Loss: 4.92754\n",
      "Epoch 867/1000, Training Loss: 35.02743, Validation Loss: 4.92688\n",
      "Epoch 868/1000, Training Loss: 35.02379, Validation Loss: 4.92622\n",
      "Epoch 869/1000, Training Loss: 35.02015, Validation Loss: 4.92557\n",
      "Epoch 870/1000, Training Loss: 35.01652, Validation Loss: 4.92491\n",
      "Epoch 871/1000, Training Loss: 35.01289, Validation Loss: 4.92425\n",
      "Epoch 872/1000, Training Loss: 35.00927, Validation Loss: 4.92360\n",
      "Epoch 873/1000, Training Loss: 35.00564, Validation Loss: 4.92294\n",
      "Epoch 874/1000, Training Loss: 35.00203, Validation Loss: 4.92229\n",
      "Epoch 875/1000, Training Loss: 34.99841, Validation Loss: 4.92163\n",
      "Epoch 876/1000, Training Loss: 34.99480, Validation Loss: 4.92098\n",
      "Epoch 877/1000, Training Loss: 34.99119, Validation Loss: 4.92033\n",
      "Epoch 878/1000, Training Loss: 34.98758, Validation Loss: 4.91967\n",
      "Epoch 879/1000, Training Loss: 34.98398, Validation Loss: 4.91902\n",
      "Epoch 880/1000, Training Loss: 34.98038, Validation Loss: 4.91837\n",
      "Epoch 881/1000, Training Loss: 34.97678, Validation Loss: 4.91772\n",
      "Epoch 882/1000, Training Loss: 34.97318, Validation Loss: 4.91707\n",
      "Epoch 883/1000, Training Loss: 34.96959, Validation Loss: 4.91642\n",
      "Epoch 884/1000, Training Loss: 34.96600, Validation Loss: 4.91577\n",
      "Epoch 885/1000, Training Loss: 34.96241, Validation Loss: 4.91512\n",
      "Epoch 886/1000, Training Loss: 34.95883, Validation Loss: 4.91447\n",
      "Epoch 887/1000, Training Loss: 34.95525, Validation Loss: 4.91382\n",
      "Epoch 888/1000, Training Loss: 34.95167, Validation Loss: 4.91318\n",
      "Epoch 889/1000, Training Loss: 34.94810, Validation Loss: 4.91253\n",
      "Epoch 890/1000, Training Loss: 34.94453, Validation Loss: 4.91189\n",
      "Epoch 891/1000, Training Loss: 34.94096, Validation Loss: 4.91124\n",
      "Epoch 892/1000, Training Loss: 34.93740, Validation Loss: 4.91060\n",
      "Epoch 893/1000, Training Loss: 34.93384, Validation Loss: 4.90995\n",
      "Epoch 894/1000, Training Loss: 34.93028, Validation Loss: 4.90931\n",
      "Epoch 895/1000, Training Loss: 34.92672, Validation Loss: 4.90866\n",
      "Epoch 896/1000, Training Loss: 34.92316, Validation Loss: 4.90802\n",
      "Epoch 897/1000, Training Loss: 34.91960, Validation Loss: 4.90738\n",
      "Epoch 898/1000, Training Loss: 34.91605, Validation Loss: 4.90673\n",
      "Epoch 899/1000, Training Loss: 34.91250, Validation Loss: 4.90609\n",
      "Epoch 900/1000, Training Loss: 34.90896, Validation Loss: 4.90545\n",
      "Epoch 901/1000, Training Loss: 34.90541, Validation Loss: 4.90480\n",
      "Epoch 902/1000, Training Loss: 34.90187, Validation Loss: 4.90416\n",
      "Epoch 903/1000, Training Loss: 34.89833, Validation Loss: 4.90352\n",
      "Epoch 904/1000, Training Loss: 34.89480, Validation Loss: 4.90288\n",
      "Epoch 905/1000, Training Loss: 34.89127, Validation Loss: 4.90224\n",
      "Epoch 906/1000, Training Loss: 34.88774, Validation Loss: 4.90161\n",
      "Epoch 907/1000, Training Loss: 34.88421, Validation Loss: 4.90097\n",
      "Epoch 908/1000, Training Loss: 34.88068, Validation Loss: 4.90033\n",
      "Epoch 909/1000, Training Loss: 34.87716, Validation Loss: 4.89969\n",
      "Epoch 910/1000, Training Loss: 34.87364, Validation Loss: 4.89906\n",
      "Epoch 911/1000, Training Loss: 34.87012, Validation Loss: 4.89842\n",
      "Epoch 912/1000, Training Loss: 34.86661, Validation Loss: 4.89778\n",
      "Epoch 913/1000, Training Loss: 34.86310, Validation Loss: 4.89715\n",
      "Epoch 914/1000, Training Loss: 34.85959, Validation Loss: 4.89652\n",
      "Epoch 915/1000, Training Loss: 34.85609, Validation Loss: 4.89588\n",
      "Epoch 916/1000, Training Loss: 34.85259, Validation Loss: 4.89525\n",
      "Epoch 917/1000, Training Loss: 34.84909, Validation Loss: 4.89461\n",
      "Epoch 918/1000, Training Loss: 34.84559, Validation Loss: 4.89398\n",
      "Epoch 919/1000, Training Loss: 34.84210, Validation Loss: 4.89335\n",
      "Epoch 920/1000, Training Loss: 34.83861, Validation Loss: 4.89272\n",
      "Epoch 921/1000, Training Loss: 34.83512, Validation Loss: 4.89209\n",
      "Epoch 922/1000, Training Loss: 34.83164, Validation Loss: 4.89146\n",
      "Epoch 923/1000, Training Loss: 34.82817, Validation Loss: 4.89083\n",
      "Epoch 924/1000, Training Loss: 34.82469, Validation Loss: 4.89020\n",
      "Epoch 925/1000, Training Loss: 34.82122, Validation Loss: 4.88957\n",
      "Epoch 926/1000, Training Loss: 34.81775, Validation Loss: 4.88894\n",
      "Epoch 927/1000, Training Loss: 34.81428, Validation Loss: 4.88831\n",
      "Epoch 928/1000, Training Loss: 34.81082, Validation Loss: 4.88769\n",
      "Epoch 929/1000, Training Loss: 34.80736, Validation Loss: 4.88706\n",
      "Epoch 930/1000, Training Loss: 34.80390, Validation Loss: 4.88644\n",
      "Epoch 931/1000, Training Loss: 34.80044, Validation Loss: 4.88581\n",
      "Epoch 932/1000, Training Loss: 34.79699, Validation Loss: 4.88518\n",
      "Epoch 933/1000, Training Loss: 34.79354, Validation Loss: 4.88456\n",
      "Epoch 934/1000, Training Loss: 34.79010, Validation Loss: 4.88393\n",
      "Epoch 935/1000, Training Loss: 34.78665, Validation Loss: 4.88331\n",
      "Epoch 936/1000, Training Loss: 34.78321, Validation Loss: 4.88268\n",
      "Epoch 937/1000, Training Loss: 34.77978, Validation Loss: 4.88206\n",
      "Epoch 938/1000, Training Loss: 34.77634, Validation Loss: 4.88144\n",
      "Epoch 939/1000, Training Loss: 34.77291, Validation Loss: 4.88082\n",
      "Epoch 940/1000, Training Loss: 34.76948, Validation Loss: 4.88020\n",
      "Epoch 941/1000, Training Loss: 34.76606, Validation Loss: 4.87958\n",
      "Epoch 942/1000, Training Loss: 34.76263, Validation Loss: 4.87896\n",
      "Epoch 943/1000, Training Loss: 34.75921, Validation Loss: 4.87834\n",
      "Epoch 944/1000, Training Loss: 34.75579, Validation Loss: 4.87772\n",
      "Epoch 945/1000, Training Loss: 34.75237, Validation Loss: 4.87710\n",
      "Epoch 946/1000, Training Loss: 34.74896, Validation Loss: 4.87648\n",
      "Epoch 947/1000, Training Loss: 34.74555, Validation Loss: 4.87587\n",
      "Epoch 948/1000, Training Loss: 34.74214, Validation Loss: 4.87525\n",
      "Epoch 949/1000, Training Loss: 34.73874, Validation Loss: 4.87463\n",
      "Epoch 950/1000, Training Loss: 34.73533, Validation Loss: 4.87402\n",
      "Epoch 951/1000, Training Loss: 34.73193, Validation Loss: 4.87340\n",
      "Epoch 952/1000, Training Loss: 34.72853, Validation Loss: 4.87279\n",
      "Epoch 953/1000, Training Loss: 34.72514, Validation Loss: 4.87217\n",
      "Epoch 954/1000, Training Loss: 34.72174, Validation Loss: 4.87156\n",
      "Epoch 955/1000, Training Loss: 34.71834, Validation Loss: 4.87094\n",
      "Epoch 956/1000, Training Loss: 34.71495, Validation Loss: 4.87033\n",
      "Epoch 957/1000, Training Loss: 34.71156, Validation Loss: 4.86972\n",
      "Epoch 958/1000, Training Loss: 34.70817, Validation Loss: 4.86910\n",
      "Epoch 959/1000, Training Loss: 34.70478, Validation Loss: 4.86849\n",
      "Epoch 960/1000, Training Loss: 34.70140, Validation Loss: 4.86788\n",
      "Epoch 961/1000, Training Loss: 34.69801, Validation Loss: 4.86727\n",
      "Epoch 962/1000, Training Loss: 34.69464, Validation Loss: 4.86666\n",
      "Epoch 963/1000, Training Loss: 34.69126, Validation Loss: 4.86605\n",
      "Epoch 964/1000, Training Loss: 34.68789, Validation Loss: 4.86544\n",
      "Epoch 965/1000, Training Loss: 34.68452, Validation Loss: 4.86483\n",
      "Epoch 966/1000, Training Loss: 34.68116, Validation Loss: 4.86422\n",
      "Epoch 967/1000, Training Loss: 34.67779, Validation Loss: 4.86361\n",
      "Epoch 968/1000, Training Loss: 34.67443, Validation Loss: 4.86301\n",
      "Epoch 969/1000, Training Loss: 34.67108, Validation Loss: 4.86240\n",
      "Epoch 970/1000, Training Loss: 34.66772, Validation Loss: 4.86180\n",
      "Epoch 971/1000, Training Loss: 34.66437, Validation Loss: 4.86119\n",
      "Epoch 972/1000, Training Loss: 34.66101, Validation Loss: 4.86058\n",
      "Epoch 973/1000, Training Loss: 34.65767, Validation Loss: 4.85998\n",
      "Epoch 974/1000, Training Loss: 34.65432, Validation Loss: 4.85937\n",
      "Epoch 975/1000, Training Loss: 34.65098, Validation Loss: 4.85877\n",
      "Epoch 976/1000, Training Loss: 34.64764, Validation Loss: 4.85817\n",
      "Epoch 977/1000, Training Loss: 34.64430, Validation Loss: 4.85756\n",
      "Epoch 978/1000, Training Loss: 34.64096, Validation Loss: 4.85696\n",
      "Epoch 979/1000, Training Loss: 34.63763, Validation Loss: 4.85636\n",
      "Epoch 980/1000, Training Loss: 34.63430, Validation Loss: 4.85576\n",
      "Epoch 981/1000, Training Loss: 34.63098, Validation Loss: 4.85516\n",
      "Epoch 982/1000, Training Loss: 34.62766, Validation Loss: 4.85456\n",
      "Epoch 983/1000, Training Loss: 34.62433, Validation Loss: 4.85396\n",
      "Epoch 984/1000, Training Loss: 34.62102, Validation Loss: 4.85336\n",
      "Epoch 985/1000, Training Loss: 34.61770, Validation Loss: 4.85276\n",
      "Epoch 986/1000, Training Loss: 34.61439, Validation Loss: 4.85216\n",
      "Epoch 987/1000, Training Loss: 34.61108, Validation Loss: 4.85156\n",
      "Epoch 988/1000, Training Loss: 34.60777, Validation Loss: 4.85096\n",
      "Epoch 989/1000, Training Loss: 34.60447, Validation Loss: 4.85037\n",
      "Epoch 990/1000, Training Loss: 34.60117, Validation Loss: 4.84977\n",
      "Epoch 991/1000, Training Loss: 34.59787, Validation Loss: 4.84917\n",
      "Epoch 992/1000, Training Loss: 34.59458, Validation Loss: 4.84858\n",
      "Epoch 993/1000, Training Loss: 34.59129, Validation Loss: 4.84798\n",
      "Epoch 994/1000, Training Loss: 34.58800, Validation Loss: 4.84739\n",
      "Epoch 995/1000, Training Loss: 34.58471, Validation Loss: 4.84679\n",
      "Epoch 996/1000, Training Loss: 34.58143, Validation Loss: 4.84620\n",
      "Epoch 997/1000, Training Loss: 34.57814, Validation Loss: 4.84561\n",
      "Epoch 998/1000, Training Loss: 34.57486, Validation Loss: 4.84501\n",
      "Epoch 999/1000, Training Loss: 34.57158, Validation Loss: 4.84442\n",
      "Epoch 1000/1000, Training Loss: 34.56831, Validation Loss: 4.84383\n",
      "Training took: 99.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_optimizer_adagrad_2 = NeuralNetwork().to(device)\n",
    "summary(model_optimizer_adagrad_2, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adagrad(model_optimizer_adagrad_2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_optimizer_adagrad_2=[]\n",
    "val_loss_list_optimizer_adagrad_2=[]\n",
    "train_accuracy_list_optimizer_adagrad_2=[]\n",
    "val_accuracy_list_optimizer_adagrad_2=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_optimizer_adagrad_2.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_optimizer_adagrad_2(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_optimizer_adagrad_2.append(train_accuracy)\n",
    "    train_loss_list_optimizer_adagrad_2.append(train_loss)\n",
    "\n",
    "    model_optimizer_adagrad_2.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_optimizer_adagrad_2(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_optimizer_adagrad_2.append(val_accuracy)\n",
    "    val_accuracy_list_optimizer_adagrad_2.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTIyB9jdk4mV",
    "outputId": "6c9c8b8f-791b-4de7-a313-6a14beba1052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable optimizer with opmizer as Adagrad: 0.7662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_optimizer_adagrad_2.eval()\n",
    "test_predictions_optimizer_adagrad_2 = model_optimizer_adagrad_2(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_optimizer_adagrad_2 = torch.round(test_predictions_optimizer_adagrad_2)\n",
    "\n",
    "test_predictions_rounded_numpy_optimizer_adagrad_2 = test_predictions_rounded_optimizer_adagrad_2.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_optimizer_adagrad_2 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_optimizer_adagrad_2)\n",
    "\n",
    "print(f\"Accuracy for variable optimizer with opmizer as Adagrad: {accuracy_optimizer_adagrad_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEZw8TcVlKlJ",
    "outputId": "cdecd99b-5474-4e93-ceed-f2396d588724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable opmizer with opmizer as Adagrad: 0.56077\n"
     ]
    }
   ],
   "source": [
    "model_optimizer_adagrad_2.eval()\n",
    "test_loss_optimizer_adagrad_2=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_optimizer_adagrad_2 = model_optimizer_adagrad_2(X_test_tensor)\n",
    "    test_loss_optimizer_adagrad_2 = loss_function(test_outputs_optimizer_adagrad_2, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable opmizer with opmizer as Adagrad: {test_loss_optimizer_adagrad_2.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep9q6qxhlmFO"
   },
   "source": [
    "Optimizer=Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQZQMxrElV9s",
    "outputId": "37a154f1-7c70-41a5-c950-5b5c8eaf9de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 40.37758, Validation Loss: 5.89584\n",
      "Epoch 2/1000, Training Loss: 40.37474, Validation Loss: 5.89532\n",
      "Epoch 3/1000, Training Loss: 40.37196, Validation Loss: 5.89481\n",
      "Epoch 4/1000, Training Loss: 40.36921, Validation Loss: 5.89430\n",
      "Epoch 5/1000, Training Loss: 40.36647, Validation Loss: 5.89380\n",
      "Epoch 6/1000, Training Loss: 40.36373, Validation Loss: 5.89329\n",
      "Epoch 7/1000, Training Loss: 40.36100, Validation Loss: 5.89279\n",
      "Epoch 8/1000, Training Loss: 40.35827, Validation Loss: 5.89228\n",
      "Epoch 9/1000, Training Loss: 40.35554, Validation Loss: 5.89178\n",
      "Epoch 10/1000, Training Loss: 40.35282, Validation Loss: 5.89128\n",
      "Epoch 11/1000, Training Loss: 40.35009, Validation Loss: 5.89078\n",
      "Epoch 12/1000, Training Loss: 40.34736, Validation Loss: 5.89027\n",
      "Epoch 13/1000, Training Loss: 40.34464, Validation Loss: 5.88977\n",
      "Epoch 14/1000, Training Loss: 40.34192, Validation Loss: 5.88927\n",
      "Epoch 15/1000, Training Loss: 40.33920, Validation Loss: 5.88877\n",
      "Epoch 16/1000, Training Loss: 40.33648, Validation Loss: 5.88826\n",
      "Epoch 17/1000, Training Loss: 40.33376, Validation Loss: 5.88776\n",
      "Epoch 18/1000, Training Loss: 40.33104, Validation Loss: 5.88726\n",
      "Epoch 19/1000, Training Loss: 40.32832, Validation Loss: 5.88676\n",
      "Epoch 20/1000, Training Loss: 40.32561, Validation Loss: 5.88626\n",
      "Epoch 21/1000, Training Loss: 40.32289, Validation Loss: 5.88576\n",
      "Epoch 22/1000, Training Loss: 40.32017, Validation Loss: 5.88525\n",
      "Epoch 23/1000, Training Loss: 40.31745, Validation Loss: 5.88475\n",
      "Epoch 24/1000, Training Loss: 40.31474, Validation Loss: 5.88425\n",
      "Epoch 25/1000, Training Loss: 40.31202, Validation Loss: 5.88375\n",
      "Epoch 26/1000, Training Loss: 40.30931, Validation Loss: 5.88325\n",
      "Epoch 27/1000, Training Loss: 40.30660, Validation Loss: 5.88275\n",
      "Epoch 28/1000, Training Loss: 40.30388, Validation Loss: 5.88225\n",
      "Epoch 29/1000, Training Loss: 40.30117, Validation Loss: 5.88175\n",
      "Epoch 30/1000, Training Loss: 40.29846, Validation Loss: 5.88125\n",
      "Epoch 31/1000, Training Loss: 40.29575, Validation Loss: 5.88075\n",
      "Epoch 32/1000, Training Loss: 40.29305, Validation Loss: 5.88025\n",
      "Epoch 33/1000, Training Loss: 40.29034, Validation Loss: 5.87975\n",
      "Epoch 34/1000, Training Loss: 40.28764, Validation Loss: 5.87925\n",
      "Epoch 35/1000, Training Loss: 40.28494, Validation Loss: 5.87875\n",
      "Epoch 36/1000, Training Loss: 40.28223, Validation Loss: 5.87825\n",
      "Epoch 37/1000, Training Loss: 40.27953, Validation Loss: 5.87775\n",
      "Epoch 38/1000, Training Loss: 40.27683, Validation Loss: 5.87725\n",
      "Epoch 39/1000, Training Loss: 40.27413, Validation Loss: 5.87675\n",
      "Epoch 40/1000, Training Loss: 40.27143, Validation Loss: 5.87625\n",
      "Epoch 41/1000, Training Loss: 40.26873, Validation Loss: 5.87575\n",
      "Epoch 42/1000, Training Loss: 40.26604, Validation Loss: 5.87525\n",
      "Epoch 43/1000, Training Loss: 40.26334, Validation Loss: 5.87475\n",
      "Epoch 44/1000, Training Loss: 40.26065, Validation Loss: 5.87425\n",
      "Epoch 45/1000, Training Loss: 40.25796, Validation Loss: 5.87375\n",
      "Epoch 46/1000, Training Loss: 40.25526, Validation Loss: 5.87325\n",
      "Epoch 47/1000, Training Loss: 40.25257, Validation Loss: 5.87275\n",
      "Epoch 48/1000, Training Loss: 40.24988, Validation Loss: 5.87225\n",
      "Epoch 49/1000, Training Loss: 40.24719, Validation Loss: 5.87175\n",
      "Epoch 50/1000, Training Loss: 40.24450, Validation Loss: 5.87125\n",
      "Epoch 51/1000, Training Loss: 40.24180, Validation Loss: 5.87076\n",
      "Epoch 52/1000, Training Loss: 40.23911, Validation Loss: 5.87026\n",
      "Epoch 53/1000, Training Loss: 40.23642, Validation Loss: 5.86976\n",
      "Epoch 54/1000, Training Loss: 40.23373, Validation Loss: 5.86926\n",
      "Epoch 55/1000, Training Loss: 40.23104, Validation Loss: 5.86876\n",
      "Epoch 56/1000, Training Loss: 40.22835, Validation Loss: 5.86826\n",
      "Epoch 57/1000, Training Loss: 40.22567, Validation Loss: 5.86776\n",
      "Epoch 58/1000, Training Loss: 40.22299, Validation Loss: 5.86727\n",
      "Epoch 59/1000, Training Loss: 40.22031, Validation Loss: 5.86677\n",
      "Epoch 60/1000, Training Loss: 40.21763, Validation Loss: 5.86627\n",
      "Epoch 61/1000, Training Loss: 40.21495, Validation Loss: 5.86578\n",
      "Epoch 62/1000, Training Loss: 40.21228, Validation Loss: 5.86528\n",
      "Epoch 63/1000, Training Loss: 40.20960, Validation Loss: 5.86478\n",
      "Epoch 64/1000, Training Loss: 40.20693, Validation Loss: 5.86429\n",
      "Epoch 65/1000, Training Loss: 40.20426, Validation Loss: 5.86379\n",
      "Epoch 66/1000, Training Loss: 40.20159, Validation Loss: 5.86330\n",
      "Epoch 67/1000, Training Loss: 40.19892, Validation Loss: 5.86280\n",
      "Epoch 68/1000, Training Loss: 40.19625, Validation Loss: 5.86231\n",
      "Epoch 69/1000, Training Loss: 40.19358, Validation Loss: 5.86181\n",
      "Epoch 70/1000, Training Loss: 40.19091, Validation Loss: 5.86132\n",
      "Epoch 71/1000, Training Loss: 40.18824, Validation Loss: 5.86082\n",
      "Epoch 72/1000, Training Loss: 40.18558, Validation Loss: 5.86033\n",
      "Epoch 73/1000, Training Loss: 40.18291, Validation Loss: 5.85983\n",
      "Epoch 74/1000, Training Loss: 40.18024, Validation Loss: 5.85934\n",
      "Epoch 75/1000, Training Loss: 40.17758, Validation Loss: 5.85884\n",
      "Epoch 76/1000, Training Loss: 40.17491, Validation Loss: 5.85835\n",
      "Epoch 77/1000, Training Loss: 40.17225, Validation Loss: 5.85785\n",
      "Epoch 78/1000, Training Loss: 40.16959, Validation Loss: 5.85736\n",
      "Epoch 79/1000, Training Loss: 40.16692, Validation Loss: 5.85686\n",
      "Epoch 80/1000, Training Loss: 40.16427, Validation Loss: 5.85637\n",
      "Epoch 81/1000, Training Loss: 40.16161, Validation Loss: 5.85587\n",
      "Epoch 82/1000, Training Loss: 40.15895, Validation Loss: 5.85538\n",
      "Epoch 83/1000, Training Loss: 40.15629, Validation Loss: 5.85488\n",
      "Epoch 84/1000, Training Loss: 40.15364, Validation Loss: 5.85439\n",
      "Epoch 85/1000, Training Loss: 40.15098, Validation Loss: 5.85390\n",
      "Epoch 86/1000, Training Loss: 40.14833, Validation Loss: 5.85340\n",
      "Epoch 87/1000, Training Loss: 40.14568, Validation Loss: 5.85291\n",
      "Epoch 88/1000, Training Loss: 40.14303, Validation Loss: 5.85241\n",
      "Epoch 89/1000, Training Loss: 40.14038, Validation Loss: 5.85192\n",
      "Epoch 90/1000, Training Loss: 40.13773, Validation Loss: 5.85143\n",
      "Epoch 91/1000, Training Loss: 40.13508, Validation Loss: 5.85093\n",
      "Epoch 92/1000, Training Loss: 40.13243, Validation Loss: 5.85044\n",
      "Epoch 93/1000, Training Loss: 40.12979, Validation Loss: 5.84995\n",
      "Epoch 94/1000, Training Loss: 40.12714, Validation Loss: 5.84945\n",
      "Epoch 95/1000, Training Loss: 40.12449, Validation Loss: 5.84896\n",
      "Epoch 96/1000, Training Loss: 40.12185, Validation Loss: 5.84846\n",
      "Epoch 97/1000, Training Loss: 40.11920, Validation Loss: 5.84797\n",
      "Epoch 98/1000, Training Loss: 40.11656, Validation Loss: 5.84748\n",
      "Epoch 99/1000, Training Loss: 40.11392, Validation Loss: 5.84698\n",
      "Epoch 100/1000, Training Loss: 40.11127, Validation Loss: 5.84649\n",
      "Epoch 101/1000, Training Loss: 40.10863, Validation Loss: 5.84600\n",
      "Epoch 102/1000, Training Loss: 40.10599, Validation Loss: 5.84551\n",
      "Epoch 103/1000, Training Loss: 40.10336, Validation Loss: 5.84501\n",
      "Epoch 104/1000, Training Loss: 40.10072, Validation Loss: 5.84452\n",
      "Epoch 105/1000, Training Loss: 40.09809, Validation Loss: 5.84403\n",
      "Epoch 106/1000, Training Loss: 40.09545, Validation Loss: 5.84354\n",
      "Epoch 107/1000, Training Loss: 40.09282, Validation Loss: 5.84305\n",
      "Epoch 108/1000, Training Loss: 40.09019, Validation Loss: 5.84256\n",
      "Epoch 109/1000, Training Loss: 40.08755, Validation Loss: 5.84207\n",
      "Epoch 110/1000, Training Loss: 40.08492, Validation Loss: 5.84158\n",
      "Epoch 111/1000, Training Loss: 40.08230, Validation Loss: 5.84108\n",
      "Epoch 112/1000, Training Loss: 40.07967, Validation Loss: 5.84059\n",
      "Epoch 113/1000, Training Loss: 40.07704, Validation Loss: 5.84010\n",
      "Epoch 114/1000, Training Loss: 40.07441, Validation Loss: 5.83961\n",
      "Epoch 115/1000, Training Loss: 40.07179, Validation Loss: 5.83912\n",
      "Epoch 116/1000, Training Loss: 40.06916, Validation Loss: 5.83863\n",
      "Epoch 117/1000, Training Loss: 40.06654, Validation Loss: 5.83814\n",
      "Epoch 118/1000, Training Loss: 40.06392, Validation Loss: 5.83766\n",
      "Epoch 119/1000, Training Loss: 40.06130, Validation Loss: 5.83717\n",
      "Epoch 120/1000, Training Loss: 40.05868, Validation Loss: 5.83668\n",
      "Epoch 121/1000, Training Loss: 40.05605, Validation Loss: 5.83619\n",
      "Epoch 122/1000, Training Loss: 40.05343, Validation Loss: 5.83570\n",
      "Epoch 123/1000, Training Loss: 40.05081, Validation Loss: 5.83521\n",
      "Epoch 124/1000, Training Loss: 40.04819, Validation Loss: 5.83472\n",
      "Epoch 125/1000, Training Loss: 40.04558, Validation Loss: 5.83423\n",
      "Epoch 126/1000, Training Loss: 40.04296, Validation Loss: 5.83374\n",
      "Epoch 127/1000, Training Loss: 40.04034, Validation Loss: 5.83325\n",
      "Epoch 128/1000, Training Loss: 40.03772, Validation Loss: 5.83277\n",
      "Epoch 129/1000, Training Loss: 40.03511, Validation Loss: 5.83228\n",
      "Epoch 130/1000, Training Loss: 40.03249, Validation Loss: 5.83179\n",
      "Epoch 131/1000, Training Loss: 40.02988, Validation Loss: 5.83130\n",
      "Epoch 132/1000, Training Loss: 40.02727, Validation Loss: 5.83081\n",
      "Epoch 133/1000, Training Loss: 40.02465, Validation Loss: 5.83033\n",
      "Epoch 134/1000, Training Loss: 40.02203, Validation Loss: 5.82984\n",
      "Epoch 135/1000, Training Loss: 40.01942, Validation Loss: 5.82935\n",
      "Epoch 136/1000, Training Loss: 40.01681, Validation Loss: 5.82886\n",
      "Epoch 137/1000, Training Loss: 40.01420, Validation Loss: 5.82837\n",
      "Epoch 138/1000, Training Loss: 40.01159, Validation Loss: 5.82789\n",
      "Epoch 139/1000, Training Loss: 40.00898, Validation Loss: 5.82740\n",
      "Epoch 140/1000, Training Loss: 40.00637, Validation Loss: 5.82691\n",
      "Epoch 141/1000, Training Loss: 40.00377, Validation Loss: 5.82642\n",
      "Epoch 142/1000, Training Loss: 40.00116, Validation Loss: 5.82594\n",
      "Epoch 143/1000, Training Loss: 39.99856, Validation Loss: 5.82545\n",
      "Epoch 144/1000, Training Loss: 39.99596, Validation Loss: 5.82497\n",
      "Epoch 145/1000, Training Loss: 39.99336, Validation Loss: 5.82448\n",
      "Epoch 146/1000, Training Loss: 39.99077, Validation Loss: 5.82399\n",
      "Epoch 147/1000, Training Loss: 39.98817, Validation Loss: 5.82351\n",
      "Epoch 148/1000, Training Loss: 39.98556, Validation Loss: 5.82302\n",
      "Epoch 149/1000, Training Loss: 39.98295, Validation Loss: 5.82254\n",
      "Epoch 150/1000, Training Loss: 39.98035, Validation Loss: 5.82205\n",
      "Epoch 151/1000, Training Loss: 39.97774, Validation Loss: 5.82156\n",
      "Epoch 152/1000, Training Loss: 39.97513, Validation Loss: 5.82108\n",
      "Epoch 153/1000, Training Loss: 39.97253, Validation Loss: 5.82059\n",
      "Epoch 154/1000, Training Loss: 39.96993, Validation Loss: 5.82011\n",
      "Epoch 155/1000, Training Loss: 39.96732, Validation Loss: 5.81962\n",
      "Epoch 156/1000, Training Loss: 39.96472, Validation Loss: 5.81914\n",
      "Epoch 157/1000, Training Loss: 39.96212, Validation Loss: 5.81865\n",
      "Epoch 158/1000, Training Loss: 39.95953, Validation Loss: 5.81817\n",
      "Epoch 159/1000, Training Loss: 39.95693, Validation Loss: 5.81769\n",
      "Epoch 160/1000, Training Loss: 39.95433, Validation Loss: 5.81720\n",
      "Epoch 161/1000, Training Loss: 39.95174, Validation Loss: 5.81672\n",
      "Epoch 162/1000, Training Loss: 39.94914, Validation Loss: 5.81623\n",
      "Epoch 163/1000, Training Loss: 39.94655, Validation Loss: 5.81575\n",
      "Epoch 164/1000, Training Loss: 39.94396, Validation Loss: 5.81527\n",
      "Epoch 165/1000, Training Loss: 39.94137, Validation Loss: 5.81479\n",
      "Epoch 166/1000, Training Loss: 39.93878, Validation Loss: 5.81430\n",
      "Epoch 167/1000, Training Loss: 39.93619, Validation Loss: 5.81382\n",
      "Epoch 168/1000, Training Loss: 39.93360, Validation Loss: 5.81334\n",
      "Epoch 169/1000, Training Loss: 39.93102, Validation Loss: 5.81286\n",
      "Epoch 170/1000, Training Loss: 39.92843, Validation Loss: 5.81238\n",
      "Epoch 171/1000, Training Loss: 39.92585, Validation Loss: 5.81190\n",
      "Epoch 172/1000, Training Loss: 39.92326, Validation Loss: 5.81142\n",
      "Epoch 173/1000, Training Loss: 39.92068, Validation Loss: 5.81093\n",
      "Epoch 174/1000, Training Loss: 39.91810, Validation Loss: 5.81045\n",
      "Epoch 175/1000, Training Loss: 39.91551, Validation Loss: 5.80997\n",
      "Epoch 176/1000, Training Loss: 39.91293, Validation Loss: 5.80949\n",
      "Epoch 177/1000, Training Loss: 39.91035, Validation Loss: 5.80901\n",
      "Epoch 178/1000, Training Loss: 39.90778, Validation Loss: 5.80853\n",
      "Epoch 179/1000, Training Loss: 39.90520, Validation Loss: 5.80805\n",
      "Epoch 180/1000, Training Loss: 39.90262, Validation Loss: 5.80757\n",
      "Epoch 181/1000, Training Loss: 39.90005, Validation Loss: 5.80709\n",
      "Epoch 182/1000, Training Loss: 39.89748, Validation Loss: 5.80661\n",
      "Epoch 183/1000, Training Loss: 39.89490, Validation Loss: 5.80613\n",
      "Epoch 184/1000, Training Loss: 39.89233, Validation Loss: 5.80565\n",
      "Epoch 185/1000, Training Loss: 39.88976, Validation Loss: 5.80517\n",
      "Epoch 186/1000, Training Loss: 39.88719, Validation Loss: 5.80469\n",
      "Epoch 187/1000, Training Loss: 39.88462, Validation Loss: 5.80421\n",
      "Epoch 188/1000, Training Loss: 39.88205, Validation Loss: 5.80373\n",
      "Epoch 189/1000, Training Loss: 39.87948, Validation Loss: 5.80325\n",
      "Epoch 190/1000, Training Loss: 39.87692, Validation Loss: 5.80277\n",
      "Epoch 191/1000, Training Loss: 39.87435, Validation Loss: 5.80229\n",
      "Epoch 192/1000, Training Loss: 39.87179, Validation Loss: 5.80181\n",
      "Epoch 193/1000, Training Loss: 39.86922, Validation Loss: 5.80133\n",
      "Epoch 194/1000, Training Loss: 39.86665, Validation Loss: 5.80085\n",
      "Epoch 195/1000, Training Loss: 39.86409, Validation Loss: 5.80037\n",
      "Epoch 196/1000, Training Loss: 39.86152, Validation Loss: 5.79989\n",
      "Epoch 197/1000, Training Loss: 39.85896, Validation Loss: 5.79942\n",
      "Epoch 198/1000, Training Loss: 39.85640, Validation Loss: 5.79894\n",
      "Epoch 199/1000, Training Loss: 39.85383, Validation Loss: 5.79846\n",
      "Epoch 200/1000, Training Loss: 39.85127, Validation Loss: 5.79798\n",
      "Epoch 201/1000, Training Loss: 39.84871, Validation Loss: 5.79750\n",
      "Epoch 202/1000, Training Loss: 39.84615, Validation Loss: 5.79702\n",
      "Epoch 203/1000, Training Loss: 39.84360, Validation Loss: 5.79655\n",
      "Epoch 204/1000, Training Loss: 39.84104, Validation Loss: 5.79607\n",
      "Epoch 205/1000, Training Loss: 39.83849, Validation Loss: 5.79559\n",
      "Epoch 206/1000, Training Loss: 39.83593, Validation Loss: 5.79511\n",
      "Epoch 207/1000, Training Loss: 39.83339, Validation Loss: 5.79464\n",
      "Epoch 208/1000, Training Loss: 39.83084, Validation Loss: 5.79416\n",
      "Epoch 209/1000, Training Loss: 39.82829, Validation Loss: 5.79369\n",
      "Epoch 210/1000, Training Loss: 39.82575, Validation Loss: 5.79321\n",
      "Epoch 211/1000, Training Loss: 39.82321, Validation Loss: 5.79274\n",
      "Epoch 212/1000, Training Loss: 39.82066, Validation Loss: 5.79226\n",
      "Epoch 213/1000, Training Loss: 39.81812, Validation Loss: 5.79179\n",
      "Epoch 214/1000, Training Loss: 39.81558, Validation Loss: 5.79131\n",
      "Epoch 215/1000, Training Loss: 39.81304, Validation Loss: 5.79083\n",
      "Epoch 216/1000, Training Loss: 39.81050, Validation Loss: 5.79036\n",
      "Epoch 217/1000, Training Loss: 39.80796, Validation Loss: 5.78988\n",
      "Epoch 218/1000, Training Loss: 39.80543, Validation Loss: 5.78941\n",
      "Epoch 219/1000, Training Loss: 39.80289, Validation Loss: 5.78894\n",
      "Epoch 220/1000, Training Loss: 39.80036, Validation Loss: 5.78846\n",
      "Epoch 221/1000, Training Loss: 39.79783, Validation Loss: 5.78799\n",
      "Epoch 222/1000, Training Loss: 39.79529, Validation Loss: 5.78751\n",
      "Epoch 223/1000, Training Loss: 39.79276, Validation Loss: 5.78704\n",
      "Epoch 224/1000, Training Loss: 39.79023, Validation Loss: 5.78656\n",
      "Epoch 225/1000, Training Loss: 39.78770, Validation Loss: 5.78609\n",
      "Epoch 226/1000, Training Loss: 39.78516, Validation Loss: 5.78562\n",
      "Epoch 227/1000, Training Loss: 39.78263, Validation Loss: 5.78514\n",
      "Epoch 228/1000, Training Loss: 39.78010, Validation Loss: 5.78467\n",
      "Epoch 229/1000, Training Loss: 39.77757, Validation Loss: 5.78419\n",
      "Epoch 230/1000, Training Loss: 39.77504, Validation Loss: 5.78372\n",
      "Epoch 231/1000, Training Loss: 39.77252, Validation Loss: 5.78325\n",
      "Epoch 232/1000, Training Loss: 39.76999, Validation Loss: 5.78277\n",
      "Epoch 233/1000, Training Loss: 39.76747, Validation Loss: 5.78230\n",
      "Epoch 234/1000, Training Loss: 39.76494, Validation Loss: 5.78183\n",
      "Epoch 235/1000, Training Loss: 39.76242, Validation Loss: 5.78135\n",
      "Epoch 236/1000, Training Loss: 39.75990, Validation Loss: 5.78088\n",
      "Epoch 237/1000, Training Loss: 39.75738, Validation Loss: 5.78041\n",
      "Epoch 238/1000, Training Loss: 39.75485, Validation Loss: 5.77994\n",
      "Epoch 239/1000, Training Loss: 39.75233, Validation Loss: 5.77946\n",
      "Epoch 240/1000, Training Loss: 39.74981, Validation Loss: 5.77899\n",
      "Epoch 241/1000, Training Loss: 39.74729, Validation Loss: 5.77852\n",
      "Epoch 242/1000, Training Loss: 39.74477, Validation Loss: 5.77805\n",
      "Epoch 243/1000, Training Loss: 39.74225, Validation Loss: 5.77758\n",
      "Epoch 244/1000, Training Loss: 39.73973, Validation Loss: 5.77710\n",
      "Epoch 245/1000, Training Loss: 39.73720, Validation Loss: 5.77663\n",
      "Epoch 246/1000, Training Loss: 39.73468, Validation Loss: 5.77616\n",
      "Epoch 247/1000, Training Loss: 39.73216, Validation Loss: 5.77569\n",
      "Epoch 248/1000, Training Loss: 39.72964, Validation Loss: 5.77522\n",
      "Epoch 249/1000, Training Loss: 39.72712, Validation Loss: 5.77475\n",
      "Epoch 250/1000, Training Loss: 39.72461, Validation Loss: 5.77428\n",
      "Epoch 251/1000, Training Loss: 39.72209, Validation Loss: 5.77380\n",
      "Epoch 252/1000, Training Loss: 39.71957, Validation Loss: 5.77333\n",
      "Epoch 253/1000, Training Loss: 39.71705, Validation Loss: 5.77286\n",
      "Epoch 254/1000, Training Loss: 39.71454, Validation Loss: 5.77239\n",
      "Epoch 255/1000, Training Loss: 39.71203, Validation Loss: 5.77192\n",
      "Epoch 256/1000, Training Loss: 39.70952, Validation Loss: 5.77145\n",
      "Epoch 257/1000, Training Loss: 39.70701, Validation Loss: 5.77098\n",
      "Epoch 258/1000, Training Loss: 39.70450, Validation Loss: 5.77051\n",
      "Epoch 259/1000, Training Loss: 39.70199, Validation Loss: 5.77004\n",
      "Epoch 260/1000, Training Loss: 39.69948, Validation Loss: 5.76957\n",
      "Epoch 261/1000, Training Loss: 39.69697, Validation Loss: 5.76910\n",
      "Epoch 262/1000, Training Loss: 39.69445, Validation Loss: 5.76863\n",
      "Epoch 263/1000, Training Loss: 39.69194, Validation Loss: 5.76816\n",
      "Epoch 264/1000, Training Loss: 39.68944, Validation Loss: 5.76769\n",
      "Epoch 265/1000, Training Loss: 39.68693, Validation Loss: 5.76722\n",
      "Epoch 266/1000, Training Loss: 39.68442, Validation Loss: 5.76674\n",
      "Epoch 267/1000, Training Loss: 39.68191, Validation Loss: 5.76627\n",
      "Epoch 268/1000, Training Loss: 39.67941, Validation Loss: 5.76580\n",
      "Epoch 269/1000, Training Loss: 39.67690, Validation Loss: 5.76533\n",
      "Epoch 270/1000, Training Loss: 39.67440, Validation Loss: 5.76486\n",
      "Epoch 271/1000, Training Loss: 39.67190, Validation Loss: 5.76439\n",
      "Epoch 272/1000, Training Loss: 39.66940, Validation Loss: 5.76392\n",
      "Epoch 273/1000, Training Loss: 39.66690, Validation Loss: 5.76346\n",
      "Epoch 274/1000, Training Loss: 39.66440, Validation Loss: 5.76299\n",
      "Epoch 275/1000, Training Loss: 39.66190, Validation Loss: 5.76252\n",
      "Epoch 276/1000, Training Loss: 39.65940, Validation Loss: 5.76205\n",
      "Epoch 277/1000, Training Loss: 39.65691, Validation Loss: 5.76158\n",
      "Epoch 278/1000, Training Loss: 39.65441, Validation Loss: 5.76111\n",
      "Epoch 279/1000, Training Loss: 39.65191, Validation Loss: 5.76064\n",
      "Epoch 280/1000, Training Loss: 39.64942, Validation Loss: 5.76017\n",
      "Epoch 281/1000, Training Loss: 39.64692, Validation Loss: 5.75970\n",
      "Epoch 282/1000, Training Loss: 39.64443, Validation Loss: 5.75923\n",
      "Epoch 283/1000, Training Loss: 39.64194, Validation Loss: 5.75876\n",
      "Epoch 284/1000, Training Loss: 39.63944, Validation Loss: 5.75830\n",
      "Epoch 285/1000, Training Loss: 39.63695, Validation Loss: 5.75783\n",
      "Epoch 286/1000, Training Loss: 39.63446, Validation Loss: 5.75736\n",
      "Epoch 287/1000, Training Loss: 39.63197, Validation Loss: 5.75689\n",
      "Epoch 288/1000, Training Loss: 39.62949, Validation Loss: 5.75642\n",
      "Epoch 289/1000, Training Loss: 39.62700, Validation Loss: 5.75596\n",
      "Epoch 290/1000, Training Loss: 39.62451, Validation Loss: 5.75549\n",
      "Epoch 291/1000, Training Loss: 39.62203, Validation Loss: 5.75502\n",
      "Epoch 292/1000, Training Loss: 39.61954, Validation Loss: 5.75455\n",
      "Epoch 293/1000, Training Loss: 39.61706, Validation Loss: 5.75409\n",
      "Epoch 294/1000, Training Loss: 39.61458, Validation Loss: 5.75362\n",
      "Epoch 295/1000, Training Loss: 39.61209, Validation Loss: 5.75315\n",
      "Epoch 296/1000, Training Loss: 39.60961, Validation Loss: 5.75269\n",
      "Epoch 297/1000, Training Loss: 39.60713, Validation Loss: 5.75222\n",
      "Epoch 298/1000, Training Loss: 39.60465, Validation Loss: 5.75175\n",
      "Epoch 299/1000, Training Loss: 39.60217, Validation Loss: 5.75129\n",
      "Epoch 300/1000, Training Loss: 39.59969, Validation Loss: 5.75082\n",
      "Epoch 301/1000, Training Loss: 39.59721, Validation Loss: 5.75035\n",
      "Epoch 302/1000, Training Loss: 39.59474, Validation Loss: 5.74989\n",
      "Epoch 303/1000, Training Loss: 39.59226, Validation Loss: 5.74942\n",
      "Epoch 304/1000, Training Loss: 39.58979, Validation Loss: 5.74896\n",
      "Epoch 305/1000, Training Loss: 39.58731, Validation Loss: 5.74849\n",
      "Epoch 306/1000, Training Loss: 39.58484, Validation Loss: 5.74803\n",
      "Epoch 307/1000, Training Loss: 39.58237, Validation Loss: 5.74756\n",
      "Epoch 308/1000, Training Loss: 39.57990, Validation Loss: 5.74710\n",
      "Epoch 309/1000, Training Loss: 39.57743, Validation Loss: 5.74663\n",
      "Epoch 310/1000, Training Loss: 39.57496, Validation Loss: 5.74617\n",
      "Epoch 311/1000, Training Loss: 39.57249, Validation Loss: 5.74570\n",
      "Epoch 312/1000, Training Loss: 39.57003, Validation Loss: 5.74524\n",
      "Epoch 313/1000, Training Loss: 39.56756, Validation Loss: 5.74478\n",
      "Epoch 314/1000, Training Loss: 39.56510, Validation Loss: 5.74431\n",
      "Epoch 315/1000, Training Loss: 39.56264, Validation Loss: 5.74385\n",
      "Epoch 316/1000, Training Loss: 39.56018, Validation Loss: 5.74339\n",
      "Epoch 317/1000, Training Loss: 39.55771, Validation Loss: 5.74292\n",
      "Epoch 318/1000, Training Loss: 39.55525, Validation Loss: 5.74246\n",
      "Epoch 319/1000, Training Loss: 39.55279, Validation Loss: 5.74200\n",
      "Epoch 320/1000, Training Loss: 39.55033, Validation Loss: 5.74153\n",
      "Epoch 321/1000, Training Loss: 39.54786, Validation Loss: 5.74107\n",
      "Epoch 322/1000, Training Loss: 39.54540, Validation Loss: 5.74061\n",
      "Epoch 323/1000, Training Loss: 39.54294, Validation Loss: 5.74014\n",
      "Epoch 324/1000, Training Loss: 39.54048, Validation Loss: 5.73968\n",
      "Epoch 325/1000, Training Loss: 39.53803, Validation Loss: 5.73922\n",
      "Epoch 326/1000, Training Loss: 39.53557, Validation Loss: 5.73876\n",
      "Epoch 327/1000, Training Loss: 39.53310, Validation Loss: 5.73830\n",
      "Epoch 328/1000, Training Loss: 39.53065, Validation Loss: 5.73783\n",
      "Epoch 329/1000, Training Loss: 39.52819, Validation Loss: 5.73737\n",
      "Epoch 330/1000, Training Loss: 39.52573, Validation Loss: 5.73691\n",
      "Epoch 331/1000, Training Loss: 39.52328, Validation Loss: 5.73645\n",
      "Epoch 332/1000, Training Loss: 39.52082, Validation Loss: 5.73599\n",
      "Epoch 333/1000, Training Loss: 39.51837, Validation Loss: 5.73553\n",
      "Epoch 334/1000, Training Loss: 39.51592, Validation Loss: 5.73507\n",
      "Epoch 335/1000, Training Loss: 39.51347, Validation Loss: 5.73461\n",
      "Epoch 336/1000, Training Loss: 39.51102, Validation Loss: 5.73414\n",
      "Epoch 337/1000, Training Loss: 39.50857, Validation Loss: 5.73368\n",
      "Epoch 338/1000, Training Loss: 39.50612, Validation Loss: 5.73322\n",
      "Epoch 339/1000, Training Loss: 39.50367, Validation Loss: 5.73276\n",
      "Epoch 340/1000, Training Loss: 39.50122, Validation Loss: 5.73230\n",
      "Epoch 341/1000, Training Loss: 39.49877, Validation Loss: 5.73184\n",
      "Epoch 342/1000, Training Loss: 39.49633, Validation Loss: 5.73138\n",
      "Epoch 343/1000, Training Loss: 39.49388, Validation Loss: 5.73092\n",
      "Epoch 344/1000, Training Loss: 39.49144, Validation Loss: 5.73046\n",
      "Epoch 345/1000, Training Loss: 39.48899, Validation Loss: 5.73000\n",
      "Epoch 346/1000, Training Loss: 39.48655, Validation Loss: 5.72954\n",
      "Epoch 347/1000, Training Loss: 39.48410, Validation Loss: 5.72908\n",
      "Epoch 348/1000, Training Loss: 39.48166, Validation Loss: 5.72862\n",
      "Epoch 349/1000, Training Loss: 39.47922, Validation Loss: 5.72816\n",
      "Epoch 350/1000, Training Loss: 39.47678, Validation Loss: 5.72770\n",
      "Epoch 351/1000, Training Loss: 39.47434, Validation Loss: 5.72724\n",
      "Epoch 352/1000, Training Loss: 39.47190, Validation Loss: 5.72678\n",
      "Epoch 353/1000, Training Loss: 39.46947, Validation Loss: 5.72632\n",
      "Epoch 354/1000, Training Loss: 39.46703, Validation Loss: 5.72586\n",
      "Epoch 355/1000, Training Loss: 39.46460, Validation Loss: 5.72540\n",
      "Epoch 356/1000, Training Loss: 39.46216, Validation Loss: 5.72494\n",
      "Epoch 357/1000, Training Loss: 39.45973, Validation Loss: 5.72448\n",
      "Epoch 358/1000, Training Loss: 39.45730, Validation Loss: 5.72402\n",
      "Epoch 359/1000, Training Loss: 39.45487, Validation Loss: 5.72357\n",
      "Epoch 360/1000, Training Loss: 39.45244, Validation Loss: 5.72311\n",
      "Epoch 361/1000, Training Loss: 39.45002, Validation Loss: 5.72265\n",
      "Epoch 362/1000, Training Loss: 39.44759, Validation Loss: 5.72219\n",
      "Epoch 363/1000, Training Loss: 39.44517, Validation Loss: 5.72174\n",
      "Epoch 364/1000, Training Loss: 39.44275, Validation Loss: 5.72128\n",
      "Epoch 365/1000, Training Loss: 39.44033, Validation Loss: 5.72082\n",
      "Epoch 366/1000, Training Loss: 39.43791, Validation Loss: 5.72037\n",
      "Epoch 367/1000, Training Loss: 39.43549, Validation Loss: 5.71991\n",
      "Epoch 368/1000, Training Loss: 39.43307, Validation Loss: 5.71945\n",
      "Epoch 369/1000, Training Loss: 39.43065, Validation Loss: 5.71900\n",
      "Epoch 370/1000, Training Loss: 39.42823, Validation Loss: 5.71854\n",
      "Epoch 371/1000, Training Loss: 39.42582, Validation Loss: 5.71809\n",
      "Epoch 372/1000, Training Loss: 39.42341, Validation Loss: 5.71763\n",
      "Epoch 373/1000, Training Loss: 39.42100, Validation Loss: 5.71718\n",
      "Epoch 374/1000, Training Loss: 39.41859, Validation Loss: 5.71672\n",
      "Epoch 375/1000, Training Loss: 39.41619, Validation Loss: 5.71627\n",
      "Epoch 376/1000, Training Loss: 39.41378, Validation Loss: 5.71581\n",
      "Epoch 377/1000, Training Loss: 39.41137, Validation Loss: 5.71536\n",
      "Epoch 378/1000, Training Loss: 39.40897, Validation Loss: 5.71490\n",
      "Epoch 379/1000, Training Loss: 39.40656, Validation Loss: 5.71445\n",
      "Epoch 380/1000, Training Loss: 39.40416, Validation Loss: 5.71400\n",
      "Epoch 381/1000, Training Loss: 39.40176, Validation Loss: 5.71354\n",
      "Epoch 382/1000, Training Loss: 39.39937, Validation Loss: 5.71309\n",
      "Epoch 383/1000, Training Loss: 39.39697, Validation Loss: 5.71264\n",
      "Epoch 384/1000, Training Loss: 39.39458, Validation Loss: 5.71218\n",
      "Epoch 385/1000, Training Loss: 39.39218, Validation Loss: 5.71173\n",
      "Epoch 386/1000, Training Loss: 39.38979, Validation Loss: 5.71128\n",
      "Epoch 387/1000, Training Loss: 39.38740, Validation Loss: 5.71083\n",
      "Epoch 388/1000, Training Loss: 39.38501, Validation Loss: 5.71038\n",
      "Epoch 389/1000, Training Loss: 39.38262, Validation Loss: 5.70993\n",
      "Epoch 390/1000, Training Loss: 39.38023, Validation Loss: 5.70948\n",
      "Epoch 391/1000, Training Loss: 39.37785, Validation Loss: 5.70902\n",
      "Epoch 392/1000, Training Loss: 39.37546, Validation Loss: 5.70857\n",
      "Epoch 393/1000, Training Loss: 39.37307, Validation Loss: 5.70812\n",
      "Epoch 394/1000, Training Loss: 39.37068, Validation Loss: 5.70767\n",
      "Epoch 395/1000, Training Loss: 39.36829, Validation Loss: 5.70722\n",
      "Epoch 396/1000, Training Loss: 39.36591, Validation Loss: 5.70677\n",
      "Epoch 397/1000, Training Loss: 39.36352, Validation Loss: 5.70632\n",
      "Epoch 398/1000, Training Loss: 39.36114, Validation Loss: 5.70587\n",
      "Epoch 399/1000, Training Loss: 39.35875, Validation Loss: 5.70542\n",
      "Epoch 400/1000, Training Loss: 39.35637, Validation Loss: 5.70497\n",
      "Epoch 401/1000, Training Loss: 39.35399, Validation Loss: 5.70452\n",
      "Epoch 402/1000, Training Loss: 39.35161, Validation Loss: 5.70407\n",
      "Epoch 403/1000, Training Loss: 39.34923, Validation Loss: 5.70362\n",
      "Epoch 404/1000, Training Loss: 39.34685, Validation Loss: 5.70317\n",
      "Epoch 405/1000, Training Loss: 39.34447, Validation Loss: 5.70271\n",
      "Epoch 406/1000, Training Loss: 39.34209, Validation Loss: 5.70226\n",
      "Epoch 407/1000, Training Loss: 39.33972, Validation Loss: 5.70181\n",
      "Epoch 408/1000, Training Loss: 39.33734, Validation Loss: 5.70136\n",
      "Epoch 409/1000, Training Loss: 39.33497, Validation Loss: 5.70091\n",
      "Epoch 410/1000, Training Loss: 39.33259, Validation Loss: 5.70046\n",
      "Epoch 411/1000, Training Loss: 39.33022, Validation Loss: 5.70001\n",
      "Epoch 412/1000, Training Loss: 39.32785, Validation Loss: 5.69956\n",
      "Epoch 413/1000, Training Loss: 39.32548, Validation Loss: 5.69911\n",
      "Epoch 414/1000, Training Loss: 39.32311, Validation Loss: 5.69866\n",
      "Epoch 415/1000, Training Loss: 39.32073, Validation Loss: 5.69821\n",
      "Epoch 416/1000, Training Loss: 39.31836, Validation Loss: 5.69776\n",
      "Epoch 417/1000, Training Loss: 39.31600, Validation Loss: 5.69731\n",
      "Epoch 418/1000, Training Loss: 39.31363, Validation Loss: 5.69686\n",
      "Epoch 419/1000, Training Loss: 39.31126, Validation Loss: 5.69641\n",
      "Epoch 420/1000, Training Loss: 39.30889, Validation Loss: 5.69596\n",
      "Epoch 421/1000, Training Loss: 39.30653, Validation Loss: 5.69551\n",
      "Epoch 422/1000, Training Loss: 39.30416, Validation Loss: 5.69506\n",
      "Epoch 423/1000, Training Loss: 39.30180, Validation Loss: 5.69461\n",
      "Epoch 424/1000, Training Loss: 39.29944, Validation Loss: 5.69416\n",
      "Epoch 425/1000, Training Loss: 39.29708, Validation Loss: 5.69371\n",
      "Epoch 426/1000, Training Loss: 39.29472, Validation Loss: 5.69326\n",
      "Epoch 427/1000, Training Loss: 39.29236, Validation Loss: 5.69282\n",
      "Epoch 428/1000, Training Loss: 39.29000, Validation Loss: 5.69237\n",
      "Epoch 429/1000, Training Loss: 39.28764, Validation Loss: 5.69192\n",
      "Epoch 430/1000, Training Loss: 39.28528, Validation Loss: 5.69147\n",
      "Epoch 431/1000, Training Loss: 39.28292, Validation Loss: 5.69102\n",
      "Epoch 432/1000, Training Loss: 39.28057, Validation Loss: 5.69057\n",
      "Epoch 433/1000, Training Loss: 39.27821, Validation Loss: 5.69013\n",
      "Epoch 434/1000, Training Loss: 39.27586, Validation Loss: 5.68968\n",
      "Epoch 435/1000, Training Loss: 39.27350, Validation Loss: 5.68923\n",
      "Epoch 436/1000, Training Loss: 39.27115, Validation Loss: 5.68878\n",
      "Epoch 437/1000, Training Loss: 39.26879, Validation Loss: 5.68833\n",
      "Epoch 438/1000, Training Loss: 39.26643, Validation Loss: 5.68788\n",
      "Epoch 439/1000, Training Loss: 39.26408, Validation Loss: 5.68743\n",
      "Epoch 440/1000, Training Loss: 39.26172, Validation Loss: 5.68698\n",
      "Epoch 441/1000, Training Loss: 39.25937, Validation Loss: 5.68653\n",
      "Epoch 442/1000, Training Loss: 39.25701, Validation Loss: 5.68608\n",
      "Epoch 443/1000, Training Loss: 39.25466, Validation Loss: 5.68563\n",
      "Epoch 444/1000, Training Loss: 39.25231, Validation Loss: 5.68518\n",
      "Epoch 445/1000, Training Loss: 39.24995, Validation Loss: 5.68473\n",
      "Epoch 446/1000, Training Loss: 39.24760, Validation Loss: 5.68429\n",
      "Epoch 447/1000, Training Loss: 39.24525, Validation Loss: 5.68384\n",
      "Epoch 448/1000, Training Loss: 39.24289, Validation Loss: 5.68339\n",
      "Epoch 449/1000, Training Loss: 39.24054, Validation Loss: 5.68294\n",
      "Epoch 450/1000, Training Loss: 39.23818, Validation Loss: 5.68249\n",
      "Epoch 451/1000, Training Loss: 39.23583, Validation Loss: 5.68204\n",
      "Epoch 452/1000, Training Loss: 39.23347, Validation Loss: 5.68160\n",
      "Epoch 453/1000, Training Loss: 39.23112, Validation Loss: 5.68115\n",
      "Epoch 454/1000, Training Loss: 39.22877, Validation Loss: 5.68070\n",
      "Epoch 455/1000, Training Loss: 39.22641, Validation Loss: 5.68025\n",
      "Epoch 456/1000, Training Loss: 39.22406, Validation Loss: 5.67980\n",
      "Epoch 457/1000, Training Loss: 39.22171, Validation Loss: 5.67935\n",
      "Epoch 458/1000, Training Loss: 39.21936, Validation Loss: 5.67891\n",
      "Epoch 459/1000, Training Loss: 39.21701, Validation Loss: 5.67846\n",
      "Epoch 460/1000, Training Loss: 39.21466, Validation Loss: 5.67801\n",
      "Epoch 461/1000, Training Loss: 39.21231, Validation Loss: 5.67756\n",
      "Epoch 462/1000, Training Loss: 39.20996, Validation Loss: 5.67711\n",
      "Epoch 463/1000, Training Loss: 39.20761, Validation Loss: 5.67667\n",
      "Epoch 464/1000, Training Loss: 39.20527, Validation Loss: 5.67622\n",
      "Epoch 465/1000, Training Loss: 39.20292, Validation Loss: 5.67577\n",
      "Epoch 466/1000, Training Loss: 39.20058, Validation Loss: 5.67533\n",
      "Epoch 467/1000, Training Loss: 39.19824, Validation Loss: 5.67488\n",
      "Epoch 468/1000, Training Loss: 39.19589, Validation Loss: 5.67443\n",
      "Epoch 469/1000, Training Loss: 39.19355, Validation Loss: 5.67399\n",
      "Epoch 470/1000, Training Loss: 39.19121, Validation Loss: 5.67354\n",
      "Epoch 471/1000, Training Loss: 39.18887, Validation Loss: 5.67309\n",
      "Epoch 472/1000, Training Loss: 39.18653, Validation Loss: 5.67265\n",
      "Epoch 473/1000, Training Loss: 39.18419, Validation Loss: 5.67220\n",
      "Epoch 474/1000, Training Loss: 39.18185, Validation Loss: 5.67175\n",
      "Epoch 475/1000, Training Loss: 39.17952, Validation Loss: 5.67131\n",
      "Epoch 476/1000, Training Loss: 39.17718, Validation Loss: 5.67086\n",
      "Epoch 477/1000, Training Loss: 39.17484, Validation Loss: 5.67041\n",
      "Epoch 478/1000, Training Loss: 39.17251, Validation Loss: 5.66997\n",
      "Epoch 479/1000, Training Loss: 39.17017, Validation Loss: 5.66952\n",
      "Epoch 480/1000, Training Loss: 39.16784, Validation Loss: 5.66908\n",
      "Epoch 481/1000, Training Loss: 39.16551, Validation Loss: 5.66863\n",
      "Epoch 482/1000, Training Loss: 39.16318, Validation Loss: 5.66819\n",
      "Epoch 483/1000, Training Loss: 39.16085, Validation Loss: 5.66774\n",
      "Epoch 484/1000, Training Loss: 39.15852, Validation Loss: 5.66730\n",
      "Epoch 485/1000, Training Loss: 39.15620, Validation Loss: 5.66685\n",
      "Epoch 486/1000, Training Loss: 39.15387, Validation Loss: 5.66641\n",
      "Epoch 487/1000, Training Loss: 39.15154, Validation Loss: 5.66596\n",
      "Epoch 488/1000, Training Loss: 39.14921, Validation Loss: 5.66552\n",
      "Epoch 489/1000, Training Loss: 39.14689, Validation Loss: 5.66507\n",
      "Epoch 490/1000, Training Loss: 39.14456, Validation Loss: 5.66463\n",
      "Epoch 491/1000, Training Loss: 39.14224, Validation Loss: 5.66419\n",
      "Epoch 492/1000, Training Loss: 39.13992, Validation Loss: 5.66374\n",
      "Epoch 493/1000, Training Loss: 39.13760, Validation Loss: 5.66330\n",
      "Epoch 494/1000, Training Loss: 39.13528, Validation Loss: 5.66286\n",
      "Epoch 495/1000, Training Loss: 39.13296, Validation Loss: 5.66242\n",
      "Epoch 496/1000, Training Loss: 39.13064, Validation Loss: 5.66197\n",
      "Epoch 497/1000, Training Loss: 39.12832, Validation Loss: 5.66153\n",
      "Epoch 498/1000, Training Loss: 39.12600, Validation Loss: 5.66109\n",
      "Epoch 499/1000, Training Loss: 39.12368, Validation Loss: 5.66065\n",
      "Epoch 500/1000, Training Loss: 39.12136, Validation Loss: 5.66021\n",
      "Epoch 501/1000, Training Loss: 39.11905, Validation Loss: 5.65976\n",
      "Epoch 502/1000, Training Loss: 39.11673, Validation Loss: 5.65932\n",
      "Epoch 503/1000, Training Loss: 39.11441, Validation Loss: 5.65888\n",
      "Epoch 504/1000, Training Loss: 39.11210, Validation Loss: 5.65844\n",
      "Epoch 505/1000, Training Loss: 39.10978, Validation Loss: 5.65800\n",
      "Epoch 506/1000, Training Loss: 39.10747, Validation Loss: 5.65756\n",
      "Epoch 507/1000, Training Loss: 39.10516, Validation Loss: 5.65711\n",
      "Epoch 508/1000, Training Loss: 39.10284, Validation Loss: 5.65667\n",
      "Epoch 509/1000, Training Loss: 39.10053, Validation Loss: 5.65623\n",
      "Epoch 510/1000, Training Loss: 39.09822, Validation Loss: 5.65579\n",
      "Epoch 511/1000, Training Loss: 39.09591, Validation Loss: 5.65535\n",
      "Epoch 512/1000, Training Loss: 39.09360, Validation Loss: 5.65491\n",
      "Epoch 513/1000, Training Loss: 39.09129, Validation Loss: 5.65447\n",
      "Epoch 514/1000, Training Loss: 39.08898, Validation Loss: 5.65403\n",
      "Epoch 515/1000, Training Loss: 39.08668, Validation Loss: 5.65359\n",
      "Epoch 516/1000, Training Loss: 39.08437, Validation Loss: 5.65315\n",
      "Epoch 517/1000, Training Loss: 39.08207, Validation Loss: 5.65271\n",
      "Epoch 518/1000, Training Loss: 39.07976, Validation Loss: 5.65227\n",
      "Epoch 519/1000, Training Loss: 39.07746, Validation Loss: 5.65183\n",
      "Epoch 520/1000, Training Loss: 39.07515, Validation Loss: 5.65139\n",
      "Epoch 521/1000, Training Loss: 39.07285, Validation Loss: 5.65095\n",
      "Epoch 522/1000, Training Loss: 39.07054, Validation Loss: 5.65051\n",
      "Epoch 523/1000, Training Loss: 39.06824, Validation Loss: 5.65007\n",
      "Epoch 524/1000, Training Loss: 39.06594, Validation Loss: 5.64963\n",
      "Epoch 525/1000, Training Loss: 39.06364, Validation Loss: 5.64919\n",
      "Epoch 526/1000, Training Loss: 39.06134, Validation Loss: 5.64875\n",
      "Epoch 527/1000, Training Loss: 39.05904, Validation Loss: 5.64831\n",
      "Epoch 528/1000, Training Loss: 39.05674, Validation Loss: 5.64787\n",
      "Epoch 529/1000, Training Loss: 39.05444, Validation Loss: 5.64743\n",
      "Epoch 530/1000, Training Loss: 39.05214, Validation Loss: 5.64699\n",
      "Epoch 531/1000, Training Loss: 39.04985, Validation Loss: 5.64655\n",
      "Epoch 532/1000, Training Loss: 39.04755, Validation Loss: 5.64612\n",
      "Epoch 533/1000, Training Loss: 39.04525, Validation Loss: 5.64568\n",
      "Epoch 534/1000, Training Loss: 39.04296, Validation Loss: 5.64524\n",
      "Epoch 535/1000, Training Loss: 39.04067, Validation Loss: 5.64480\n",
      "Epoch 536/1000, Training Loss: 39.03837, Validation Loss: 5.64436\n",
      "Epoch 537/1000, Training Loss: 39.03608, Validation Loss: 5.64392\n",
      "Epoch 538/1000, Training Loss: 39.03378, Validation Loss: 5.64348\n",
      "Epoch 539/1000, Training Loss: 39.03149, Validation Loss: 5.64304\n",
      "Epoch 540/1000, Training Loss: 39.02920, Validation Loss: 5.64261\n",
      "Epoch 541/1000, Training Loss: 39.02690, Validation Loss: 5.64217\n",
      "Epoch 542/1000, Training Loss: 39.02461, Validation Loss: 5.64173\n",
      "Epoch 543/1000, Training Loss: 39.02231, Validation Loss: 5.64129\n",
      "Epoch 544/1000, Training Loss: 39.02002, Validation Loss: 5.64085\n",
      "Epoch 545/1000, Training Loss: 39.01773, Validation Loss: 5.64042\n",
      "Epoch 546/1000, Training Loss: 39.01543, Validation Loss: 5.63998\n",
      "Epoch 547/1000, Training Loss: 39.01314, Validation Loss: 5.63954\n",
      "Epoch 548/1000, Training Loss: 39.01085, Validation Loss: 5.63910\n",
      "Epoch 549/1000, Training Loss: 39.00856, Validation Loss: 5.63867\n",
      "Epoch 550/1000, Training Loss: 39.00627, Validation Loss: 5.63823\n",
      "Epoch 551/1000, Training Loss: 39.00398, Validation Loss: 5.63779\n",
      "Epoch 552/1000, Training Loss: 39.00169, Validation Loss: 5.63736\n",
      "Epoch 553/1000, Training Loss: 38.99941, Validation Loss: 5.63692\n",
      "Epoch 554/1000, Training Loss: 38.99712, Validation Loss: 5.63649\n",
      "Epoch 555/1000, Training Loss: 38.99484, Validation Loss: 5.63605\n",
      "Epoch 556/1000, Training Loss: 38.99255, Validation Loss: 5.63561\n",
      "Epoch 557/1000, Training Loss: 38.99027, Validation Loss: 5.63518\n",
      "Epoch 558/1000, Training Loss: 38.98799, Validation Loss: 5.63474\n",
      "Epoch 559/1000, Training Loss: 38.98570, Validation Loss: 5.63431\n",
      "Epoch 560/1000, Training Loss: 38.98342, Validation Loss: 5.63387\n",
      "Epoch 561/1000, Training Loss: 38.98114, Validation Loss: 5.63344\n",
      "Epoch 562/1000, Training Loss: 38.97886, Validation Loss: 5.63300\n",
      "Epoch 563/1000, Training Loss: 38.97658, Validation Loss: 5.63257\n",
      "Epoch 564/1000, Training Loss: 38.97429, Validation Loss: 5.63213\n",
      "Epoch 565/1000, Training Loss: 38.97202, Validation Loss: 5.63170\n",
      "Epoch 566/1000, Training Loss: 38.96974, Validation Loss: 5.63127\n",
      "Epoch 567/1000, Training Loss: 38.96746, Validation Loss: 5.63083\n",
      "Epoch 568/1000, Training Loss: 38.96519, Validation Loss: 5.63040\n",
      "Epoch 569/1000, Training Loss: 38.96292, Validation Loss: 5.62997\n",
      "Epoch 570/1000, Training Loss: 38.96064, Validation Loss: 5.62953\n",
      "Epoch 571/1000, Training Loss: 38.95837, Validation Loss: 5.62910\n",
      "Epoch 572/1000, Training Loss: 38.95610, Validation Loss: 5.62867\n",
      "Epoch 573/1000, Training Loss: 38.95383, Validation Loss: 5.62823\n",
      "Epoch 574/1000, Training Loss: 38.95156, Validation Loss: 5.62780\n",
      "Epoch 575/1000, Training Loss: 38.94929, Validation Loss: 5.62737\n",
      "Epoch 576/1000, Training Loss: 38.94702, Validation Loss: 5.62693\n",
      "Epoch 577/1000, Training Loss: 38.94476, Validation Loss: 5.62650\n",
      "Epoch 578/1000, Training Loss: 38.94249, Validation Loss: 5.62607\n",
      "Epoch 579/1000, Training Loss: 38.94022, Validation Loss: 5.62564\n",
      "Epoch 580/1000, Training Loss: 38.93795, Validation Loss: 5.62520\n",
      "Epoch 581/1000, Training Loss: 38.93568, Validation Loss: 5.62477\n",
      "Epoch 582/1000, Training Loss: 38.93341, Validation Loss: 5.62434\n",
      "Epoch 583/1000, Training Loss: 38.93114, Validation Loss: 5.62391\n",
      "Epoch 584/1000, Training Loss: 38.92888, Validation Loss: 5.62347\n",
      "Epoch 585/1000, Training Loss: 38.92661, Validation Loss: 5.62304\n",
      "Epoch 586/1000, Training Loss: 38.92434, Validation Loss: 5.62261\n",
      "Epoch 587/1000, Training Loss: 38.92208, Validation Loss: 5.62218\n",
      "Epoch 588/1000, Training Loss: 38.91982, Validation Loss: 5.62174\n",
      "Epoch 589/1000, Training Loss: 38.91756, Validation Loss: 5.62131\n",
      "Epoch 590/1000, Training Loss: 38.91530, Validation Loss: 5.62088\n",
      "Epoch 591/1000, Training Loss: 38.91304, Validation Loss: 5.62045\n",
      "Epoch 592/1000, Training Loss: 38.91078, Validation Loss: 5.62002\n",
      "Epoch 593/1000, Training Loss: 38.90853, Validation Loss: 5.61959\n",
      "Epoch 594/1000, Training Loss: 38.90627, Validation Loss: 5.61916\n",
      "Epoch 595/1000, Training Loss: 38.90402, Validation Loss: 5.61872\n",
      "Epoch 596/1000, Training Loss: 38.90176, Validation Loss: 5.61829\n",
      "Epoch 597/1000, Training Loss: 38.89951, Validation Loss: 5.61786\n",
      "Epoch 598/1000, Training Loss: 38.89726, Validation Loss: 5.61743\n",
      "Epoch 599/1000, Training Loss: 38.89501, Validation Loss: 5.61700\n",
      "Epoch 600/1000, Training Loss: 38.89276, Validation Loss: 5.61657\n",
      "Epoch 601/1000, Training Loss: 38.89051, Validation Loss: 5.61614\n",
      "Epoch 602/1000, Training Loss: 38.88826, Validation Loss: 5.61571\n",
      "Epoch 603/1000, Training Loss: 38.88601, Validation Loss: 5.61528\n",
      "Epoch 604/1000, Training Loss: 38.88376, Validation Loss: 5.61485\n",
      "Epoch 605/1000, Training Loss: 38.88152, Validation Loss: 5.61442\n",
      "Epoch 606/1000, Training Loss: 38.87927, Validation Loss: 5.61399\n",
      "Epoch 607/1000, Training Loss: 38.87703, Validation Loss: 5.61356\n",
      "Epoch 608/1000, Training Loss: 38.87479, Validation Loss: 5.61313\n",
      "Epoch 609/1000, Training Loss: 38.87254, Validation Loss: 5.61270\n",
      "Epoch 610/1000, Training Loss: 38.87030, Validation Loss: 5.61227\n",
      "Epoch 611/1000, Training Loss: 38.86806, Validation Loss: 5.61184\n",
      "Epoch 612/1000, Training Loss: 38.86582, Validation Loss: 5.61141\n",
      "Epoch 613/1000, Training Loss: 38.86358, Validation Loss: 5.61098\n",
      "Epoch 614/1000, Training Loss: 38.86133, Validation Loss: 5.61055\n",
      "Epoch 615/1000, Training Loss: 38.85909, Validation Loss: 5.61012\n",
      "Epoch 616/1000, Training Loss: 38.85686, Validation Loss: 5.60969\n",
      "Epoch 617/1000, Training Loss: 38.85462, Validation Loss: 5.60926\n",
      "Epoch 618/1000, Training Loss: 38.85239, Validation Loss: 5.60883\n",
      "Epoch 619/1000, Training Loss: 38.85015, Validation Loss: 5.60840\n",
      "Epoch 620/1000, Training Loss: 38.84792, Validation Loss: 5.60797\n",
      "Epoch 621/1000, Training Loss: 38.84569, Validation Loss: 5.60754\n",
      "Epoch 622/1000, Training Loss: 38.84346, Validation Loss: 5.60712\n",
      "Epoch 623/1000, Training Loss: 38.84123, Validation Loss: 5.60669\n",
      "Epoch 624/1000, Training Loss: 38.83900, Validation Loss: 5.60626\n",
      "Epoch 625/1000, Training Loss: 38.83677, Validation Loss: 5.60583\n",
      "Epoch 626/1000, Training Loss: 38.83454, Validation Loss: 5.60540\n",
      "Epoch 627/1000, Training Loss: 38.83231, Validation Loss: 5.60497\n",
      "Epoch 628/1000, Training Loss: 38.83008, Validation Loss: 5.60454\n",
      "Epoch 629/1000, Training Loss: 38.82785, Validation Loss: 5.60412\n",
      "Epoch 630/1000, Training Loss: 38.82562, Validation Loss: 5.60369\n",
      "Epoch 631/1000, Training Loss: 38.82339, Validation Loss: 5.60326\n",
      "Epoch 632/1000, Training Loss: 38.82116, Validation Loss: 5.60283\n",
      "Epoch 633/1000, Training Loss: 38.81893, Validation Loss: 5.60240\n",
      "Epoch 634/1000, Training Loss: 38.81671, Validation Loss: 5.60197\n",
      "Epoch 635/1000, Training Loss: 38.81448, Validation Loss: 5.60155\n",
      "Epoch 636/1000, Training Loss: 38.81226, Validation Loss: 5.60112\n",
      "Epoch 637/1000, Training Loss: 38.81004, Validation Loss: 5.60069\n",
      "Epoch 638/1000, Training Loss: 38.80782, Validation Loss: 5.60027\n",
      "Epoch 639/1000, Training Loss: 38.80560, Validation Loss: 5.59984\n",
      "Epoch 640/1000, Training Loss: 38.80338, Validation Loss: 5.59941\n",
      "Epoch 641/1000, Training Loss: 38.80116, Validation Loss: 5.59898\n",
      "Epoch 642/1000, Training Loss: 38.79894, Validation Loss: 5.59856\n",
      "Epoch 643/1000, Training Loss: 38.79672, Validation Loss: 5.59813\n",
      "Epoch 644/1000, Training Loss: 38.79450, Validation Loss: 5.59771\n",
      "Epoch 645/1000, Training Loss: 38.79229, Validation Loss: 5.59728\n",
      "Epoch 646/1000, Training Loss: 38.79007, Validation Loss: 5.59685\n",
      "Epoch 647/1000, Training Loss: 38.78785, Validation Loss: 5.59643\n",
      "Epoch 648/1000, Training Loss: 38.78563, Validation Loss: 5.59600\n",
      "Epoch 649/1000, Training Loss: 38.78342, Validation Loss: 5.59558\n",
      "Epoch 650/1000, Training Loss: 38.78120, Validation Loss: 5.59515\n",
      "Epoch 651/1000, Training Loss: 38.77899, Validation Loss: 5.59473\n",
      "Epoch 652/1000, Training Loss: 38.77678, Validation Loss: 5.59430\n",
      "Epoch 653/1000, Training Loss: 38.77457, Validation Loss: 5.59388\n",
      "Epoch 654/1000, Training Loss: 38.77236, Validation Loss: 5.59345\n",
      "Epoch 655/1000, Training Loss: 38.77015, Validation Loss: 5.59303\n",
      "Epoch 656/1000, Training Loss: 38.76794, Validation Loss: 5.59260\n",
      "Epoch 657/1000, Training Loss: 38.76573, Validation Loss: 5.59218\n",
      "Epoch 658/1000, Training Loss: 38.76352, Validation Loss: 5.59175\n",
      "Epoch 659/1000, Training Loss: 38.76131, Validation Loss: 5.59133\n",
      "Epoch 660/1000, Training Loss: 38.75910, Validation Loss: 5.59091\n",
      "Epoch 661/1000, Training Loss: 38.75689, Validation Loss: 5.59048\n",
      "Epoch 662/1000, Training Loss: 38.75469, Validation Loss: 5.59006\n",
      "Epoch 663/1000, Training Loss: 38.75248, Validation Loss: 5.58963\n",
      "Epoch 664/1000, Training Loss: 38.75028, Validation Loss: 5.58921\n",
      "Epoch 665/1000, Training Loss: 38.74808, Validation Loss: 5.58879\n",
      "Epoch 666/1000, Training Loss: 38.74587, Validation Loss: 5.58836\n",
      "Epoch 667/1000, Training Loss: 38.74367, Validation Loss: 5.58794\n",
      "Epoch 668/1000, Training Loss: 38.74147, Validation Loss: 5.58752\n",
      "Epoch 669/1000, Training Loss: 38.73927, Validation Loss: 5.58709\n",
      "Epoch 670/1000, Training Loss: 38.73708, Validation Loss: 5.58667\n",
      "Epoch 671/1000, Training Loss: 38.73488, Validation Loss: 5.58625\n",
      "Epoch 672/1000, Training Loss: 38.73268, Validation Loss: 5.58582\n",
      "Epoch 673/1000, Training Loss: 38.73049, Validation Loss: 5.58540\n",
      "Epoch 674/1000, Training Loss: 38.72829, Validation Loss: 5.58498\n",
      "Epoch 675/1000, Training Loss: 38.72610, Validation Loss: 5.58455\n",
      "Epoch 676/1000, Training Loss: 38.72390, Validation Loss: 5.58413\n",
      "Epoch 677/1000, Training Loss: 38.72171, Validation Loss: 5.58371\n",
      "Epoch 678/1000, Training Loss: 38.71952, Validation Loss: 5.58328\n",
      "Epoch 679/1000, Training Loss: 38.71733, Validation Loss: 5.58286\n",
      "Epoch 680/1000, Training Loss: 38.71514, Validation Loss: 5.58244\n",
      "Epoch 681/1000, Training Loss: 38.71295, Validation Loss: 5.58202\n",
      "Epoch 682/1000, Training Loss: 38.71076, Validation Loss: 5.58159\n",
      "Epoch 683/1000, Training Loss: 38.70858, Validation Loss: 5.58117\n",
      "Epoch 684/1000, Training Loss: 38.70639, Validation Loss: 5.58075\n",
      "Epoch 685/1000, Training Loss: 38.70421, Validation Loss: 5.58033\n",
      "Epoch 686/1000, Training Loss: 38.70202, Validation Loss: 5.57990\n",
      "Epoch 687/1000, Training Loss: 38.69984, Validation Loss: 5.57948\n",
      "Epoch 688/1000, Training Loss: 38.69766, Validation Loss: 5.57906\n",
      "Epoch 689/1000, Training Loss: 38.69547, Validation Loss: 5.57864\n",
      "Epoch 690/1000, Training Loss: 38.69329, Validation Loss: 5.57821\n",
      "Epoch 691/1000, Training Loss: 38.69110, Validation Loss: 5.57779\n",
      "Epoch 692/1000, Training Loss: 38.68892, Validation Loss: 5.57737\n",
      "Epoch 693/1000, Training Loss: 38.68674, Validation Loss: 5.57695\n",
      "Epoch 694/1000, Training Loss: 38.68456, Validation Loss: 5.57653\n",
      "Epoch 695/1000, Training Loss: 38.68238, Validation Loss: 5.57610\n",
      "Epoch 696/1000, Training Loss: 38.68020, Validation Loss: 5.57568\n",
      "Epoch 697/1000, Training Loss: 38.67802, Validation Loss: 5.57526\n",
      "Epoch 698/1000, Training Loss: 38.67584, Validation Loss: 5.57484\n",
      "Epoch 699/1000, Training Loss: 38.67366, Validation Loss: 5.57442\n",
      "Epoch 700/1000, Training Loss: 38.67148, Validation Loss: 5.57400\n",
      "Epoch 701/1000, Training Loss: 38.66931, Validation Loss: 5.57358\n",
      "Epoch 702/1000, Training Loss: 38.66713, Validation Loss: 5.57315\n",
      "Epoch 703/1000, Training Loss: 38.66496, Validation Loss: 5.57273\n",
      "Epoch 704/1000, Training Loss: 38.66279, Validation Loss: 5.57231\n",
      "Epoch 705/1000, Training Loss: 38.66061, Validation Loss: 5.57189\n",
      "Epoch 706/1000, Training Loss: 38.65844, Validation Loss: 5.57147\n",
      "Epoch 707/1000, Training Loss: 38.65627, Validation Loss: 5.57105\n",
      "Epoch 708/1000, Training Loss: 38.65410, Validation Loss: 5.57063\n",
      "Epoch 709/1000, Training Loss: 38.65193, Validation Loss: 5.57021\n",
      "Epoch 710/1000, Training Loss: 38.64976, Validation Loss: 5.56979\n",
      "Epoch 711/1000, Training Loss: 38.64759, Validation Loss: 5.56937\n",
      "Epoch 712/1000, Training Loss: 38.64543, Validation Loss: 5.56895\n",
      "Epoch 713/1000, Training Loss: 38.64326, Validation Loss: 5.56853\n",
      "Epoch 714/1000, Training Loss: 38.64109, Validation Loss: 5.56811\n",
      "Epoch 715/1000, Training Loss: 38.63893, Validation Loss: 5.56769\n",
      "Epoch 716/1000, Training Loss: 38.63676, Validation Loss: 5.56728\n",
      "Epoch 717/1000, Training Loss: 38.63460, Validation Loss: 5.56686\n",
      "Epoch 718/1000, Training Loss: 38.63243, Validation Loss: 5.56644\n",
      "Epoch 719/1000, Training Loss: 38.63027, Validation Loss: 5.56602\n",
      "Epoch 720/1000, Training Loss: 38.62811, Validation Loss: 5.56560\n",
      "Epoch 721/1000, Training Loss: 38.62594, Validation Loss: 5.56518\n",
      "Epoch 722/1000, Training Loss: 38.62378, Validation Loss: 5.56476\n",
      "Epoch 723/1000, Training Loss: 38.62162, Validation Loss: 5.56435\n",
      "Epoch 724/1000, Training Loss: 38.61945, Validation Loss: 5.56393\n",
      "Epoch 725/1000, Training Loss: 38.61729, Validation Loss: 5.56351\n",
      "Epoch 726/1000, Training Loss: 38.61513, Validation Loss: 5.56309\n",
      "Epoch 727/1000, Training Loss: 38.61297, Validation Loss: 5.56267\n",
      "Epoch 728/1000, Training Loss: 38.61081, Validation Loss: 5.56226\n",
      "Epoch 729/1000, Training Loss: 38.60865, Validation Loss: 5.56184\n",
      "Epoch 730/1000, Training Loss: 38.60649, Validation Loss: 5.56142\n",
      "Epoch 731/1000, Training Loss: 38.60433, Validation Loss: 5.56100\n",
      "Epoch 732/1000, Training Loss: 38.60217, Validation Loss: 5.56059\n",
      "Epoch 733/1000, Training Loss: 38.60001, Validation Loss: 5.56017\n",
      "Epoch 734/1000, Training Loss: 38.59786, Validation Loss: 5.55975\n",
      "Epoch 735/1000, Training Loss: 38.59570, Validation Loss: 5.55934\n",
      "Epoch 736/1000, Training Loss: 38.59354, Validation Loss: 5.55892\n",
      "Epoch 737/1000, Training Loss: 38.59139, Validation Loss: 5.55850\n",
      "Epoch 738/1000, Training Loss: 38.58923, Validation Loss: 5.55808\n",
      "Epoch 739/1000, Training Loss: 38.58708, Validation Loss: 5.55767\n",
      "Epoch 740/1000, Training Loss: 38.58492, Validation Loss: 5.55725\n",
      "Epoch 741/1000, Training Loss: 38.58277, Validation Loss: 5.55683\n",
      "Epoch 742/1000, Training Loss: 38.58061, Validation Loss: 5.55642\n",
      "Epoch 743/1000, Training Loss: 38.57846, Validation Loss: 5.55600\n",
      "Epoch 744/1000, Training Loss: 38.57630, Validation Loss: 5.55559\n",
      "Epoch 745/1000, Training Loss: 38.57415, Validation Loss: 5.55517\n",
      "Epoch 746/1000, Training Loss: 38.57200, Validation Loss: 5.55475\n",
      "Epoch 747/1000, Training Loss: 38.56985, Validation Loss: 5.55434\n",
      "Epoch 748/1000, Training Loss: 38.56770, Validation Loss: 5.55392\n",
      "Epoch 749/1000, Training Loss: 38.56555, Validation Loss: 5.55351\n",
      "Epoch 750/1000, Training Loss: 38.56340, Validation Loss: 5.55309\n",
      "Epoch 751/1000, Training Loss: 38.56125, Validation Loss: 5.55267\n",
      "Epoch 752/1000, Training Loss: 38.55910, Validation Loss: 5.55226\n",
      "Epoch 753/1000, Training Loss: 38.55696, Validation Loss: 5.55184\n",
      "Epoch 754/1000, Training Loss: 38.55481, Validation Loss: 5.55143\n",
      "Epoch 755/1000, Training Loss: 38.55266, Validation Loss: 5.55101\n",
      "Epoch 756/1000, Training Loss: 38.55052, Validation Loss: 5.55060\n",
      "Epoch 757/1000, Training Loss: 38.54837, Validation Loss: 5.55018\n",
      "Epoch 758/1000, Training Loss: 38.54623, Validation Loss: 5.54977\n",
      "Epoch 759/1000, Training Loss: 38.54408, Validation Loss: 5.54936\n",
      "Epoch 760/1000, Training Loss: 38.54194, Validation Loss: 5.54894\n",
      "Epoch 761/1000, Training Loss: 38.53980, Validation Loss: 5.54853\n",
      "Epoch 762/1000, Training Loss: 38.53766, Validation Loss: 5.54811\n",
      "Epoch 763/1000, Training Loss: 38.53553, Validation Loss: 5.54770\n",
      "Epoch 764/1000, Training Loss: 38.53339, Validation Loss: 5.54728\n",
      "Epoch 765/1000, Training Loss: 38.53125, Validation Loss: 5.54687\n",
      "Epoch 766/1000, Training Loss: 38.52911, Validation Loss: 5.54646\n",
      "Epoch 767/1000, Training Loss: 38.52698, Validation Loss: 5.54604\n",
      "Epoch 768/1000, Training Loss: 38.52484, Validation Loss: 5.54563\n",
      "Epoch 769/1000, Training Loss: 38.52271, Validation Loss: 5.54522\n",
      "Epoch 770/1000, Training Loss: 38.52057, Validation Loss: 5.54480\n",
      "Epoch 771/1000, Training Loss: 38.51844, Validation Loss: 5.54439\n",
      "Epoch 772/1000, Training Loss: 38.51631, Validation Loss: 5.54398\n",
      "Epoch 773/1000, Training Loss: 38.51418, Validation Loss: 5.54356\n",
      "Epoch 774/1000, Training Loss: 38.51204, Validation Loss: 5.54315\n",
      "Epoch 775/1000, Training Loss: 38.50991, Validation Loss: 5.54273\n",
      "Epoch 776/1000, Training Loss: 38.50778, Validation Loss: 5.54232\n",
      "Epoch 777/1000, Training Loss: 38.50565, Validation Loss: 5.54191\n",
      "Epoch 778/1000, Training Loss: 38.50352, Validation Loss: 5.54150\n",
      "Epoch 779/1000, Training Loss: 38.50140, Validation Loss: 5.54108\n",
      "Epoch 780/1000, Training Loss: 38.49927, Validation Loss: 5.54067\n",
      "Epoch 781/1000, Training Loss: 38.49714, Validation Loss: 5.54026\n",
      "Epoch 782/1000, Training Loss: 38.49502, Validation Loss: 5.53984\n",
      "Epoch 783/1000, Training Loss: 38.49289, Validation Loss: 5.53943\n",
      "Epoch 784/1000, Training Loss: 38.49077, Validation Loss: 5.53902\n",
      "Epoch 785/1000, Training Loss: 38.48864, Validation Loss: 5.53861\n",
      "Epoch 786/1000, Training Loss: 38.48652, Validation Loss: 5.53820\n",
      "Epoch 787/1000, Training Loss: 38.48439, Validation Loss: 5.53779\n",
      "Epoch 788/1000, Training Loss: 38.48227, Validation Loss: 5.53737\n",
      "Epoch 789/1000, Training Loss: 38.48015, Validation Loss: 5.53696\n",
      "Epoch 790/1000, Training Loss: 38.47803, Validation Loss: 5.53655\n",
      "Epoch 791/1000, Training Loss: 38.47591, Validation Loss: 5.53614\n",
      "Epoch 792/1000, Training Loss: 38.47379, Validation Loss: 5.53573\n",
      "Epoch 793/1000, Training Loss: 38.47167, Validation Loss: 5.53532\n",
      "Epoch 794/1000, Training Loss: 38.46955, Validation Loss: 5.53491\n",
      "Epoch 795/1000, Training Loss: 38.46742, Validation Loss: 5.53450\n",
      "Epoch 796/1000, Training Loss: 38.46530, Validation Loss: 5.53408\n",
      "Epoch 797/1000, Training Loss: 38.46318, Validation Loss: 5.53367\n",
      "Epoch 798/1000, Training Loss: 38.46106, Validation Loss: 5.53326\n",
      "Epoch 799/1000, Training Loss: 38.45895, Validation Loss: 5.53285\n",
      "Epoch 800/1000, Training Loss: 38.45683, Validation Loss: 5.53244\n",
      "Epoch 801/1000, Training Loss: 38.45472, Validation Loss: 5.53203\n",
      "Epoch 802/1000, Training Loss: 38.45261, Validation Loss: 5.53162\n",
      "Epoch 803/1000, Training Loss: 38.45049, Validation Loss: 5.53121\n",
      "Epoch 804/1000, Training Loss: 38.44838, Validation Loss: 5.53080\n",
      "Epoch 805/1000, Training Loss: 38.44627, Validation Loss: 5.53039\n",
      "Epoch 806/1000, Training Loss: 38.44415, Validation Loss: 5.52998\n",
      "Epoch 807/1000, Training Loss: 38.44204, Validation Loss: 5.52957\n",
      "Epoch 808/1000, Training Loss: 38.43993, Validation Loss: 5.52916\n",
      "Epoch 809/1000, Training Loss: 38.43782, Validation Loss: 5.52875\n",
      "Epoch 810/1000, Training Loss: 38.43572, Validation Loss: 5.52834\n",
      "Epoch 811/1000, Training Loss: 38.43361, Validation Loss: 5.52793\n",
      "Epoch 812/1000, Training Loss: 38.43150, Validation Loss: 5.52752\n",
      "Epoch 813/1000, Training Loss: 38.42940, Validation Loss: 5.52711\n",
      "Epoch 814/1000, Training Loss: 38.42729, Validation Loss: 5.52670\n",
      "Epoch 815/1000, Training Loss: 38.42518, Validation Loss: 5.52629\n",
      "Epoch 816/1000, Training Loss: 38.42308, Validation Loss: 5.52588\n",
      "Epoch 817/1000, Training Loss: 38.42098, Validation Loss: 5.52547\n",
      "Epoch 818/1000, Training Loss: 38.41887, Validation Loss: 5.52507\n",
      "Epoch 819/1000, Training Loss: 38.41677, Validation Loss: 5.52466\n",
      "Epoch 820/1000, Training Loss: 38.41466, Validation Loss: 5.52425\n",
      "Epoch 821/1000, Training Loss: 38.41256, Validation Loss: 5.52384\n",
      "Epoch 822/1000, Training Loss: 38.41046, Validation Loss: 5.52343\n",
      "Epoch 823/1000, Training Loss: 38.40835, Validation Loss: 5.52302\n",
      "Epoch 824/1000, Training Loss: 38.40625, Validation Loss: 5.52261\n",
      "Epoch 825/1000, Training Loss: 38.40415, Validation Loss: 5.52220\n",
      "Epoch 826/1000, Training Loss: 38.40205, Validation Loss: 5.52179\n",
      "Epoch 827/1000, Training Loss: 38.39995, Validation Loss: 5.52138\n",
      "Epoch 828/1000, Training Loss: 38.39785, Validation Loss: 5.52097\n",
      "Epoch 829/1000, Training Loss: 38.39575, Validation Loss: 5.52056\n",
      "Epoch 830/1000, Training Loss: 38.39365, Validation Loss: 5.52015\n",
      "Epoch 831/1000, Training Loss: 38.39155, Validation Loss: 5.51974\n",
      "Epoch 832/1000, Training Loss: 38.38945, Validation Loss: 5.51933\n",
      "Epoch 833/1000, Training Loss: 38.38735, Validation Loss: 5.51892\n",
      "Epoch 834/1000, Training Loss: 38.38526, Validation Loss: 5.51852\n",
      "Epoch 835/1000, Training Loss: 38.38316, Validation Loss: 5.51811\n",
      "Epoch 836/1000, Training Loss: 38.38107, Validation Loss: 5.51770\n",
      "Epoch 837/1000, Training Loss: 38.37897, Validation Loss: 5.51729\n",
      "Epoch 838/1000, Training Loss: 38.37688, Validation Loss: 5.51688\n",
      "Epoch 839/1000, Training Loss: 38.37478, Validation Loss: 5.51647\n",
      "Epoch 840/1000, Training Loss: 38.37269, Validation Loss: 5.51606\n",
      "Epoch 841/1000, Training Loss: 38.37059, Validation Loss: 5.51565\n",
      "Epoch 842/1000, Training Loss: 38.36850, Validation Loss: 5.51524\n",
      "Epoch 843/1000, Training Loss: 38.36640, Validation Loss: 5.51484\n",
      "Epoch 844/1000, Training Loss: 38.36430, Validation Loss: 5.51443\n",
      "Epoch 845/1000, Training Loss: 38.36221, Validation Loss: 5.51402\n",
      "Epoch 846/1000, Training Loss: 38.36012, Validation Loss: 5.51361\n",
      "Epoch 847/1000, Training Loss: 38.35803, Validation Loss: 5.51320\n",
      "Epoch 848/1000, Training Loss: 38.35594, Validation Loss: 5.51280\n",
      "Epoch 849/1000, Training Loss: 38.35385, Validation Loss: 5.51239\n",
      "Epoch 850/1000, Training Loss: 38.35175, Validation Loss: 5.51198\n",
      "Epoch 851/1000, Training Loss: 38.34967, Validation Loss: 5.51157\n",
      "Epoch 852/1000, Training Loss: 38.34757, Validation Loss: 5.51117\n",
      "Epoch 853/1000, Training Loss: 38.34548, Validation Loss: 5.51076\n",
      "Epoch 854/1000, Training Loss: 38.34339, Validation Loss: 5.51035\n",
      "Epoch 855/1000, Training Loss: 38.34130, Validation Loss: 5.50994\n",
      "Epoch 856/1000, Training Loss: 38.33921, Validation Loss: 5.50954\n",
      "Epoch 857/1000, Training Loss: 38.33712, Validation Loss: 5.50913\n",
      "Epoch 858/1000, Training Loss: 38.33503, Validation Loss: 5.50872\n",
      "Epoch 859/1000, Training Loss: 38.33295, Validation Loss: 5.50832\n",
      "Epoch 860/1000, Training Loss: 38.33086, Validation Loss: 5.50791\n",
      "Epoch 861/1000, Training Loss: 38.32877, Validation Loss: 5.50750\n",
      "Epoch 862/1000, Training Loss: 38.32668, Validation Loss: 5.50710\n",
      "Epoch 863/1000, Training Loss: 38.32459, Validation Loss: 5.50669\n",
      "Epoch 864/1000, Training Loss: 38.32251, Validation Loss: 5.50629\n",
      "Epoch 865/1000, Training Loss: 38.32042, Validation Loss: 5.50588\n",
      "Epoch 866/1000, Training Loss: 38.31833, Validation Loss: 5.50548\n",
      "Epoch 867/1000, Training Loss: 38.31625, Validation Loss: 5.50507\n",
      "Epoch 868/1000, Training Loss: 38.31416, Validation Loss: 5.50466\n",
      "Epoch 869/1000, Training Loss: 38.31208, Validation Loss: 5.50426\n",
      "Epoch 870/1000, Training Loss: 38.30999, Validation Loss: 5.50385\n",
      "Epoch 871/1000, Training Loss: 38.30791, Validation Loss: 5.50345\n",
      "Epoch 872/1000, Training Loss: 38.30582, Validation Loss: 5.50304\n",
      "Epoch 873/1000, Training Loss: 38.30374, Validation Loss: 5.50264\n",
      "Epoch 874/1000, Training Loss: 38.30165, Validation Loss: 5.50223\n",
      "Epoch 875/1000, Training Loss: 38.29957, Validation Loss: 5.50183\n",
      "Epoch 876/1000, Training Loss: 38.29749, Validation Loss: 5.50142\n",
      "Epoch 877/1000, Training Loss: 38.29540, Validation Loss: 5.50102\n",
      "Epoch 878/1000, Training Loss: 38.29332, Validation Loss: 5.50061\n",
      "Epoch 879/1000, Training Loss: 38.29124, Validation Loss: 5.50021\n",
      "Epoch 880/1000, Training Loss: 38.28916, Validation Loss: 5.49980\n",
      "Epoch 881/1000, Training Loss: 38.28708, Validation Loss: 5.49940\n",
      "Epoch 882/1000, Training Loss: 38.28500, Validation Loss: 5.49900\n",
      "Epoch 883/1000, Training Loss: 38.28293, Validation Loss: 5.49859\n",
      "Epoch 884/1000, Training Loss: 38.28085, Validation Loss: 5.49819\n",
      "Epoch 885/1000, Training Loss: 38.27877, Validation Loss: 5.49778\n",
      "Epoch 886/1000, Training Loss: 38.27669, Validation Loss: 5.49738\n",
      "Epoch 887/1000, Training Loss: 38.27462, Validation Loss: 5.49698\n",
      "Epoch 888/1000, Training Loss: 38.27254, Validation Loss: 5.49657\n",
      "Epoch 889/1000, Training Loss: 38.27046, Validation Loss: 5.49617\n",
      "Epoch 890/1000, Training Loss: 38.26838, Validation Loss: 5.49576\n",
      "Epoch 891/1000, Training Loss: 38.26631, Validation Loss: 5.49536\n",
      "Epoch 892/1000, Training Loss: 38.26423, Validation Loss: 5.49496\n",
      "Epoch 893/1000, Training Loss: 38.26216, Validation Loss: 5.49455\n",
      "Epoch 894/1000, Training Loss: 38.26009, Validation Loss: 5.49415\n",
      "Epoch 895/1000, Training Loss: 38.25801, Validation Loss: 5.49375\n",
      "Epoch 896/1000, Training Loss: 38.25594, Validation Loss: 5.49334\n",
      "Epoch 897/1000, Training Loss: 38.25387, Validation Loss: 5.49294\n",
      "Epoch 898/1000, Training Loss: 38.25180, Validation Loss: 5.49254\n",
      "Epoch 899/1000, Training Loss: 38.24973, Validation Loss: 5.49213\n",
      "Epoch 900/1000, Training Loss: 38.24766, Validation Loss: 5.49173\n",
      "Epoch 901/1000, Training Loss: 38.24559, Validation Loss: 5.49133\n",
      "Epoch 902/1000, Training Loss: 38.24352, Validation Loss: 5.49093\n",
      "Epoch 903/1000, Training Loss: 38.24145, Validation Loss: 5.49052\n",
      "Epoch 904/1000, Training Loss: 38.23938, Validation Loss: 5.49012\n",
      "Epoch 905/1000, Training Loss: 38.23732, Validation Loss: 5.48972\n",
      "Epoch 906/1000, Training Loss: 38.23525, Validation Loss: 5.48932\n",
      "Epoch 907/1000, Training Loss: 38.23318, Validation Loss: 5.48892\n",
      "Epoch 908/1000, Training Loss: 38.23112, Validation Loss: 5.48851\n",
      "Epoch 909/1000, Training Loss: 38.22905, Validation Loss: 5.48811\n",
      "Epoch 910/1000, Training Loss: 38.22699, Validation Loss: 5.48771\n",
      "Epoch 911/1000, Training Loss: 38.22492, Validation Loss: 5.48731\n",
      "Epoch 912/1000, Training Loss: 38.22286, Validation Loss: 5.48691\n",
      "Epoch 913/1000, Training Loss: 38.22080, Validation Loss: 5.48651\n",
      "Epoch 914/1000, Training Loss: 38.21873, Validation Loss: 5.48611\n",
      "Epoch 915/1000, Training Loss: 38.21667, Validation Loss: 5.48571\n",
      "Epoch 916/1000, Training Loss: 38.21461, Validation Loss: 5.48530\n",
      "Epoch 917/1000, Training Loss: 38.21255, Validation Loss: 5.48490\n",
      "Epoch 918/1000, Training Loss: 38.21049, Validation Loss: 5.48450\n",
      "Epoch 919/1000, Training Loss: 38.20843, Validation Loss: 5.48410\n",
      "Epoch 920/1000, Training Loss: 38.20637, Validation Loss: 5.48370\n",
      "Epoch 921/1000, Training Loss: 38.20431, Validation Loss: 5.48330\n",
      "Epoch 922/1000, Training Loss: 38.20225, Validation Loss: 5.48290\n",
      "Epoch 923/1000, Training Loss: 38.20019, Validation Loss: 5.48250\n",
      "Epoch 924/1000, Training Loss: 38.19814, Validation Loss: 5.48210\n",
      "Epoch 925/1000, Training Loss: 38.19609, Validation Loss: 5.48171\n",
      "Epoch 926/1000, Training Loss: 38.19403, Validation Loss: 5.48131\n",
      "Epoch 927/1000, Training Loss: 38.19198, Validation Loss: 5.48091\n",
      "Epoch 928/1000, Training Loss: 38.18992, Validation Loss: 5.48051\n",
      "Epoch 929/1000, Training Loss: 38.18787, Validation Loss: 5.48011\n",
      "Epoch 930/1000, Training Loss: 38.18582, Validation Loss: 5.47971\n",
      "Epoch 931/1000, Training Loss: 38.18377, Validation Loss: 5.47931\n",
      "Epoch 932/1000, Training Loss: 38.18172, Validation Loss: 5.47891\n",
      "Epoch 933/1000, Training Loss: 38.17967, Validation Loss: 5.47851\n",
      "Epoch 934/1000, Training Loss: 38.17761, Validation Loss: 5.47811\n",
      "Epoch 935/1000, Training Loss: 38.17556, Validation Loss: 5.47771\n",
      "Epoch 936/1000, Training Loss: 38.17351, Validation Loss: 5.47731\n",
      "Epoch 937/1000, Training Loss: 38.17146, Validation Loss: 5.47691\n",
      "Epoch 938/1000, Training Loss: 38.16941, Validation Loss: 5.47651\n",
      "Epoch 939/1000, Training Loss: 38.16736, Validation Loss: 5.47612\n",
      "Epoch 940/1000, Training Loss: 38.16531, Validation Loss: 5.47572\n",
      "Epoch 941/1000, Training Loss: 38.16326, Validation Loss: 5.47532\n",
      "Epoch 942/1000, Training Loss: 38.16121, Validation Loss: 5.47492\n",
      "Epoch 943/1000, Training Loss: 38.15916, Validation Loss: 5.47452\n",
      "Epoch 944/1000, Training Loss: 38.15712, Validation Loss: 5.47412\n",
      "Epoch 945/1000, Training Loss: 38.15507, Validation Loss: 5.47372\n",
      "Epoch 946/1000, Training Loss: 38.15303, Validation Loss: 5.47332\n",
      "Epoch 947/1000, Training Loss: 38.15098, Validation Loss: 5.47293\n",
      "Epoch 948/1000, Training Loss: 38.14894, Validation Loss: 5.47253\n",
      "Epoch 949/1000, Training Loss: 38.14690, Validation Loss: 5.47213\n",
      "Epoch 950/1000, Training Loss: 38.14486, Validation Loss: 5.47173\n",
      "Epoch 951/1000, Training Loss: 38.14281, Validation Loss: 5.47133\n",
      "Epoch 952/1000, Training Loss: 38.14078, Validation Loss: 5.47094\n",
      "Epoch 953/1000, Training Loss: 38.13874, Validation Loss: 5.47054\n",
      "Epoch 954/1000, Training Loss: 38.13670, Validation Loss: 5.47014\n",
      "Epoch 955/1000, Training Loss: 38.13466, Validation Loss: 5.46974\n",
      "Epoch 956/1000, Training Loss: 38.13263, Validation Loss: 5.46935\n",
      "Epoch 957/1000, Training Loss: 38.13059, Validation Loss: 5.46895\n",
      "Epoch 958/1000, Training Loss: 38.12856, Validation Loss: 5.46855\n",
      "Epoch 959/1000, Training Loss: 38.12653, Validation Loss: 5.46816\n",
      "Epoch 960/1000, Training Loss: 38.12450, Validation Loss: 5.46776\n",
      "Epoch 961/1000, Training Loss: 38.12248, Validation Loss: 5.46736\n",
      "Epoch 962/1000, Training Loss: 38.12045, Validation Loss: 5.46697\n",
      "Epoch 963/1000, Training Loss: 38.11842, Validation Loss: 5.46657\n",
      "Epoch 964/1000, Training Loss: 38.11640, Validation Loss: 5.46618\n",
      "Epoch 965/1000, Training Loss: 38.11437, Validation Loss: 5.46578\n",
      "Epoch 966/1000, Training Loss: 38.11235, Validation Loss: 5.46539\n",
      "Epoch 967/1000, Training Loss: 38.11032, Validation Loss: 5.46499\n",
      "Epoch 968/1000, Training Loss: 38.10830, Validation Loss: 5.46459\n",
      "Epoch 969/1000, Training Loss: 38.10628, Validation Loss: 5.46420\n",
      "Epoch 970/1000, Training Loss: 38.10426, Validation Loss: 5.46380\n",
      "Epoch 971/1000, Training Loss: 38.10223, Validation Loss: 5.46341\n",
      "Epoch 972/1000, Training Loss: 38.10021, Validation Loss: 5.46301\n",
      "Epoch 973/1000, Training Loss: 38.09819, Validation Loss: 5.46262\n",
      "Epoch 974/1000, Training Loss: 38.09618, Validation Loss: 5.46222\n",
      "Epoch 975/1000, Training Loss: 38.09416, Validation Loss: 5.46183\n",
      "Epoch 976/1000, Training Loss: 38.09214, Validation Loss: 5.46144\n",
      "Epoch 977/1000, Training Loss: 38.09012, Validation Loss: 5.46104\n",
      "Epoch 978/1000, Training Loss: 38.08811, Validation Loss: 5.46065\n",
      "Epoch 979/1000, Training Loss: 38.08609, Validation Loss: 5.46025\n",
      "Epoch 980/1000, Training Loss: 38.08408, Validation Loss: 5.45986\n",
      "Epoch 981/1000, Training Loss: 38.08206, Validation Loss: 5.45946\n",
      "Epoch 982/1000, Training Loss: 38.08005, Validation Loss: 5.45907\n",
      "Epoch 983/1000, Training Loss: 38.07803, Validation Loss: 5.45868\n",
      "Epoch 984/1000, Training Loss: 38.07602, Validation Loss: 5.45828\n",
      "Epoch 985/1000, Training Loss: 38.07400, Validation Loss: 5.45789\n",
      "Epoch 986/1000, Training Loss: 38.07199, Validation Loss: 5.45750\n",
      "Epoch 987/1000, Training Loss: 38.06998, Validation Loss: 5.45710\n",
      "Epoch 988/1000, Training Loss: 38.06797, Validation Loss: 5.45671\n",
      "Epoch 989/1000, Training Loss: 38.06595, Validation Loss: 5.45631\n",
      "Epoch 990/1000, Training Loss: 38.06394, Validation Loss: 5.45592\n",
      "Epoch 991/1000, Training Loss: 38.06193, Validation Loss: 5.45553\n",
      "Epoch 992/1000, Training Loss: 38.05992, Validation Loss: 5.45513\n",
      "Epoch 993/1000, Training Loss: 38.05791, Validation Loss: 5.45474\n",
      "Epoch 994/1000, Training Loss: 38.05590, Validation Loss: 5.45434\n",
      "Epoch 995/1000, Training Loss: 38.05390, Validation Loss: 5.45395\n",
      "Epoch 996/1000, Training Loss: 38.05189, Validation Loss: 5.45356\n",
      "Epoch 997/1000, Training Loss: 38.04988, Validation Loss: 5.45316\n",
      "Epoch 998/1000, Training Loss: 38.04788, Validation Loss: 5.45277\n",
      "Epoch 999/1000, Training Loss: 38.04587, Validation Loss: 5.45238\n",
      "Epoch 1000/1000, Training Loss: 38.04387, Validation Loss: 5.45198\n",
      "Training took: 124.62 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_optimizer_adadelta_3 = NeuralNetwork().to(device)\n",
    "summary(model_optimizer_adadelta_3, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adadelta(model_optimizer_adadelta_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_optimizer_adadelta_3=[]\n",
    "val_loss_list_optimizer_adadelta_3=[]\n",
    "train_accuracy_list_optimizer_adadelta_3=[]\n",
    "val_accuracy_list_optimizer_adadelta_3=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_optimizer_adadelta_3.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_optimizer_adadelta_3(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_optimizer_adadelta_3.append(train_accuracy)\n",
    "    train_loss_list_optimizer_adadelta_3.append(train_loss)\n",
    "\n",
    "    model_optimizer_adadelta_3.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_optimizer_adadelta_3(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_optimizer_adadelta_3.append(val_accuracy)\n",
    "    val_accuracy_list_optimizer_adadelta_3.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaF1gp0mmNqW",
    "outputId": "daffac4e-5443-46c3-90a2-c718ebdba7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable optimizer with opmizer as Adadelta: 0.6494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_optimizer_adadelta_3.eval()\n",
    "test_predictions_optimizer_adadelta_3 = model_optimizer_adadelta_3(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_optimizer_adadelta_3 = torch.round(test_predictions_optimizer_adadelta_3)\n",
    "\n",
    "test_predictions_rounded_numpy_optimizer_adadelta_3 = test_predictions_rounded_optimizer_adadelta_3.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_optimizer_adadelta_3 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_optimizer_adadelta_3)\n",
    "\n",
    "print(f\"Accuracy for variable optimizer with opmizer as Adadelta: {accuracy_optimizer_adadelta_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ll3G6JMmSSK",
    "outputId": "a90931b3-2ac4-4c17-9749-4e80fe1d7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable opmizer with opmizer as Adadelta: 0.62053\n"
     ]
    }
   ],
   "source": [
    "model_optimizer_adadelta_3.eval()\n",
    "test_loss_optimizer_adadelta_3=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_optimizer_adadelta_3 = model_optimizer_adadelta_3(X_test_tensor)\n",
    "    test_loss_optimizer_adadelta_3 = loss_function(test_outputs_optimizer_adadelta_3, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable opmizer with opmizer as Adadelta: {test_loss_optimizer_adadelta_3.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NfdnrNtnOoy"
   },
   "source": [
    "Taking Batch Size as the Hyperparameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYHBBtwWnXbA"
   },
   "source": [
    "Batch Size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRdN5sr9nh08",
    "outputId": "8226a04d-4802-48bf-8521-0fb78a06e84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 27.67620, Validation Loss: 4.07605\n",
      "Epoch 2/1000, Training Loss: 27.66783, Validation Loss: 4.07473\n",
      "Epoch 3/1000, Training Loss: 27.65949, Validation Loss: 4.07341\n",
      "Epoch 4/1000, Training Loss: 27.65117, Validation Loss: 4.07209\n",
      "Epoch 5/1000, Training Loss: 27.64288, Validation Loss: 4.07078\n",
      "Epoch 6/1000, Training Loss: 27.63461, Validation Loss: 4.06947\n",
      "Epoch 7/1000, Training Loss: 27.62638, Validation Loss: 4.06817\n",
      "Epoch 8/1000, Training Loss: 27.61817, Validation Loss: 4.06687\n",
      "Epoch 9/1000, Training Loss: 27.60999, Validation Loss: 4.06557\n",
      "Epoch 10/1000, Training Loss: 27.60183, Validation Loss: 4.06428\n",
      "Epoch 11/1000, Training Loss: 27.59369, Validation Loss: 4.06299\n",
      "Epoch 12/1000, Training Loss: 27.58558, Validation Loss: 4.06170\n",
      "Epoch 13/1000, Training Loss: 27.57750, Validation Loss: 4.06042\n",
      "Epoch 14/1000, Training Loss: 27.56944, Validation Loss: 4.05914\n",
      "Epoch 15/1000, Training Loss: 27.56141, Validation Loss: 4.05786\n",
      "Epoch 16/1000, Training Loss: 27.55340, Validation Loss: 4.05659\n",
      "Epoch 17/1000, Training Loss: 27.54541, Validation Loss: 4.05532\n",
      "Epoch 18/1000, Training Loss: 27.53744, Validation Loss: 4.05405\n",
      "Epoch 19/1000, Training Loss: 27.52949, Validation Loss: 4.05279\n",
      "Epoch 20/1000, Training Loss: 27.52156, Validation Loss: 4.05153\n",
      "Epoch 21/1000, Training Loss: 27.51366, Validation Loss: 4.05027\n",
      "Epoch 22/1000, Training Loss: 27.50579, Validation Loss: 4.04902\n",
      "Epoch 23/1000, Training Loss: 27.49793, Validation Loss: 4.04777\n",
      "Epoch 24/1000, Training Loss: 27.49010, Validation Loss: 4.04652\n",
      "Epoch 25/1000, Training Loss: 27.48229, Validation Loss: 4.04528\n",
      "Epoch 26/1000, Training Loss: 27.47451, Validation Loss: 4.04404\n",
      "Epoch 27/1000, Training Loss: 27.46675, Validation Loss: 4.04281\n",
      "Epoch 28/1000, Training Loss: 27.45901, Validation Loss: 4.04157\n",
      "Epoch 29/1000, Training Loss: 27.45129, Validation Loss: 4.04035\n",
      "Epoch 30/1000, Training Loss: 27.44359, Validation Loss: 4.03912\n",
      "Epoch 31/1000, Training Loss: 27.43591, Validation Loss: 4.03790\n",
      "Epoch 32/1000, Training Loss: 27.42826, Validation Loss: 4.03668\n",
      "Epoch 33/1000, Training Loss: 27.42063, Validation Loss: 4.03546\n",
      "Epoch 34/1000, Training Loss: 27.41302, Validation Loss: 4.03425\n",
      "Epoch 35/1000, Training Loss: 27.40543, Validation Loss: 4.03304\n",
      "Epoch 36/1000, Training Loss: 27.39786, Validation Loss: 4.03183\n",
      "Epoch 37/1000, Training Loss: 27.39030, Validation Loss: 4.03062\n",
      "Epoch 38/1000, Training Loss: 27.38277, Validation Loss: 4.02942\n",
      "Epoch 39/1000, Training Loss: 27.37525, Validation Loss: 4.02822\n",
      "Epoch 40/1000, Training Loss: 27.36776, Validation Loss: 4.02702\n",
      "Epoch 41/1000, Training Loss: 27.36028, Validation Loss: 4.02582\n",
      "Epoch 42/1000, Training Loss: 27.35282, Validation Loss: 4.02463\n",
      "Epoch 43/1000, Training Loss: 27.34539, Validation Loss: 4.02344\n",
      "Epoch 44/1000, Training Loss: 27.33797, Validation Loss: 4.02226\n",
      "Epoch 45/1000, Training Loss: 27.33057, Validation Loss: 4.02107\n",
      "Epoch 46/1000, Training Loss: 27.32319, Validation Loss: 4.01989\n",
      "Epoch 47/1000, Training Loss: 27.31583, Validation Loss: 4.01871\n",
      "Epoch 48/1000, Training Loss: 27.30848, Validation Loss: 4.01754\n",
      "Epoch 49/1000, Training Loss: 27.30116, Validation Loss: 4.01636\n",
      "Epoch 50/1000, Training Loss: 27.29385, Validation Loss: 4.01519\n",
      "Epoch 51/1000, Training Loss: 27.28656, Validation Loss: 4.01402\n",
      "Epoch 52/1000, Training Loss: 27.27929, Validation Loss: 4.01286\n",
      "Epoch 53/1000, Training Loss: 27.27204, Validation Loss: 4.01169\n",
      "Epoch 54/1000, Training Loss: 27.26480, Validation Loss: 4.01053\n",
      "Epoch 55/1000, Training Loss: 27.25759, Validation Loss: 4.00937\n",
      "Epoch 56/1000, Training Loss: 27.25038, Validation Loss: 4.00821\n",
      "Epoch 57/1000, Training Loss: 27.24319, Validation Loss: 4.00706\n",
      "Epoch 58/1000, Training Loss: 27.23603, Validation Loss: 4.00590\n",
      "Epoch 59/1000, Training Loss: 27.22888, Validation Loss: 4.00475\n",
      "Epoch 60/1000, Training Loss: 27.22174, Validation Loss: 4.00360\n",
      "Epoch 61/1000, Training Loss: 27.21462, Validation Loss: 4.00246\n",
      "Epoch 62/1000, Training Loss: 27.20752, Validation Loss: 4.00131\n",
      "Epoch 63/1000, Training Loss: 27.20044, Validation Loss: 4.00017\n",
      "Epoch 64/1000, Training Loss: 27.19337, Validation Loss: 3.99903\n",
      "Epoch 65/1000, Training Loss: 27.18631, Validation Loss: 3.99790\n",
      "Epoch 66/1000, Training Loss: 27.17928, Validation Loss: 3.99676\n",
      "Epoch 67/1000, Training Loss: 27.17226, Validation Loss: 3.99563\n",
      "Epoch 68/1000, Training Loss: 27.16527, Validation Loss: 3.99450\n",
      "Epoch 69/1000, Training Loss: 27.15828, Validation Loss: 3.99337\n",
      "Epoch 70/1000, Training Loss: 27.15131, Validation Loss: 3.99225\n",
      "Epoch 71/1000, Training Loss: 27.14436, Validation Loss: 3.99112\n",
      "Epoch 72/1000, Training Loss: 27.13742, Validation Loss: 3.99000\n",
      "Epoch 73/1000, Training Loss: 27.13050, Validation Loss: 3.98888\n",
      "Epoch 74/1000, Training Loss: 27.12359, Validation Loss: 3.98777\n",
      "Epoch 75/1000, Training Loss: 27.11671, Validation Loss: 3.98665\n",
      "Epoch 76/1000, Training Loss: 27.10983, Validation Loss: 3.98554\n",
      "Epoch 77/1000, Training Loss: 27.10297, Validation Loss: 3.98443\n",
      "Epoch 78/1000, Training Loss: 27.09612, Validation Loss: 3.98332\n",
      "Epoch 79/1000, Training Loss: 27.08929, Validation Loss: 3.98221\n",
      "Epoch 80/1000, Training Loss: 27.08248, Validation Loss: 3.98111\n",
      "Epoch 81/1000, Training Loss: 27.07568, Validation Loss: 3.98000\n",
      "Epoch 82/1000, Training Loss: 27.06889, Validation Loss: 3.97890\n",
      "Epoch 83/1000, Training Loss: 27.06212, Validation Loss: 3.97780\n",
      "Epoch 84/1000, Training Loss: 27.05537, Validation Loss: 3.97670\n",
      "Epoch 85/1000, Training Loss: 27.04862, Validation Loss: 3.97561\n",
      "Epoch 86/1000, Training Loss: 27.04189, Validation Loss: 3.97451\n",
      "Epoch 87/1000, Training Loss: 27.03518, Validation Loss: 3.97342\n",
      "Epoch 88/1000, Training Loss: 27.02847, Validation Loss: 3.97233\n",
      "Epoch 89/1000, Training Loss: 27.02179, Validation Loss: 3.97124\n",
      "Epoch 90/1000, Training Loss: 27.01512, Validation Loss: 3.97015\n",
      "Epoch 91/1000, Training Loss: 27.00846, Validation Loss: 3.96906\n",
      "Epoch 92/1000, Training Loss: 27.00181, Validation Loss: 3.96798\n",
      "Epoch 93/1000, Training Loss: 26.99518, Validation Loss: 3.96689\n",
      "Epoch 94/1000, Training Loss: 26.98855, Validation Loss: 3.96581\n",
      "Epoch 95/1000, Training Loss: 26.98194, Validation Loss: 3.96474\n",
      "Epoch 96/1000, Training Loss: 26.97533, Validation Loss: 3.96366\n",
      "Epoch 97/1000, Training Loss: 26.96874, Validation Loss: 3.96258\n",
      "Epoch 98/1000, Training Loss: 26.96217, Validation Loss: 3.96151\n",
      "Epoch 99/1000, Training Loss: 26.95561, Validation Loss: 3.96044\n",
      "Epoch 100/1000, Training Loss: 26.94907, Validation Loss: 3.95937\n",
      "Epoch 101/1000, Training Loss: 26.94254, Validation Loss: 3.95830\n",
      "Epoch 102/1000, Training Loss: 26.93602, Validation Loss: 3.95723\n",
      "Epoch 103/1000, Training Loss: 26.92951, Validation Loss: 3.95617\n",
      "Epoch 104/1000, Training Loss: 26.92301, Validation Loss: 3.95511\n",
      "Epoch 105/1000, Training Loss: 26.91654, Validation Loss: 3.95405\n",
      "Epoch 106/1000, Training Loss: 26.91007, Validation Loss: 3.95299\n",
      "Epoch 107/1000, Training Loss: 26.90362, Validation Loss: 3.95193\n",
      "Epoch 108/1000, Training Loss: 26.89718, Validation Loss: 3.95087\n",
      "Epoch 109/1000, Training Loss: 26.89076, Validation Loss: 3.94982\n",
      "Epoch 110/1000, Training Loss: 26.88434, Validation Loss: 3.94876\n",
      "Epoch 111/1000, Training Loss: 26.87794, Validation Loss: 3.94771\n",
      "Epoch 112/1000, Training Loss: 26.87155, Validation Loss: 3.94666\n",
      "Epoch 113/1000, Training Loss: 26.86517, Validation Loss: 3.94561\n",
      "Epoch 114/1000, Training Loss: 26.85880, Validation Loss: 3.94457\n",
      "Epoch 115/1000, Training Loss: 26.85245, Validation Loss: 3.94353\n",
      "Epoch 116/1000, Training Loss: 26.84611, Validation Loss: 3.94248\n",
      "Epoch 117/1000, Training Loss: 26.83977, Validation Loss: 3.94144\n",
      "Epoch 118/1000, Training Loss: 26.83346, Validation Loss: 3.94040\n",
      "Epoch 119/1000, Training Loss: 26.82715, Validation Loss: 3.93937\n",
      "Epoch 120/1000, Training Loss: 26.82085, Validation Loss: 3.93833\n",
      "Epoch 121/1000, Training Loss: 26.81457, Validation Loss: 3.93730\n",
      "Epoch 122/1000, Training Loss: 26.80829, Validation Loss: 3.93626\n",
      "Epoch 123/1000, Training Loss: 26.80203, Validation Loss: 3.93523\n",
      "Epoch 124/1000, Training Loss: 26.79578, Validation Loss: 3.93420\n",
      "Epoch 125/1000, Training Loss: 26.78953, Validation Loss: 3.93317\n",
      "Epoch 126/1000, Training Loss: 26.78330, Validation Loss: 3.93214\n",
      "Epoch 127/1000, Training Loss: 26.77708, Validation Loss: 3.93111\n",
      "Epoch 128/1000, Training Loss: 26.77087, Validation Loss: 3.93008\n",
      "Epoch 129/1000, Training Loss: 26.76467, Validation Loss: 3.92906\n",
      "Epoch 130/1000, Training Loss: 26.75847, Validation Loss: 3.92803\n",
      "Epoch 131/1000, Training Loss: 26.75230, Validation Loss: 3.92701\n",
      "Epoch 132/1000, Training Loss: 26.74613, Validation Loss: 3.92599\n",
      "Epoch 133/1000, Training Loss: 26.73997, Validation Loss: 3.92497\n",
      "Epoch 134/1000, Training Loss: 26.73382, Validation Loss: 3.92395\n",
      "Epoch 135/1000, Training Loss: 26.72768, Validation Loss: 3.92293\n",
      "Epoch 136/1000, Training Loss: 26.72154, Validation Loss: 3.92192\n",
      "Epoch 137/1000, Training Loss: 26.71542, Validation Loss: 3.92090\n",
      "Epoch 138/1000, Training Loss: 26.70930, Validation Loss: 3.91989\n",
      "Epoch 139/1000, Training Loss: 26.70319, Validation Loss: 3.91888\n",
      "Epoch 140/1000, Training Loss: 26.69710, Validation Loss: 3.91786\n",
      "Epoch 141/1000, Training Loss: 26.69102, Validation Loss: 3.91686\n",
      "Epoch 142/1000, Training Loss: 26.68494, Validation Loss: 3.91585\n",
      "Epoch 143/1000, Training Loss: 26.67887, Validation Loss: 3.91484\n",
      "Epoch 144/1000, Training Loss: 26.67282, Validation Loss: 3.91384\n",
      "Epoch 145/1000, Training Loss: 26.66677, Validation Loss: 3.91283\n",
      "Epoch 146/1000, Training Loss: 26.66074, Validation Loss: 3.91183\n",
      "Epoch 147/1000, Training Loss: 26.65471, Validation Loss: 3.91083\n",
      "Epoch 148/1000, Training Loss: 26.64869, Validation Loss: 3.90982\n",
      "Epoch 149/1000, Training Loss: 26.64267, Validation Loss: 3.90882\n",
      "Epoch 150/1000, Training Loss: 26.63667, Validation Loss: 3.90782\n",
      "Epoch 151/1000, Training Loss: 26.63067, Validation Loss: 3.90682\n",
      "Epoch 152/1000, Training Loss: 26.62468, Validation Loss: 3.90582\n",
      "Epoch 153/1000, Training Loss: 26.61870, Validation Loss: 3.90483\n",
      "Epoch 154/1000, Training Loss: 26.61273, Validation Loss: 3.90383\n",
      "Epoch 155/1000, Training Loss: 26.60677, Validation Loss: 3.90284\n",
      "Epoch 156/1000, Training Loss: 26.60081, Validation Loss: 3.90184\n",
      "Epoch 157/1000, Training Loss: 26.59486, Validation Loss: 3.90085\n",
      "Epoch 158/1000, Training Loss: 26.58892, Validation Loss: 3.89986\n",
      "Epoch 159/1000, Training Loss: 26.58299, Validation Loss: 3.89887\n",
      "Epoch 160/1000, Training Loss: 26.57707, Validation Loss: 3.89788\n",
      "Epoch 161/1000, Training Loss: 26.57116, Validation Loss: 3.89689\n",
      "Epoch 162/1000, Training Loss: 26.56525, Validation Loss: 3.89590\n",
      "Epoch 163/1000, Training Loss: 26.55935, Validation Loss: 3.89491\n",
      "Epoch 164/1000, Training Loss: 26.55346, Validation Loss: 3.89393\n",
      "Epoch 165/1000, Training Loss: 26.54757, Validation Loss: 3.89294\n",
      "Epoch 166/1000, Training Loss: 26.54169, Validation Loss: 3.89196\n",
      "Epoch 167/1000, Training Loss: 26.53582, Validation Loss: 3.89097\n",
      "Epoch 168/1000, Training Loss: 26.52995, Validation Loss: 3.88999\n",
      "Epoch 169/1000, Training Loss: 26.52409, Validation Loss: 3.88901\n",
      "Epoch 170/1000, Training Loss: 26.51825, Validation Loss: 3.88803\n",
      "Epoch 171/1000, Training Loss: 26.51242, Validation Loss: 3.88705\n",
      "Epoch 172/1000, Training Loss: 26.50659, Validation Loss: 3.88607\n",
      "Epoch 173/1000, Training Loss: 26.50077, Validation Loss: 3.88509\n",
      "Epoch 174/1000, Training Loss: 26.49496, Validation Loss: 3.88412\n",
      "Epoch 175/1000, Training Loss: 26.48916, Validation Loss: 3.88314\n",
      "Epoch 176/1000, Training Loss: 26.48336, Validation Loss: 3.88217\n",
      "Epoch 177/1000, Training Loss: 26.47757, Validation Loss: 3.88119\n",
      "Epoch 178/1000, Training Loss: 26.47178, Validation Loss: 3.88022\n",
      "Epoch 179/1000, Training Loss: 26.46601, Validation Loss: 3.87925\n",
      "Epoch 180/1000, Training Loss: 26.46024, Validation Loss: 3.87828\n",
      "Epoch 181/1000, Training Loss: 26.45447, Validation Loss: 3.87731\n",
      "Epoch 182/1000, Training Loss: 26.44872, Validation Loss: 3.87634\n",
      "Epoch 183/1000, Training Loss: 26.44297, Validation Loss: 3.87537\n",
      "Epoch 184/1000, Training Loss: 26.43723, Validation Loss: 3.87441\n",
      "Epoch 185/1000, Training Loss: 26.43149, Validation Loss: 3.87344\n",
      "Epoch 186/1000, Training Loss: 26.42576, Validation Loss: 3.87247\n",
      "Epoch 187/1000, Training Loss: 26.42004, Validation Loss: 3.87151\n",
      "Epoch 188/1000, Training Loss: 26.41432, Validation Loss: 3.87055\n",
      "Epoch 189/1000, Training Loss: 26.40861, Validation Loss: 3.86958\n",
      "Epoch 190/1000, Training Loss: 26.40291, Validation Loss: 3.86862\n",
      "Epoch 191/1000, Training Loss: 26.39721, Validation Loss: 3.86766\n",
      "Epoch 192/1000, Training Loss: 26.39151, Validation Loss: 3.86670\n",
      "Epoch 193/1000, Training Loss: 26.38583, Validation Loss: 3.86574\n",
      "Epoch 194/1000, Training Loss: 26.38015, Validation Loss: 3.86479\n",
      "Epoch 195/1000, Training Loss: 26.37447, Validation Loss: 3.86383\n",
      "Epoch 196/1000, Training Loss: 26.36881, Validation Loss: 3.86287\n",
      "Epoch 197/1000, Training Loss: 26.36315, Validation Loss: 3.86192\n",
      "Epoch 198/1000, Training Loss: 26.35749, Validation Loss: 3.86096\n",
      "Epoch 199/1000, Training Loss: 26.35184, Validation Loss: 3.86001\n",
      "Epoch 200/1000, Training Loss: 26.34620, Validation Loss: 3.85906\n",
      "Epoch 201/1000, Training Loss: 26.34057, Validation Loss: 3.85810\n",
      "Epoch 202/1000, Training Loss: 26.33493, Validation Loss: 3.85715\n",
      "Epoch 203/1000, Training Loss: 26.32931, Validation Loss: 3.85620\n",
      "Epoch 204/1000, Training Loss: 26.32369, Validation Loss: 3.85525\n",
      "Epoch 205/1000, Training Loss: 26.31808, Validation Loss: 3.85430\n",
      "Epoch 206/1000, Training Loss: 26.31247, Validation Loss: 3.85335\n",
      "Epoch 207/1000, Training Loss: 26.30687, Validation Loss: 3.85240\n",
      "Epoch 208/1000, Training Loss: 26.30128, Validation Loss: 3.85145\n",
      "Epoch 209/1000, Training Loss: 26.29569, Validation Loss: 3.85051\n",
      "Epoch 210/1000, Training Loss: 26.29012, Validation Loss: 3.84956\n",
      "Epoch 211/1000, Training Loss: 26.28454, Validation Loss: 3.84862\n",
      "Epoch 212/1000, Training Loss: 26.27898, Validation Loss: 3.84767\n",
      "Epoch 213/1000, Training Loss: 26.27341, Validation Loss: 3.84673\n",
      "Epoch 214/1000, Training Loss: 26.26785, Validation Loss: 3.84578\n",
      "Epoch 215/1000, Training Loss: 26.26230, Validation Loss: 3.84484\n",
      "Epoch 216/1000, Training Loss: 26.25675, Validation Loss: 3.84390\n",
      "Epoch 217/1000, Training Loss: 26.25121, Validation Loss: 3.84296\n",
      "Epoch 218/1000, Training Loss: 26.24568, Validation Loss: 3.84202\n",
      "Epoch 219/1000, Training Loss: 26.24015, Validation Loss: 3.84108\n",
      "Epoch 220/1000, Training Loss: 26.23462, Validation Loss: 3.84014\n",
      "Epoch 221/1000, Training Loss: 26.22910, Validation Loss: 3.83920\n",
      "Epoch 222/1000, Training Loss: 26.22358, Validation Loss: 3.83826\n",
      "Epoch 223/1000, Training Loss: 26.21807, Validation Loss: 3.83732\n",
      "Epoch 224/1000, Training Loss: 26.21257, Validation Loss: 3.83638\n",
      "Epoch 225/1000, Training Loss: 26.20707, Validation Loss: 3.83544\n",
      "Epoch 226/1000, Training Loss: 26.20157, Validation Loss: 3.83450\n",
      "Epoch 227/1000, Training Loss: 26.19609, Validation Loss: 3.83357\n",
      "Epoch 228/1000, Training Loss: 26.19061, Validation Loss: 3.83263\n",
      "Epoch 229/1000, Training Loss: 26.18513, Validation Loss: 3.83170\n",
      "Epoch 230/1000, Training Loss: 26.17967, Validation Loss: 3.83076\n",
      "Epoch 231/1000, Training Loss: 26.17421, Validation Loss: 3.82983\n",
      "Epoch 232/1000, Training Loss: 26.16875, Validation Loss: 3.82890\n",
      "Epoch 233/1000, Training Loss: 26.16330, Validation Loss: 3.82796\n",
      "Epoch 234/1000, Training Loss: 26.15785, Validation Loss: 3.82703\n",
      "Epoch 235/1000, Training Loss: 26.15240, Validation Loss: 3.82610\n",
      "Epoch 236/1000, Training Loss: 26.14697, Validation Loss: 3.82517\n",
      "Epoch 237/1000, Training Loss: 26.14153, Validation Loss: 3.82423\n",
      "Epoch 238/1000, Training Loss: 26.13610, Validation Loss: 3.82330\n",
      "Epoch 239/1000, Training Loss: 26.13067, Validation Loss: 3.82237\n",
      "Epoch 240/1000, Training Loss: 26.12525, Validation Loss: 3.82144\n",
      "Epoch 241/1000, Training Loss: 26.11983, Validation Loss: 3.82051\n",
      "Epoch 242/1000, Training Loss: 26.11442, Validation Loss: 3.81958\n",
      "Epoch 243/1000, Training Loss: 26.10901, Validation Loss: 3.81865\n",
      "Epoch 244/1000, Training Loss: 26.10360, Validation Loss: 3.81773\n",
      "Epoch 245/1000, Training Loss: 26.09820, Validation Loss: 3.81680\n",
      "Epoch 246/1000, Training Loss: 26.09280, Validation Loss: 3.81587\n",
      "Epoch 247/1000, Training Loss: 26.08741, Validation Loss: 3.81494\n",
      "Epoch 248/1000, Training Loss: 26.08202, Validation Loss: 3.81401\n",
      "Epoch 249/1000, Training Loss: 26.07664, Validation Loss: 3.81308\n",
      "Epoch 250/1000, Training Loss: 26.07125, Validation Loss: 3.81216\n",
      "Epoch 251/1000, Training Loss: 26.06588, Validation Loss: 3.81123\n",
      "Epoch 252/1000, Training Loss: 26.06051, Validation Loss: 3.81031\n",
      "Epoch 253/1000, Training Loss: 26.05514, Validation Loss: 3.80938\n",
      "Epoch 254/1000, Training Loss: 26.04977, Validation Loss: 3.80845\n",
      "Epoch 255/1000, Training Loss: 26.04442, Validation Loss: 3.80753\n",
      "Epoch 256/1000, Training Loss: 26.03907, Validation Loss: 3.80661\n",
      "Epoch 257/1000, Training Loss: 26.03373, Validation Loss: 3.80568\n",
      "Epoch 258/1000, Training Loss: 26.02840, Validation Loss: 3.80476\n",
      "Epoch 259/1000, Training Loss: 26.02306, Validation Loss: 3.80384\n",
      "Epoch 260/1000, Training Loss: 26.01774, Validation Loss: 3.80292\n",
      "Epoch 261/1000, Training Loss: 26.01241, Validation Loss: 3.80200\n",
      "Epoch 262/1000, Training Loss: 26.00709, Validation Loss: 3.80108\n",
      "Epoch 263/1000, Training Loss: 26.00177, Validation Loss: 3.80016\n",
      "Epoch 264/1000, Training Loss: 25.99646, Validation Loss: 3.79924\n",
      "Epoch 265/1000, Training Loss: 25.99115, Validation Loss: 3.79832\n",
      "Epoch 266/1000, Training Loss: 25.98585, Validation Loss: 3.79740\n",
      "Epoch 267/1000, Training Loss: 25.98055, Validation Loss: 3.79649\n",
      "Epoch 268/1000, Training Loss: 25.97525, Validation Loss: 3.79557\n",
      "Epoch 269/1000, Training Loss: 25.96995, Validation Loss: 3.79465\n",
      "Epoch 270/1000, Training Loss: 25.96466, Validation Loss: 3.79374\n",
      "Epoch 271/1000, Training Loss: 25.95938, Validation Loss: 3.79282\n",
      "Epoch 272/1000, Training Loss: 25.95410, Validation Loss: 3.79191\n",
      "Epoch 273/1000, Training Loss: 25.94882, Validation Loss: 3.79099\n",
      "Epoch 274/1000, Training Loss: 25.94355, Validation Loss: 3.79008\n",
      "Epoch 275/1000, Training Loss: 25.93828, Validation Loss: 3.78916\n",
      "Epoch 276/1000, Training Loss: 25.93301, Validation Loss: 3.78825\n",
      "Epoch 277/1000, Training Loss: 25.92775, Validation Loss: 3.78733\n",
      "Epoch 278/1000, Training Loss: 25.92250, Validation Loss: 3.78642\n",
      "Epoch 279/1000, Training Loss: 25.91725, Validation Loss: 3.78551\n",
      "Epoch 280/1000, Training Loss: 25.91201, Validation Loss: 3.78460\n",
      "Epoch 281/1000, Training Loss: 25.90678, Validation Loss: 3.78369\n",
      "Epoch 282/1000, Training Loss: 25.90155, Validation Loss: 3.78278\n",
      "Epoch 283/1000, Training Loss: 25.89633, Validation Loss: 3.78187\n",
      "Epoch 284/1000, Training Loss: 25.89110, Validation Loss: 3.78096\n",
      "Epoch 285/1000, Training Loss: 25.88588, Validation Loss: 3.78005\n",
      "Epoch 286/1000, Training Loss: 25.88066, Validation Loss: 3.77914\n",
      "Epoch 287/1000, Training Loss: 25.87545, Validation Loss: 3.77824\n",
      "Epoch 288/1000, Training Loss: 25.87023, Validation Loss: 3.77733\n",
      "Epoch 289/1000, Training Loss: 25.86502, Validation Loss: 3.77642\n",
      "Epoch 290/1000, Training Loss: 25.85981, Validation Loss: 3.77551\n",
      "Epoch 291/1000, Training Loss: 25.85461, Validation Loss: 3.77460\n",
      "Epoch 292/1000, Training Loss: 25.84940, Validation Loss: 3.77370\n",
      "Epoch 293/1000, Training Loss: 25.84420, Validation Loss: 3.77279\n",
      "Epoch 294/1000, Training Loss: 25.83900, Validation Loss: 3.77188\n",
      "Epoch 295/1000, Training Loss: 25.83381, Validation Loss: 3.77098\n",
      "Epoch 296/1000, Training Loss: 25.82862, Validation Loss: 3.77007\n",
      "Epoch 297/1000, Training Loss: 25.82343, Validation Loss: 3.76916\n",
      "Epoch 298/1000, Training Loss: 25.81825, Validation Loss: 3.76826\n",
      "Epoch 299/1000, Training Loss: 25.81307, Validation Loss: 3.76735\n",
      "Epoch 300/1000, Training Loss: 25.80789, Validation Loss: 3.76645\n",
      "Epoch 301/1000, Training Loss: 25.80272, Validation Loss: 3.76554\n",
      "Epoch 302/1000, Training Loss: 25.79755, Validation Loss: 3.76463\n",
      "Epoch 303/1000, Training Loss: 25.79239, Validation Loss: 3.76373\n",
      "Epoch 304/1000, Training Loss: 25.78722, Validation Loss: 3.76282\n",
      "Epoch 305/1000, Training Loss: 25.78207, Validation Loss: 3.76191\n",
      "Epoch 306/1000, Training Loss: 25.77691, Validation Loss: 3.76101\n",
      "Epoch 307/1000, Training Loss: 25.77176, Validation Loss: 3.76010\n",
      "Epoch 308/1000, Training Loss: 25.76661, Validation Loss: 3.75919\n",
      "Epoch 309/1000, Training Loss: 25.76146, Validation Loss: 3.75829\n",
      "Epoch 310/1000, Training Loss: 25.75631, Validation Loss: 3.75738\n",
      "Epoch 311/1000, Training Loss: 25.75117, Validation Loss: 3.75647\n",
      "Epoch 312/1000, Training Loss: 25.74602, Validation Loss: 3.75556\n",
      "Epoch 313/1000, Training Loss: 25.74088, Validation Loss: 3.75466\n",
      "Epoch 314/1000, Training Loss: 25.73574, Validation Loss: 3.75375\n",
      "Epoch 315/1000, Training Loss: 25.73060, Validation Loss: 3.75284\n",
      "Epoch 316/1000, Training Loss: 25.72547, Validation Loss: 3.75193\n",
      "Epoch 317/1000, Training Loss: 25.72034, Validation Loss: 3.75103\n",
      "Epoch 318/1000, Training Loss: 25.71521, Validation Loss: 3.75012\n",
      "Epoch 319/1000, Training Loss: 25.71008, Validation Loss: 3.74921\n",
      "Epoch 320/1000, Training Loss: 25.70495, Validation Loss: 3.74831\n",
      "Epoch 321/1000, Training Loss: 25.69983, Validation Loss: 3.74740\n",
      "Epoch 322/1000, Training Loss: 25.69471, Validation Loss: 3.74649\n",
      "Epoch 323/1000, Training Loss: 25.68959, Validation Loss: 3.74558\n",
      "Epoch 324/1000, Training Loss: 25.68448, Validation Loss: 3.74467\n",
      "Epoch 325/1000, Training Loss: 25.67936, Validation Loss: 3.74377\n",
      "Epoch 326/1000, Training Loss: 25.67424, Validation Loss: 3.74286\n",
      "Epoch 327/1000, Training Loss: 25.66913, Validation Loss: 3.74195\n",
      "Epoch 328/1000, Training Loss: 25.66403, Validation Loss: 3.74104\n",
      "Epoch 329/1000, Training Loss: 25.65892, Validation Loss: 3.74013\n",
      "Epoch 330/1000, Training Loss: 25.65382, Validation Loss: 3.73922\n",
      "Epoch 331/1000, Training Loss: 25.64872, Validation Loss: 3.73831\n",
      "Epoch 332/1000, Training Loss: 25.64362, Validation Loss: 3.73740\n",
      "Epoch 333/1000, Training Loss: 25.63852, Validation Loss: 3.73650\n",
      "Epoch 334/1000, Training Loss: 25.63343, Validation Loss: 3.73559\n",
      "Epoch 335/1000, Training Loss: 25.62834, Validation Loss: 3.73468\n",
      "Epoch 336/1000, Training Loss: 25.62324, Validation Loss: 3.73377\n",
      "Epoch 337/1000, Training Loss: 25.61814, Validation Loss: 3.73286\n",
      "Epoch 338/1000, Training Loss: 25.61303, Validation Loss: 3.73195\n",
      "Epoch 339/1000, Training Loss: 25.60793, Validation Loss: 3.73104\n",
      "Epoch 340/1000, Training Loss: 25.60282, Validation Loss: 3.73013\n",
      "Epoch 341/1000, Training Loss: 25.59773, Validation Loss: 3.72922\n",
      "Epoch 342/1000, Training Loss: 25.59264, Validation Loss: 3.72831\n",
      "Epoch 343/1000, Training Loss: 25.58755, Validation Loss: 3.72740\n",
      "Epoch 344/1000, Training Loss: 25.58247, Validation Loss: 3.72649\n",
      "Epoch 345/1000, Training Loss: 25.57739, Validation Loss: 3.72558\n",
      "Epoch 346/1000, Training Loss: 25.57231, Validation Loss: 3.72467\n",
      "Epoch 347/1000, Training Loss: 25.56723, Validation Loss: 3.72376\n",
      "Epoch 348/1000, Training Loss: 25.56214, Validation Loss: 3.72285\n",
      "Epoch 349/1000, Training Loss: 25.55707, Validation Loss: 3.72194\n",
      "Epoch 350/1000, Training Loss: 25.55199, Validation Loss: 3.72103\n",
      "Epoch 351/1000, Training Loss: 25.54691, Validation Loss: 3.72012\n",
      "Epoch 352/1000, Training Loss: 25.54183, Validation Loss: 3.71921\n",
      "Epoch 353/1000, Training Loss: 25.53676, Validation Loss: 3.71830\n",
      "Epoch 354/1000, Training Loss: 25.53169, Validation Loss: 3.71740\n",
      "Epoch 355/1000, Training Loss: 25.52663, Validation Loss: 3.71649\n",
      "Epoch 356/1000, Training Loss: 25.52156, Validation Loss: 3.71558\n",
      "Epoch 357/1000, Training Loss: 25.51649, Validation Loss: 3.71467\n",
      "Epoch 358/1000, Training Loss: 25.51143, Validation Loss: 3.71376\n",
      "Epoch 359/1000, Training Loss: 25.50636, Validation Loss: 3.71285\n",
      "Epoch 360/1000, Training Loss: 25.50129, Validation Loss: 3.71194\n",
      "Epoch 361/1000, Training Loss: 25.49623, Validation Loss: 3.71103\n",
      "Epoch 362/1000, Training Loss: 25.49117, Validation Loss: 3.71012\n",
      "Epoch 363/1000, Training Loss: 25.48611, Validation Loss: 3.70922\n",
      "Epoch 364/1000, Training Loss: 25.48105, Validation Loss: 3.70831\n",
      "Epoch 365/1000, Training Loss: 25.47599, Validation Loss: 3.70740\n",
      "Epoch 366/1000, Training Loss: 25.47094, Validation Loss: 3.70649\n",
      "Epoch 367/1000, Training Loss: 25.46589, Validation Loss: 3.70558\n",
      "Epoch 368/1000, Training Loss: 25.46084, Validation Loss: 3.70467\n",
      "Epoch 369/1000, Training Loss: 25.45579, Validation Loss: 3.70376\n",
      "Epoch 370/1000, Training Loss: 25.45075, Validation Loss: 3.70285\n",
      "Epoch 371/1000, Training Loss: 25.44571, Validation Loss: 3.70194\n",
      "Epoch 372/1000, Training Loss: 25.44067, Validation Loss: 3.70103\n",
      "Epoch 373/1000, Training Loss: 25.43564, Validation Loss: 3.70012\n",
      "Epoch 374/1000, Training Loss: 25.43060, Validation Loss: 3.69922\n",
      "Epoch 375/1000, Training Loss: 25.42557, Validation Loss: 3.69831\n",
      "Epoch 376/1000, Training Loss: 25.42053, Validation Loss: 3.69740\n",
      "Epoch 377/1000, Training Loss: 25.41550, Validation Loss: 3.69649\n",
      "Epoch 378/1000, Training Loss: 25.41047, Validation Loss: 3.69558\n",
      "Epoch 379/1000, Training Loss: 25.40544, Validation Loss: 3.69467\n",
      "Epoch 380/1000, Training Loss: 25.40041, Validation Loss: 3.69376\n",
      "Epoch 381/1000, Training Loss: 25.39539, Validation Loss: 3.69286\n",
      "Epoch 382/1000, Training Loss: 25.39037, Validation Loss: 3.69195\n",
      "Epoch 383/1000, Training Loss: 25.38535, Validation Loss: 3.69104\n",
      "Epoch 384/1000, Training Loss: 25.38033, Validation Loss: 3.69013\n",
      "Epoch 385/1000, Training Loss: 25.37531, Validation Loss: 3.68922\n",
      "Epoch 386/1000, Training Loss: 25.37029, Validation Loss: 3.68831\n",
      "Epoch 387/1000, Training Loss: 25.36528, Validation Loss: 3.68740\n",
      "Epoch 388/1000, Training Loss: 25.36026, Validation Loss: 3.68649\n",
      "Epoch 389/1000, Training Loss: 25.35525, Validation Loss: 3.68559\n",
      "Epoch 390/1000, Training Loss: 25.35024, Validation Loss: 3.68468\n",
      "Epoch 391/1000, Training Loss: 25.34524, Validation Loss: 3.68377\n",
      "Epoch 392/1000, Training Loss: 25.34024, Validation Loss: 3.68286\n",
      "Epoch 393/1000, Training Loss: 25.33524, Validation Loss: 3.68195\n",
      "Epoch 394/1000, Training Loss: 25.33024, Validation Loss: 3.68104\n",
      "Epoch 395/1000, Training Loss: 25.32524, Validation Loss: 3.68014\n",
      "Epoch 396/1000, Training Loss: 25.32025, Validation Loss: 3.67923\n",
      "Epoch 397/1000, Training Loss: 25.31526, Validation Loss: 3.67832\n",
      "Epoch 398/1000, Training Loss: 25.31027, Validation Loss: 3.67741\n",
      "Epoch 399/1000, Training Loss: 25.30528, Validation Loss: 3.67651\n",
      "Epoch 400/1000, Training Loss: 25.30030, Validation Loss: 3.67560\n",
      "Epoch 401/1000, Training Loss: 25.29532, Validation Loss: 3.67469\n",
      "Epoch 402/1000, Training Loss: 25.29035, Validation Loss: 3.67379\n",
      "Epoch 403/1000, Training Loss: 25.28537, Validation Loss: 3.67288\n",
      "Epoch 404/1000, Training Loss: 25.28040, Validation Loss: 3.67198\n",
      "Epoch 405/1000, Training Loss: 25.27543, Validation Loss: 3.67107\n",
      "Epoch 406/1000, Training Loss: 25.27047, Validation Loss: 3.67017\n",
      "Epoch 407/1000, Training Loss: 25.26551, Validation Loss: 3.66926\n",
      "Epoch 408/1000, Training Loss: 25.26055, Validation Loss: 3.66836\n",
      "Epoch 409/1000, Training Loss: 25.25559, Validation Loss: 3.66746\n",
      "Epoch 410/1000, Training Loss: 25.25064, Validation Loss: 3.66655\n",
      "Epoch 411/1000, Training Loss: 25.24569, Validation Loss: 3.66565\n",
      "Epoch 412/1000, Training Loss: 25.24074, Validation Loss: 3.66475\n",
      "Epoch 413/1000, Training Loss: 25.23580, Validation Loss: 3.66385\n",
      "Epoch 414/1000, Training Loss: 25.23085, Validation Loss: 3.66294\n",
      "Epoch 415/1000, Training Loss: 25.22591, Validation Loss: 3.66204\n",
      "Epoch 416/1000, Training Loss: 25.22097, Validation Loss: 3.66114\n",
      "Epoch 417/1000, Training Loss: 25.21604, Validation Loss: 3.66024\n",
      "Epoch 418/1000, Training Loss: 25.21110, Validation Loss: 3.65933\n",
      "Epoch 419/1000, Training Loss: 25.20617, Validation Loss: 3.65843\n",
      "Epoch 420/1000, Training Loss: 25.20124, Validation Loss: 3.65753\n",
      "Epoch 421/1000, Training Loss: 25.19631, Validation Loss: 3.65663\n",
      "Epoch 422/1000, Training Loss: 25.19139, Validation Loss: 3.65573\n",
      "Epoch 423/1000, Training Loss: 25.18647, Validation Loss: 3.65483\n",
      "Epoch 424/1000, Training Loss: 25.18155, Validation Loss: 3.65393\n",
      "Epoch 425/1000, Training Loss: 25.17663, Validation Loss: 3.65303\n",
      "Epoch 426/1000, Training Loss: 25.17170, Validation Loss: 3.65213\n",
      "Epoch 427/1000, Training Loss: 25.16678, Validation Loss: 3.65123\n",
      "Epoch 428/1000, Training Loss: 25.16186, Validation Loss: 3.65033\n",
      "Epoch 429/1000, Training Loss: 25.15693, Validation Loss: 3.64943\n",
      "Epoch 430/1000, Training Loss: 25.15201, Validation Loss: 3.64852\n",
      "Epoch 431/1000, Training Loss: 25.14710, Validation Loss: 3.64762\n",
      "Epoch 432/1000, Training Loss: 25.14218, Validation Loss: 3.64672\n",
      "Epoch 433/1000, Training Loss: 25.13726, Validation Loss: 3.64582\n",
      "Epoch 434/1000, Training Loss: 25.13234, Validation Loss: 3.64491\n",
      "Epoch 435/1000, Training Loss: 25.12743, Validation Loss: 3.64401\n",
      "Epoch 436/1000, Training Loss: 25.12252, Validation Loss: 3.64311\n",
      "Epoch 437/1000, Training Loss: 25.11761, Validation Loss: 3.64221\n",
      "Epoch 438/1000, Training Loss: 25.11270, Validation Loss: 3.64131\n",
      "Epoch 439/1000, Training Loss: 25.10780, Validation Loss: 3.64040\n",
      "Epoch 440/1000, Training Loss: 25.10289, Validation Loss: 3.63950\n",
      "Epoch 441/1000, Training Loss: 25.09799, Validation Loss: 3.63860\n",
      "Epoch 442/1000, Training Loss: 25.09309, Validation Loss: 3.63770\n",
      "Epoch 443/1000, Training Loss: 25.08819, Validation Loss: 3.63680\n",
      "Epoch 444/1000, Training Loss: 25.08328, Validation Loss: 3.63590\n",
      "Epoch 445/1000, Training Loss: 25.07838, Validation Loss: 3.63500\n",
      "Epoch 446/1000, Training Loss: 25.07349, Validation Loss: 3.63410\n",
      "Epoch 447/1000, Training Loss: 25.06859, Validation Loss: 3.63320\n",
      "Epoch 448/1000, Training Loss: 25.06370, Validation Loss: 3.63230\n",
      "Epoch 449/1000, Training Loss: 25.05880, Validation Loss: 3.63140\n",
      "Epoch 450/1000, Training Loss: 25.05392, Validation Loss: 3.63050\n",
      "Epoch 451/1000, Training Loss: 25.04903, Validation Loss: 3.62960\n",
      "Epoch 452/1000, Training Loss: 25.04415, Validation Loss: 3.62870\n",
      "Epoch 453/1000, Training Loss: 25.03927, Validation Loss: 3.62780\n",
      "Epoch 454/1000, Training Loss: 25.03440, Validation Loss: 3.62690\n",
      "Epoch 455/1000, Training Loss: 25.02952, Validation Loss: 3.62601\n",
      "Epoch 456/1000, Training Loss: 25.02465, Validation Loss: 3.62511\n",
      "Epoch 457/1000, Training Loss: 25.01977, Validation Loss: 3.62421\n",
      "Epoch 458/1000, Training Loss: 25.01490, Validation Loss: 3.62331\n",
      "Epoch 459/1000, Training Loss: 25.01002, Validation Loss: 3.62241\n",
      "Epoch 460/1000, Training Loss: 25.00515, Validation Loss: 3.62151\n",
      "Epoch 461/1000, Training Loss: 25.00028, Validation Loss: 3.62062\n",
      "Epoch 462/1000, Training Loss: 24.99541, Validation Loss: 3.61972\n",
      "Epoch 463/1000, Training Loss: 24.99054, Validation Loss: 3.61882\n",
      "Epoch 464/1000, Training Loss: 24.98567, Validation Loss: 3.61792\n",
      "Epoch 465/1000, Training Loss: 24.98080, Validation Loss: 3.61702\n",
      "Epoch 466/1000, Training Loss: 24.97593, Validation Loss: 3.61612\n",
      "Epoch 467/1000, Training Loss: 24.97107, Validation Loss: 3.61522\n",
      "Epoch 468/1000, Training Loss: 24.96620, Validation Loss: 3.61432\n",
      "Epoch 469/1000, Training Loss: 24.96134, Validation Loss: 3.61343\n",
      "Epoch 470/1000, Training Loss: 24.95647, Validation Loss: 3.61253\n",
      "Epoch 471/1000, Training Loss: 24.95161, Validation Loss: 3.61163\n",
      "Epoch 472/1000, Training Loss: 24.94674, Validation Loss: 3.61073\n",
      "Epoch 473/1000, Training Loss: 24.94188, Validation Loss: 3.60983\n",
      "Epoch 474/1000, Training Loss: 24.93702, Validation Loss: 3.60893\n",
      "Epoch 475/1000, Training Loss: 24.93216, Validation Loss: 3.60804\n",
      "Epoch 476/1000, Training Loss: 24.92731, Validation Loss: 3.60714\n",
      "Epoch 477/1000, Training Loss: 24.92245, Validation Loss: 3.60624\n",
      "Epoch 478/1000, Training Loss: 24.91760, Validation Loss: 3.60534\n",
      "Epoch 479/1000, Training Loss: 24.91275, Validation Loss: 3.60445\n",
      "Epoch 480/1000, Training Loss: 24.90790, Validation Loss: 3.60355\n",
      "Epoch 481/1000, Training Loss: 24.90305, Validation Loss: 3.60265\n",
      "Epoch 482/1000, Training Loss: 24.89821, Validation Loss: 3.60176\n",
      "Epoch 483/1000, Training Loss: 24.89337, Validation Loss: 3.60086\n",
      "Epoch 484/1000, Training Loss: 24.88852, Validation Loss: 3.59996\n",
      "Epoch 485/1000, Training Loss: 24.88368, Validation Loss: 3.59907\n",
      "Epoch 486/1000, Training Loss: 24.87885, Validation Loss: 3.59817\n",
      "Epoch 487/1000, Training Loss: 24.87401, Validation Loss: 3.59727\n",
      "Epoch 488/1000, Training Loss: 24.86917, Validation Loss: 3.59638\n",
      "Epoch 489/1000, Training Loss: 24.86434, Validation Loss: 3.59548\n",
      "Epoch 490/1000, Training Loss: 24.85950, Validation Loss: 3.59458\n",
      "Epoch 491/1000, Training Loss: 24.85467, Validation Loss: 3.59369\n",
      "Epoch 492/1000, Training Loss: 24.84983, Validation Loss: 3.59279\n",
      "Epoch 493/1000, Training Loss: 24.84500, Validation Loss: 3.59190\n",
      "Epoch 494/1000, Training Loss: 24.84018, Validation Loss: 3.59100\n",
      "Epoch 495/1000, Training Loss: 24.83535, Validation Loss: 3.59011\n",
      "Epoch 496/1000, Training Loss: 24.83052, Validation Loss: 3.58921\n",
      "Epoch 497/1000, Training Loss: 24.82569, Validation Loss: 3.58831\n",
      "Epoch 498/1000, Training Loss: 24.82086, Validation Loss: 3.58742\n",
      "Epoch 499/1000, Training Loss: 24.81603, Validation Loss: 3.58652\n",
      "Epoch 500/1000, Training Loss: 24.81120, Validation Loss: 3.58562\n",
      "Epoch 501/1000, Training Loss: 24.80638, Validation Loss: 3.58472\n",
      "Epoch 502/1000, Training Loss: 24.80155, Validation Loss: 3.58383\n",
      "Epoch 503/1000, Training Loss: 24.79672, Validation Loss: 3.58293\n",
      "Epoch 504/1000, Training Loss: 24.79190, Validation Loss: 3.58204\n",
      "Epoch 505/1000, Training Loss: 24.78707, Validation Loss: 3.58115\n",
      "Epoch 506/1000, Training Loss: 24.78225, Validation Loss: 3.58025\n",
      "Epoch 507/1000, Training Loss: 24.77743, Validation Loss: 3.57936\n",
      "Epoch 508/1000, Training Loss: 24.77261, Validation Loss: 3.57846\n",
      "Epoch 509/1000, Training Loss: 24.76779, Validation Loss: 3.57757\n",
      "Epoch 510/1000, Training Loss: 24.76297, Validation Loss: 3.57667\n",
      "Epoch 511/1000, Training Loss: 24.75816, Validation Loss: 3.57578\n",
      "Epoch 512/1000, Training Loss: 24.75335, Validation Loss: 3.57489\n",
      "Epoch 513/1000, Training Loss: 24.74854, Validation Loss: 3.57399\n",
      "Epoch 514/1000, Training Loss: 24.74372, Validation Loss: 3.57310\n",
      "Epoch 515/1000, Training Loss: 24.73891, Validation Loss: 3.57220\n",
      "Epoch 516/1000, Training Loss: 24.73410, Validation Loss: 3.57131\n",
      "Epoch 517/1000, Training Loss: 24.72930, Validation Loss: 3.57041\n",
      "Epoch 518/1000, Training Loss: 24.72449, Validation Loss: 3.56952\n",
      "Epoch 519/1000, Training Loss: 24.71969, Validation Loss: 3.56862\n",
      "Epoch 520/1000, Training Loss: 24.71489, Validation Loss: 3.56773\n",
      "Epoch 521/1000, Training Loss: 24.71009, Validation Loss: 3.56684\n",
      "Epoch 522/1000, Training Loss: 24.70530, Validation Loss: 3.56594\n",
      "Epoch 523/1000, Training Loss: 24.70051, Validation Loss: 3.56505\n",
      "Epoch 524/1000, Training Loss: 24.69572, Validation Loss: 3.56415\n",
      "Epoch 525/1000, Training Loss: 24.69093, Validation Loss: 3.56326\n",
      "Epoch 526/1000, Training Loss: 24.68614, Validation Loss: 3.56237\n",
      "Epoch 527/1000, Training Loss: 24.68135, Validation Loss: 3.56147\n",
      "Epoch 528/1000, Training Loss: 24.67656, Validation Loss: 3.56058\n",
      "Epoch 529/1000, Training Loss: 24.67177, Validation Loss: 3.55969\n",
      "Epoch 530/1000, Training Loss: 24.66698, Validation Loss: 3.55879\n",
      "Epoch 531/1000, Training Loss: 24.66220, Validation Loss: 3.55790\n",
      "Epoch 532/1000, Training Loss: 24.65741, Validation Loss: 3.55701\n",
      "Epoch 533/1000, Training Loss: 24.65263, Validation Loss: 3.55611\n",
      "Epoch 534/1000, Training Loss: 24.64785, Validation Loss: 3.55522\n",
      "Epoch 535/1000, Training Loss: 24.64307, Validation Loss: 3.55433\n",
      "Epoch 536/1000, Training Loss: 24.63829, Validation Loss: 3.55344\n",
      "Epoch 537/1000, Training Loss: 24.63351, Validation Loss: 3.55255\n",
      "Epoch 538/1000, Training Loss: 24.62873, Validation Loss: 3.55165\n",
      "Epoch 539/1000, Training Loss: 24.62396, Validation Loss: 3.55076\n",
      "Epoch 540/1000, Training Loss: 24.61919, Validation Loss: 3.54987\n",
      "Epoch 541/1000, Training Loss: 24.61442, Validation Loss: 3.54898\n",
      "Epoch 542/1000, Training Loss: 24.60965, Validation Loss: 3.54809\n",
      "Epoch 543/1000, Training Loss: 24.60488, Validation Loss: 3.54720\n",
      "Epoch 544/1000, Training Loss: 24.60011, Validation Loss: 3.54631\n",
      "Epoch 545/1000, Training Loss: 24.59535, Validation Loss: 3.54542\n",
      "Epoch 546/1000, Training Loss: 24.59059, Validation Loss: 3.54453\n",
      "Epoch 547/1000, Training Loss: 24.58582, Validation Loss: 3.54364\n",
      "Epoch 548/1000, Training Loss: 24.58106, Validation Loss: 3.54275\n",
      "Epoch 549/1000, Training Loss: 24.57630, Validation Loss: 3.54186\n",
      "Epoch 550/1000, Training Loss: 24.57154, Validation Loss: 3.54097\n",
      "Epoch 551/1000, Training Loss: 24.56678, Validation Loss: 3.54008\n",
      "Epoch 552/1000, Training Loss: 24.56202, Validation Loss: 3.53919\n",
      "Epoch 553/1000, Training Loss: 24.55726, Validation Loss: 3.53830\n",
      "Epoch 554/1000, Training Loss: 24.55250, Validation Loss: 3.53741\n",
      "Epoch 555/1000, Training Loss: 24.54774, Validation Loss: 3.53652\n",
      "Epoch 556/1000, Training Loss: 24.54299, Validation Loss: 3.53563\n",
      "Epoch 557/1000, Training Loss: 24.53823, Validation Loss: 3.53474\n",
      "Epoch 558/1000, Training Loss: 24.53347, Validation Loss: 3.53385\n",
      "Epoch 559/1000, Training Loss: 24.52871, Validation Loss: 3.53296\n",
      "Epoch 560/1000, Training Loss: 24.52396, Validation Loss: 3.53207\n",
      "Epoch 561/1000, Training Loss: 24.51920, Validation Loss: 3.53118\n",
      "Epoch 562/1000, Training Loss: 24.51445, Validation Loss: 3.53029\n",
      "Epoch 563/1000, Training Loss: 24.50969, Validation Loss: 3.52940\n",
      "Epoch 564/1000, Training Loss: 24.50494, Validation Loss: 3.52851\n",
      "Epoch 565/1000, Training Loss: 24.50020, Validation Loss: 3.52762\n",
      "Epoch 566/1000, Training Loss: 24.49545, Validation Loss: 3.52673\n",
      "Epoch 567/1000, Training Loss: 24.49071, Validation Loss: 3.52584\n",
      "Epoch 568/1000, Training Loss: 24.48596, Validation Loss: 3.52495\n",
      "Epoch 569/1000, Training Loss: 24.48122, Validation Loss: 3.52406\n",
      "Epoch 570/1000, Training Loss: 24.47648, Validation Loss: 3.52317\n",
      "Epoch 571/1000, Training Loss: 24.47175, Validation Loss: 3.52228\n",
      "Epoch 572/1000, Training Loss: 24.46701, Validation Loss: 3.52139\n",
      "Epoch 573/1000, Training Loss: 24.46228, Validation Loss: 3.52050\n",
      "Epoch 574/1000, Training Loss: 24.45755, Validation Loss: 3.51961\n",
      "Epoch 575/1000, Training Loss: 24.45282, Validation Loss: 3.51872\n",
      "Epoch 576/1000, Training Loss: 24.44809, Validation Loss: 3.51783\n",
      "Epoch 577/1000, Training Loss: 24.44336, Validation Loss: 3.51694\n",
      "Epoch 578/1000, Training Loss: 24.43863, Validation Loss: 3.51605\n",
      "Epoch 579/1000, Training Loss: 24.43390, Validation Loss: 3.51516\n",
      "Epoch 580/1000, Training Loss: 24.42917, Validation Loss: 3.51427\n",
      "Epoch 581/1000, Training Loss: 24.42445, Validation Loss: 3.51338\n",
      "Epoch 582/1000, Training Loss: 24.41972, Validation Loss: 3.51249\n",
      "Epoch 583/1000, Training Loss: 24.41499, Validation Loss: 3.51160\n",
      "Epoch 584/1000, Training Loss: 24.41027, Validation Loss: 3.51071\n",
      "Epoch 585/1000, Training Loss: 24.40555, Validation Loss: 3.50982\n",
      "Epoch 586/1000, Training Loss: 24.40082, Validation Loss: 3.50893\n",
      "Epoch 587/1000, Training Loss: 24.39610, Validation Loss: 3.50804\n",
      "Epoch 588/1000, Training Loss: 24.39138, Validation Loss: 3.50715\n",
      "Epoch 589/1000, Training Loss: 24.38666, Validation Loss: 3.50627\n",
      "Epoch 590/1000, Training Loss: 24.38194, Validation Loss: 3.50538\n",
      "Epoch 591/1000, Training Loss: 24.37722, Validation Loss: 3.50449\n",
      "Epoch 592/1000, Training Loss: 24.37251, Validation Loss: 3.50360\n",
      "Epoch 593/1000, Training Loss: 24.36779, Validation Loss: 3.50271\n",
      "Epoch 594/1000, Training Loss: 24.36308, Validation Loss: 3.50182\n",
      "Epoch 595/1000, Training Loss: 24.35838, Validation Loss: 3.50093\n",
      "Epoch 596/1000, Training Loss: 24.35367, Validation Loss: 3.50004\n",
      "Epoch 597/1000, Training Loss: 24.34896, Validation Loss: 3.49916\n",
      "Epoch 598/1000, Training Loss: 24.34426, Validation Loss: 3.49827\n",
      "Epoch 599/1000, Training Loss: 24.33956, Validation Loss: 3.49738\n",
      "Epoch 600/1000, Training Loss: 24.33486, Validation Loss: 3.49649\n",
      "Epoch 601/1000, Training Loss: 24.33016, Validation Loss: 3.49560\n",
      "Epoch 602/1000, Training Loss: 24.32546, Validation Loss: 3.49472\n",
      "Epoch 603/1000, Training Loss: 24.32076, Validation Loss: 3.49383\n",
      "Epoch 604/1000, Training Loss: 24.31606, Validation Loss: 3.49294\n",
      "Epoch 605/1000, Training Loss: 24.31137, Validation Loss: 3.49205\n",
      "Epoch 606/1000, Training Loss: 24.30667, Validation Loss: 3.49116\n",
      "Epoch 607/1000, Training Loss: 24.30197, Validation Loss: 3.49028\n",
      "Epoch 608/1000, Training Loss: 24.29727, Validation Loss: 3.48939\n",
      "Epoch 609/1000, Training Loss: 24.29258, Validation Loss: 3.48850\n",
      "Epoch 610/1000, Training Loss: 24.28788, Validation Loss: 3.48761\n",
      "Epoch 611/1000, Training Loss: 24.28319, Validation Loss: 3.48672\n",
      "Epoch 612/1000, Training Loss: 24.27850, Validation Loss: 3.48584\n",
      "Epoch 613/1000, Training Loss: 24.27381, Validation Loss: 3.48495\n",
      "Epoch 614/1000, Training Loss: 24.26912, Validation Loss: 3.48406\n",
      "Epoch 615/1000, Training Loss: 24.26443, Validation Loss: 3.48317\n",
      "Epoch 616/1000, Training Loss: 24.25974, Validation Loss: 3.48228\n",
      "Epoch 617/1000, Training Loss: 24.25506, Validation Loss: 3.48140\n",
      "Epoch 618/1000, Training Loss: 24.25037, Validation Loss: 3.48051\n",
      "Epoch 619/1000, Training Loss: 24.24569, Validation Loss: 3.47962\n",
      "Epoch 620/1000, Training Loss: 24.24100, Validation Loss: 3.47874\n",
      "Epoch 621/1000, Training Loss: 24.23632, Validation Loss: 3.47785\n",
      "Epoch 622/1000, Training Loss: 24.23163, Validation Loss: 3.47696\n",
      "Epoch 623/1000, Training Loss: 24.22695, Validation Loss: 3.47608\n",
      "Epoch 624/1000, Training Loss: 24.22227, Validation Loss: 3.47519\n",
      "Epoch 625/1000, Training Loss: 24.21759, Validation Loss: 3.47430\n",
      "Epoch 626/1000, Training Loss: 24.21292, Validation Loss: 3.47342\n",
      "Epoch 627/1000, Training Loss: 24.20824, Validation Loss: 3.47253\n",
      "Epoch 628/1000, Training Loss: 24.20357, Validation Loss: 3.47165\n",
      "Epoch 629/1000, Training Loss: 24.19890, Validation Loss: 3.47076\n",
      "Epoch 630/1000, Training Loss: 24.19423, Validation Loss: 3.46988\n",
      "Epoch 631/1000, Training Loss: 24.18956, Validation Loss: 3.46899\n",
      "Epoch 632/1000, Training Loss: 24.18489, Validation Loss: 3.46811\n",
      "Epoch 633/1000, Training Loss: 24.18022, Validation Loss: 3.46722\n",
      "Epoch 634/1000, Training Loss: 24.17555, Validation Loss: 3.46633\n",
      "Epoch 635/1000, Training Loss: 24.17088, Validation Loss: 3.46545\n",
      "Epoch 636/1000, Training Loss: 24.16621, Validation Loss: 3.46456\n",
      "Epoch 637/1000, Training Loss: 24.16155, Validation Loss: 3.46368\n",
      "Epoch 638/1000, Training Loss: 24.15688, Validation Loss: 3.46279\n",
      "Epoch 639/1000, Training Loss: 24.15222, Validation Loss: 3.46191\n",
      "Epoch 640/1000, Training Loss: 24.14756, Validation Loss: 3.46102\n",
      "Epoch 641/1000, Training Loss: 24.14291, Validation Loss: 3.46014\n",
      "Epoch 642/1000, Training Loss: 24.13825, Validation Loss: 3.45926\n",
      "Epoch 643/1000, Training Loss: 24.13360, Validation Loss: 3.45837\n",
      "Epoch 644/1000, Training Loss: 24.12895, Validation Loss: 3.45749\n",
      "Epoch 645/1000, Training Loss: 24.12430, Validation Loss: 3.45660\n",
      "Epoch 646/1000, Training Loss: 24.11965, Validation Loss: 3.45572\n",
      "Epoch 647/1000, Training Loss: 24.11500, Validation Loss: 3.45484\n",
      "Epoch 648/1000, Training Loss: 24.11036, Validation Loss: 3.45396\n",
      "Epoch 649/1000, Training Loss: 24.10572, Validation Loss: 3.45307\n",
      "Epoch 650/1000, Training Loss: 24.10107, Validation Loss: 3.45219\n",
      "Epoch 651/1000, Training Loss: 24.09643, Validation Loss: 3.45130\n",
      "Epoch 652/1000, Training Loss: 24.09179, Validation Loss: 3.45042\n",
      "Epoch 653/1000, Training Loss: 24.08715, Validation Loss: 3.44954\n",
      "Epoch 654/1000, Training Loss: 24.08251, Validation Loss: 3.44866\n",
      "Epoch 655/1000, Training Loss: 24.07788, Validation Loss: 3.44777\n",
      "Epoch 656/1000, Training Loss: 24.07325, Validation Loss: 3.44689\n",
      "Epoch 657/1000, Training Loss: 24.06862, Validation Loss: 3.44601\n",
      "Epoch 658/1000, Training Loss: 24.06400, Validation Loss: 3.44513\n",
      "Epoch 659/1000, Training Loss: 24.05938, Validation Loss: 3.44425\n",
      "Epoch 660/1000, Training Loss: 24.05476, Validation Loss: 3.44337\n",
      "Epoch 661/1000, Training Loss: 24.05014, Validation Loss: 3.44248\n",
      "Epoch 662/1000, Training Loss: 24.04552, Validation Loss: 3.44160\n",
      "Epoch 663/1000, Training Loss: 24.04090, Validation Loss: 3.44072\n",
      "Epoch 664/1000, Training Loss: 24.03629, Validation Loss: 3.43984\n",
      "Epoch 665/1000, Training Loss: 24.03168, Validation Loss: 3.43896\n",
      "Epoch 666/1000, Training Loss: 24.02707, Validation Loss: 3.43808\n",
      "Epoch 667/1000, Training Loss: 24.02246, Validation Loss: 3.43720\n",
      "Epoch 668/1000, Training Loss: 24.01785, Validation Loss: 3.43632\n",
      "Epoch 669/1000, Training Loss: 24.01324, Validation Loss: 3.43545\n",
      "Epoch 670/1000, Training Loss: 24.00864, Validation Loss: 3.43457\n",
      "Epoch 671/1000, Training Loss: 24.00404, Validation Loss: 3.43369\n",
      "Epoch 672/1000, Training Loss: 23.99944, Validation Loss: 3.43281\n",
      "Epoch 673/1000, Training Loss: 23.99484, Validation Loss: 3.43193\n",
      "Epoch 674/1000, Training Loss: 23.99026, Validation Loss: 3.43106\n",
      "Epoch 675/1000, Training Loss: 23.98567, Validation Loss: 3.43018\n",
      "Epoch 676/1000, Training Loss: 23.98109, Validation Loss: 3.42931\n",
      "Epoch 677/1000, Training Loss: 23.97651, Validation Loss: 3.42843\n",
      "Epoch 678/1000, Training Loss: 23.97193, Validation Loss: 3.42756\n",
      "Epoch 679/1000, Training Loss: 23.96735, Validation Loss: 3.42668\n",
      "Epoch 680/1000, Training Loss: 23.96278, Validation Loss: 3.42581\n",
      "Epoch 681/1000, Training Loss: 23.95820, Validation Loss: 3.42494\n",
      "Epoch 682/1000, Training Loss: 23.95363, Validation Loss: 3.42406\n",
      "Epoch 683/1000, Training Loss: 23.94906, Validation Loss: 3.42319\n",
      "Epoch 684/1000, Training Loss: 23.94449, Validation Loss: 3.42232\n",
      "Epoch 685/1000, Training Loss: 23.93993, Validation Loss: 3.42144\n",
      "Epoch 686/1000, Training Loss: 23.93537, Validation Loss: 3.42057\n",
      "Epoch 687/1000, Training Loss: 23.93082, Validation Loss: 3.41970\n",
      "Epoch 688/1000, Training Loss: 23.92627, Validation Loss: 3.41883\n",
      "Epoch 689/1000, Training Loss: 23.92172, Validation Loss: 3.41797\n",
      "Epoch 690/1000, Training Loss: 23.91718, Validation Loss: 3.41710\n",
      "Epoch 691/1000, Training Loss: 23.91264, Validation Loss: 3.41623\n",
      "Epoch 692/1000, Training Loss: 23.90810, Validation Loss: 3.41536\n",
      "Epoch 693/1000, Training Loss: 23.90357, Validation Loss: 3.41449\n",
      "Epoch 694/1000, Training Loss: 23.89903, Validation Loss: 3.41362\n",
      "Epoch 695/1000, Training Loss: 23.89450, Validation Loss: 3.41276\n",
      "Epoch 696/1000, Training Loss: 23.88997, Validation Loss: 3.41189\n",
      "Epoch 697/1000, Training Loss: 23.88545, Validation Loss: 3.41102\n",
      "Epoch 698/1000, Training Loss: 23.88092, Validation Loss: 3.41015\n",
      "Epoch 699/1000, Training Loss: 23.87640, Validation Loss: 3.40929\n",
      "Epoch 700/1000, Training Loss: 23.87187, Validation Loss: 3.40842\n",
      "Epoch 701/1000, Training Loss: 23.86735, Validation Loss: 3.40755\n",
      "Epoch 702/1000, Training Loss: 23.86283, Validation Loss: 3.40669\n",
      "Epoch 703/1000, Training Loss: 23.85832, Validation Loss: 3.40582\n",
      "Epoch 704/1000, Training Loss: 23.85380, Validation Loss: 3.40495\n",
      "Epoch 705/1000, Training Loss: 23.84929, Validation Loss: 3.40409\n",
      "Epoch 706/1000, Training Loss: 23.84478, Validation Loss: 3.40322\n",
      "Epoch 707/1000, Training Loss: 23.84027, Validation Loss: 3.40236\n",
      "Epoch 708/1000, Training Loss: 23.83576, Validation Loss: 3.40149\n",
      "Epoch 709/1000, Training Loss: 23.83126, Validation Loss: 3.40063\n",
      "Epoch 710/1000, Training Loss: 23.82677, Validation Loss: 3.39976\n",
      "Epoch 711/1000, Training Loss: 23.82228, Validation Loss: 3.39890\n",
      "Epoch 712/1000, Training Loss: 23.81779, Validation Loss: 3.39803\n",
      "Epoch 713/1000, Training Loss: 23.81331, Validation Loss: 3.39717\n",
      "Epoch 714/1000, Training Loss: 23.80883, Validation Loss: 3.39631\n",
      "Epoch 715/1000, Training Loss: 23.80435, Validation Loss: 3.39544\n",
      "Epoch 716/1000, Training Loss: 23.79986, Validation Loss: 3.39458\n",
      "Epoch 717/1000, Training Loss: 23.79538, Validation Loss: 3.39372\n",
      "Epoch 718/1000, Training Loss: 23.79091, Validation Loss: 3.39286\n",
      "Epoch 719/1000, Training Loss: 23.78643, Validation Loss: 3.39199\n",
      "Epoch 720/1000, Training Loss: 23.78196, Validation Loss: 3.39113\n",
      "Epoch 721/1000, Training Loss: 23.77748, Validation Loss: 3.39027\n",
      "Epoch 722/1000, Training Loss: 23.77301, Validation Loss: 3.38941\n",
      "Epoch 723/1000, Training Loss: 23.76854, Validation Loss: 3.38855\n",
      "Epoch 724/1000, Training Loss: 23.76407, Validation Loss: 3.38768\n",
      "Epoch 725/1000, Training Loss: 23.75961, Validation Loss: 3.38682\n",
      "Epoch 726/1000, Training Loss: 23.75514, Validation Loss: 3.38596\n",
      "Epoch 727/1000, Training Loss: 23.75067, Validation Loss: 3.38510\n",
      "Epoch 728/1000, Training Loss: 23.74621, Validation Loss: 3.38424\n",
      "Epoch 729/1000, Training Loss: 23.74175, Validation Loss: 3.38337\n",
      "Epoch 730/1000, Training Loss: 23.73729, Validation Loss: 3.38251\n",
      "Epoch 731/1000, Training Loss: 23.73283, Validation Loss: 3.38165\n",
      "Epoch 732/1000, Training Loss: 23.72838, Validation Loss: 3.38079\n",
      "Epoch 733/1000, Training Loss: 23.72393, Validation Loss: 3.37993\n",
      "Epoch 734/1000, Training Loss: 23.71948, Validation Loss: 3.37907\n",
      "Epoch 735/1000, Training Loss: 23.71504, Validation Loss: 3.37821\n",
      "Epoch 736/1000, Training Loss: 23.71060, Validation Loss: 3.37735\n",
      "Epoch 737/1000, Training Loss: 23.70617, Validation Loss: 3.37649\n",
      "Epoch 738/1000, Training Loss: 23.70173, Validation Loss: 3.37564\n",
      "Epoch 739/1000, Training Loss: 23.69731, Validation Loss: 3.37478\n",
      "Epoch 740/1000, Training Loss: 23.69288, Validation Loss: 3.37392\n",
      "Epoch 741/1000, Training Loss: 23.68845, Validation Loss: 3.37307\n",
      "Epoch 742/1000, Training Loss: 23.68403, Validation Loss: 3.37221\n",
      "Epoch 743/1000, Training Loss: 23.67961, Validation Loss: 3.37135\n",
      "Epoch 744/1000, Training Loss: 23.67519, Validation Loss: 3.37050\n",
      "Epoch 745/1000, Training Loss: 23.67077, Validation Loss: 3.36964\n",
      "Epoch 746/1000, Training Loss: 23.66635, Validation Loss: 3.36878\n",
      "Epoch 747/1000, Training Loss: 23.66194, Validation Loss: 3.36793\n",
      "Epoch 748/1000, Training Loss: 23.65753, Validation Loss: 3.36707\n",
      "Epoch 749/1000, Training Loss: 23.65312, Validation Loss: 3.36622\n",
      "Epoch 750/1000, Training Loss: 23.64872, Validation Loss: 3.36537\n",
      "Epoch 751/1000, Training Loss: 23.64432, Validation Loss: 3.36451\n",
      "Epoch 752/1000, Training Loss: 23.63991, Validation Loss: 3.36366\n",
      "Epoch 753/1000, Training Loss: 23.63552, Validation Loss: 3.36281\n",
      "Epoch 754/1000, Training Loss: 23.63112, Validation Loss: 3.36195\n",
      "Epoch 755/1000, Training Loss: 23.62673, Validation Loss: 3.36110\n",
      "Epoch 756/1000, Training Loss: 23.62234, Validation Loss: 3.36025\n",
      "Epoch 757/1000, Training Loss: 23.61795, Validation Loss: 3.35940\n",
      "Epoch 758/1000, Training Loss: 23.61356, Validation Loss: 3.35855\n",
      "Epoch 759/1000, Training Loss: 23.60917, Validation Loss: 3.35769\n",
      "Epoch 760/1000, Training Loss: 23.60478, Validation Loss: 3.35684\n",
      "Epoch 761/1000, Training Loss: 23.60040, Validation Loss: 3.35599\n",
      "Epoch 762/1000, Training Loss: 23.59602, Validation Loss: 3.35514\n",
      "Epoch 763/1000, Training Loss: 23.59164, Validation Loss: 3.35429\n",
      "Epoch 764/1000, Training Loss: 23.58726, Validation Loss: 3.35344\n",
      "Epoch 765/1000, Training Loss: 23.58289, Validation Loss: 3.35259\n",
      "Epoch 766/1000, Training Loss: 23.57851, Validation Loss: 3.35174\n",
      "Epoch 767/1000, Training Loss: 23.57414, Validation Loss: 3.35089\n",
      "Epoch 768/1000, Training Loss: 23.56977, Validation Loss: 3.35005\n",
      "Epoch 769/1000, Training Loss: 23.56540, Validation Loss: 3.34920\n",
      "Epoch 770/1000, Training Loss: 23.56104, Validation Loss: 3.34835\n",
      "Epoch 771/1000, Training Loss: 23.55669, Validation Loss: 3.34751\n",
      "Epoch 772/1000, Training Loss: 23.55233, Validation Loss: 3.34666\n",
      "Epoch 773/1000, Training Loss: 23.54798, Validation Loss: 3.34582\n",
      "Epoch 774/1000, Training Loss: 23.54363, Validation Loss: 3.34497\n",
      "Epoch 775/1000, Training Loss: 23.53929, Validation Loss: 3.34413\n",
      "Epoch 776/1000, Training Loss: 23.53495, Validation Loss: 3.34328\n",
      "Epoch 777/1000, Training Loss: 23.53062, Validation Loss: 3.34244\n",
      "Epoch 778/1000, Training Loss: 23.52628, Validation Loss: 3.34160\n",
      "Epoch 779/1000, Training Loss: 23.52195, Validation Loss: 3.34076\n",
      "Epoch 780/1000, Training Loss: 23.51763, Validation Loss: 3.33992\n",
      "Epoch 781/1000, Training Loss: 23.51330, Validation Loss: 3.33907\n",
      "Epoch 782/1000, Training Loss: 23.50898, Validation Loss: 3.33823\n",
      "Epoch 783/1000, Training Loss: 23.50466, Validation Loss: 3.33739\n",
      "Epoch 784/1000, Training Loss: 23.50035, Validation Loss: 3.33655\n",
      "Epoch 785/1000, Training Loss: 23.49603, Validation Loss: 3.33571\n",
      "Epoch 786/1000, Training Loss: 23.49172, Validation Loss: 3.33488\n",
      "Epoch 787/1000, Training Loss: 23.48741, Validation Loss: 3.33404\n",
      "Epoch 788/1000, Training Loss: 23.48311, Validation Loss: 3.33320\n",
      "Epoch 789/1000, Training Loss: 23.47881, Validation Loss: 3.33236\n",
      "Epoch 790/1000, Training Loss: 23.47452, Validation Loss: 3.33152\n",
      "Epoch 791/1000, Training Loss: 23.47023, Validation Loss: 3.33069\n",
      "Epoch 792/1000, Training Loss: 23.46594, Validation Loss: 3.32985\n",
      "Epoch 793/1000, Training Loss: 23.46166, Validation Loss: 3.32902\n",
      "Epoch 794/1000, Training Loss: 23.45737, Validation Loss: 3.32818\n",
      "Epoch 795/1000, Training Loss: 23.45310, Validation Loss: 3.32735\n",
      "Epoch 796/1000, Training Loss: 23.44882, Validation Loss: 3.32651\n",
      "Epoch 797/1000, Training Loss: 23.44455, Validation Loss: 3.32568\n",
      "Epoch 798/1000, Training Loss: 23.44028, Validation Loss: 3.32485\n",
      "Epoch 799/1000, Training Loss: 23.43601, Validation Loss: 3.32402\n",
      "Epoch 800/1000, Training Loss: 23.43174, Validation Loss: 3.32318\n",
      "Epoch 801/1000, Training Loss: 23.42748, Validation Loss: 3.32235\n",
      "Epoch 802/1000, Training Loss: 23.42322, Validation Loss: 3.32152\n",
      "Epoch 803/1000, Training Loss: 23.41896, Validation Loss: 3.32069\n",
      "Epoch 804/1000, Training Loss: 23.41470, Validation Loss: 3.31986\n",
      "Epoch 805/1000, Training Loss: 23.41044, Validation Loss: 3.31903\n",
      "Epoch 806/1000, Training Loss: 23.40618, Validation Loss: 3.31820\n",
      "Epoch 807/1000, Training Loss: 23.40193, Validation Loss: 3.31737\n",
      "Epoch 808/1000, Training Loss: 23.39768, Validation Loss: 3.31654\n",
      "Epoch 809/1000, Training Loss: 23.39343, Validation Loss: 3.31571\n",
      "Epoch 810/1000, Training Loss: 23.38918, Validation Loss: 3.31488\n",
      "Epoch 811/1000, Training Loss: 23.38494, Validation Loss: 3.31406\n",
      "Epoch 812/1000, Training Loss: 23.38070, Validation Loss: 3.31323\n",
      "Epoch 813/1000, Training Loss: 23.37646, Validation Loss: 3.31240\n",
      "Epoch 814/1000, Training Loss: 23.37223, Validation Loss: 3.31158\n",
      "Epoch 815/1000, Training Loss: 23.36800, Validation Loss: 3.31075\n",
      "Epoch 816/1000, Training Loss: 23.36377, Validation Loss: 3.30993\n",
      "Epoch 817/1000, Training Loss: 23.35954, Validation Loss: 3.30910\n",
      "Epoch 818/1000, Training Loss: 23.35532, Validation Loss: 3.30828\n",
      "Epoch 819/1000, Training Loss: 23.35110, Validation Loss: 3.30745\n",
      "Epoch 820/1000, Training Loss: 23.34689, Validation Loss: 3.30663\n",
      "Epoch 821/1000, Training Loss: 23.34268, Validation Loss: 3.30581\n",
      "Epoch 822/1000, Training Loss: 23.33847, Validation Loss: 3.30498\n",
      "Epoch 823/1000, Training Loss: 23.33427, Validation Loss: 3.30416\n",
      "Epoch 824/1000, Training Loss: 23.33007, Validation Loss: 3.30334\n",
      "Epoch 825/1000, Training Loss: 23.32587, Validation Loss: 3.30252\n",
      "Epoch 826/1000, Training Loss: 23.32167, Validation Loss: 3.30170\n",
      "Epoch 827/1000, Training Loss: 23.31748, Validation Loss: 3.30088\n",
      "Epoch 828/1000, Training Loss: 23.31329, Validation Loss: 3.30006\n",
      "Epoch 829/1000, Training Loss: 23.30910, Validation Loss: 3.29924\n",
      "Epoch 830/1000, Training Loss: 23.30492, Validation Loss: 3.29842\n",
      "Epoch 831/1000, Training Loss: 23.30074, Validation Loss: 3.29760\n",
      "Epoch 832/1000, Training Loss: 23.29656, Validation Loss: 3.29679\n",
      "Epoch 833/1000, Training Loss: 23.29239, Validation Loss: 3.29597\n",
      "Epoch 834/1000, Training Loss: 23.28823, Validation Loss: 3.29515\n",
      "Epoch 835/1000, Training Loss: 23.28406, Validation Loss: 3.29434\n",
      "Epoch 836/1000, Training Loss: 23.27990, Validation Loss: 3.29352\n",
      "Epoch 837/1000, Training Loss: 23.27574, Validation Loss: 3.29271\n",
      "Epoch 838/1000, Training Loss: 23.27158, Validation Loss: 3.29189\n",
      "Epoch 839/1000, Training Loss: 23.26742, Validation Loss: 3.29108\n",
      "Epoch 840/1000, Training Loss: 23.26327, Validation Loss: 3.29026\n",
      "Epoch 841/1000, Training Loss: 23.25912, Validation Loss: 3.28945\n",
      "Epoch 842/1000, Training Loss: 23.25497, Validation Loss: 3.28864\n",
      "Epoch 843/1000, Training Loss: 23.25082, Validation Loss: 3.28783\n",
      "Epoch 844/1000, Training Loss: 23.24668, Validation Loss: 3.28701\n",
      "Epoch 845/1000, Training Loss: 23.24254, Validation Loss: 3.28620\n",
      "Epoch 846/1000, Training Loss: 23.23840, Validation Loss: 3.28539\n",
      "Epoch 847/1000, Training Loss: 23.23427, Validation Loss: 3.28458\n",
      "Epoch 848/1000, Training Loss: 23.23014, Validation Loss: 3.28377\n",
      "Epoch 849/1000, Training Loss: 23.22601, Validation Loss: 3.28296\n",
      "Epoch 850/1000, Training Loss: 23.22189, Validation Loss: 3.28215\n",
      "Epoch 851/1000, Training Loss: 23.21777, Validation Loss: 3.28135\n",
      "Epoch 852/1000, Training Loss: 23.21365, Validation Loss: 3.28054\n",
      "Epoch 853/1000, Training Loss: 23.20953, Validation Loss: 3.27973\n",
      "Epoch 854/1000, Training Loss: 23.20542, Validation Loss: 3.27892\n",
      "Epoch 855/1000, Training Loss: 23.20131, Validation Loss: 3.27812\n",
      "Epoch 856/1000, Training Loss: 23.19721, Validation Loss: 3.27731\n",
      "Epoch 857/1000, Training Loss: 23.19310, Validation Loss: 3.27650\n",
      "Epoch 858/1000, Training Loss: 23.18900, Validation Loss: 3.27570\n",
      "Epoch 859/1000, Training Loss: 23.18491, Validation Loss: 3.27489\n",
      "Epoch 860/1000, Training Loss: 23.18082, Validation Loss: 3.27409\n",
      "Epoch 861/1000, Training Loss: 23.17673, Validation Loss: 3.27328\n",
      "Epoch 862/1000, Training Loss: 23.17265, Validation Loss: 3.27248\n",
      "Epoch 863/1000, Training Loss: 23.16857, Validation Loss: 3.27168\n",
      "Epoch 864/1000, Training Loss: 23.16449, Validation Loss: 3.27087\n",
      "Epoch 865/1000, Training Loss: 23.16042, Validation Loss: 3.27007\n",
      "Epoch 866/1000, Training Loss: 23.15635, Validation Loss: 3.26927\n",
      "Epoch 867/1000, Training Loss: 23.15229, Validation Loss: 3.26847\n",
      "Epoch 868/1000, Training Loss: 23.14823, Validation Loss: 3.26767\n",
      "Epoch 869/1000, Training Loss: 23.14418, Validation Loss: 3.26686\n",
      "Epoch 870/1000, Training Loss: 23.14013, Validation Loss: 3.26607\n",
      "Epoch 871/1000, Training Loss: 23.13608, Validation Loss: 3.26527\n",
      "Epoch 872/1000, Training Loss: 23.13204, Validation Loss: 3.26447\n",
      "Epoch 873/1000, Training Loss: 23.12800, Validation Loss: 3.26367\n",
      "Epoch 874/1000, Training Loss: 23.12396, Validation Loss: 3.26287\n",
      "Epoch 875/1000, Training Loss: 23.11993, Validation Loss: 3.26207\n",
      "Epoch 876/1000, Training Loss: 23.11590, Validation Loss: 3.26128\n",
      "Epoch 877/1000, Training Loss: 23.11187, Validation Loss: 3.26048\n",
      "Epoch 878/1000, Training Loss: 23.10785, Validation Loss: 3.25969\n",
      "Epoch 879/1000, Training Loss: 23.10383, Validation Loss: 3.25889\n",
      "Epoch 880/1000, Training Loss: 23.09982, Validation Loss: 3.25810\n",
      "Epoch 881/1000, Training Loss: 23.09580, Validation Loss: 3.25731\n",
      "Epoch 882/1000, Training Loss: 23.09179, Validation Loss: 3.25651\n",
      "Epoch 883/1000, Training Loss: 23.08779, Validation Loss: 3.25572\n",
      "Epoch 884/1000, Training Loss: 23.08378, Validation Loss: 3.25493\n",
      "Epoch 885/1000, Training Loss: 23.07978, Validation Loss: 3.25414\n",
      "Epoch 886/1000, Training Loss: 23.07579, Validation Loss: 3.25335\n",
      "Epoch 887/1000, Training Loss: 23.07179, Validation Loss: 3.25256\n",
      "Epoch 888/1000, Training Loss: 23.06780, Validation Loss: 3.25177\n",
      "Epoch 889/1000, Training Loss: 23.06381, Validation Loss: 3.25098\n",
      "Epoch 890/1000, Training Loss: 23.05983, Validation Loss: 3.25019\n",
      "Epoch 891/1000, Training Loss: 23.05585, Validation Loss: 3.24941\n",
      "Epoch 892/1000, Training Loss: 23.05188, Validation Loss: 3.24862\n",
      "Epoch 893/1000, Training Loss: 23.04791, Validation Loss: 3.24784\n",
      "Epoch 894/1000, Training Loss: 23.04394, Validation Loss: 3.24705\n",
      "Epoch 895/1000, Training Loss: 23.03997, Validation Loss: 3.24627\n",
      "Epoch 896/1000, Training Loss: 23.03601, Validation Loss: 3.24548\n",
      "Epoch 897/1000, Training Loss: 23.03205, Validation Loss: 3.24470\n",
      "Epoch 898/1000, Training Loss: 23.02809, Validation Loss: 3.24392\n",
      "Epoch 899/1000, Training Loss: 23.02414, Validation Loss: 3.24314\n",
      "Epoch 900/1000, Training Loss: 23.02018, Validation Loss: 3.24236\n",
      "Epoch 901/1000, Training Loss: 23.01624, Validation Loss: 3.24157\n",
      "Epoch 902/1000, Training Loss: 23.01230, Validation Loss: 3.24079\n",
      "Epoch 903/1000, Training Loss: 23.00836, Validation Loss: 3.24001\n",
      "Epoch 904/1000, Training Loss: 23.00442, Validation Loss: 3.23924\n",
      "Epoch 905/1000, Training Loss: 23.00050, Validation Loss: 3.23846\n",
      "Epoch 906/1000, Training Loss: 22.99658, Validation Loss: 3.23768\n",
      "Epoch 907/1000, Training Loss: 22.99266, Validation Loss: 3.23690\n",
      "Epoch 908/1000, Training Loss: 22.98874, Validation Loss: 3.23613\n",
      "Epoch 909/1000, Training Loss: 22.98484, Validation Loss: 3.23535\n",
      "Epoch 910/1000, Training Loss: 22.98093, Validation Loss: 3.23458\n",
      "Epoch 911/1000, Training Loss: 22.97703, Validation Loss: 3.23381\n",
      "Epoch 912/1000, Training Loss: 22.97313, Validation Loss: 3.23303\n",
      "Epoch 913/1000, Training Loss: 22.96924, Validation Loss: 3.23226\n",
      "Epoch 914/1000, Training Loss: 22.96534, Validation Loss: 3.23149\n",
      "Epoch 915/1000, Training Loss: 22.96145, Validation Loss: 3.23071\n",
      "Epoch 916/1000, Training Loss: 22.95757, Validation Loss: 3.22994\n",
      "Epoch 917/1000, Training Loss: 22.95368, Validation Loss: 3.22917\n",
      "Epoch 918/1000, Training Loss: 22.94980, Validation Loss: 3.22840\n",
      "Epoch 919/1000, Training Loss: 22.94592, Validation Loss: 3.22763\n",
      "Epoch 920/1000, Training Loss: 22.94205, Validation Loss: 3.22686\n",
      "Epoch 921/1000, Training Loss: 22.93817, Validation Loss: 3.22609\n",
      "Epoch 922/1000, Training Loss: 22.93431, Validation Loss: 3.22532\n",
      "Epoch 923/1000, Training Loss: 22.93044, Validation Loss: 3.22456\n",
      "Epoch 924/1000, Training Loss: 22.92658, Validation Loss: 3.22379\n",
      "Epoch 925/1000, Training Loss: 22.92272, Validation Loss: 3.22302\n",
      "Epoch 926/1000, Training Loss: 22.91886, Validation Loss: 3.22226\n",
      "Epoch 927/1000, Training Loss: 22.91501, Validation Loss: 3.22149\n",
      "Epoch 928/1000, Training Loss: 22.91116, Validation Loss: 3.22072\n",
      "Epoch 929/1000, Training Loss: 22.90732, Validation Loss: 3.21996\n",
      "Epoch 930/1000, Training Loss: 22.90347, Validation Loss: 3.21919\n",
      "Epoch 931/1000, Training Loss: 22.89963, Validation Loss: 3.21843\n",
      "Epoch 932/1000, Training Loss: 22.89580, Validation Loss: 3.21767\n",
      "Epoch 933/1000, Training Loss: 22.89197, Validation Loss: 3.21690\n",
      "Epoch 934/1000, Training Loss: 22.88813, Validation Loss: 3.21614\n",
      "Epoch 935/1000, Training Loss: 22.88431, Validation Loss: 3.21538\n",
      "Epoch 936/1000, Training Loss: 22.88048, Validation Loss: 3.21462\n",
      "Epoch 937/1000, Training Loss: 22.87666, Validation Loss: 3.21386\n",
      "Epoch 938/1000, Training Loss: 22.87285, Validation Loss: 3.21310\n",
      "Epoch 939/1000, Training Loss: 22.86903, Validation Loss: 3.21234\n",
      "Epoch 940/1000, Training Loss: 22.86522, Validation Loss: 3.21158\n",
      "Epoch 941/1000, Training Loss: 22.86142, Validation Loss: 3.21082\n",
      "Epoch 942/1000, Training Loss: 22.85761, Validation Loss: 3.21006\n",
      "Epoch 943/1000, Training Loss: 22.85381, Validation Loss: 3.20930\n",
      "Epoch 944/1000, Training Loss: 22.85002, Validation Loss: 3.20854\n",
      "Epoch 945/1000, Training Loss: 22.84623, Validation Loss: 3.20779\n",
      "Epoch 946/1000, Training Loss: 22.84244, Validation Loss: 3.20703\n",
      "Epoch 947/1000, Training Loss: 22.83864, Validation Loss: 3.20627\n",
      "Epoch 948/1000, Training Loss: 22.83486, Validation Loss: 3.20552\n",
      "Epoch 949/1000, Training Loss: 22.83107, Validation Loss: 3.20476\n",
      "Epoch 950/1000, Training Loss: 22.82729, Validation Loss: 3.20400\n",
      "Epoch 951/1000, Training Loss: 22.82350, Validation Loss: 3.20325\n",
      "Epoch 952/1000, Training Loss: 22.81973, Validation Loss: 3.20249\n",
      "Epoch 953/1000, Training Loss: 22.81596, Validation Loss: 3.20174\n",
      "Epoch 954/1000, Training Loss: 22.81219, Validation Loss: 3.20098\n",
      "Epoch 955/1000, Training Loss: 22.80843, Validation Loss: 3.20023\n",
      "Epoch 956/1000, Training Loss: 22.80467, Validation Loss: 3.19948\n",
      "Epoch 957/1000, Training Loss: 22.80091, Validation Loss: 3.19872\n",
      "Epoch 958/1000, Training Loss: 22.79716, Validation Loss: 3.19797\n",
      "Epoch 959/1000, Training Loss: 22.79341, Validation Loss: 3.19722\n",
      "Epoch 960/1000, Training Loss: 22.78967, Validation Loss: 3.19647\n",
      "Epoch 961/1000, Training Loss: 22.78592, Validation Loss: 3.19572\n",
      "Epoch 962/1000, Training Loss: 22.78218, Validation Loss: 3.19497\n",
      "Epoch 963/1000, Training Loss: 22.77844, Validation Loss: 3.19422\n",
      "Epoch 964/1000, Training Loss: 22.77471, Validation Loss: 3.19347\n",
      "Epoch 965/1000, Training Loss: 22.77098, Validation Loss: 3.19272\n",
      "Epoch 966/1000, Training Loss: 22.76725, Validation Loss: 3.19197\n",
      "Epoch 967/1000, Training Loss: 22.76352, Validation Loss: 3.19122\n",
      "Epoch 968/1000, Training Loss: 22.75980, Validation Loss: 3.19048\n",
      "Epoch 969/1000, Training Loss: 22.75608, Validation Loss: 3.18973\n",
      "Epoch 970/1000, Training Loss: 22.75236, Validation Loss: 3.18898\n",
      "Epoch 971/1000, Training Loss: 22.74865, Validation Loss: 3.18824\n",
      "Epoch 972/1000, Training Loss: 22.74494, Validation Loss: 3.18750\n",
      "Epoch 973/1000, Training Loss: 22.74123, Validation Loss: 3.18675\n",
      "Epoch 974/1000, Training Loss: 22.73753, Validation Loss: 3.18601\n",
      "Epoch 975/1000, Training Loss: 22.73383, Validation Loss: 3.18527\n",
      "Epoch 976/1000, Training Loss: 22.73014, Validation Loss: 3.18452\n",
      "Epoch 977/1000, Training Loss: 22.72645, Validation Loss: 3.18378\n",
      "Epoch 978/1000, Training Loss: 22.72276, Validation Loss: 3.18304\n",
      "Epoch 979/1000, Training Loss: 22.71908, Validation Loss: 3.18230\n",
      "Epoch 980/1000, Training Loss: 22.71540, Validation Loss: 3.18156\n",
      "Epoch 981/1000, Training Loss: 22.71173, Validation Loss: 3.18083\n",
      "Epoch 982/1000, Training Loss: 22.70806, Validation Loss: 3.18009\n",
      "Epoch 983/1000, Training Loss: 22.70439, Validation Loss: 3.17935\n",
      "Epoch 984/1000, Training Loss: 22.70073, Validation Loss: 3.17861\n",
      "Epoch 985/1000, Training Loss: 22.69707, Validation Loss: 3.17788\n",
      "Epoch 986/1000, Training Loss: 22.69341, Validation Loss: 3.17714\n",
      "Epoch 987/1000, Training Loss: 22.68976, Validation Loss: 3.17641\n",
      "Epoch 988/1000, Training Loss: 22.68611, Validation Loss: 3.17567\n",
      "Epoch 989/1000, Training Loss: 22.68246, Validation Loss: 3.17494\n",
      "Epoch 990/1000, Training Loss: 22.67882, Validation Loss: 3.17421\n",
      "Epoch 991/1000, Training Loss: 22.67518, Validation Loss: 3.17347\n",
      "Epoch 992/1000, Training Loss: 22.67155, Validation Loss: 3.17274\n",
      "Epoch 993/1000, Training Loss: 22.66792, Validation Loss: 3.17201\n",
      "Epoch 994/1000, Training Loss: 22.66429, Validation Loss: 3.17128\n",
      "Epoch 995/1000, Training Loss: 22.66067, Validation Loss: 3.17055\n",
      "Epoch 996/1000, Training Loss: 22.65706, Validation Loss: 3.16982\n",
      "Epoch 997/1000, Training Loss: 22.65345, Validation Loss: 3.16909\n",
      "Epoch 998/1000, Training Loss: 22.64984, Validation Loss: 3.16837\n",
      "Epoch 999/1000, Training Loss: 22.64624, Validation Loss: 3.16764\n",
      "Epoch 1000/1000, Training Loss: 22.64264, Validation Loss: 3.16691\n",
      "Training took: 60.48 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_batch_15_1 = NeuralNetwork().to(device)\n",
    "summary(model_batch_15_1, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 15\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_batch_15_1.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_batch_15_1=[]\n",
    "val_loss_list_batch_15_1=[]\n",
    "train_accuracy_list_batch_15_1=[]\n",
    "val_accuracy_list_batch_15_1=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_batch_15_1.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_batch_15_1(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_batch_15_1.append(train_accuracy)\n",
    "    train_loss_list_batch_15_1.append(train_loss)\n",
    "\n",
    "    model_batch_15_1.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_batch_15_1(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_batch_15_1.append(val_accuracy)\n",
    "    val_accuracy_list_batch_15_1.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtclFYd4oCl-",
    "outputId": "8fbaf3d3-f873-48ef-a4fd-7a415ae99896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable batch size with batch size as 15: 0.7143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_batch_15_1.eval()\n",
    "test_predictions_batch_15_1 = model_batch_15_1(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_batch_15_1 = torch.round(test_predictions_batch_15_1)\n",
    "\n",
    "test_predictions_rounded_numpy_batch_15_1 = test_predictions_rounded_batch_15_1.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_batch_15_1 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_batch_15_1)\n",
    "\n",
    "print(f\"Accuracy for variable batch size with batch size as 15: {accuracy_batch_15_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QX5iiUpol_W",
    "outputId": "f496eefa-0344-42a1-f058-916d71f3575b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable batch size with batch size as 15: 0.55935\n"
     ]
    }
   ],
   "source": [
    "model_batch_15_1.eval()\n",
    "test_loss_batch_15_1=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_batch_15_1 = model_batch_15_1(X_test_tensor)\n",
    "    test_loss_batch_15_1 = loss_function(test_outputs_batch_15_1, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable batch size with batch size as 15: {test_loss_batch_15_1.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHAkyzQp1jU"
   },
   "source": [
    "Batch Size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAFUi8AUo1D9",
    "outputId": "532330be-80a0-4ed2-816d-48d8b62637e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 22.49111, Validation Loss: 3.68470\n",
      "Epoch 2/1000, Training Loss: 22.48345, Validation Loss: 3.68326\n",
      "Epoch 3/1000, Training Loss: 22.47583, Validation Loss: 3.68182\n",
      "Epoch 4/1000, Training Loss: 22.46822, Validation Loss: 3.68039\n",
      "Epoch 5/1000, Training Loss: 22.46064, Validation Loss: 3.67896\n",
      "Epoch 6/1000, Training Loss: 22.45309, Validation Loss: 3.67754\n",
      "Epoch 7/1000, Training Loss: 22.44556, Validation Loss: 3.67612\n",
      "Epoch 8/1000, Training Loss: 22.43805, Validation Loss: 3.67471\n",
      "Epoch 9/1000, Training Loss: 22.43056, Validation Loss: 3.67330\n",
      "Epoch 10/1000, Training Loss: 22.42310, Validation Loss: 3.67189\n",
      "Epoch 11/1000, Training Loss: 22.41566, Validation Loss: 3.67049\n",
      "Epoch 12/1000, Training Loss: 22.40825, Validation Loss: 3.66909\n",
      "Epoch 13/1000, Training Loss: 22.40086, Validation Loss: 3.66770\n",
      "Epoch 14/1000, Training Loss: 22.39350, Validation Loss: 3.66631\n",
      "Epoch 15/1000, Training Loss: 22.38617, Validation Loss: 3.66493\n",
      "Epoch 16/1000, Training Loss: 22.37886, Validation Loss: 3.66355\n",
      "Epoch 17/1000, Training Loss: 22.37157, Validation Loss: 3.66217\n",
      "Epoch 18/1000, Training Loss: 22.36430, Validation Loss: 3.66080\n",
      "Epoch 19/1000, Training Loss: 22.35706, Validation Loss: 3.65944\n",
      "Epoch 20/1000, Training Loss: 22.34985, Validation Loss: 3.65807\n",
      "Epoch 21/1000, Training Loss: 22.34267, Validation Loss: 3.65671\n",
      "Epoch 22/1000, Training Loss: 22.33551, Validation Loss: 3.65536\n",
      "Epoch 23/1000, Training Loss: 22.32837, Validation Loss: 3.65401\n",
      "Epoch 24/1000, Training Loss: 22.32126, Validation Loss: 3.65266\n",
      "Epoch 25/1000, Training Loss: 22.31418, Validation Loss: 3.65132\n",
      "Epoch 26/1000, Training Loss: 22.30712, Validation Loss: 3.64998\n",
      "Epoch 27/1000, Training Loss: 22.30008, Validation Loss: 3.64865\n",
      "Epoch 28/1000, Training Loss: 22.29308, Validation Loss: 3.64732\n",
      "Epoch 29/1000, Training Loss: 22.28610, Validation Loss: 3.64600\n",
      "Epoch 30/1000, Training Loss: 22.27915, Validation Loss: 3.64468\n",
      "Epoch 31/1000, Training Loss: 22.27221, Validation Loss: 3.64336\n",
      "Epoch 32/1000, Training Loss: 22.26530, Validation Loss: 3.64204\n",
      "Epoch 33/1000, Training Loss: 22.25840, Validation Loss: 3.64073\n",
      "Epoch 34/1000, Training Loss: 22.25154, Validation Loss: 3.63943\n",
      "Epoch 35/1000, Training Loss: 22.24469, Validation Loss: 3.63812\n",
      "Epoch 36/1000, Training Loss: 22.23786, Validation Loss: 3.63683\n",
      "Epoch 37/1000, Training Loss: 22.23105, Validation Loss: 3.63553\n",
      "Epoch 38/1000, Training Loss: 22.22427, Validation Loss: 3.63424\n",
      "Epoch 39/1000, Training Loss: 22.21751, Validation Loss: 3.63295\n",
      "Epoch 40/1000, Training Loss: 22.21076, Validation Loss: 3.63167\n",
      "Epoch 41/1000, Training Loss: 22.20404, Validation Loss: 3.63038\n",
      "Epoch 42/1000, Training Loss: 22.19734, Validation Loss: 3.62911\n",
      "Epoch 43/1000, Training Loss: 22.19066, Validation Loss: 3.62783\n",
      "Epoch 44/1000, Training Loss: 22.18400, Validation Loss: 3.62656\n",
      "Epoch 45/1000, Training Loss: 22.17736, Validation Loss: 3.62529\n",
      "Epoch 46/1000, Training Loss: 22.17075, Validation Loss: 3.62403\n",
      "Epoch 47/1000, Training Loss: 22.16415, Validation Loss: 3.62277\n",
      "Epoch 48/1000, Training Loss: 22.15758, Validation Loss: 3.62151\n",
      "Epoch 49/1000, Training Loss: 22.15102, Validation Loss: 3.62026\n",
      "Epoch 50/1000, Training Loss: 22.14449, Validation Loss: 3.61901\n",
      "Epoch 51/1000, Training Loss: 22.13797, Validation Loss: 3.61776\n",
      "Epoch 52/1000, Training Loss: 22.13147, Validation Loss: 3.61652\n",
      "Epoch 53/1000, Training Loss: 22.12498, Validation Loss: 3.61528\n",
      "Epoch 54/1000, Training Loss: 22.11852, Validation Loss: 3.61404\n",
      "Epoch 55/1000, Training Loss: 22.11207, Validation Loss: 3.61281\n",
      "Epoch 56/1000, Training Loss: 22.10564, Validation Loss: 3.61158\n",
      "Epoch 57/1000, Training Loss: 22.09923, Validation Loss: 3.61035\n",
      "Epoch 58/1000, Training Loss: 22.09284, Validation Loss: 3.60913\n",
      "Epoch 59/1000, Training Loss: 22.08648, Validation Loss: 3.60791\n",
      "Epoch 60/1000, Training Loss: 22.08013, Validation Loss: 3.60669\n",
      "Epoch 61/1000, Training Loss: 22.07380, Validation Loss: 3.60548\n",
      "Epoch 62/1000, Training Loss: 22.06749, Validation Loss: 3.60427\n",
      "Epoch 63/1000, Training Loss: 22.06121, Validation Loss: 3.60307\n",
      "Epoch 64/1000, Training Loss: 22.05493, Validation Loss: 3.60186\n",
      "Epoch 65/1000, Training Loss: 22.04868, Validation Loss: 3.60066\n",
      "Epoch 66/1000, Training Loss: 22.04244, Validation Loss: 3.59947\n",
      "Epoch 67/1000, Training Loss: 22.03622, Validation Loss: 3.59827\n",
      "Epoch 68/1000, Training Loss: 22.03001, Validation Loss: 3.59708\n",
      "Epoch 69/1000, Training Loss: 22.02383, Validation Loss: 3.59590\n",
      "Epoch 70/1000, Training Loss: 22.01766, Validation Loss: 3.59472\n",
      "Epoch 71/1000, Training Loss: 22.01152, Validation Loss: 3.59354\n",
      "Epoch 72/1000, Training Loss: 22.00540, Validation Loss: 3.59236\n",
      "Epoch 73/1000, Training Loss: 21.99929, Validation Loss: 3.59119\n",
      "Epoch 74/1000, Training Loss: 21.99320, Validation Loss: 3.59002\n",
      "Epoch 75/1000, Training Loss: 21.98714, Validation Loss: 3.58885\n",
      "Epoch 76/1000, Training Loss: 21.98109, Validation Loss: 3.58769\n",
      "Epoch 77/1000, Training Loss: 21.97505, Validation Loss: 3.58653\n",
      "Epoch 78/1000, Training Loss: 21.96904, Validation Loss: 3.58537\n",
      "Epoch 79/1000, Training Loss: 21.96304, Validation Loss: 3.58421\n",
      "Epoch 80/1000, Training Loss: 21.95705, Validation Loss: 3.58306\n",
      "Epoch 81/1000, Training Loss: 21.95109, Validation Loss: 3.58191\n",
      "Epoch 82/1000, Training Loss: 21.94515, Validation Loss: 3.58077\n",
      "Epoch 83/1000, Training Loss: 21.93922, Validation Loss: 3.57963\n",
      "Epoch 84/1000, Training Loss: 21.93331, Validation Loss: 3.57849\n",
      "Epoch 85/1000, Training Loss: 21.92742, Validation Loss: 3.57735\n",
      "Epoch 86/1000, Training Loss: 21.92154, Validation Loss: 3.57622\n",
      "Epoch 87/1000, Training Loss: 21.91568, Validation Loss: 3.57509\n",
      "Epoch 88/1000, Training Loss: 21.90984, Validation Loss: 3.57396\n",
      "Epoch 89/1000, Training Loss: 21.90401, Validation Loss: 3.57283\n",
      "Epoch 90/1000, Training Loss: 21.89821, Validation Loss: 3.57171\n",
      "Epoch 91/1000, Training Loss: 21.89241, Validation Loss: 3.57059\n",
      "Epoch 92/1000, Training Loss: 21.88664, Validation Loss: 3.56948\n",
      "Epoch 93/1000, Training Loss: 21.88088, Validation Loss: 3.56836\n",
      "Epoch 94/1000, Training Loss: 21.87513, Validation Loss: 3.56725\n",
      "Epoch 95/1000, Training Loss: 21.86940, Validation Loss: 3.56614\n",
      "Epoch 96/1000, Training Loss: 21.86369, Validation Loss: 3.56504\n",
      "Epoch 97/1000, Training Loss: 21.85799, Validation Loss: 3.56393\n",
      "Epoch 98/1000, Training Loss: 21.85230, Validation Loss: 3.56283\n",
      "Epoch 99/1000, Training Loss: 21.84663, Validation Loss: 3.56174\n",
      "Epoch 100/1000, Training Loss: 21.84097, Validation Loss: 3.56064\n",
      "Epoch 101/1000, Training Loss: 21.83533, Validation Loss: 3.55955\n",
      "Epoch 102/1000, Training Loss: 21.82970, Validation Loss: 3.55846\n",
      "Epoch 103/1000, Training Loss: 21.82409, Validation Loss: 3.55737\n",
      "Epoch 104/1000, Training Loss: 21.81850, Validation Loss: 3.55629\n",
      "Epoch 105/1000, Training Loss: 21.81292, Validation Loss: 3.55521\n",
      "Epoch 106/1000, Training Loss: 21.80736, Validation Loss: 3.55414\n",
      "Epoch 107/1000, Training Loss: 21.80181, Validation Loss: 3.55306\n",
      "Epoch 108/1000, Training Loss: 21.79628, Validation Loss: 3.55199\n",
      "Epoch 109/1000, Training Loss: 21.79076, Validation Loss: 3.55092\n",
      "Epoch 110/1000, Training Loss: 21.78526, Validation Loss: 3.54986\n",
      "Epoch 111/1000, Training Loss: 21.77976, Validation Loss: 3.54879\n",
      "Epoch 112/1000, Training Loss: 21.77429, Validation Loss: 3.54774\n",
      "Epoch 113/1000, Training Loss: 21.76882, Validation Loss: 3.54668\n",
      "Epoch 114/1000, Training Loss: 21.76338, Validation Loss: 3.54562\n",
      "Epoch 115/1000, Training Loss: 21.75794, Validation Loss: 3.54457\n",
      "Epoch 116/1000, Training Loss: 21.75252, Validation Loss: 3.54352\n",
      "Epoch 117/1000, Training Loss: 21.74711, Validation Loss: 3.54247\n",
      "Epoch 118/1000, Training Loss: 21.74171, Validation Loss: 3.54143\n",
      "Epoch 119/1000, Training Loss: 21.73633, Validation Loss: 3.54039\n",
      "Epoch 120/1000, Training Loss: 21.73097, Validation Loss: 3.53935\n",
      "Epoch 121/1000, Training Loss: 21.72561, Validation Loss: 3.53831\n",
      "Epoch 122/1000, Training Loss: 21.72027, Validation Loss: 3.53727\n",
      "Epoch 123/1000, Training Loss: 21.71495, Validation Loss: 3.53624\n",
      "Epoch 124/1000, Training Loss: 21.70964, Validation Loss: 3.53521\n",
      "Epoch 125/1000, Training Loss: 21.70434, Validation Loss: 3.53418\n",
      "Epoch 126/1000, Training Loss: 21.69906, Validation Loss: 3.53316\n",
      "Epoch 127/1000, Training Loss: 21.69380, Validation Loss: 3.53214\n",
      "Epoch 128/1000, Training Loss: 21.68854, Validation Loss: 3.53112\n",
      "Epoch 129/1000, Training Loss: 21.68330, Validation Loss: 3.53010\n",
      "Epoch 130/1000, Training Loss: 21.67808, Validation Loss: 3.52908\n",
      "Epoch 131/1000, Training Loss: 21.67286, Validation Loss: 3.52807\n",
      "Epoch 132/1000, Training Loss: 21.66766, Validation Loss: 3.52706\n",
      "Epoch 133/1000, Training Loss: 21.66247, Validation Loss: 3.52605\n",
      "Epoch 134/1000, Training Loss: 21.65730, Validation Loss: 3.52505\n",
      "Epoch 135/1000, Training Loss: 21.65214, Validation Loss: 3.52404\n",
      "Epoch 136/1000, Training Loss: 21.64699, Validation Loss: 3.52304\n",
      "Epoch 137/1000, Training Loss: 21.64186, Validation Loss: 3.52204\n",
      "Epoch 138/1000, Training Loss: 21.63673, Validation Loss: 3.52104\n",
      "Epoch 139/1000, Training Loss: 21.63162, Validation Loss: 3.52005\n",
      "Epoch 140/1000, Training Loss: 21.62652, Validation Loss: 3.51906\n",
      "Epoch 141/1000, Training Loss: 21.62144, Validation Loss: 3.51807\n",
      "Epoch 142/1000, Training Loss: 21.61637, Validation Loss: 3.51708\n",
      "Epoch 143/1000, Training Loss: 21.61131, Validation Loss: 3.51609\n",
      "Epoch 144/1000, Training Loss: 21.60626, Validation Loss: 3.51511\n",
      "Epoch 145/1000, Training Loss: 21.60122, Validation Loss: 3.51413\n",
      "Epoch 146/1000, Training Loss: 21.59620, Validation Loss: 3.51314\n",
      "Epoch 147/1000, Training Loss: 21.59119, Validation Loss: 3.51217\n",
      "Epoch 148/1000, Training Loss: 21.58619, Validation Loss: 3.51119\n",
      "Epoch 149/1000, Training Loss: 21.58120, Validation Loss: 3.51022\n",
      "Epoch 150/1000, Training Loss: 21.57623, Validation Loss: 3.50925\n",
      "Epoch 151/1000, Training Loss: 21.57127, Validation Loss: 3.50828\n",
      "Epoch 152/1000, Training Loss: 21.56632, Validation Loss: 3.50731\n",
      "Epoch 153/1000, Training Loss: 21.56138, Validation Loss: 3.50634\n",
      "Epoch 154/1000, Training Loss: 21.55645, Validation Loss: 3.50538\n",
      "Epoch 155/1000, Training Loss: 21.55154, Validation Loss: 3.50442\n",
      "Epoch 156/1000, Training Loss: 21.54663, Validation Loss: 3.50346\n",
      "Epoch 157/1000, Training Loss: 21.54175, Validation Loss: 3.50250\n",
      "Epoch 158/1000, Training Loss: 21.53687, Validation Loss: 3.50155\n",
      "Epoch 159/1000, Training Loss: 21.53201, Validation Loss: 3.50060\n",
      "Epoch 160/1000, Training Loss: 21.52716, Validation Loss: 3.49965\n",
      "Epoch 161/1000, Training Loss: 21.52231, Validation Loss: 3.49870\n",
      "Epoch 162/1000, Training Loss: 21.51748, Validation Loss: 3.49775\n",
      "Epoch 163/1000, Training Loss: 21.51266, Validation Loss: 3.49681\n",
      "Epoch 164/1000, Training Loss: 21.50785, Validation Loss: 3.49587\n",
      "Epoch 165/1000, Training Loss: 21.50305, Validation Loss: 3.49493\n",
      "Epoch 166/1000, Training Loss: 21.49827, Validation Loss: 3.49399\n",
      "Epoch 167/1000, Training Loss: 21.49350, Validation Loss: 3.49305\n",
      "Epoch 168/1000, Training Loss: 21.48873, Validation Loss: 3.49212\n",
      "Epoch 169/1000, Training Loss: 21.48398, Validation Loss: 3.49119\n",
      "Epoch 170/1000, Training Loss: 21.47924, Validation Loss: 3.49026\n",
      "Epoch 171/1000, Training Loss: 21.47452, Validation Loss: 3.48933\n",
      "Epoch 172/1000, Training Loss: 21.46980, Validation Loss: 3.48841\n",
      "Epoch 173/1000, Training Loss: 21.46510, Validation Loss: 3.48749\n",
      "Epoch 174/1000, Training Loss: 21.46041, Validation Loss: 3.48657\n",
      "Epoch 175/1000, Training Loss: 21.45573, Validation Loss: 3.48565\n",
      "Epoch 176/1000, Training Loss: 21.45106, Validation Loss: 3.48473\n",
      "Epoch 177/1000, Training Loss: 21.44640, Validation Loss: 3.48382\n",
      "Epoch 178/1000, Training Loss: 21.44176, Validation Loss: 3.48290\n",
      "Epoch 179/1000, Training Loss: 21.43712, Validation Loss: 3.48199\n",
      "Epoch 180/1000, Training Loss: 21.43250, Validation Loss: 3.48108\n",
      "Epoch 181/1000, Training Loss: 21.42789, Validation Loss: 3.48018\n",
      "Epoch 182/1000, Training Loss: 21.42329, Validation Loss: 3.47927\n",
      "Epoch 183/1000, Training Loss: 21.41871, Validation Loss: 3.47837\n",
      "Epoch 184/1000, Training Loss: 21.41413, Validation Loss: 3.47747\n",
      "Epoch 185/1000, Training Loss: 21.40956, Validation Loss: 3.47657\n",
      "Epoch 186/1000, Training Loss: 21.40500, Validation Loss: 3.47567\n",
      "Epoch 187/1000, Training Loss: 21.40045, Validation Loss: 3.47477\n",
      "Epoch 188/1000, Training Loss: 21.39591, Validation Loss: 3.47388\n",
      "Epoch 189/1000, Training Loss: 21.39139, Validation Loss: 3.47299\n",
      "Epoch 190/1000, Training Loss: 21.38687, Validation Loss: 3.47210\n",
      "Epoch 191/1000, Training Loss: 21.38236, Validation Loss: 3.47121\n",
      "Epoch 192/1000, Training Loss: 21.37786, Validation Loss: 3.47032\n",
      "Epoch 193/1000, Training Loss: 21.37337, Validation Loss: 3.46944\n",
      "Epoch 194/1000, Training Loss: 21.36889, Validation Loss: 3.46855\n",
      "Epoch 195/1000, Training Loss: 21.36442, Validation Loss: 3.46767\n",
      "Epoch 196/1000, Training Loss: 21.35996, Validation Loss: 3.46679\n",
      "Epoch 197/1000, Training Loss: 21.35551, Validation Loss: 3.46592\n",
      "Epoch 198/1000, Training Loss: 21.35107, Validation Loss: 3.46504\n",
      "Epoch 199/1000, Training Loss: 21.34664, Validation Loss: 3.46416\n",
      "Epoch 200/1000, Training Loss: 21.34222, Validation Loss: 3.46329\n",
      "Epoch 201/1000, Training Loss: 21.33781, Validation Loss: 3.46242\n",
      "Epoch 202/1000, Training Loss: 21.33341, Validation Loss: 3.46155\n",
      "Epoch 203/1000, Training Loss: 21.32902, Validation Loss: 3.46068\n",
      "Epoch 204/1000, Training Loss: 21.32464, Validation Loss: 3.45981\n",
      "Epoch 205/1000, Training Loss: 21.32027, Validation Loss: 3.45895\n",
      "Epoch 206/1000, Training Loss: 21.31591, Validation Loss: 3.45808\n",
      "Epoch 207/1000, Training Loss: 21.31155, Validation Loss: 3.45722\n",
      "Epoch 208/1000, Training Loss: 21.30721, Validation Loss: 3.45636\n",
      "Epoch 209/1000, Training Loss: 21.30288, Validation Loss: 3.45550\n",
      "Epoch 210/1000, Training Loss: 21.29855, Validation Loss: 3.45464\n",
      "Epoch 211/1000, Training Loss: 21.29423, Validation Loss: 3.45378\n",
      "Epoch 212/1000, Training Loss: 21.28993, Validation Loss: 3.45293\n",
      "Epoch 213/1000, Training Loss: 21.28562, Validation Loss: 3.45207\n",
      "Epoch 214/1000, Training Loss: 21.28133, Validation Loss: 3.45122\n",
      "Epoch 215/1000, Training Loss: 21.27705, Validation Loss: 3.45037\n",
      "Epoch 216/1000, Training Loss: 21.27277, Validation Loss: 3.44952\n",
      "Epoch 217/1000, Training Loss: 21.26849, Validation Loss: 3.44868\n",
      "Epoch 218/1000, Training Loss: 21.26423, Validation Loss: 3.44783\n",
      "Epoch 219/1000, Training Loss: 21.25997, Validation Loss: 3.44698\n",
      "Epoch 220/1000, Training Loss: 21.25572, Validation Loss: 3.44614\n",
      "Epoch 221/1000, Training Loss: 21.25148, Validation Loss: 3.44530\n",
      "Epoch 222/1000, Training Loss: 21.24725, Validation Loss: 3.44446\n",
      "Epoch 223/1000, Training Loss: 21.24303, Validation Loss: 3.44362\n",
      "Epoch 224/1000, Training Loss: 21.23882, Validation Loss: 3.44278\n",
      "Epoch 225/1000, Training Loss: 21.23462, Validation Loss: 3.44195\n",
      "Epoch 226/1000, Training Loss: 21.23042, Validation Loss: 3.44111\n",
      "Epoch 227/1000, Training Loss: 21.22624, Validation Loss: 3.44028\n",
      "Epoch 228/1000, Training Loss: 21.22206, Validation Loss: 3.43945\n",
      "Epoch 229/1000, Training Loss: 21.21790, Validation Loss: 3.43862\n",
      "Epoch 230/1000, Training Loss: 21.21375, Validation Loss: 3.43780\n",
      "Epoch 231/1000, Training Loss: 21.20960, Validation Loss: 3.43697\n",
      "Epoch 232/1000, Training Loss: 21.20546, Validation Loss: 3.43615\n",
      "Epoch 233/1000, Training Loss: 21.20133, Validation Loss: 3.43533\n",
      "Epoch 234/1000, Training Loss: 21.19721, Validation Loss: 3.43451\n",
      "Epoch 235/1000, Training Loss: 21.19310, Validation Loss: 3.43369\n",
      "Epoch 236/1000, Training Loss: 21.18899, Validation Loss: 3.43287\n",
      "Epoch 237/1000, Training Loss: 21.18489, Validation Loss: 3.43205\n",
      "Epoch 238/1000, Training Loss: 21.18080, Validation Loss: 3.43124\n",
      "Epoch 239/1000, Training Loss: 21.17672, Validation Loss: 3.43042\n",
      "Epoch 240/1000, Training Loss: 21.17265, Validation Loss: 3.42961\n",
      "Epoch 241/1000, Training Loss: 21.16858, Validation Loss: 3.42880\n",
      "Epoch 242/1000, Training Loss: 21.16452, Validation Loss: 3.42799\n",
      "Epoch 243/1000, Training Loss: 21.16047, Validation Loss: 3.42718\n",
      "Epoch 244/1000, Training Loss: 21.15643, Validation Loss: 3.42638\n",
      "Epoch 245/1000, Training Loss: 21.15240, Validation Loss: 3.42557\n",
      "Epoch 246/1000, Training Loss: 21.14837, Validation Loss: 3.42477\n",
      "Epoch 247/1000, Training Loss: 21.14436, Validation Loss: 3.42396\n",
      "Epoch 248/1000, Training Loss: 21.14035, Validation Loss: 3.42316\n",
      "Epoch 249/1000, Training Loss: 21.13635, Validation Loss: 3.42236\n",
      "Epoch 250/1000, Training Loss: 21.13235, Validation Loss: 3.42157\n",
      "Epoch 251/1000, Training Loss: 21.12837, Validation Loss: 3.42077\n",
      "Epoch 252/1000, Training Loss: 21.12439, Validation Loss: 3.41998\n",
      "Epoch 253/1000, Training Loss: 21.12042, Validation Loss: 3.41918\n",
      "Epoch 254/1000, Training Loss: 21.11646, Validation Loss: 3.41839\n",
      "Epoch 255/1000, Training Loss: 21.11251, Validation Loss: 3.41760\n",
      "Epoch 256/1000, Training Loss: 21.10857, Validation Loss: 3.41681\n",
      "Epoch 257/1000, Training Loss: 21.10463, Validation Loss: 3.41602\n",
      "Epoch 258/1000, Training Loss: 21.10070, Validation Loss: 3.41524\n",
      "Epoch 259/1000, Training Loss: 21.09679, Validation Loss: 3.41445\n",
      "Epoch 260/1000, Training Loss: 21.09288, Validation Loss: 3.41367\n",
      "Epoch 261/1000, Training Loss: 21.08897, Validation Loss: 3.41288\n",
      "Epoch 262/1000, Training Loss: 21.08508, Validation Loss: 3.41210\n",
      "Epoch 263/1000, Training Loss: 21.08119, Validation Loss: 3.41132\n",
      "Epoch 264/1000, Training Loss: 21.07731, Validation Loss: 3.41054\n",
      "Epoch 265/1000, Training Loss: 21.07343, Validation Loss: 3.40977\n",
      "Epoch 266/1000, Training Loss: 21.06957, Validation Loss: 3.40899\n",
      "Epoch 267/1000, Training Loss: 21.06571, Validation Loss: 3.40822\n",
      "Epoch 268/1000, Training Loss: 21.06185, Validation Loss: 3.40744\n",
      "Epoch 269/1000, Training Loss: 21.05801, Validation Loss: 3.40667\n",
      "Epoch 270/1000, Training Loss: 21.05417, Validation Loss: 3.40590\n",
      "Epoch 271/1000, Training Loss: 21.05034, Validation Loss: 3.40513\n",
      "Epoch 272/1000, Training Loss: 21.04652, Validation Loss: 3.40436\n",
      "Epoch 273/1000, Training Loss: 21.04271, Validation Loss: 3.40360\n",
      "Epoch 274/1000, Training Loss: 21.03890, Validation Loss: 3.40283\n",
      "Epoch 275/1000, Training Loss: 21.03511, Validation Loss: 3.40207\n",
      "Epoch 276/1000, Training Loss: 21.03132, Validation Loss: 3.40130\n",
      "Epoch 277/1000, Training Loss: 21.02754, Validation Loss: 3.40054\n",
      "Epoch 278/1000, Training Loss: 21.02377, Validation Loss: 3.39978\n",
      "Epoch 279/1000, Training Loss: 21.02000, Validation Loss: 3.39902\n",
      "Epoch 280/1000, Training Loss: 21.01625, Validation Loss: 3.39827\n",
      "Epoch 281/1000, Training Loss: 21.01250, Validation Loss: 3.39751\n",
      "Epoch 282/1000, Training Loss: 21.00876, Validation Loss: 3.39675\n",
      "Epoch 283/1000, Training Loss: 21.00503, Validation Loss: 3.39600\n",
      "Epoch 284/1000, Training Loss: 21.00130, Validation Loss: 3.39525\n",
      "Epoch 285/1000, Training Loss: 20.99758, Validation Loss: 3.39450\n",
      "Epoch 286/1000, Training Loss: 20.99387, Validation Loss: 3.39374\n",
      "Epoch 287/1000, Training Loss: 20.99016, Validation Loss: 3.39300\n",
      "Epoch 288/1000, Training Loss: 20.98647, Validation Loss: 3.39225\n",
      "Epoch 289/1000, Training Loss: 20.98277, Validation Loss: 3.39150\n",
      "Epoch 290/1000, Training Loss: 20.97909, Validation Loss: 3.39075\n",
      "Epoch 291/1000, Training Loss: 20.97541, Validation Loss: 3.39001\n",
      "Epoch 292/1000, Training Loss: 20.97173, Validation Loss: 3.38927\n",
      "Epoch 293/1000, Training Loss: 20.96807, Validation Loss: 3.38852\n",
      "Epoch 294/1000, Training Loss: 20.96441, Validation Loss: 3.38778\n",
      "Epoch 295/1000, Training Loss: 20.96075, Validation Loss: 3.38704\n",
      "Epoch 296/1000, Training Loss: 20.95711, Validation Loss: 3.38630\n",
      "Epoch 297/1000, Training Loss: 20.95347, Validation Loss: 3.38556\n",
      "Epoch 298/1000, Training Loss: 20.94983, Validation Loss: 3.38483\n",
      "Epoch 299/1000, Training Loss: 20.94620, Validation Loss: 3.38409\n",
      "Epoch 300/1000, Training Loss: 20.94258, Validation Loss: 3.38336\n",
      "Epoch 301/1000, Training Loss: 20.93896, Validation Loss: 3.38262\n",
      "Epoch 302/1000, Training Loss: 20.93535, Validation Loss: 3.38189\n",
      "Epoch 303/1000, Training Loss: 20.93174, Validation Loss: 3.38116\n",
      "Epoch 304/1000, Training Loss: 20.92814, Validation Loss: 3.38043\n",
      "Epoch 305/1000, Training Loss: 20.92454, Validation Loss: 3.37970\n",
      "Epoch 306/1000, Training Loss: 20.92095, Validation Loss: 3.37897\n",
      "Epoch 307/1000, Training Loss: 20.91737, Validation Loss: 3.37824\n",
      "Epoch 308/1000, Training Loss: 20.91380, Validation Loss: 3.37752\n",
      "Epoch 309/1000, Training Loss: 20.91023, Validation Loss: 3.37679\n",
      "Epoch 310/1000, Training Loss: 20.90667, Validation Loss: 3.37607\n",
      "Epoch 311/1000, Training Loss: 20.90312, Validation Loss: 3.37535\n",
      "Epoch 312/1000, Training Loss: 20.89958, Validation Loss: 3.37463\n",
      "Epoch 313/1000, Training Loss: 20.89604, Validation Loss: 3.37391\n",
      "Epoch 314/1000, Training Loss: 20.89251, Validation Loss: 3.37319\n",
      "Epoch 315/1000, Training Loss: 20.88898, Validation Loss: 3.37247\n",
      "Epoch 316/1000, Training Loss: 20.88546, Validation Loss: 3.37175\n",
      "Epoch 317/1000, Training Loss: 20.88194, Validation Loss: 3.37104\n",
      "Epoch 318/1000, Training Loss: 20.87843, Validation Loss: 3.37033\n",
      "Epoch 319/1000, Training Loss: 20.87493, Validation Loss: 3.36961\n",
      "Epoch 320/1000, Training Loss: 20.87143, Validation Loss: 3.36890\n",
      "Epoch 321/1000, Training Loss: 20.86794, Validation Loss: 3.36819\n",
      "Epoch 322/1000, Training Loss: 20.86445, Validation Loss: 3.36748\n",
      "Epoch 323/1000, Training Loss: 20.86097, Validation Loss: 3.36677\n",
      "Epoch 324/1000, Training Loss: 20.85749, Validation Loss: 3.36606\n",
      "Epoch 325/1000, Training Loss: 20.85402, Validation Loss: 3.36535\n",
      "Epoch 326/1000, Training Loss: 20.85056, Validation Loss: 3.36465\n",
      "Epoch 327/1000, Training Loss: 20.84710, Validation Loss: 3.36394\n",
      "Epoch 328/1000, Training Loss: 20.84364, Validation Loss: 3.36324\n",
      "Epoch 329/1000, Training Loss: 20.84020, Validation Loss: 3.36253\n",
      "Epoch 330/1000, Training Loss: 20.83676, Validation Loss: 3.36183\n",
      "Epoch 331/1000, Training Loss: 20.83332, Validation Loss: 3.36113\n",
      "Epoch 332/1000, Training Loss: 20.82989, Validation Loss: 3.36043\n",
      "Epoch 333/1000, Training Loss: 20.82646, Validation Loss: 3.35973\n",
      "Epoch 334/1000, Training Loss: 20.82305, Validation Loss: 3.35903\n",
      "Epoch 335/1000, Training Loss: 20.81963, Validation Loss: 3.35833\n",
      "Epoch 336/1000, Training Loss: 20.81622, Validation Loss: 3.35764\n",
      "Epoch 337/1000, Training Loss: 20.81282, Validation Loss: 3.35694\n",
      "Epoch 338/1000, Training Loss: 20.80942, Validation Loss: 3.35625\n",
      "Epoch 339/1000, Training Loss: 20.80602, Validation Loss: 3.35555\n",
      "Epoch 340/1000, Training Loss: 20.80263, Validation Loss: 3.35486\n",
      "Epoch 341/1000, Training Loss: 20.79925, Validation Loss: 3.35417\n",
      "Epoch 342/1000, Training Loss: 20.79587, Validation Loss: 3.35348\n",
      "Epoch 343/1000, Training Loss: 20.79250, Validation Loss: 3.35279\n",
      "Epoch 344/1000, Training Loss: 20.78913, Validation Loss: 3.35210\n",
      "Epoch 345/1000, Training Loss: 20.78577, Validation Loss: 3.35141\n",
      "Epoch 346/1000, Training Loss: 20.78241, Validation Loss: 3.35073\n",
      "Epoch 347/1000, Training Loss: 20.77906, Validation Loss: 3.35004\n",
      "Epoch 348/1000, Training Loss: 20.77572, Validation Loss: 3.34936\n",
      "Epoch 349/1000, Training Loss: 20.77238, Validation Loss: 3.34867\n",
      "Epoch 350/1000, Training Loss: 20.76904, Validation Loss: 3.34799\n",
      "Epoch 351/1000, Training Loss: 20.76572, Validation Loss: 3.34731\n",
      "Epoch 352/1000, Training Loss: 20.76239, Validation Loss: 3.34663\n",
      "Epoch 353/1000, Training Loss: 20.75908, Validation Loss: 3.34595\n",
      "Epoch 354/1000, Training Loss: 20.75576, Validation Loss: 3.34527\n",
      "Epoch 355/1000, Training Loss: 20.75246, Validation Loss: 3.34459\n",
      "Epoch 356/1000, Training Loss: 20.74915, Validation Loss: 3.34391\n",
      "Epoch 357/1000, Training Loss: 20.74585, Validation Loss: 3.34323\n",
      "Epoch 358/1000, Training Loss: 20.74256, Validation Loss: 3.34256\n",
      "Epoch 359/1000, Training Loss: 20.73927, Validation Loss: 3.34188\n",
      "Epoch 360/1000, Training Loss: 20.73598, Validation Loss: 3.34121\n",
      "Epoch 361/1000, Training Loss: 20.73270, Validation Loss: 3.34053\n",
      "Epoch 362/1000, Training Loss: 20.72943, Validation Loss: 3.33986\n",
      "Epoch 363/1000, Training Loss: 20.72616, Validation Loss: 3.33919\n",
      "Epoch 364/1000, Training Loss: 20.72289, Validation Loss: 3.33852\n",
      "Epoch 365/1000, Training Loss: 20.71963, Validation Loss: 3.33784\n",
      "Epoch 366/1000, Training Loss: 20.71638, Validation Loss: 3.33717\n",
      "Epoch 367/1000, Training Loss: 20.71313, Validation Loss: 3.33651\n",
      "Epoch 368/1000, Training Loss: 20.70988, Validation Loss: 3.33584\n",
      "Epoch 369/1000, Training Loss: 20.70664, Validation Loss: 3.33517\n",
      "Epoch 370/1000, Training Loss: 20.70340, Validation Loss: 3.33450\n",
      "Epoch 371/1000, Training Loss: 20.70017, Validation Loss: 3.33384\n",
      "Epoch 372/1000, Training Loss: 20.69693, Validation Loss: 3.33317\n",
      "Epoch 373/1000, Training Loss: 20.69371, Validation Loss: 3.33251\n",
      "Epoch 374/1000, Training Loss: 20.69049, Validation Loss: 3.33185\n",
      "Epoch 375/1000, Training Loss: 20.68727, Validation Loss: 3.33119\n",
      "Epoch 376/1000, Training Loss: 20.68406, Validation Loss: 3.33052\n",
      "Epoch 377/1000, Training Loss: 20.68085, Validation Loss: 3.32986\n",
      "Epoch 378/1000, Training Loss: 20.67765, Validation Loss: 3.32920\n",
      "Epoch 379/1000, Training Loss: 20.67444, Validation Loss: 3.32855\n",
      "Epoch 380/1000, Training Loss: 20.67125, Validation Loss: 3.32789\n",
      "Epoch 381/1000, Training Loss: 20.66806, Validation Loss: 3.32723\n",
      "Epoch 382/1000, Training Loss: 20.66487, Validation Loss: 3.32658\n",
      "Epoch 383/1000, Training Loss: 20.66169, Validation Loss: 3.32592\n",
      "Epoch 384/1000, Training Loss: 20.65851, Validation Loss: 3.32527\n",
      "Epoch 385/1000, Training Loss: 20.65534, Validation Loss: 3.32461\n",
      "Epoch 386/1000, Training Loss: 20.65217, Validation Loss: 3.32396\n",
      "Epoch 387/1000, Training Loss: 20.64901, Validation Loss: 3.32330\n",
      "Epoch 388/1000, Training Loss: 20.64585, Validation Loss: 3.32265\n",
      "Epoch 389/1000, Training Loss: 20.64269, Validation Loss: 3.32200\n",
      "Epoch 390/1000, Training Loss: 20.63954, Validation Loss: 3.32134\n",
      "Epoch 391/1000, Training Loss: 20.63640, Validation Loss: 3.32069\n",
      "Epoch 392/1000, Training Loss: 20.63326, Validation Loss: 3.32005\n",
      "Epoch 393/1000, Training Loss: 20.63012, Validation Loss: 3.31940\n",
      "Epoch 394/1000, Training Loss: 20.62699, Validation Loss: 3.31875\n",
      "Epoch 395/1000, Training Loss: 20.62386, Validation Loss: 3.31810\n",
      "Epoch 396/1000, Training Loss: 20.62074, Validation Loss: 3.31746\n",
      "Epoch 397/1000, Training Loss: 20.61762, Validation Loss: 3.31681\n",
      "Epoch 398/1000, Training Loss: 20.61450, Validation Loss: 3.31616\n",
      "Epoch 399/1000, Training Loss: 20.61139, Validation Loss: 3.31552\n",
      "Epoch 400/1000, Training Loss: 20.60829, Validation Loss: 3.31488\n",
      "Epoch 401/1000, Training Loss: 20.60519, Validation Loss: 3.31424\n",
      "Epoch 402/1000, Training Loss: 20.60209, Validation Loss: 3.31359\n",
      "Epoch 403/1000, Training Loss: 20.59900, Validation Loss: 3.31295\n",
      "Epoch 404/1000, Training Loss: 20.59592, Validation Loss: 3.31231\n",
      "Epoch 405/1000, Training Loss: 20.59283, Validation Loss: 3.31167\n",
      "Epoch 406/1000, Training Loss: 20.58976, Validation Loss: 3.31103\n",
      "Epoch 407/1000, Training Loss: 20.58668, Validation Loss: 3.31040\n",
      "Epoch 408/1000, Training Loss: 20.58361, Validation Loss: 3.30976\n",
      "Epoch 409/1000, Training Loss: 20.58054, Validation Loss: 3.30912\n",
      "Epoch 410/1000, Training Loss: 20.57748, Validation Loss: 3.30848\n",
      "Epoch 411/1000, Training Loss: 20.57442, Validation Loss: 3.30785\n",
      "Epoch 412/1000, Training Loss: 20.57137, Validation Loss: 3.30722\n",
      "Epoch 413/1000, Training Loss: 20.56832, Validation Loss: 3.30658\n",
      "Epoch 414/1000, Training Loss: 20.56527, Validation Loss: 3.30595\n",
      "Epoch 415/1000, Training Loss: 20.56223, Validation Loss: 3.30531\n",
      "Epoch 416/1000, Training Loss: 20.55919, Validation Loss: 3.30468\n",
      "Epoch 417/1000, Training Loss: 20.55616, Validation Loss: 3.30405\n",
      "Epoch 418/1000, Training Loss: 20.55313, Validation Loss: 3.30341\n",
      "Epoch 419/1000, Training Loss: 20.55010, Validation Loss: 3.30278\n",
      "Epoch 420/1000, Training Loss: 20.54708, Validation Loss: 3.30215\n",
      "Epoch 421/1000, Training Loss: 20.54405, Validation Loss: 3.30152\n",
      "Epoch 422/1000, Training Loss: 20.54104, Validation Loss: 3.30089\n",
      "Epoch 423/1000, Training Loss: 20.53802, Validation Loss: 3.30027\n",
      "Epoch 424/1000, Training Loss: 20.53501, Validation Loss: 3.29964\n",
      "Epoch 425/1000, Training Loss: 20.53201, Validation Loss: 3.29901\n",
      "Epoch 426/1000, Training Loss: 20.52901, Validation Loss: 3.29838\n",
      "Epoch 427/1000, Training Loss: 20.52601, Validation Loss: 3.29776\n",
      "Epoch 428/1000, Training Loss: 20.52302, Validation Loss: 3.29713\n",
      "Epoch 429/1000, Training Loss: 20.52003, Validation Loss: 3.29651\n",
      "Epoch 430/1000, Training Loss: 20.51704, Validation Loss: 3.29589\n",
      "Epoch 431/1000, Training Loss: 20.51406, Validation Loss: 3.29527\n",
      "Epoch 432/1000, Training Loss: 20.51108, Validation Loss: 3.29464\n",
      "Epoch 433/1000, Training Loss: 20.50811, Validation Loss: 3.29402\n",
      "Epoch 434/1000, Training Loss: 20.50514, Validation Loss: 3.29340\n",
      "Epoch 435/1000, Training Loss: 20.50217, Validation Loss: 3.29278\n",
      "Epoch 436/1000, Training Loss: 20.49921, Validation Loss: 3.29216\n",
      "Epoch 437/1000, Training Loss: 20.49625, Validation Loss: 3.29154\n",
      "Epoch 438/1000, Training Loss: 20.49329, Validation Loss: 3.29092\n",
      "Epoch 439/1000, Training Loss: 20.49034, Validation Loss: 3.29031\n",
      "Epoch 440/1000, Training Loss: 20.48739, Validation Loss: 3.28969\n",
      "Epoch 441/1000, Training Loss: 20.48445, Validation Loss: 3.28908\n",
      "Epoch 442/1000, Training Loss: 20.48151, Validation Loss: 3.28846\n",
      "Epoch 443/1000, Training Loss: 20.47857, Validation Loss: 3.28785\n",
      "Epoch 444/1000, Training Loss: 20.47564, Validation Loss: 3.28723\n",
      "Epoch 445/1000, Training Loss: 20.47271, Validation Loss: 3.28662\n",
      "Epoch 446/1000, Training Loss: 20.46978, Validation Loss: 3.28601\n",
      "Epoch 447/1000, Training Loss: 20.46686, Validation Loss: 3.28540\n",
      "Epoch 448/1000, Training Loss: 20.46394, Validation Loss: 3.28479\n",
      "Epoch 449/1000, Training Loss: 20.46102, Validation Loss: 3.28418\n",
      "Epoch 450/1000, Training Loss: 20.45811, Validation Loss: 3.28357\n",
      "Epoch 451/1000, Training Loss: 20.45520, Validation Loss: 3.28296\n",
      "Epoch 452/1000, Training Loss: 20.45229, Validation Loss: 3.28235\n",
      "Epoch 453/1000, Training Loss: 20.44939, Validation Loss: 3.28174\n",
      "Epoch 454/1000, Training Loss: 20.44649, Validation Loss: 3.28114\n",
      "Epoch 455/1000, Training Loss: 20.44360, Validation Loss: 3.28053\n",
      "Epoch 456/1000, Training Loss: 20.44070, Validation Loss: 3.27992\n",
      "Epoch 457/1000, Training Loss: 20.43782, Validation Loss: 3.27932\n",
      "Epoch 458/1000, Training Loss: 20.43493, Validation Loss: 3.27871\n",
      "Epoch 459/1000, Training Loss: 20.43205, Validation Loss: 3.27811\n",
      "Epoch 460/1000, Training Loss: 20.42917, Validation Loss: 3.27750\n",
      "Epoch 461/1000, Training Loss: 20.42629, Validation Loss: 3.27690\n",
      "Epoch 462/1000, Training Loss: 20.42342, Validation Loss: 3.27629\n",
      "Epoch 463/1000, Training Loss: 20.42055, Validation Loss: 3.27569\n",
      "Epoch 464/1000, Training Loss: 20.41768, Validation Loss: 3.27509\n",
      "Epoch 465/1000, Training Loss: 20.41482, Validation Loss: 3.27448\n",
      "Epoch 466/1000, Training Loss: 20.41196, Validation Loss: 3.27388\n",
      "Epoch 467/1000, Training Loss: 20.40911, Validation Loss: 3.27328\n",
      "Epoch 468/1000, Training Loss: 20.40625, Validation Loss: 3.27268\n",
      "Epoch 469/1000, Training Loss: 20.40340, Validation Loss: 3.27207\n",
      "Epoch 470/1000, Training Loss: 20.40055, Validation Loss: 3.27147\n",
      "Epoch 471/1000, Training Loss: 20.39770, Validation Loss: 3.27087\n",
      "Epoch 472/1000, Training Loss: 20.39486, Validation Loss: 3.27027\n",
      "Epoch 473/1000, Training Loss: 20.39202, Validation Loss: 3.26967\n",
      "Epoch 474/1000, Training Loss: 20.38918, Validation Loss: 3.26908\n",
      "Epoch 475/1000, Training Loss: 20.38635, Validation Loss: 3.26848\n",
      "Epoch 476/1000, Training Loss: 20.38352, Validation Loss: 3.26788\n",
      "Epoch 477/1000, Training Loss: 20.38069, Validation Loss: 3.26729\n",
      "Epoch 478/1000, Training Loss: 20.37787, Validation Loss: 3.26669\n",
      "Epoch 479/1000, Training Loss: 20.37505, Validation Loss: 3.26610\n",
      "Epoch 480/1000, Training Loss: 20.37223, Validation Loss: 3.26550\n",
      "Epoch 481/1000, Training Loss: 20.36941, Validation Loss: 3.26491\n",
      "Epoch 482/1000, Training Loss: 20.36660, Validation Loss: 3.26431\n",
      "Epoch 483/1000, Training Loss: 20.36379, Validation Loss: 3.26372\n",
      "Epoch 484/1000, Training Loss: 20.36098, Validation Loss: 3.26312\n",
      "Epoch 485/1000, Training Loss: 20.35817, Validation Loss: 3.26253\n",
      "Epoch 486/1000, Training Loss: 20.35537, Validation Loss: 3.26194\n",
      "Epoch 487/1000, Training Loss: 20.35257, Validation Loss: 3.26135\n",
      "Epoch 488/1000, Training Loss: 20.34977, Validation Loss: 3.26076\n",
      "Epoch 489/1000, Training Loss: 20.34698, Validation Loss: 3.26017\n",
      "Epoch 490/1000, Training Loss: 20.34419, Validation Loss: 3.25958\n",
      "Epoch 491/1000, Training Loss: 20.34140, Validation Loss: 3.25899\n",
      "Epoch 492/1000, Training Loss: 20.33861, Validation Loss: 3.25840\n",
      "Epoch 493/1000, Training Loss: 20.33583, Validation Loss: 3.25781\n",
      "Epoch 494/1000, Training Loss: 20.33305, Validation Loss: 3.25722\n",
      "Epoch 495/1000, Training Loss: 20.33028, Validation Loss: 3.25663\n",
      "Epoch 496/1000, Training Loss: 20.32751, Validation Loss: 3.25605\n",
      "Epoch 497/1000, Training Loss: 20.32474, Validation Loss: 3.25546\n",
      "Epoch 498/1000, Training Loss: 20.32197, Validation Loss: 3.25488\n",
      "Epoch 499/1000, Training Loss: 20.31921, Validation Loss: 3.25429\n",
      "Epoch 500/1000, Training Loss: 20.31645, Validation Loss: 3.25371\n",
      "Epoch 501/1000, Training Loss: 20.31369, Validation Loss: 3.25312\n",
      "Epoch 502/1000, Training Loss: 20.31094, Validation Loss: 3.25254\n",
      "Epoch 503/1000, Training Loss: 20.30819, Validation Loss: 3.25196\n",
      "Epoch 504/1000, Training Loss: 20.30544, Validation Loss: 3.25138\n",
      "Epoch 505/1000, Training Loss: 20.30269, Validation Loss: 3.25079\n",
      "Epoch 506/1000, Training Loss: 20.29994, Validation Loss: 3.25021\n",
      "Epoch 507/1000, Training Loss: 20.29720, Validation Loss: 3.24963\n",
      "Epoch 508/1000, Training Loss: 20.29446, Validation Loss: 3.24905\n",
      "Epoch 509/1000, Training Loss: 20.29173, Validation Loss: 3.24847\n",
      "Epoch 510/1000, Training Loss: 20.28899, Validation Loss: 3.24789\n",
      "Epoch 511/1000, Training Loss: 20.28626, Validation Loss: 3.24732\n",
      "Epoch 512/1000, Training Loss: 20.28354, Validation Loss: 3.24674\n",
      "Epoch 513/1000, Training Loss: 20.28081, Validation Loss: 3.24616\n",
      "Epoch 514/1000, Training Loss: 20.27809, Validation Loss: 3.24558\n",
      "Epoch 515/1000, Training Loss: 20.27538, Validation Loss: 3.24501\n",
      "Epoch 516/1000, Training Loss: 20.27266, Validation Loss: 3.24443\n",
      "Epoch 517/1000, Training Loss: 20.26995, Validation Loss: 3.24385\n",
      "Epoch 518/1000, Training Loss: 20.26724, Validation Loss: 3.24328\n",
      "Epoch 519/1000, Training Loss: 20.26453, Validation Loss: 3.24270\n",
      "Epoch 520/1000, Training Loss: 20.26182, Validation Loss: 3.24213\n",
      "Epoch 521/1000, Training Loss: 20.25912, Validation Loss: 3.24155\n",
      "Epoch 522/1000, Training Loss: 20.25641, Validation Loss: 3.24098\n",
      "Epoch 523/1000, Training Loss: 20.25371, Validation Loss: 3.24040\n",
      "Epoch 524/1000, Training Loss: 20.25101, Validation Loss: 3.23983\n",
      "Epoch 525/1000, Training Loss: 20.24832, Validation Loss: 3.23926\n",
      "Epoch 526/1000, Training Loss: 20.24562, Validation Loss: 3.23868\n",
      "Epoch 527/1000, Training Loss: 20.24293, Validation Loss: 3.23811\n",
      "Epoch 528/1000, Training Loss: 20.24024, Validation Loss: 3.23754\n",
      "Epoch 529/1000, Training Loss: 20.23755, Validation Loss: 3.23697\n",
      "Epoch 530/1000, Training Loss: 20.23487, Validation Loss: 3.23640\n",
      "Epoch 531/1000, Training Loss: 20.23219, Validation Loss: 3.23582\n",
      "Epoch 532/1000, Training Loss: 20.22950, Validation Loss: 3.23525\n",
      "Epoch 533/1000, Training Loss: 20.22683, Validation Loss: 3.23468\n",
      "Epoch 534/1000, Training Loss: 20.22415, Validation Loss: 3.23411\n",
      "Epoch 535/1000, Training Loss: 20.22148, Validation Loss: 3.23355\n",
      "Epoch 536/1000, Training Loss: 20.21880, Validation Loss: 3.23298\n",
      "Epoch 537/1000, Training Loss: 20.21614, Validation Loss: 3.23241\n",
      "Epoch 538/1000, Training Loss: 20.21347, Validation Loss: 3.23184\n",
      "Epoch 539/1000, Training Loss: 20.21081, Validation Loss: 3.23127\n",
      "Epoch 540/1000, Training Loss: 20.20816, Validation Loss: 3.23071\n",
      "Epoch 541/1000, Training Loss: 20.20550, Validation Loss: 3.23014\n",
      "Epoch 542/1000, Training Loss: 20.20285, Validation Loss: 3.22957\n",
      "Epoch 543/1000, Training Loss: 20.20020, Validation Loss: 3.22901\n",
      "Epoch 544/1000, Training Loss: 20.19755, Validation Loss: 3.22844\n",
      "Epoch 545/1000, Training Loss: 20.19490, Validation Loss: 3.22788\n",
      "Epoch 546/1000, Training Loss: 20.19226, Validation Loss: 3.22731\n",
      "Epoch 547/1000, Training Loss: 20.18962, Validation Loss: 3.22675\n",
      "Epoch 548/1000, Training Loss: 20.18698, Validation Loss: 3.22619\n",
      "Epoch 549/1000, Training Loss: 20.18434, Validation Loss: 3.22562\n",
      "Epoch 550/1000, Training Loss: 20.18171, Validation Loss: 3.22506\n",
      "Epoch 551/1000, Training Loss: 20.17908, Validation Loss: 3.22450\n",
      "Epoch 552/1000, Training Loss: 20.17645, Validation Loss: 3.22394\n",
      "Epoch 553/1000, Training Loss: 20.17382, Validation Loss: 3.22337\n",
      "Epoch 554/1000, Training Loss: 20.17119, Validation Loss: 3.22281\n",
      "Epoch 555/1000, Training Loss: 20.16856, Validation Loss: 3.22225\n",
      "Epoch 556/1000, Training Loss: 20.16594, Validation Loss: 3.22169\n",
      "Epoch 557/1000, Training Loss: 20.16332, Validation Loss: 3.22113\n",
      "Epoch 558/1000, Training Loss: 20.16070, Validation Loss: 3.22057\n",
      "Epoch 559/1000, Training Loss: 20.15808, Validation Loss: 3.22001\n",
      "Epoch 560/1000, Training Loss: 20.15547, Validation Loss: 3.21945\n",
      "Epoch 561/1000, Training Loss: 20.15286, Validation Loss: 3.21889\n",
      "Epoch 562/1000, Training Loss: 20.15025, Validation Loss: 3.21834\n",
      "Epoch 563/1000, Training Loss: 20.14765, Validation Loss: 3.21778\n",
      "Epoch 564/1000, Training Loss: 20.14504, Validation Loss: 3.21722\n",
      "Epoch 565/1000, Training Loss: 20.14244, Validation Loss: 3.21667\n",
      "Epoch 566/1000, Training Loss: 20.13984, Validation Loss: 3.21611\n",
      "Epoch 567/1000, Training Loss: 20.13724, Validation Loss: 3.21555\n",
      "Epoch 568/1000, Training Loss: 20.13465, Validation Loss: 3.21500\n",
      "Epoch 569/1000, Training Loss: 20.13205, Validation Loss: 3.21444\n",
      "Epoch 570/1000, Training Loss: 20.12946, Validation Loss: 3.21389\n",
      "Epoch 571/1000, Training Loss: 20.12687, Validation Loss: 3.21333\n",
      "Epoch 572/1000, Training Loss: 20.12428, Validation Loss: 3.21278\n",
      "Epoch 573/1000, Training Loss: 20.12170, Validation Loss: 3.21223\n",
      "Epoch 574/1000, Training Loss: 20.11911, Validation Loss: 3.21167\n",
      "Epoch 575/1000, Training Loss: 20.11653, Validation Loss: 3.21112\n",
      "Epoch 576/1000, Training Loss: 20.11395, Validation Loss: 3.21057\n",
      "Epoch 577/1000, Training Loss: 20.11138, Validation Loss: 3.21001\n",
      "Epoch 578/1000, Training Loss: 20.10880, Validation Loss: 3.20946\n",
      "Epoch 579/1000, Training Loss: 20.10623, Validation Loss: 3.20891\n",
      "Epoch 580/1000, Training Loss: 20.10366, Validation Loss: 3.20836\n",
      "Epoch 581/1000, Training Loss: 20.10109, Validation Loss: 3.20781\n",
      "Epoch 582/1000, Training Loss: 20.09852, Validation Loss: 3.20726\n",
      "Epoch 583/1000, Training Loss: 20.09596, Validation Loss: 3.20671\n",
      "Epoch 584/1000, Training Loss: 20.09340, Validation Loss: 3.20616\n",
      "Epoch 585/1000, Training Loss: 20.09083, Validation Loss: 3.20561\n",
      "Epoch 586/1000, Training Loss: 20.08827, Validation Loss: 3.20506\n",
      "Epoch 587/1000, Training Loss: 20.08572, Validation Loss: 3.20451\n",
      "Epoch 588/1000, Training Loss: 20.08316, Validation Loss: 3.20396\n",
      "Epoch 589/1000, Training Loss: 20.08061, Validation Loss: 3.20342\n",
      "Epoch 590/1000, Training Loss: 20.07806, Validation Loss: 3.20287\n",
      "Epoch 591/1000, Training Loss: 20.07552, Validation Loss: 3.20232\n",
      "Epoch 592/1000, Training Loss: 20.07297, Validation Loss: 3.20177\n",
      "Epoch 593/1000, Training Loss: 20.07043, Validation Loss: 3.20123\n",
      "Epoch 594/1000, Training Loss: 20.06788, Validation Loss: 3.20068\n",
      "Epoch 595/1000, Training Loss: 20.06534, Validation Loss: 3.20014\n",
      "Epoch 596/1000, Training Loss: 20.06281, Validation Loss: 3.19959\n",
      "Epoch 597/1000, Training Loss: 20.06027, Validation Loss: 3.19904\n",
      "Epoch 598/1000, Training Loss: 20.05774, Validation Loss: 3.19850\n",
      "Epoch 599/1000, Training Loss: 20.05520, Validation Loss: 3.19796\n",
      "Epoch 600/1000, Training Loss: 20.05267, Validation Loss: 3.19741\n",
      "Epoch 601/1000, Training Loss: 20.05014, Validation Loss: 3.19687\n",
      "Epoch 602/1000, Training Loss: 20.04762, Validation Loss: 3.19632\n",
      "Epoch 603/1000, Training Loss: 20.04510, Validation Loss: 3.19578\n",
      "Epoch 604/1000, Training Loss: 20.04257, Validation Loss: 3.19524\n",
      "Epoch 605/1000, Training Loss: 20.04005, Validation Loss: 3.19469\n",
      "Epoch 606/1000, Training Loss: 20.03754, Validation Loss: 3.19415\n",
      "Epoch 607/1000, Training Loss: 20.03502, Validation Loss: 3.19361\n",
      "Epoch 608/1000, Training Loss: 20.03250, Validation Loss: 3.19307\n",
      "Epoch 609/1000, Training Loss: 20.02999, Validation Loss: 3.19253\n",
      "Epoch 610/1000, Training Loss: 20.02748, Validation Loss: 3.19198\n",
      "Epoch 611/1000, Training Loss: 20.02497, Validation Loss: 3.19144\n",
      "Epoch 612/1000, Training Loss: 20.02247, Validation Loss: 3.19090\n",
      "Epoch 613/1000, Training Loss: 20.01996, Validation Loss: 3.19036\n",
      "Epoch 614/1000, Training Loss: 20.01746, Validation Loss: 3.18982\n",
      "Epoch 615/1000, Training Loss: 20.01496, Validation Loss: 3.18928\n",
      "Epoch 616/1000, Training Loss: 20.01246, Validation Loss: 3.18874\n",
      "Epoch 617/1000, Training Loss: 20.00996, Validation Loss: 3.18820\n",
      "Epoch 618/1000, Training Loss: 20.00746, Validation Loss: 3.18766\n",
      "Epoch 619/1000, Training Loss: 20.00497, Validation Loss: 3.18713\n",
      "Epoch 620/1000, Training Loss: 20.00248, Validation Loss: 3.18659\n",
      "Epoch 621/1000, Training Loss: 19.99999, Validation Loss: 3.18605\n",
      "Epoch 622/1000, Training Loss: 19.99750, Validation Loss: 3.18551\n",
      "Epoch 623/1000, Training Loss: 19.99501, Validation Loss: 3.18497\n",
      "Epoch 624/1000, Training Loss: 19.99252, Validation Loss: 3.18444\n",
      "Epoch 625/1000, Training Loss: 19.99004, Validation Loss: 3.18390\n",
      "Epoch 626/1000, Training Loss: 19.98755, Validation Loss: 3.18336\n",
      "Epoch 627/1000, Training Loss: 19.98507, Validation Loss: 3.18283\n",
      "Epoch 628/1000, Training Loss: 19.98259, Validation Loss: 3.18229\n",
      "Epoch 629/1000, Training Loss: 19.98012, Validation Loss: 3.18176\n",
      "Epoch 630/1000, Training Loss: 19.97764, Validation Loss: 3.18122\n",
      "Epoch 631/1000, Training Loss: 19.97516, Validation Loss: 3.18069\n",
      "Epoch 632/1000, Training Loss: 19.97269, Validation Loss: 3.18015\n",
      "Epoch 633/1000, Training Loss: 19.97022, Validation Loss: 3.17962\n",
      "Epoch 634/1000, Training Loss: 19.96774, Validation Loss: 3.17908\n",
      "Epoch 635/1000, Training Loss: 19.96527, Validation Loss: 3.17855\n",
      "Epoch 636/1000, Training Loss: 19.96280, Validation Loss: 3.17801\n",
      "Epoch 637/1000, Training Loss: 19.96033, Validation Loss: 3.17748\n",
      "Epoch 638/1000, Training Loss: 19.95787, Validation Loss: 3.17695\n",
      "Epoch 639/1000, Training Loss: 19.95540, Validation Loss: 3.17641\n",
      "Epoch 640/1000, Training Loss: 19.95293, Validation Loss: 3.17588\n",
      "Epoch 641/1000, Training Loss: 19.95047, Validation Loss: 3.17534\n",
      "Epoch 642/1000, Training Loss: 19.94800, Validation Loss: 3.17481\n",
      "Epoch 643/1000, Training Loss: 19.94554, Validation Loss: 3.17428\n",
      "Epoch 644/1000, Training Loss: 19.94308, Validation Loss: 3.17374\n",
      "Epoch 645/1000, Training Loss: 19.94062, Validation Loss: 3.17321\n",
      "Epoch 646/1000, Training Loss: 19.93816, Validation Loss: 3.17268\n",
      "Epoch 647/1000, Training Loss: 19.93571, Validation Loss: 3.17214\n",
      "Epoch 648/1000, Training Loss: 19.93325, Validation Loss: 3.17161\n",
      "Epoch 649/1000, Training Loss: 19.93080, Validation Loss: 3.17108\n",
      "Epoch 650/1000, Training Loss: 19.92835, Validation Loss: 3.17055\n",
      "Epoch 651/1000, Training Loss: 19.92590, Validation Loss: 3.17001\n",
      "Epoch 652/1000, Training Loss: 19.92345, Validation Loss: 3.16948\n",
      "Epoch 653/1000, Training Loss: 19.92100, Validation Loss: 3.16895\n",
      "Epoch 654/1000, Training Loss: 19.91856, Validation Loss: 3.16842\n",
      "Epoch 655/1000, Training Loss: 19.91611, Validation Loss: 3.16789\n",
      "Epoch 656/1000, Training Loss: 19.91367, Validation Loss: 3.16735\n",
      "Epoch 657/1000, Training Loss: 19.91123, Validation Loss: 3.16682\n",
      "Epoch 658/1000, Training Loss: 19.90879, Validation Loss: 3.16629\n",
      "Epoch 659/1000, Training Loss: 19.90635, Validation Loss: 3.16576\n",
      "Epoch 660/1000, Training Loss: 19.90391, Validation Loss: 3.16523\n",
      "Epoch 661/1000, Training Loss: 19.90147, Validation Loss: 3.16470\n",
      "Epoch 662/1000, Training Loss: 19.89903, Validation Loss: 3.16417\n",
      "Epoch 663/1000, Training Loss: 19.89660, Validation Loss: 3.16364\n",
      "Epoch 664/1000, Training Loss: 19.89416, Validation Loss: 3.16311\n",
      "Epoch 665/1000, Training Loss: 19.89173, Validation Loss: 3.16258\n",
      "Epoch 666/1000, Training Loss: 19.88930, Validation Loss: 3.16205\n",
      "Epoch 667/1000, Training Loss: 19.88687, Validation Loss: 3.16153\n",
      "Epoch 668/1000, Training Loss: 19.88444, Validation Loss: 3.16100\n",
      "Epoch 669/1000, Training Loss: 19.88201, Validation Loss: 3.16047\n",
      "Epoch 670/1000, Training Loss: 19.87958, Validation Loss: 3.15994\n",
      "Epoch 671/1000, Training Loss: 19.87716, Validation Loss: 3.15941\n",
      "Epoch 672/1000, Training Loss: 19.87474, Validation Loss: 3.15888\n",
      "Epoch 673/1000, Training Loss: 19.87232, Validation Loss: 3.15835\n",
      "Epoch 674/1000, Training Loss: 19.86990, Validation Loss: 3.15783\n",
      "Epoch 675/1000, Training Loss: 19.86748, Validation Loss: 3.15730\n",
      "Epoch 676/1000, Training Loss: 19.86507, Validation Loss: 3.15677\n",
      "Epoch 677/1000, Training Loss: 19.86265, Validation Loss: 3.15624\n",
      "Epoch 678/1000, Training Loss: 19.86024, Validation Loss: 3.15572\n",
      "Epoch 679/1000, Training Loss: 19.85783, Validation Loss: 3.15519\n",
      "Epoch 680/1000, Training Loss: 19.85542, Validation Loss: 3.15466\n",
      "Epoch 681/1000, Training Loss: 19.85301, Validation Loss: 3.15414\n",
      "Epoch 682/1000, Training Loss: 19.85060, Validation Loss: 3.15361\n",
      "Epoch 683/1000, Training Loss: 19.84819, Validation Loss: 3.15308\n",
      "Epoch 684/1000, Training Loss: 19.84579, Validation Loss: 3.15256\n",
      "Epoch 685/1000, Training Loss: 19.84338, Validation Loss: 3.15203\n",
      "Epoch 686/1000, Training Loss: 19.84098, Validation Loss: 3.15151\n",
      "Epoch 687/1000, Training Loss: 19.83858, Validation Loss: 3.15098\n",
      "Epoch 688/1000, Training Loss: 19.83618, Validation Loss: 3.15046\n",
      "Epoch 689/1000, Training Loss: 19.83378, Validation Loss: 3.14993\n",
      "Epoch 690/1000, Training Loss: 19.83139, Validation Loss: 3.14941\n",
      "Epoch 691/1000, Training Loss: 19.82899, Validation Loss: 3.14889\n",
      "Epoch 692/1000, Training Loss: 19.82659, Validation Loss: 3.14836\n",
      "Epoch 693/1000, Training Loss: 19.82420, Validation Loss: 3.14784\n",
      "Epoch 694/1000, Training Loss: 19.82180, Validation Loss: 3.14732\n",
      "Epoch 695/1000, Training Loss: 19.81941, Validation Loss: 3.14679\n",
      "Epoch 696/1000, Training Loss: 19.81702, Validation Loss: 3.14627\n",
      "Epoch 697/1000, Training Loss: 19.81463, Validation Loss: 3.14575\n",
      "Epoch 698/1000, Training Loss: 19.81224, Validation Loss: 3.14522\n",
      "Epoch 699/1000, Training Loss: 19.80985, Validation Loss: 3.14470\n",
      "Epoch 700/1000, Training Loss: 19.80747, Validation Loss: 3.14418\n",
      "Epoch 701/1000, Training Loss: 19.80508, Validation Loss: 3.14366\n",
      "Epoch 702/1000, Training Loss: 19.80270, Validation Loss: 3.14314\n",
      "Epoch 703/1000, Training Loss: 19.80032, Validation Loss: 3.14262\n",
      "Epoch 704/1000, Training Loss: 19.79794, Validation Loss: 3.14209\n",
      "Epoch 705/1000, Training Loss: 19.79556, Validation Loss: 3.14157\n",
      "Epoch 706/1000, Training Loss: 19.79318, Validation Loss: 3.14105\n",
      "Epoch 707/1000, Training Loss: 19.79081, Validation Loss: 3.14053\n",
      "Epoch 708/1000, Training Loss: 19.78843, Validation Loss: 3.14001\n",
      "Epoch 709/1000, Training Loss: 19.78606, Validation Loss: 3.13949\n",
      "Epoch 710/1000, Training Loss: 19.78368, Validation Loss: 3.13897\n",
      "Epoch 711/1000, Training Loss: 19.78131, Validation Loss: 3.13845\n",
      "Epoch 712/1000, Training Loss: 19.77894, Validation Loss: 3.13793\n",
      "Epoch 713/1000, Training Loss: 19.77657, Validation Loss: 3.13741\n",
      "Epoch 714/1000, Training Loss: 19.77420, Validation Loss: 3.13689\n",
      "Epoch 715/1000, Training Loss: 19.77183, Validation Loss: 3.13637\n",
      "Epoch 716/1000, Training Loss: 19.76947, Validation Loss: 3.13585\n",
      "Epoch 717/1000, Training Loss: 19.76710, Validation Loss: 3.13533\n",
      "Epoch 718/1000, Training Loss: 19.76474, Validation Loss: 3.13482\n",
      "Epoch 719/1000, Training Loss: 19.76238, Validation Loss: 3.13430\n",
      "Epoch 720/1000, Training Loss: 19.76002, Validation Loss: 3.13378\n",
      "Epoch 721/1000, Training Loss: 19.75766, Validation Loss: 3.13326\n",
      "Epoch 722/1000, Training Loss: 19.75530, Validation Loss: 3.13274\n",
      "Epoch 723/1000, Training Loss: 19.75294, Validation Loss: 3.13223\n",
      "Epoch 724/1000, Training Loss: 19.75059, Validation Loss: 3.13171\n",
      "Epoch 725/1000, Training Loss: 19.74823, Validation Loss: 3.13119\n",
      "Epoch 726/1000, Training Loss: 19.74588, Validation Loss: 3.13068\n",
      "Epoch 727/1000, Training Loss: 19.74352, Validation Loss: 3.13016\n",
      "Epoch 728/1000, Training Loss: 19.74117, Validation Loss: 3.12964\n",
      "Epoch 729/1000, Training Loss: 19.73882, Validation Loss: 3.12913\n",
      "Epoch 730/1000, Training Loss: 19.73648, Validation Loss: 3.12861\n",
      "Epoch 731/1000, Training Loss: 19.73413, Validation Loss: 3.12810\n",
      "Epoch 732/1000, Training Loss: 19.73179, Validation Loss: 3.12758\n",
      "Epoch 733/1000, Training Loss: 19.72944, Validation Loss: 3.12707\n",
      "Epoch 734/1000, Training Loss: 19.72710, Validation Loss: 3.12655\n",
      "Epoch 735/1000, Training Loss: 19.72475, Validation Loss: 3.12604\n",
      "Epoch 736/1000, Training Loss: 19.72241, Validation Loss: 3.12552\n",
      "Epoch 737/1000, Training Loss: 19.72006, Validation Loss: 3.12501\n",
      "Epoch 738/1000, Training Loss: 19.71772, Validation Loss: 3.12449\n",
      "Epoch 739/1000, Training Loss: 19.71538, Validation Loss: 3.12398\n",
      "Epoch 740/1000, Training Loss: 19.71304, Validation Loss: 3.12346\n",
      "Epoch 741/1000, Training Loss: 19.71070, Validation Loss: 3.12295\n",
      "Epoch 742/1000, Training Loss: 19.70836, Validation Loss: 3.12243\n",
      "Epoch 743/1000, Training Loss: 19.70602, Validation Loss: 3.12192\n",
      "Epoch 744/1000, Training Loss: 19.70368, Validation Loss: 3.12141\n",
      "Epoch 745/1000, Training Loss: 19.70135, Validation Loss: 3.12089\n",
      "Epoch 746/1000, Training Loss: 19.69901, Validation Loss: 3.12038\n",
      "Epoch 747/1000, Training Loss: 19.69668, Validation Loss: 3.11986\n",
      "Epoch 748/1000, Training Loss: 19.69435, Validation Loss: 3.11935\n",
      "Epoch 749/1000, Training Loss: 19.69201, Validation Loss: 3.11884\n",
      "Epoch 750/1000, Training Loss: 19.68968, Validation Loss: 3.11833\n",
      "Epoch 751/1000, Training Loss: 19.68735, Validation Loss: 3.11781\n",
      "Epoch 752/1000, Training Loss: 19.68502, Validation Loss: 3.11730\n",
      "Epoch 753/1000, Training Loss: 19.68270, Validation Loss: 3.11679\n",
      "Epoch 754/1000, Training Loss: 19.68037, Validation Loss: 3.11628\n",
      "Epoch 755/1000, Training Loss: 19.67804, Validation Loss: 3.11577\n",
      "Epoch 756/1000, Training Loss: 19.67571, Validation Loss: 3.11525\n",
      "Epoch 757/1000, Training Loss: 19.67339, Validation Loss: 3.11474\n",
      "Epoch 758/1000, Training Loss: 19.67106, Validation Loss: 3.11423\n",
      "Epoch 759/1000, Training Loss: 19.66874, Validation Loss: 3.11372\n",
      "Epoch 760/1000, Training Loss: 19.66641, Validation Loss: 3.11321\n",
      "Epoch 761/1000, Training Loss: 19.66409, Validation Loss: 3.11270\n",
      "Epoch 762/1000, Training Loss: 19.66177, Validation Loss: 3.11219\n",
      "Epoch 763/1000, Training Loss: 19.65944, Validation Loss: 3.11168\n",
      "Epoch 764/1000, Training Loss: 19.65712, Validation Loss: 3.11116\n",
      "Epoch 765/1000, Training Loss: 19.65480, Validation Loss: 3.11065\n",
      "Epoch 766/1000, Training Loss: 19.65248, Validation Loss: 3.11014\n",
      "Epoch 767/1000, Training Loss: 19.65016, Validation Loss: 3.10963\n",
      "Epoch 768/1000, Training Loss: 19.64784, Validation Loss: 3.10912\n",
      "Epoch 769/1000, Training Loss: 19.64553, Validation Loss: 3.10861\n",
      "Epoch 770/1000, Training Loss: 19.64321, Validation Loss: 3.10810\n",
      "Epoch 771/1000, Training Loss: 19.64089, Validation Loss: 3.10759\n",
      "Epoch 772/1000, Training Loss: 19.63858, Validation Loss: 3.10708\n",
      "Epoch 773/1000, Training Loss: 19.63626, Validation Loss: 3.10657\n",
      "Epoch 774/1000, Training Loss: 19.63394, Validation Loss: 3.10606\n",
      "Epoch 775/1000, Training Loss: 19.63163, Validation Loss: 3.10555\n",
      "Epoch 776/1000, Training Loss: 19.62932, Validation Loss: 3.10504\n",
      "Epoch 777/1000, Training Loss: 19.62700, Validation Loss: 3.10453\n",
      "Epoch 778/1000, Training Loss: 19.62469, Validation Loss: 3.10402\n",
      "Epoch 779/1000, Training Loss: 19.62237, Validation Loss: 3.10351\n",
      "Epoch 780/1000, Training Loss: 19.62006, Validation Loss: 3.10300\n",
      "Epoch 781/1000, Training Loss: 19.61775, Validation Loss: 3.10249\n",
      "Epoch 782/1000, Training Loss: 19.61544, Validation Loss: 3.10198\n",
      "Epoch 783/1000, Training Loss: 19.61313, Validation Loss: 3.10147\n",
      "Epoch 784/1000, Training Loss: 19.61083, Validation Loss: 3.10096\n",
      "Epoch 785/1000, Training Loss: 19.60852, Validation Loss: 3.10046\n",
      "Epoch 786/1000, Training Loss: 19.60621, Validation Loss: 3.09995\n",
      "Epoch 787/1000, Training Loss: 19.60391, Validation Loss: 3.09944\n",
      "Epoch 788/1000, Training Loss: 19.60160, Validation Loss: 3.09893\n",
      "Epoch 789/1000, Training Loss: 19.59930, Validation Loss: 3.09842\n",
      "Epoch 790/1000, Training Loss: 19.59699, Validation Loss: 3.09791\n",
      "Epoch 791/1000, Training Loss: 19.59469, Validation Loss: 3.09740\n",
      "Epoch 792/1000, Training Loss: 19.59239, Validation Loss: 3.09690\n",
      "Epoch 793/1000, Training Loss: 19.59008, Validation Loss: 3.09639\n",
      "Epoch 794/1000, Training Loss: 19.58778, Validation Loss: 3.09588\n",
      "Epoch 795/1000, Training Loss: 19.58548, Validation Loss: 3.09537\n",
      "Epoch 796/1000, Training Loss: 19.58318, Validation Loss: 3.09486\n",
      "Epoch 797/1000, Training Loss: 19.58087, Validation Loss: 3.09436\n",
      "Epoch 798/1000, Training Loss: 19.57857, Validation Loss: 3.09385\n",
      "Epoch 799/1000, Training Loss: 19.57627, Validation Loss: 3.09334\n",
      "Epoch 800/1000, Training Loss: 19.57397, Validation Loss: 3.09283\n",
      "Epoch 801/1000, Training Loss: 19.57167, Validation Loss: 3.09233\n",
      "Epoch 802/1000, Training Loss: 19.56937, Validation Loss: 3.09182\n",
      "Epoch 803/1000, Training Loss: 19.56707, Validation Loss: 3.09131\n",
      "Epoch 804/1000, Training Loss: 19.56477, Validation Loss: 3.09080\n",
      "Epoch 805/1000, Training Loss: 19.56247, Validation Loss: 3.09030\n",
      "Epoch 806/1000, Training Loss: 19.56018, Validation Loss: 3.08979\n",
      "Epoch 807/1000, Training Loss: 19.55788, Validation Loss: 3.08928\n",
      "Epoch 808/1000, Training Loss: 19.55558, Validation Loss: 3.08878\n",
      "Epoch 809/1000, Training Loss: 19.55328, Validation Loss: 3.08827\n",
      "Epoch 810/1000, Training Loss: 19.55099, Validation Loss: 3.08776\n",
      "Epoch 811/1000, Training Loss: 19.54870, Validation Loss: 3.08726\n",
      "Epoch 812/1000, Training Loss: 19.54640, Validation Loss: 3.08675\n",
      "Epoch 813/1000, Training Loss: 19.54411, Validation Loss: 3.08624\n",
      "Epoch 814/1000, Training Loss: 19.54181, Validation Loss: 3.08574\n",
      "Epoch 815/1000, Training Loss: 19.53952, Validation Loss: 3.08523\n",
      "Epoch 816/1000, Training Loss: 19.53722, Validation Loss: 3.08472\n",
      "Epoch 817/1000, Training Loss: 19.53493, Validation Loss: 3.08422\n",
      "Epoch 818/1000, Training Loss: 19.53264, Validation Loss: 3.08371\n",
      "Epoch 819/1000, Training Loss: 19.53035, Validation Loss: 3.08320\n",
      "Epoch 820/1000, Training Loss: 19.52806, Validation Loss: 3.08270\n",
      "Epoch 821/1000, Training Loss: 19.52577, Validation Loss: 3.08219\n",
      "Epoch 822/1000, Training Loss: 19.52347, Validation Loss: 3.08168\n",
      "Epoch 823/1000, Training Loss: 19.52118, Validation Loss: 3.08118\n",
      "Epoch 824/1000, Training Loss: 19.51888, Validation Loss: 3.08067\n",
      "Epoch 825/1000, Training Loss: 19.51659, Validation Loss: 3.08017\n",
      "Epoch 826/1000, Training Loss: 19.51430, Validation Loss: 3.07966\n",
      "Epoch 827/1000, Training Loss: 19.51201, Validation Loss: 3.07915\n",
      "Epoch 828/1000, Training Loss: 19.50972, Validation Loss: 3.07865\n",
      "Epoch 829/1000, Training Loss: 19.50743, Validation Loss: 3.07814\n",
      "Epoch 830/1000, Training Loss: 19.50513, Validation Loss: 3.07764\n",
      "Epoch 831/1000, Training Loss: 19.50284, Validation Loss: 3.07713\n",
      "Epoch 832/1000, Training Loss: 19.50055, Validation Loss: 3.07663\n",
      "Epoch 833/1000, Training Loss: 19.49826, Validation Loss: 3.07612\n",
      "Epoch 834/1000, Training Loss: 19.49597, Validation Loss: 3.07562\n",
      "Epoch 835/1000, Training Loss: 19.49368, Validation Loss: 3.07511\n",
      "Epoch 836/1000, Training Loss: 19.49139, Validation Loss: 3.07460\n",
      "Epoch 837/1000, Training Loss: 19.48910, Validation Loss: 3.07410\n",
      "Epoch 838/1000, Training Loss: 19.48681, Validation Loss: 3.07359\n",
      "Epoch 839/1000, Training Loss: 19.48453, Validation Loss: 3.07309\n",
      "Epoch 840/1000, Training Loss: 19.48224, Validation Loss: 3.07258\n",
      "Epoch 841/1000, Training Loss: 19.47995, Validation Loss: 3.07208\n",
      "Epoch 842/1000, Training Loss: 19.47767, Validation Loss: 3.07157\n",
      "Epoch 843/1000, Training Loss: 19.47538, Validation Loss: 3.07107\n",
      "Epoch 844/1000, Training Loss: 19.47310, Validation Loss: 3.07056\n",
      "Epoch 845/1000, Training Loss: 19.47082, Validation Loss: 3.07006\n",
      "Epoch 846/1000, Training Loss: 19.46853, Validation Loss: 3.06955\n",
      "Epoch 847/1000, Training Loss: 19.46625, Validation Loss: 3.06905\n",
      "Epoch 848/1000, Training Loss: 19.46396, Validation Loss: 3.06854\n",
      "Epoch 849/1000, Training Loss: 19.46168, Validation Loss: 3.06804\n",
      "Epoch 850/1000, Training Loss: 19.45939, Validation Loss: 3.06753\n",
      "Epoch 851/1000, Training Loss: 19.45711, Validation Loss: 3.06703\n",
      "Epoch 852/1000, Training Loss: 19.45483, Validation Loss: 3.06652\n",
      "Epoch 853/1000, Training Loss: 19.45254, Validation Loss: 3.06602\n",
      "Epoch 854/1000, Training Loss: 19.45026, Validation Loss: 3.06551\n",
      "Epoch 855/1000, Training Loss: 19.44797, Validation Loss: 3.06501\n",
      "Epoch 856/1000, Training Loss: 19.44569, Validation Loss: 3.06450\n",
      "Epoch 857/1000, Training Loss: 19.44340, Validation Loss: 3.06400\n",
      "Epoch 858/1000, Training Loss: 19.44112, Validation Loss: 3.06349\n",
      "Epoch 859/1000, Training Loss: 19.43884, Validation Loss: 3.06299\n",
      "Epoch 860/1000, Training Loss: 19.43655, Validation Loss: 3.06248\n",
      "Epoch 861/1000, Training Loss: 19.43427, Validation Loss: 3.06198\n",
      "Epoch 862/1000, Training Loss: 19.43198, Validation Loss: 3.06147\n",
      "Epoch 863/1000, Training Loss: 19.42970, Validation Loss: 3.06097\n",
      "Epoch 864/1000, Training Loss: 19.42742, Validation Loss: 3.06046\n",
      "Epoch 865/1000, Training Loss: 19.42514, Validation Loss: 3.05996\n",
      "Epoch 866/1000, Training Loss: 19.42286, Validation Loss: 3.05946\n",
      "Epoch 867/1000, Training Loss: 19.42058, Validation Loss: 3.05895\n",
      "Epoch 868/1000, Training Loss: 19.41830, Validation Loss: 3.05845\n",
      "Epoch 869/1000, Training Loss: 19.41602, Validation Loss: 3.05794\n",
      "Epoch 870/1000, Training Loss: 19.41374, Validation Loss: 3.05744\n",
      "Epoch 871/1000, Training Loss: 19.41147, Validation Loss: 3.05693\n",
      "Epoch 872/1000, Training Loss: 19.40919, Validation Loss: 3.05643\n",
      "Epoch 873/1000, Training Loss: 19.40691, Validation Loss: 3.05593\n",
      "Epoch 874/1000, Training Loss: 19.40464, Validation Loss: 3.05542\n",
      "Epoch 875/1000, Training Loss: 19.40236, Validation Loss: 3.05492\n",
      "Epoch 876/1000, Training Loss: 19.40009, Validation Loss: 3.05442\n",
      "Epoch 877/1000, Training Loss: 19.39781, Validation Loss: 3.05391\n",
      "Epoch 878/1000, Training Loss: 19.39554, Validation Loss: 3.05341\n",
      "Epoch 879/1000, Training Loss: 19.39326, Validation Loss: 3.05291\n",
      "Epoch 880/1000, Training Loss: 19.39099, Validation Loss: 3.05240\n",
      "Epoch 881/1000, Training Loss: 19.38872, Validation Loss: 3.05190\n",
      "Epoch 882/1000, Training Loss: 19.38644, Validation Loss: 3.05140\n",
      "Epoch 883/1000, Training Loss: 19.38417, Validation Loss: 3.05090\n",
      "Epoch 884/1000, Training Loss: 19.38189, Validation Loss: 3.05039\n",
      "Epoch 885/1000, Training Loss: 19.37962, Validation Loss: 3.04989\n",
      "Epoch 886/1000, Training Loss: 19.37735, Validation Loss: 3.04939\n",
      "Epoch 887/1000, Training Loss: 19.37508, Validation Loss: 3.04889\n",
      "Epoch 888/1000, Training Loss: 19.37281, Validation Loss: 3.04838\n",
      "Epoch 889/1000, Training Loss: 19.37054, Validation Loss: 3.04788\n",
      "Epoch 890/1000, Training Loss: 19.36827, Validation Loss: 3.04738\n",
      "Epoch 891/1000, Training Loss: 19.36600, Validation Loss: 3.04688\n",
      "Epoch 892/1000, Training Loss: 19.36373, Validation Loss: 3.04638\n",
      "Epoch 893/1000, Training Loss: 19.36145, Validation Loss: 3.04587\n",
      "Epoch 894/1000, Training Loss: 19.35918, Validation Loss: 3.04537\n",
      "Epoch 895/1000, Training Loss: 19.35691, Validation Loss: 3.04487\n",
      "Epoch 896/1000, Training Loss: 19.35464, Validation Loss: 3.04437\n",
      "Epoch 897/1000, Training Loss: 19.35237, Validation Loss: 3.04387\n",
      "Epoch 898/1000, Training Loss: 19.35010, Validation Loss: 3.04336\n",
      "Epoch 899/1000, Training Loss: 19.34783, Validation Loss: 3.04286\n",
      "Epoch 900/1000, Training Loss: 19.34556, Validation Loss: 3.04236\n",
      "Epoch 901/1000, Training Loss: 19.34329, Validation Loss: 3.04186\n",
      "Epoch 902/1000, Training Loss: 19.34102, Validation Loss: 3.04135\n",
      "Epoch 903/1000, Training Loss: 19.33876, Validation Loss: 3.04085\n",
      "Epoch 904/1000, Training Loss: 19.33649, Validation Loss: 3.04035\n",
      "Epoch 905/1000, Training Loss: 19.33422, Validation Loss: 3.03985\n",
      "Epoch 906/1000, Training Loss: 19.33195, Validation Loss: 3.03935\n",
      "Epoch 907/1000, Training Loss: 19.32968, Validation Loss: 3.03884\n",
      "Epoch 908/1000, Training Loss: 19.32741, Validation Loss: 3.03834\n",
      "Epoch 909/1000, Training Loss: 19.32514, Validation Loss: 3.03784\n",
      "Epoch 910/1000, Training Loss: 19.32287, Validation Loss: 3.03734\n",
      "Epoch 911/1000, Training Loss: 19.32060, Validation Loss: 3.03683\n",
      "Epoch 912/1000, Training Loss: 19.31833, Validation Loss: 3.03633\n",
      "Epoch 913/1000, Training Loss: 19.31606, Validation Loss: 3.03583\n",
      "Epoch 914/1000, Training Loss: 19.31379, Validation Loss: 3.03533\n",
      "Epoch 915/1000, Training Loss: 19.31152, Validation Loss: 3.03482\n",
      "Epoch 916/1000, Training Loss: 19.30926, Validation Loss: 3.03432\n",
      "Epoch 917/1000, Training Loss: 19.30699, Validation Loss: 3.03382\n",
      "Epoch 918/1000, Training Loss: 19.30472, Validation Loss: 3.03332\n",
      "Epoch 919/1000, Training Loss: 19.30245, Validation Loss: 3.03281\n",
      "Epoch 920/1000, Training Loss: 19.30018, Validation Loss: 3.03231\n",
      "Epoch 921/1000, Training Loss: 19.29791, Validation Loss: 3.03181\n",
      "Epoch 922/1000, Training Loss: 19.29564, Validation Loss: 3.03131\n",
      "Epoch 923/1000, Training Loss: 19.29338, Validation Loss: 3.03080\n",
      "Epoch 924/1000, Training Loss: 19.29111, Validation Loss: 3.03030\n",
      "Epoch 925/1000, Training Loss: 19.28884, Validation Loss: 3.02980\n",
      "Epoch 926/1000, Training Loss: 19.28657, Validation Loss: 3.02930\n",
      "Epoch 927/1000, Training Loss: 19.28430, Validation Loss: 3.02879\n",
      "Epoch 928/1000, Training Loss: 19.28204, Validation Loss: 3.02829\n",
      "Epoch 929/1000, Training Loss: 19.27977, Validation Loss: 3.02779\n",
      "Epoch 930/1000, Training Loss: 19.27751, Validation Loss: 3.02729\n",
      "Epoch 931/1000, Training Loss: 19.27524, Validation Loss: 3.02679\n",
      "Epoch 932/1000, Training Loss: 19.27297, Validation Loss: 3.02628\n",
      "Epoch 933/1000, Training Loss: 19.27071, Validation Loss: 3.02578\n",
      "Epoch 934/1000, Training Loss: 19.26844, Validation Loss: 3.02528\n",
      "Epoch 935/1000, Training Loss: 19.26618, Validation Loss: 3.02478\n",
      "Epoch 936/1000, Training Loss: 19.26392, Validation Loss: 3.02428\n",
      "Epoch 937/1000, Training Loss: 19.26165, Validation Loss: 3.02377\n",
      "Epoch 938/1000, Training Loss: 19.25939, Validation Loss: 3.02327\n",
      "Epoch 939/1000, Training Loss: 19.25712, Validation Loss: 3.02277\n",
      "Epoch 940/1000, Training Loss: 19.25486, Validation Loss: 3.02227\n",
      "Epoch 941/1000, Training Loss: 19.25259, Validation Loss: 3.02177\n",
      "Epoch 942/1000, Training Loss: 19.25033, Validation Loss: 3.02127\n",
      "Epoch 943/1000, Training Loss: 19.24807, Validation Loss: 3.02077\n",
      "Epoch 944/1000, Training Loss: 19.24581, Validation Loss: 3.02026\n",
      "Epoch 945/1000, Training Loss: 19.24354, Validation Loss: 3.01976\n",
      "Epoch 946/1000, Training Loss: 19.24128, Validation Loss: 3.01926\n",
      "Epoch 947/1000, Training Loss: 19.23902, Validation Loss: 3.01876\n",
      "Epoch 948/1000, Training Loss: 19.23675, Validation Loss: 3.01826\n",
      "Epoch 949/1000, Training Loss: 19.23449, Validation Loss: 3.01776\n",
      "Epoch 950/1000, Training Loss: 19.23223, Validation Loss: 3.01726\n",
      "Epoch 951/1000, Training Loss: 19.22996, Validation Loss: 3.01676\n",
      "Epoch 952/1000, Training Loss: 19.22770, Validation Loss: 3.01625\n",
      "Epoch 953/1000, Training Loss: 19.22544, Validation Loss: 3.01575\n",
      "Epoch 954/1000, Training Loss: 19.22318, Validation Loss: 3.01525\n",
      "Epoch 955/1000, Training Loss: 19.22091, Validation Loss: 3.01475\n",
      "Epoch 956/1000, Training Loss: 19.21865, Validation Loss: 3.01425\n",
      "Epoch 957/1000, Training Loss: 19.21639, Validation Loss: 3.01375\n",
      "Epoch 958/1000, Training Loss: 19.21413, Validation Loss: 3.01325\n",
      "Epoch 959/1000, Training Loss: 19.21186, Validation Loss: 3.01275\n",
      "Epoch 960/1000, Training Loss: 19.20960, Validation Loss: 3.01224\n",
      "Epoch 961/1000, Training Loss: 19.20734, Validation Loss: 3.01174\n",
      "Epoch 962/1000, Training Loss: 19.20508, Validation Loss: 3.01124\n",
      "Epoch 963/1000, Training Loss: 19.20282, Validation Loss: 3.01074\n",
      "Epoch 964/1000, Training Loss: 19.20056, Validation Loss: 3.01024\n",
      "Epoch 965/1000, Training Loss: 19.19830, Validation Loss: 3.00974\n",
      "Epoch 966/1000, Training Loss: 19.19603, Validation Loss: 3.00924\n",
      "Epoch 967/1000, Training Loss: 19.19377, Validation Loss: 3.00874\n",
      "Epoch 968/1000, Training Loss: 19.19151, Validation Loss: 3.00824\n",
      "Epoch 969/1000, Training Loss: 19.18925, Validation Loss: 3.00774\n",
      "Epoch 970/1000, Training Loss: 19.18699, Validation Loss: 3.00723\n",
      "Epoch 971/1000, Training Loss: 19.18473, Validation Loss: 3.00673\n",
      "Epoch 972/1000, Training Loss: 19.18247, Validation Loss: 3.00623\n",
      "Epoch 973/1000, Training Loss: 19.18022, Validation Loss: 3.00573\n",
      "Epoch 974/1000, Training Loss: 19.17795, Validation Loss: 3.00523\n",
      "Epoch 975/1000, Training Loss: 19.17569, Validation Loss: 3.00473\n",
      "Epoch 976/1000, Training Loss: 19.17344, Validation Loss: 3.00423\n",
      "Epoch 977/1000, Training Loss: 19.17117, Validation Loss: 3.00373\n",
      "Epoch 978/1000, Training Loss: 19.16891, Validation Loss: 3.00323\n",
      "Epoch 979/1000, Training Loss: 19.16665, Validation Loss: 3.00273\n",
      "Epoch 980/1000, Training Loss: 19.16439, Validation Loss: 3.00223\n",
      "Epoch 981/1000, Training Loss: 19.16213, Validation Loss: 3.00173\n",
      "Epoch 982/1000, Training Loss: 19.15987, Validation Loss: 3.00123\n",
      "Epoch 983/1000, Training Loss: 19.15761, Validation Loss: 3.00073\n",
      "Epoch 984/1000, Training Loss: 19.15535, Validation Loss: 3.00023\n",
      "Epoch 985/1000, Training Loss: 19.15309, Validation Loss: 2.99973\n",
      "Epoch 986/1000, Training Loss: 19.15083, Validation Loss: 2.99923\n",
      "Epoch 987/1000, Training Loss: 19.14856, Validation Loss: 2.99872\n",
      "Epoch 988/1000, Training Loss: 19.14630, Validation Loss: 2.99822\n",
      "Epoch 989/1000, Training Loss: 19.14404, Validation Loss: 2.99772\n",
      "Epoch 990/1000, Training Loss: 19.14178, Validation Loss: 2.99722\n",
      "Epoch 991/1000, Training Loss: 19.13952, Validation Loss: 2.99672\n",
      "Epoch 992/1000, Training Loss: 19.13726, Validation Loss: 2.99622\n",
      "Epoch 993/1000, Training Loss: 19.13500, Validation Loss: 2.99572\n",
      "Epoch 994/1000, Training Loss: 19.13274, Validation Loss: 2.99522\n",
      "Epoch 995/1000, Training Loss: 19.13047, Validation Loss: 2.99472\n",
      "Epoch 996/1000, Training Loss: 19.12821, Validation Loss: 2.99422\n",
      "Epoch 997/1000, Training Loss: 19.12595, Validation Loss: 2.99372\n",
      "Epoch 998/1000, Training Loss: 19.12369, Validation Loss: 2.99322\n",
      "Epoch 999/1000, Training Loss: 19.12143, Validation Loss: 2.99271\n",
      "Epoch 1000/1000, Training Loss: 19.11917, Validation Loss: 2.99221\n",
      "Training took: 47.91 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_batch_20_2 = NeuralNetwork().to(device)\n",
    "summary(model_batch_20_2, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 20\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_batch_20_2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_batch_20_2=[]\n",
    "val_loss_list_batch_20_2=[]\n",
    "train_accuracy_list_batch_20_2=[]\n",
    "val_accuracy_list_batch_20_2=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_batch_20_2.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_batch_20_2(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_batch_20_2.append(train_accuracy)\n",
    "    train_loss_list_batch_20_2.append(train_loss)\n",
    "\n",
    "    model_batch_20_2.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_batch_20_2(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_batch_20_2.append(val_accuracy)\n",
    "    val_accuracy_list_batch_20_2.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWwOcSWco5Cp",
    "outputId": "fdd682bc-10d6-407e-9c66-141181c50c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable batch size with batch size as 20: 0.6753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_batch_20_2.eval()\n",
    "test_predictions_batch_20_2 = model_batch_20_2(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_batch_20_2 = torch.round(test_predictions_batch_20_2)\n",
    "\n",
    "test_predictions_rounded_numpy_batch_20_2 = test_predictions_rounded_batch_20_2.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_batch_20_2 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_batch_20_2)\n",
    "\n",
    "print(f\"Accuracy for variable batch size with batch size as 20: {accuracy_batch_20_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1q-zb4Co8Zw",
    "outputId": "1250ba17-2544-455b-ad84-348dbdafdc0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable batch size with batch size as 20: 0.60650\n"
     ]
    }
   ],
   "source": [
    "model_batch_20_2.eval()\n",
    "test_loss_batch_20_2=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_batch_20_2 = model_batch_20_2(X_test_tensor)\n",
    "    test_loss_batch_20_2 = loss_function(test_outputs_batch_20_2, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable batch size with batch size as 20: {test_loss_batch_20_2.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilmj-CSzp3qO"
   },
   "source": [
    "Batch Size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSxqpfIKpDT-",
    "outputId": "e099f633-148a-4bd1-c4c4-da15242368c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 14.90600, Validation Loss: 2.16497\n",
      "Epoch 2/1000, Training Loss: 14.90145, Validation Loss: 2.16425\n",
      "Epoch 3/1000, Training Loss: 14.89691, Validation Loss: 2.16353\n",
      "Epoch 4/1000, Training Loss: 14.89238, Validation Loss: 2.16281\n",
      "Epoch 5/1000, Training Loss: 14.88787, Validation Loss: 2.16210\n",
      "Epoch 6/1000, Training Loss: 14.88336, Validation Loss: 2.16138\n",
      "Epoch 7/1000, Training Loss: 14.87887, Validation Loss: 2.16067\n",
      "Epoch 8/1000, Training Loss: 14.87440, Validation Loss: 2.15996\n",
      "Epoch 9/1000, Training Loss: 14.86993, Validation Loss: 2.15926\n",
      "Epoch 10/1000, Training Loss: 14.86548, Validation Loss: 2.15855\n",
      "Epoch 11/1000, Training Loss: 14.86104, Validation Loss: 2.15785\n",
      "Epoch 12/1000, Training Loss: 14.85661, Validation Loss: 2.15715\n",
      "Epoch 13/1000, Training Loss: 14.85220, Validation Loss: 2.15645\n",
      "Epoch 14/1000, Training Loss: 14.84779, Validation Loss: 2.15575\n",
      "Epoch 15/1000, Training Loss: 14.84340, Validation Loss: 2.15505\n",
      "Epoch 16/1000, Training Loss: 14.83902, Validation Loss: 2.15436\n",
      "Epoch 17/1000, Training Loss: 14.83466, Validation Loss: 2.15367\n",
      "Epoch 18/1000, Training Loss: 14.83030, Validation Loss: 2.15298\n",
      "Epoch 19/1000, Training Loss: 14.82595, Validation Loss: 2.15229\n",
      "Epoch 20/1000, Training Loss: 14.82162, Validation Loss: 2.15160\n",
      "Epoch 21/1000, Training Loss: 14.81730, Validation Loss: 2.15091\n",
      "Epoch 22/1000, Training Loss: 14.81298, Validation Loss: 2.15023\n",
      "Epoch 23/1000, Training Loss: 14.80868, Validation Loss: 2.14954\n",
      "Epoch 24/1000, Training Loss: 14.80439, Validation Loss: 2.14886\n",
      "Epoch 25/1000, Training Loss: 14.80010, Validation Loss: 2.14818\n",
      "Epoch 26/1000, Training Loss: 14.79584, Validation Loss: 2.14751\n",
      "Epoch 27/1000, Training Loss: 14.79158, Validation Loss: 2.14683\n",
      "Epoch 28/1000, Training Loss: 14.78733, Validation Loss: 2.14616\n",
      "Epoch 29/1000, Training Loss: 14.78309, Validation Loss: 2.14549\n",
      "Epoch 30/1000, Training Loss: 14.77887, Validation Loss: 2.14482\n",
      "Epoch 31/1000, Training Loss: 14.77465, Validation Loss: 2.14415\n",
      "Epoch 32/1000, Training Loss: 14.77045, Validation Loss: 2.14348\n",
      "Epoch 33/1000, Training Loss: 14.76626, Validation Loss: 2.14282\n",
      "Epoch 34/1000, Training Loss: 14.76207, Validation Loss: 2.14215\n",
      "Epoch 35/1000, Training Loss: 14.75790, Validation Loss: 2.14149\n",
      "Epoch 36/1000, Training Loss: 14.75374, Validation Loss: 2.14083\n",
      "Epoch 37/1000, Training Loss: 14.74960, Validation Loss: 2.14017\n",
      "Epoch 38/1000, Training Loss: 14.74546, Validation Loss: 2.13952\n",
      "Epoch 39/1000, Training Loss: 14.74133, Validation Loss: 2.13886\n",
      "Epoch 40/1000, Training Loss: 14.73722, Validation Loss: 2.13821\n",
      "Epoch 41/1000, Training Loss: 14.73311, Validation Loss: 2.13756\n",
      "Epoch 42/1000, Training Loss: 14.72901, Validation Loss: 2.13691\n",
      "Epoch 43/1000, Training Loss: 14.72493, Validation Loss: 2.13626\n",
      "Epoch 44/1000, Training Loss: 14.72085, Validation Loss: 2.13561\n",
      "Epoch 45/1000, Training Loss: 14.71679, Validation Loss: 2.13497\n",
      "Epoch 46/1000, Training Loss: 14.71273, Validation Loss: 2.13432\n",
      "Epoch 47/1000, Training Loss: 14.70869, Validation Loss: 2.13368\n",
      "Epoch 48/1000, Training Loss: 14.70466, Validation Loss: 2.13304\n",
      "Epoch 49/1000, Training Loss: 14.70063, Validation Loss: 2.13240\n",
      "Epoch 50/1000, Training Loss: 14.69662, Validation Loss: 2.13176\n",
      "Epoch 51/1000, Training Loss: 14.69262, Validation Loss: 2.13113\n",
      "Epoch 52/1000, Training Loss: 14.68862, Validation Loss: 2.13049\n",
      "Epoch 53/1000, Training Loss: 14.68464, Validation Loss: 2.12986\n",
      "Epoch 54/1000, Training Loss: 14.68066, Validation Loss: 2.12923\n",
      "Epoch 55/1000, Training Loss: 14.67670, Validation Loss: 2.12860\n",
      "Epoch 56/1000, Training Loss: 14.67274, Validation Loss: 2.12797\n",
      "Epoch 57/1000, Training Loss: 14.66880, Validation Loss: 2.12734\n",
      "Epoch 58/1000, Training Loss: 14.66486, Validation Loss: 2.12672\n",
      "Epoch 59/1000, Training Loss: 14.66094, Validation Loss: 2.12609\n",
      "Epoch 60/1000, Training Loss: 14.65702, Validation Loss: 2.12547\n",
      "Epoch 61/1000, Training Loss: 14.65311, Validation Loss: 2.12485\n",
      "Epoch 62/1000, Training Loss: 14.64922, Validation Loss: 2.12423\n",
      "Epoch 63/1000, Training Loss: 14.64533, Validation Loss: 2.12361\n",
      "Epoch 64/1000, Training Loss: 14.64145, Validation Loss: 2.12299\n",
      "Epoch 65/1000, Training Loss: 14.63758, Validation Loss: 2.12238\n",
      "Epoch 66/1000, Training Loss: 14.63372, Validation Loss: 2.12176\n",
      "Epoch 67/1000, Training Loss: 14.62987, Validation Loss: 2.12115\n",
      "Epoch 68/1000, Training Loss: 14.62603, Validation Loss: 2.12054\n",
      "Epoch 69/1000, Training Loss: 14.62220, Validation Loss: 2.11993\n",
      "Epoch 70/1000, Training Loss: 14.61838, Validation Loss: 2.11932\n",
      "Epoch 71/1000, Training Loss: 14.61457, Validation Loss: 2.11872\n",
      "Epoch 72/1000, Training Loss: 14.61076, Validation Loss: 2.11811\n",
      "Epoch 73/1000, Training Loss: 14.60697, Validation Loss: 2.11751\n",
      "Epoch 74/1000, Training Loss: 14.60318, Validation Loss: 2.11690\n",
      "Epoch 75/1000, Training Loss: 14.59941, Validation Loss: 2.11630\n",
      "Epoch 76/1000, Training Loss: 14.59564, Validation Loss: 2.11570\n",
      "Epoch 77/1000, Training Loss: 14.59188, Validation Loss: 2.11510\n",
      "Epoch 78/1000, Training Loss: 14.58813, Validation Loss: 2.11451\n",
      "Epoch 79/1000, Training Loss: 14.58439, Validation Loss: 2.11391\n",
      "Epoch 80/1000, Training Loss: 14.58066, Validation Loss: 2.11331\n",
      "Epoch 81/1000, Training Loss: 14.57693, Validation Loss: 2.11272\n",
      "Epoch 82/1000, Training Loss: 14.57322, Validation Loss: 2.11213\n",
      "Epoch 83/1000, Training Loss: 14.56951, Validation Loss: 2.11154\n",
      "Epoch 84/1000, Training Loss: 14.56582, Validation Loss: 2.11095\n",
      "Epoch 85/1000, Training Loss: 14.56213, Validation Loss: 2.11036\n",
      "Epoch 86/1000, Training Loss: 14.55845, Validation Loss: 2.10978\n",
      "Epoch 87/1000, Training Loss: 14.55478, Validation Loss: 2.10919\n",
      "Epoch 88/1000, Training Loss: 14.55111, Validation Loss: 2.10861\n",
      "Epoch 89/1000, Training Loss: 14.54746, Validation Loss: 2.10802\n",
      "Epoch 90/1000, Training Loss: 14.54381, Validation Loss: 2.10744\n",
      "Epoch 91/1000, Training Loss: 14.54017, Validation Loss: 2.10686\n",
      "Epoch 92/1000, Training Loss: 14.53654, Validation Loss: 2.10628\n",
      "Epoch 93/1000, Training Loss: 14.53292, Validation Loss: 2.10571\n",
      "Epoch 94/1000, Training Loss: 14.52931, Validation Loss: 2.10513\n",
      "Epoch 95/1000, Training Loss: 14.52570, Validation Loss: 2.10455\n",
      "Epoch 96/1000, Training Loss: 14.52210, Validation Loss: 2.10398\n",
      "Epoch 97/1000, Training Loss: 14.51851, Validation Loss: 2.10341\n",
      "Epoch 98/1000, Training Loss: 14.51493, Validation Loss: 2.10284\n",
      "Epoch 99/1000, Training Loss: 14.51135, Validation Loss: 2.10227\n",
      "Epoch 100/1000, Training Loss: 14.50779, Validation Loss: 2.10170\n",
      "Epoch 101/1000, Training Loss: 14.50423, Validation Loss: 2.10113\n",
      "Epoch 102/1000, Training Loss: 14.50068, Validation Loss: 2.10056\n",
      "Epoch 103/1000, Training Loss: 14.49713, Validation Loss: 2.10000\n",
      "Epoch 104/1000, Training Loss: 14.49360, Validation Loss: 2.09944\n",
      "Epoch 105/1000, Training Loss: 14.49007, Validation Loss: 2.09887\n",
      "Epoch 106/1000, Training Loss: 14.48655, Validation Loss: 2.09831\n",
      "Epoch 107/1000, Training Loss: 14.48304, Validation Loss: 2.09775\n",
      "Epoch 108/1000, Training Loss: 14.47954, Validation Loss: 2.09719\n",
      "Epoch 109/1000, Training Loss: 14.47604, Validation Loss: 2.09664\n",
      "Epoch 110/1000, Training Loss: 14.47255, Validation Loss: 2.09608\n",
      "Epoch 111/1000, Training Loss: 14.46907, Validation Loss: 2.09553\n",
      "Epoch 112/1000, Training Loss: 14.46560, Validation Loss: 2.09497\n",
      "Epoch 113/1000, Training Loss: 14.46214, Validation Loss: 2.09442\n",
      "Epoch 114/1000, Training Loss: 14.45868, Validation Loss: 2.09387\n",
      "Epoch 115/1000, Training Loss: 14.45523, Validation Loss: 2.09332\n",
      "Epoch 116/1000, Training Loss: 14.45179, Validation Loss: 2.09277\n",
      "Epoch 117/1000, Training Loss: 14.44836, Validation Loss: 2.09222\n",
      "Epoch 118/1000, Training Loss: 14.44493, Validation Loss: 2.09167\n",
      "Epoch 119/1000, Training Loss: 14.44152, Validation Loss: 2.09113\n",
      "Epoch 120/1000, Training Loss: 14.43810, Validation Loss: 2.09058\n",
      "Epoch 121/1000, Training Loss: 14.43470, Validation Loss: 2.09004\n",
      "Epoch 122/1000, Training Loss: 14.43130, Validation Loss: 2.08950\n",
      "Epoch 123/1000, Training Loss: 14.42791, Validation Loss: 2.08896\n",
      "Epoch 124/1000, Training Loss: 14.42453, Validation Loss: 2.08842\n",
      "Epoch 125/1000, Training Loss: 14.42115, Validation Loss: 2.08788\n",
      "Epoch 126/1000, Training Loss: 14.41778, Validation Loss: 2.08734\n",
      "Epoch 127/1000, Training Loss: 14.41442, Validation Loss: 2.08680\n",
      "Epoch 128/1000, Training Loss: 14.41107, Validation Loss: 2.08626\n",
      "Epoch 129/1000, Training Loss: 14.40772, Validation Loss: 2.08573\n",
      "Epoch 130/1000, Training Loss: 14.40438, Validation Loss: 2.08520\n",
      "Epoch 131/1000, Training Loss: 14.40104, Validation Loss: 2.08466\n",
      "Epoch 132/1000, Training Loss: 14.39771, Validation Loss: 2.08413\n",
      "Epoch 133/1000, Training Loss: 14.39439, Validation Loss: 2.08360\n",
      "Epoch 134/1000, Training Loss: 14.39108, Validation Loss: 2.08307\n",
      "Epoch 135/1000, Training Loss: 14.38777, Validation Loss: 2.08254\n",
      "Epoch 136/1000, Training Loss: 14.38447, Validation Loss: 2.08201\n",
      "Epoch 137/1000, Training Loss: 14.38118, Validation Loss: 2.08148\n",
      "Epoch 138/1000, Training Loss: 14.37789, Validation Loss: 2.08096\n",
      "Epoch 139/1000, Training Loss: 14.37461, Validation Loss: 2.08043\n",
      "Epoch 140/1000, Training Loss: 14.37134, Validation Loss: 2.07991\n",
      "Epoch 141/1000, Training Loss: 14.36808, Validation Loss: 2.07939\n",
      "Epoch 142/1000, Training Loss: 14.36482, Validation Loss: 2.07887\n",
      "Epoch 143/1000, Training Loss: 14.36157, Validation Loss: 2.07835\n",
      "Epoch 144/1000, Training Loss: 14.35832, Validation Loss: 2.07783\n",
      "Epoch 145/1000, Training Loss: 14.35509, Validation Loss: 2.07731\n",
      "Epoch 146/1000, Training Loss: 14.35186, Validation Loss: 2.07679\n",
      "Epoch 147/1000, Training Loss: 14.34863, Validation Loss: 2.07627\n",
      "Epoch 148/1000, Training Loss: 14.34542, Validation Loss: 2.07576\n",
      "Epoch 149/1000, Training Loss: 14.34220, Validation Loss: 2.07524\n",
      "Epoch 150/1000, Training Loss: 14.33900, Validation Loss: 2.07473\n",
      "Epoch 151/1000, Training Loss: 14.33581, Validation Loss: 2.07422\n",
      "Epoch 152/1000, Training Loss: 14.33262, Validation Loss: 2.07371\n",
      "Epoch 153/1000, Training Loss: 14.32943, Validation Loss: 2.07319\n",
      "Epoch 154/1000, Training Loss: 14.32625, Validation Loss: 2.07268\n",
      "Epoch 155/1000, Training Loss: 14.32308, Validation Loss: 2.07218\n",
      "Epoch 156/1000, Training Loss: 14.31992, Validation Loss: 2.07167\n",
      "Epoch 157/1000, Training Loss: 14.31677, Validation Loss: 2.07116\n",
      "Epoch 158/1000, Training Loss: 14.31363, Validation Loss: 2.07066\n",
      "Epoch 159/1000, Training Loss: 14.31050, Validation Loss: 2.07015\n",
      "Epoch 160/1000, Training Loss: 14.30737, Validation Loss: 2.06965\n",
      "Epoch 161/1000, Training Loss: 14.30424, Validation Loss: 2.06915\n",
      "Epoch 162/1000, Training Loss: 14.30113, Validation Loss: 2.06865\n",
      "Epoch 163/1000, Training Loss: 14.29802, Validation Loss: 2.06815\n",
      "Epoch 164/1000, Training Loss: 14.29491, Validation Loss: 2.06765\n",
      "Epoch 165/1000, Training Loss: 14.29181, Validation Loss: 2.06715\n",
      "Epoch 166/1000, Training Loss: 14.28872, Validation Loss: 2.06665\n",
      "Epoch 167/1000, Training Loss: 14.28563, Validation Loss: 2.06616\n",
      "Epoch 168/1000, Training Loss: 14.28255, Validation Loss: 2.06566\n",
      "Epoch 169/1000, Training Loss: 14.27948, Validation Loss: 2.06517\n",
      "Epoch 170/1000, Training Loss: 14.27641, Validation Loss: 2.06467\n",
      "Epoch 171/1000, Training Loss: 14.27335, Validation Loss: 2.06418\n",
      "Epoch 172/1000, Training Loss: 14.27030, Validation Loss: 2.06369\n",
      "Epoch 173/1000, Training Loss: 14.26725, Validation Loss: 2.06320\n",
      "Epoch 174/1000, Training Loss: 14.26421, Validation Loss: 2.06271\n",
      "Epoch 175/1000, Training Loss: 14.26118, Validation Loss: 2.06222\n",
      "Epoch 176/1000, Training Loss: 14.25815, Validation Loss: 2.06173\n",
      "Epoch 177/1000, Training Loss: 14.25513, Validation Loss: 2.06124\n",
      "Epoch 178/1000, Training Loss: 14.25212, Validation Loss: 2.06076\n",
      "Epoch 179/1000, Training Loss: 14.24911, Validation Loss: 2.06027\n",
      "Epoch 180/1000, Training Loss: 14.24610, Validation Loss: 2.05978\n",
      "Epoch 181/1000, Training Loss: 14.24310, Validation Loss: 2.05930\n",
      "Epoch 182/1000, Training Loss: 14.24011, Validation Loss: 2.05882\n",
      "Epoch 183/1000, Training Loss: 14.23712, Validation Loss: 2.05834\n",
      "Epoch 184/1000, Training Loss: 14.23414, Validation Loss: 2.05785\n",
      "Epoch 185/1000, Training Loss: 14.23116, Validation Loss: 2.05737\n",
      "Epoch 186/1000, Training Loss: 14.22819, Validation Loss: 2.05689\n",
      "Epoch 187/1000, Training Loss: 14.22523, Validation Loss: 2.05641\n",
      "Epoch 188/1000, Training Loss: 14.22227, Validation Loss: 2.05594\n",
      "Epoch 189/1000, Training Loss: 14.21931, Validation Loss: 2.05546\n",
      "Epoch 190/1000, Training Loss: 14.21636, Validation Loss: 2.05498\n",
      "Epoch 191/1000, Training Loss: 14.21342, Validation Loss: 2.05451\n",
      "Epoch 192/1000, Training Loss: 14.21048, Validation Loss: 2.05403\n",
      "Epoch 193/1000, Training Loss: 14.20755, Validation Loss: 2.05356\n",
      "Epoch 194/1000, Training Loss: 14.20463, Validation Loss: 2.05308\n",
      "Epoch 195/1000, Training Loss: 14.20171, Validation Loss: 2.05261\n",
      "Epoch 196/1000, Training Loss: 14.19879, Validation Loss: 2.05214\n",
      "Epoch 197/1000, Training Loss: 14.19588, Validation Loss: 2.05167\n",
      "Epoch 198/1000, Training Loss: 14.19298, Validation Loss: 2.05120\n",
      "Epoch 199/1000, Training Loss: 14.19008, Validation Loss: 2.05073\n",
      "Epoch 200/1000, Training Loss: 14.18719, Validation Loss: 2.05026\n",
      "Epoch 201/1000, Training Loss: 14.18430, Validation Loss: 2.04980\n",
      "Epoch 202/1000, Training Loss: 14.18142, Validation Loss: 2.04933\n",
      "Epoch 203/1000, Training Loss: 14.17854, Validation Loss: 2.04886\n",
      "Epoch 204/1000, Training Loss: 14.17567, Validation Loss: 2.04840\n",
      "Epoch 205/1000, Training Loss: 14.17280, Validation Loss: 2.04793\n",
      "Epoch 206/1000, Training Loss: 14.16994, Validation Loss: 2.04747\n",
      "Epoch 207/1000, Training Loss: 14.16709, Validation Loss: 2.04701\n",
      "Epoch 208/1000, Training Loss: 14.16424, Validation Loss: 2.04655\n",
      "Epoch 209/1000, Training Loss: 14.16139, Validation Loss: 2.04609\n",
      "Epoch 210/1000, Training Loss: 14.15855, Validation Loss: 2.04563\n",
      "Epoch 211/1000, Training Loss: 14.15572, Validation Loss: 2.04517\n",
      "Epoch 212/1000, Training Loss: 14.15289, Validation Loss: 2.04471\n",
      "Epoch 213/1000, Training Loss: 14.15006, Validation Loss: 2.04425\n",
      "Epoch 214/1000, Training Loss: 14.14724, Validation Loss: 2.04380\n",
      "Epoch 215/1000, Training Loss: 14.14443, Validation Loss: 2.04334\n",
      "Epoch 216/1000, Training Loss: 14.14162, Validation Loss: 2.04289\n",
      "Epoch 217/1000, Training Loss: 14.13881, Validation Loss: 2.04243\n",
      "Epoch 218/1000, Training Loss: 14.13601, Validation Loss: 2.04198\n",
      "Epoch 219/1000, Training Loss: 14.13322, Validation Loss: 2.04153\n",
      "Epoch 220/1000, Training Loss: 14.13043, Validation Loss: 2.04107\n",
      "Epoch 221/1000, Training Loss: 14.12765, Validation Loss: 2.04062\n",
      "Epoch 222/1000, Training Loss: 14.12487, Validation Loss: 2.04017\n",
      "Epoch 223/1000, Training Loss: 14.12209, Validation Loss: 2.03972\n",
      "Epoch 224/1000, Training Loss: 14.11933, Validation Loss: 2.03928\n",
      "Epoch 225/1000, Training Loss: 14.11656, Validation Loss: 2.03883\n",
      "Epoch 226/1000, Training Loss: 14.11380, Validation Loss: 2.03838\n",
      "Epoch 227/1000, Training Loss: 14.11105, Validation Loss: 2.03793\n",
      "Epoch 228/1000, Training Loss: 14.10830, Validation Loss: 2.03749\n",
      "Epoch 229/1000, Training Loss: 14.10555, Validation Loss: 2.03704\n",
      "Epoch 230/1000, Training Loss: 14.10281, Validation Loss: 2.03660\n",
      "Epoch 231/1000, Training Loss: 14.10007, Validation Loss: 2.03615\n",
      "Epoch 232/1000, Training Loss: 14.09734, Validation Loss: 2.03571\n",
      "Epoch 233/1000, Training Loss: 14.09462, Validation Loss: 2.03527\n",
      "Epoch 234/1000, Training Loss: 14.09190, Validation Loss: 2.03483\n",
      "Epoch 235/1000, Training Loss: 14.08918, Validation Loss: 2.03439\n",
      "Epoch 236/1000, Training Loss: 14.08647, Validation Loss: 2.03395\n",
      "Epoch 237/1000, Training Loss: 14.08376, Validation Loss: 2.03351\n",
      "Epoch 238/1000, Training Loss: 14.08106, Validation Loss: 2.03307\n",
      "Epoch 239/1000, Training Loss: 14.07836, Validation Loss: 2.03263\n",
      "Epoch 240/1000, Training Loss: 14.07567, Validation Loss: 2.03220\n",
      "Epoch 241/1000, Training Loss: 14.07298, Validation Loss: 2.03176\n",
      "Epoch 242/1000, Training Loss: 14.07030, Validation Loss: 2.03132\n",
      "Epoch 243/1000, Training Loss: 14.06762, Validation Loss: 2.03089\n",
      "Epoch 244/1000, Training Loss: 14.06494, Validation Loss: 2.03045\n",
      "Epoch 245/1000, Training Loss: 14.06227, Validation Loss: 2.03002\n",
      "Epoch 246/1000, Training Loss: 14.05960, Validation Loss: 2.02959\n",
      "Epoch 247/1000, Training Loss: 14.05694, Validation Loss: 2.02915\n",
      "Epoch 248/1000, Training Loss: 14.05428, Validation Loss: 2.02872\n",
      "Epoch 249/1000, Training Loss: 14.05162, Validation Loss: 2.02829\n",
      "Epoch 250/1000, Training Loss: 14.04897, Validation Loss: 2.02786\n",
      "Epoch 251/1000, Training Loss: 14.04633, Validation Loss: 2.02743\n",
      "Epoch 252/1000, Training Loss: 14.04369, Validation Loss: 2.02700\n",
      "Epoch 253/1000, Training Loss: 14.04105, Validation Loss: 2.02657\n",
      "Epoch 254/1000, Training Loss: 14.03842, Validation Loss: 2.02614\n",
      "Epoch 255/1000, Training Loss: 14.03579, Validation Loss: 2.02572\n",
      "Epoch 256/1000, Training Loss: 14.03317, Validation Loss: 2.02529\n",
      "Epoch 257/1000, Training Loss: 14.03055, Validation Loss: 2.02487\n",
      "Epoch 258/1000, Training Loss: 14.02794, Validation Loss: 2.02444\n",
      "Epoch 259/1000, Training Loss: 14.02533, Validation Loss: 2.02402\n",
      "Epoch 260/1000, Training Loss: 14.02272, Validation Loss: 2.02359\n",
      "Epoch 261/1000, Training Loss: 14.02012, Validation Loss: 2.02317\n",
      "Epoch 262/1000, Training Loss: 14.01752, Validation Loss: 2.02275\n",
      "Epoch 263/1000, Training Loss: 14.01492, Validation Loss: 2.02233\n",
      "Epoch 264/1000, Training Loss: 14.01233, Validation Loss: 2.02191\n",
      "Epoch 265/1000, Training Loss: 14.00975, Validation Loss: 2.02148\n",
      "Epoch 266/1000, Training Loss: 14.00716, Validation Loss: 2.02107\n",
      "Epoch 267/1000, Training Loss: 14.00458, Validation Loss: 2.02065\n",
      "Epoch 268/1000, Training Loss: 14.00201, Validation Loss: 2.02023\n",
      "Epoch 269/1000, Training Loss: 13.99943, Validation Loss: 2.01981\n",
      "Epoch 270/1000, Training Loss: 13.99687, Validation Loss: 2.01939\n",
      "Epoch 271/1000, Training Loss: 13.99430, Validation Loss: 2.01898\n",
      "Epoch 272/1000, Training Loss: 13.99174, Validation Loss: 2.01856\n",
      "Epoch 273/1000, Training Loss: 13.98919, Validation Loss: 2.01814\n",
      "Epoch 274/1000, Training Loss: 13.98664, Validation Loss: 2.01773\n",
      "Epoch 275/1000, Training Loss: 13.98409, Validation Loss: 2.01731\n",
      "Epoch 276/1000, Training Loss: 13.98154, Validation Loss: 2.01690\n",
      "Epoch 277/1000, Training Loss: 13.97900, Validation Loss: 2.01649\n",
      "Epoch 278/1000, Training Loss: 13.97647, Validation Loss: 2.01608\n",
      "Epoch 279/1000, Training Loss: 13.97394, Validation Loss: 2.01566\n",
      "Epoch 280/1000, Training Loss: 13.97141, Validation Loss: 2.01525\n",
      "Epoch 281/1000, Training Loss: 13.96889, Validation Loss: 2.01484\n",
      "Epoch 282/1000, Training Loss: 13.96637, Validation Loss: 2.01443\n",
      "Epoch 283/1000, Training Loss: 13.96385, Validation Loss: 2.01402\n",
      "Epoch 284/1000, Training Loss: 13.96134, Validation Loss: 2.01362\n",
      "Epoch 285/1000, Training Loss: 13.95884, Validation Loss: 2.01321\n",
      "Epoch 286/1000, Training Loss: 13.95633, Validation Loss: 2.01280\n",
      "Epoch 287/1000, Training Loss: 13.95383, Validation Loss: 2.01239\n",
      "Epoch 288/1000, Training Loss: 13.95134, Validation Loss: 2.01199\n",
      "Epoch 289/1000, Training Loss: 13.94885, Validation Loss: 2.01158\n",
      "Epoch 290/1000, Training Loss: 13.94636, Validation Loss: 2.01118\n",
      "Epoch 291/1000, Training Loss: 13.94388, Validation Loss: 2.01077\n",
      "Epoch 292/1000, Training Loss: 13.94140, Validation Loss: 2.01037\n",
      "Epoch 293/1000, Training Loss: 13.93892, Validation Loss: 2.00996\n",
      "Epoch 294/1000, Training Loss: 13.93645, Validation Loss: 2.00956\n",
      "Epoch 295/1000, Training Loss: 13.93399, Validation Loss: 2.00916\n",
      "Epoch 296/1000, Training Loss: 13.93152, Validation Loss: 2.00876\n",
      "Epoch 297/1000, Training Loss: 13.92907, Validation Loss: 2.00835\n",
      "Epoch 298/1000, Training Loss: 13.92661, Validation Loss: 2.00795\n",
      "Epoch 299/1000, Training Loss: 13.92416, Validation Loss: 2.00755\n",
      "Epoch 300/1000, Training Loss: 13.92171, Validation Loss: 2.00715\n",
      "Epoch 301/1000, Training Loss: 13.91927, Validation Loss: 2.00676\n",
      "Epoch 302/1000, Training Loss: 13.91683, Validation Loss: 2.00636\n",
      "Epoch 303/1000, Training Loss: 13.91439, Validation Loss: 2.00596\n",
      "Epoch 304/1000, Training Loss: 13.91196, Validation Loss: 2.00556\n",
      "Epoch 305/1000, Training Loss: 13.90953, Validation Loss: 2.00516\n",
      "Epoch 306/1000, Training Loss: 13.90710, Validation Loss: 2.00477\n",
      "Epoch 307/1000, Training Loss: 13.90468, Validation Loss: 2.00437\n",
      "Epoch 308/1000, Training Loss: 13.90226, Validation Loss: 2.00397\n",
      "Epoch 309/1000, Training Loss: 13.89984, Validation Loss: 2.00358\n",
      "Epoch 310/1000, Training Loss: 13.89743, Validation Loss: 2.00318\n",
      "Epoch 311/1000, Training Loss: 13.89502, Validation Loss: 2.00279\n",
      "Epoch 312/1000, Training Loss: 13.89262, Validation Loss: 2.00239\n",
      "Epoch 313/1000, Training Loss: 13.89021, Validation Loss: 2.00200\n",
      "Epoch 314/1000, Training Loss: 13.88781, Validation Loss: 2.00161\n",
      "Epoch 315/1000, Training Loss: 13.88542, Validation Loss: 2.00122\n",
      "Epoch 316/1000, Training Loss: 13.88303, Validation Loss: 2.00083\n",
      "Epoch 317/1000, Training Loss: 13.88064, Validation Loss: 2.00043\n",
      "Epoch 318/1000, Training Loss: 13.87826, Validation Loss: 2.00004\n",
      "Epoch 319/1000, Training Loss: 13.87588, Validation Loss: 1.99965\n",
      "Epoch 320/1000, Training Loss: 13.87350, Validation Loss: 1.99926\n",
      "Epoch 321/1000, Training Loss: 13.87113, Validation Loss: 1.99888\n",
      "Epoch 322/1000, Training Loss: 13.86876, Validation Loss: 1.99849\n",
      "Epoch 323/1000, Training Loss: 13.86639, Validation Loss: 1.99810\n",
      "Epoch 324/1000, Training Loss: 13.86403, Validation Loss: 1.99771\n",
      "Epoch 325/1000, Training Loss: 13.86167, Validation Loss: 1.99733\n",
      "Epoch 326/1000, Training Loss: 13.85931, Validation Loss: 1.99694\n",
      "Epoch 327/1000, Training Loss: 13.85696, Validation Loss: 1.99655\n",
      "Epoch 328/1000, Training Loss: 13.85461, Validation Loss: 1.99617\n",
      "Epoch 329/1000, Training Loss: 13.85227, Validation Loss: 1.99578\n",
      "Epoch 330/1000, Training Loss: 13.84993, Validation Loss: 1.99540\n",
      "Epoch 331/1000, Training Loss: 13.84759, Validation Loss: 1.99502\n",
      "Epoch 332/1000, Training Loss: 13.84525, Validation Loss: 1.99463\n",
      "Epoch 333/1000, Training Loss: 13.84292, Validation Loss: 1.99425\n",
      "Epoch 334/1000, Training Loss: 13.84059, Validation Loss: 1.99387\n",
      "Epoch 335/1000, Training Loss: 13.83826, Validation Loss: 1.99349\n",
      "Epoch 336/1000, Training Loss: 13.83594, Validation Loss: 1.99311\n",
      "Epoch 337/1000, Training Loss: 13.83362, Validation Loss: 1.99272\n",
      "Epoch 338/1000, Training Loss: 13.83131, Validation Loss: 1.99234\n",
      "Epoch 339/1000, Training Loss: 13.82899, Validation Loss: 1.99196\n",
      "Epoch 340/1000, Training Loss: 13.82668, Validation Loss: 1.99158\n",
      "Epoch 341/1000, Training Loss: 13.82438, Validation Loss: 1.99120\n",
      "Epoch 342/1000, Training Loss: 13.82207, Validation Loss: 1.99083\n",
      "Epoch 343/1000, Training Loss: 13.81977, Validation Loss: 1.99045\n",
      "Epoch 344/1000, Training Loss: 13.81747, Validation Loss: 1.99007\n",
      "Epoch 345/1000, Training Loss: 13.81518, Validation Loss: 1.98969\n",
      "Epoch 346/1000, Training Loss: 13.81289, Validation Loss: 1.98932\n",
      "Epoch 347/1000, Training Loss: 13.81060, Validation Loss: 1.98894\n",
      "Epoch 348/1000, Training Loss: 13.80832, Validation Loss: 1.98856\n",
      "Epoch 349/1000, Training Loss: 13.80604, Validation Loss: 1.98819\n",
      "Epoch 350/1000, Training Loss: 13.80376, Validation Loss: 1.98781\n",
      "Epoch 351/1000, Training Loss: 13.80149, Validation Loss: 1.98744\n",
      "Epoch 352/1000, Training Loss: 13.79922, Validation Loss: 1.98707\n",
      "Epoch 353/1000, Training Loss: 13.79695, Validation Loss: 1.98669\n",
      "Epoch 354/1000, Training Loss: 13.79469, Validation Loss: 1.98632\n",
      "Epoch 355/1000, Training Loss: 13.79243, Validation Loss: 1.98595\n",
      "Epoch 356/1000, Training Loss: 13.79017, Validation Loss: 1.98558\n",
      "Epoch 357/1000, Training Loss: 13.78791, Validation Loss: 1.98521\n",
      "Epoch 358/1000, Training Loss: 13.78566, Validation Loss: 1.98483\n",
      "Epoch 359/1000, Training Loss: 13.78341, Validation Loss: 1.98446\n",
      "Epoch 360/1000, Training Loss: 13.78117, Validation Loss: 1.98409\n",
      "Epoch 361/1000, Training Loss: 13.77893, Validation Loss: 1.98372\n",
      "Epoch 362/1000, Training Loss: 13.77670, Validation Loss: 1.98336\n",
      "Epoch 363/1000, Training Loss: 13.77446, Validation Loss: 1.98299\n",
      "Epoch 364/1000, Training Loss: 13.77223, Validation Loss: 1.98262\n",
      "Epoch 365/1000, Training Loss: 13.77001, Validation Loss: 1.98225\n",
      "Epoch 366/1000, Training Loss: 13.76778, Validation Loss: 1.98188\n",
      "Epoch 367/1000, Training Loss: 13.76556, Validation Loss: 1.98152\n",
      "Epoch 368/1000, Training Loss: 13.76334, Validation Loss: 1.98115\n",
      "Epoch 369/1000, Training Loss: 13.76113, Validation Loss: 1.98079\n",
      "Epoch 370/1000, Training Loss: 13.75891, Validation Loss: 1.98042\n",
      "Epoch 371/1000, Training Loss: 13.75670, Validation Loss: 1.98005\n",
      "Epoch 372/1000, Training Loss: 13.75449, Validation Loss: 1.97969\n",
      "Epoch 373/1000, Training Loss: 13.75229, Validation Loss: 1.97932\n",
      "Epoch 374/1000, Training Loss: 13.75008, Validation Loss: 1.97896\n",
      "Epoch 375/1000, Training Loss: 13.74788, Validation Loss: 1.97860\n",
      "Epoch 376/1000, Training Loss: 13.74569, Validation Loss: 1.97823\n",
      "Epoch 377/1000, Training Loss: 13.74349, Validation Loss: 1.97787\n",
      "Epoch 378/1000, Training Loss: 13.74130, Validation Loss: 1.97751\n",
      "Epoch 379/1000, Training Loss: 13.73911, Validation Loss: 1.97715\n",
      "Epoch 380/1000, Training Loss: 13.73693, Validation Loss: 1.97679\n",
      "Epoch 381/1000, Training Loss: 13.73474, Validation Loss: 1.97643\n",
      "Epoch 382/1000, Training Loss: 13.73257, Validation Loss: 1.97607\n",
      "Epoch 383/1000, Training Loss: 13.73039, Validation Loss: 1.97571\n",
      "Epoch 384/1000, Training Loss: 13.72822, Validation Loss: 1.97535\n",
      "Epoch 385/1000, Training Loss: 13.72604, Validation Loss: 1.97499\n",
      "Epoch 386/1000, Training Loss: 13.72388, Validation Loss: 1.97463\n",
      "Epoch 387/1000, Training Loss: 13.72171, Validation Loss: 1.97427\n",
      "Epoch 388/1000, Training Loss: 13.71955, Validation Loss: 1.97391\n",
      "Epoch 389/1000, Training Loss: 13.71739, Validation Loss: 1.97356\n",
      "Epoch 390/1000, Training Loss: 13.71524, Validation Loss: 1.97320\n",
      "Epoch 391/1000, Training Loss: 13.71309, Validation Loss: 1.97284\n",
      "Epoch 392/1000, Training Loss: 13.71094, Validation Loss: 1.97249\n",
      "Epoch 393/1000, Training Loss: 13.70879, Validation Loss: 1.97213\n",
      "Epoch 394/1000, Training Loss: 13.70665, Validation Loss: 1.97178\n",
      "Epoch 395/1000, Training Loss: 13.70451, Validation Loss: 1.97142\n",
      "Epoch 396/1000, Training Loss: 13.70236, Validation Loss: 1.97107\n",
      "Epoch 397/1000, Training Loss: 13.70023, Validation Loss: 1.97071\n",
      "Epoch 398/1000, Training Loss: 13.69809, Validation Loss: 1.97036\n",
      "Epoch 399/1000, Training Loss: 13.69596, Validation Loss: 1.97000\n",
      "Epoch 400/1000, Training Loss: 13.69383, Validation Loss: 1.96965\n",
      "Epoch 401/1000, Training Loss: 13.69170, Validation Loss: 1.96930\n",
      "Epoch 402/1000, Training Loss: 13.68957, Validation Loss: 1.96895\n",
      "Epoch 403/1000, Training Loss: 13.68745, Validation Loss: 1.96860\n",
      "Epoch 404/1000, Training Loss: 13.68533, Validation Loss: 1.96824\n",
      "Epoch 405/1000, Training Loss: 13.68322, Validation Loss: 1.96789\n",
      "Epoch 406/1000, Training Loss: 13.68110, Validation Loss: 1.96754\n",
      "Epoch 407/1000, Training Loss: 13.67899, Validation Loss: 1.96719\n",
      "Epoch 408/1000, Training Loss: 13.67688, Validation Loss: 1.96684\n",
      "Epoch 409/1000, Training Loss: 13.67477, Validation Loss: 1.96649\n",
      "Epoch 410/1000, Training Loss: 13.67267, Validation Loss: 1.96614\n",
      "Epoch 411/1000, Training Loss: 13.67056, Validation Loss: 1.96580\n",
      "Epoch 412/1000, Training Loss: 13.66846, Validation Loss: 1.96545\n",
      "Epoch 413/1000, Training Loss: 13.66637, Validation Loss: 1.96510\n",
      "Epoch 414/1000, Training Loss: 13.66427, Validation Loss: 1.96475\n",
      "Epoch 415/1000, Training Loss: 13.66218, Validation Loss: 1.96440\n",
      "Epoch 416/1000, Training Loss: 13.66009, Validation Loss: 1.96406\n",
      "Epoch 417/1000, Training Loss: 13.65800, Validation Loss: 1.96371\n",
      "Epoch 418/1000, Training Loss: 13.65592, Validation Loss: 1.96337\n",
      "Epoch 419/1000, Training Loss: 13.65383, Validation Loss: 1.96302\n",
      "Epoch 420/1000, Training Loss: 13.65176, Validation Loss: 1.96268\n",
      "Epoch 421/1000, Training Loss: 13.64968, Validation Loss: 1.96233\n",
      "Epoch 422/1000, Training Loss: 13.64761, Validation Loss: 1.96199\n",
      "Epoch 423/1000, Training Loss: 13.64554, Validation Loss: 1.96164\n",
      "Epoch 424/1000, Training Loss: 13.64347, Validation Loss: 1.96130\n",
      "Epoch 425/1000, Training Loss: 13.64140, Validation Loss: 1.96096\n",
      "Epoch 426/1000, Training Loss: 13.63934, Validation Loss: 1.96061\n",
      "Epoch 427/1000, Training Loss: 13.63728, Validation Loss: 1.96027\n",
      "Epoch 428/1000, Training Loss: 13.63522, Validation Loss: 1.95993\n",
      "Epoch 429/1000, Training Loss: 13.63316, Validation Loss: 1.95959\n",
      "Epoch 430/1000, Training Loss: 13.63111, Validation Loss: 1.95925\n",
      "Epoch 431/1000, Training Loss: 13.62906, Validation Loss: 1.95890\n",
      "Epoch 432/1000, Training Loss: 13.62701, Validation Loss: 1.95856\n",
      "Epoch 433/1000, Training Loss: 13.62496, Validation Loss: 1.95822\n",
      "Epoch 434/1000, Training Loss: 13.62292, Validation Loss: 1.95788\n",
      "Epoch 435/1000, Training Loss: 13.62088, Validation Loss: 1.95754\n",
      "Epoch 436/1000, Training Loss: 13.61884, Validation Loss: 1.95720\n",
      "Epoch 437/1000, Training Loss: 13.61680, Validation Loss: 1.95686\n",
      "Epoch 438/1000, Training Loss: 13.61477, Validation Loss: 1.95652\n",
      "Epoch 439/1000, Training Loss: 13.61274, Validation Loss: 1.95618\n",
      "Epoch 440/1000, Training Loss: 13.61071, Validation Loss: 1.95584\n",
      "Epoch 441/1000, Training Loss: 13.60868, Validation Loss: 1.95550\n",
      "Epoch 442/1000, Training Loss: 13.60666, Validation Loss: 1.95517\n",
      "Epoch 443/1000, Training Loss: 13.60463, Validation Loss: 1.95483\n",
      "Epoch 444/1000, Training Loss: 13.60261, Validation Loss: 1.95449\n",
      "Epoch 445/1000, Training Loss: 13.60059, Validation Loss: 1.95415\n",
      "Epoch 446/1000, Training Loss: 13.59858, Validation Loss: 1.95382\n",
      "Epoch 447/1000, Training Loss: 13.59657, Validation Loss: 1.95348\n",
      "Epoch 448/1000, Training Loss: 13.59455, Validation Loss: 1.95314\n",
      "Epoch 449/1000, Training Loss: 13.59254, Validation Loss: 1.95281\n",
      "Epoch 450/1000, Training Loss: 13.59054, Validation Loss: 1.95247\n",
      "Epoch 451/1000, Training Loss: 13.58853, Validation Loss: 1.95214\n",
      "Epoch 452/1000, Training Loss: 13.58653, Validation Loss: 1.95180\n",
      "Epoch 453/1000, Training Loss: 13.58453, Validation Loss: 1.95147\n",
      "Epoch 454/1000, Training Loss: 13.58253, Validation Loss: 1.95113\n",
      "Epoch 455/1000, Training Loss: 13.58053, Validation Loss: 1.95080\n",
      "Epoch 456/1000, Training Loss: 13.57854, Validation Loss: 1.95046\n",
      "Epoch 457/1000, Training Loss: 13.57654, Validation Loss: 1.95013\n",
      "Epoch 458/1000, Training Loss: 13.57455, Validation Loss: 1.94980\n",
      "Epoch 459/1000, Training Loss: 13.57256, Validation Loss: 1.94946\n",
      "Epoch 460/1000, Training Loss: 13.57057, Validation Loss: 1.94913\n",
      "Epoch 461/1000, Training Loss: 13.56859, Validation Loss: 1.94880\n",
      "Epoch 462/1000, Training Loss: 13.56660, Validation Loss: 1.94847\n",
      "Epoch 463/1000, Training Loss: 13.56462, Validation Loss: 1.94813\n",
      "Epoch 464/1000, Training Loss: 13.56264, Validation Loss: 1.94780\n",
      "Epoch 465/1000, Training Loss: 13.56067, Validation Loss: 1.94747\n",
      "Epoch 466/1000, Training Loss: 13.55869, Validation Loss: 1.94714\n",
      "Epoch 467/1000, Training Loss: 13.55672, Validation Loss: 1.94681\n",
      "Epoch 468/1000, Training Loss: 13.55475, Validation Loss: 1.94648\n",
      "Epoch 469/1000, Training Loss: 13.55278, Validation Loss: 1.94615\n",
      "Epoch 470/1000, Training Loss: 13.55081, Validation Loss: 1.94582\n",
      "Epoch 471/1000, Training Loss: 13.54884, Validation Loss: 1.94549\n",
      "Epoch 472/1000, Training Loss: 13.54688, Validation Loss: 1.94516\n",
      "Epoch 473/1000, Training Loss: 13.54492, Validation Loss: 1.94484\n",
      "Epoch 474/1000, Training Loss: 13.54296, Validation Loss: 1.94451\n",
      "Epoch 475/1000, Training Loss: 13.54101, Validation Loss: 1.94418\n",
      "Epoch 476/1000, Training Loss: 13.53905, Validation Loss: 1.94385\n",
      "Epoch 477/1000, Training Loss: 13.53710, Validation Loss: 1.94353\n",
      "Epoch 478/1000, Training Loss: 13.53515, Validation Loss: 1.94320\n",
      "Epoch 479/1000, Training Loss: 13.53320, Validation Loss: 1.94287\n",
      "Epoch 480/1000, Training Loss: 13.53126, Validation Loss: 1.94255\n",
      "Epoch 481/1000, Training Loss: 13.52932, Validation Loss: 1.94222\n",
      "Epoch 482/1000, Training Loss: 13.52738, Validation Loss: 1.94190\n",
      "Epoch 483/1000, Training Loss: 13.52544, Validation Loss: 1.94157\n",
      "Epoch 484/1000, Training Loss: 13.52350, Validation Loss: 1.94125\n",
      "Epoch 485/1000, Training Loss: 13.52156, Validation Loss: 1.94092\n",
      "Epoch 486/1000, Training Loss: 13.51963, Validation Loss: 1.94060\n",
      "Epoch 487/1000, Training Loss: 13.51770, Validation Loss: 1.94027\n",
      "Epoch 488/1000, Training Loss: 13.51577, Validation Loss: 1.93995\n",
      "Epoch 489/1000, Training Loss: 13.51384, Validation Loss: 1.93963\n",
      "Epoch 490/1000, Training Loss: 13.51192, Validation Loss: 1.93930\n",
      "Epoch 491/1000, Training Loss: 13.50999, Validation Loss: 1.93898\n",
      "Epoch 492/1000, Training Loss: 13.50807, Validation Loss: 1.93866\n",
      "Epoch 493/1000, Training Loss: 13.50615, Validation Loss: 1.93834\n",
      "Epoch 494/1000, Training Loss: 13.50424, Validation Loss: 1.93801\n",
      "Epoch 495/1000, Training Loss: 13.50232, Validation Loss: 1.93769\n",
      "Epoch 496/1000, Training Loss: 13.50041, Validation Loss: 1.93737\n",
      "Epoch 497/1000, Training Loss: 13.49850, Validation Loss: 1.93705\n",
      "Epoch 498/1000, Training Loss: 13.49659, Validation Loss: 1.93673\n",
      "Epoch 499/1000, Training Loss: 13.49468, Validation Loss: 1.93641\n",
      "Epoch 500/1000, Training Loss: 13.49277, Validation Loss: 1.93609\n",
      "Epoch 501/1000, Training Loss: 13.49087, Validation Loss: 1.93577\n",
      "Epoch 502/1000, Training Loss: 13.48897, Validation Loss: 1.93545\n",
      "Epoch 503/1000, Training Loss: 13.48707, Validation Loss: 1.93513\n",
      "Epoch 504/1000, Training Loss: 13.48517, Validation Loss: 1.93481\n",
      "Epoch 505/1000, Training Loss: 13.48327, Validation Loss: 1.93449\n",
      "Epoch 506/1000, Training Loss: 13.48138, Validation Loss: 1.93417\n",
      "Epoch 507/1000, Training Loss: 13.47949, Validation Loss: 1.93386\n",
      "Epoch 508/1000, Training Loss: 13.47760, Validation Loss: 1.93354\n",
      "Epoch 509/1000, Training Loss: 13.47571, Validation Loss: 1.93322\n",
      "Epoch 510/1000, Training Loss: 13.47382, Validation Loss: 1.93290\n",
      "Epoch 511/1000, Training Loss: 13.47194, Validation Loss: 1.93259\n",
      "Epoch 512/1000, Training Loss: 13.47005, Validation Loss: 1.93227\n",
      "Epoch 513/1000, Training Loss: 13.46817, Validation Loss: 1.93195\n",
      "Epoch 514/1000, Training Loss: 13.46629, Validation Loss: 1.93164\n",
      "Epoch 515/1000, Training Loss: 13.46442, Validation Loss: 1.93132\n",
      "Epoch 516/1000, Training Loss: 13.46254, Validation Loss: 1.93100\n",
      "Epoch 517/1000, Training Loss: 13.46067, Validation Loss: 1.93069\n",
      "Epoch 518/1000, Training Loss: 13.45880, Validation Loss: 1.93037\n",
      "Epoch 519/1000, Training Loss: 13.45693, Validation Loss: 1.93006\n",
      "Epoch 520/1000, Training Loss: 13.45506, Validation Loss: 1.92975\n",
      "Epoch 521/1000, Training Loss: 13.45319, Validation Loss: 1.92943\n",
      "Epoch 522/1000, Training Loss: 13.45133, Validation Loss: 1.92912\n",
      "Epoch 523/1000, Training Loss: 13.44946, Validation Loss: 1.92880\n",
      "Epoch 524/1000, Training Loss: 13.44760, Validation Loss: 1.92849\n",
      "Epoch 525/1000, Training Loss: 13.44574, Validation Loss: 1.92818\n",
      "Epoch 526/1000, Training Loss: 13.44388, Validation Loss: 1.92786\n",
      "Epoch 527/1000, Training Loss: 13.44202, Validation Loss: 1.92755\n",
      "Epoch 528/1000, Training Loss: 13.44017, Validation Loss: 1.92724\n",
      "Epoch 529/1000, Training Loss: 13.43831, Validation Loss: 1.92693\n",
      "Epoch 530/1000, Training Loss: 13.43646, Validation Loss: 1.92661\n",
      "Epoch 531/1000, Training Loss: 13.43461, Validation Loss: 1.92630\n",
      "Epoch 532/1000, Training Loss: 13.43277, Validation Loss: 1.92599\n",
      "Epoch 533/1000, Training Loss: 13.43092, Validation Loss: 1.92568\n",
      "Epoch 534/1000, Training Loss: 13.42908, Validation Loss: 1.92537\n",
      "Epoch 535/1000, Training Loss: 13.42723, Validation Loss: 1.92506\n",
      "Epoch 536/1000, Training Loss: 13.42539, Validation Loss: 1.92475\n",
      "Epoch 537/1000, Training Loss: 13.42355, Validation Loss: 1.92444\n",
      "Epoch 538/1000, Training Loss: 13.42171, Validation Loss: 1.92413\n",
      "Epoch 539/1000, Training Loss: 13.41988, Validation Loss: 1.92382\n",
      "Epoch 540/1000, Training Loss: 13.41804, Validation Loss: 1.92351\n",
      "Epoch 541/1000, Training Loss: 13.41621, Validation Loss: 1.92320\n",
      "Epoch 542/1000, Training Loss: 13.41438, Validation Loss: 1.92289\n",
      "Epoch 543/1000, Training Loss: 13.41255, Validation Loss: 1.92258\n",
      "Epoch 544/1000, Training Loss: 13.41072, Validation Loss: 1.92227\n",
      "Epoch 545/1000, Training Loss: 13.40889, Validation Loss: 1.92196\n",
      "Epoch 546/1000, Training Loss: 13.40707, Validation Loss: 1.92165\n",
      "Epoch 547/1000, Training Loss: 13.40525, Validation Loss: 1.92135\n",
      "Epoch 548/1000, Training Loss: 13.40343, Validation Loss: 1.92104\n",
      "Epoch 549/1000, Training Loss: 13.40161, Validation Loss: 1.92073\n",
      "Epoch 550/1000, Training Loss: 13.39979, Validation Loss: 1.92042\n",
      "Epoch 551/1000, Training Loss: 13.39798, Validation Loss: 1.92012\n",
      "Epoch 552/1000, Training Loss: 13.39616, Validation Loss: 1.91981\n",
      "Epoch 553/1000, Training Loss: 13.39435, Validation Loss: 1.91950\n",
      "Epoch 554/1000, Training Loss: 13.39254, Validation Loss: 1.91919\n",
      "Epoch 555/1000, Training Loss: 13.39073, Validation Loss: 1.91889\n",
      "Epoch 556/1000, Training Loss: 13.38892, Validation Loss: 1.91858\n",
      "Epoch 557/1000, Training Loss: 13.38712, Validation Loss: 1.91827\n",
      "Epoch 558/1000, Training Loss: 13.38532, Validation Loss: 1.91797\n",
      "Epoch 559/1000, Training Loss: 13.38351, Validation Loss: 1.91766\n",
      "Epoch 560/1000, Training Loss: 13.38171, Validation Loss: 1.91736\n",
      "Epoch 561/1000, Training Loss: 13.37992, Validation Loss: 1.91705\n",
      "Epoch 562/1000, Training Loss: 13.37812, Validation Loss: 1.91675\n",
      "Epoch 563/1000, Training Loss: 13.37632, Validation Loss: 1.91644\n",
      "Epoch 564/1000, Training Loss: 13.37453, Validation Loss: 1.91614\n",
      "Epoch 565/1000, Training Loss: 13.37274, Validation Loss: 1.91584\n",
      "Epoch 566/1000, Training Loss: 13.37095, Validation Loss: 1.91553\n",
      "Epoch 567/1000, Training Loss: 13.36916, Validation Loss: 1.91523\n",
      "Epoch 568/1000, Training Loss: 13.36737, Validation Loss: 1.91493\n",
      "Epoch 569/1000, Training Loss: 13.36557, Validation Loss: 1.91462\n",
      "Epoch 570/1000, Training Loss: 13.36378, Validation Loss: 1.91432\n",
      "Epoch 571/1000, Training Loss: 13.36199, Validation Loss: 1.91402\n",
      "Epoch 572/1000, Training Loss: 13.36020, Validation Loss: 1.91371\n",
      "Epoch 573/1000, Training Loss: 13.35841, Validation Loss: 1.91341\n",
      "Epoch 574/1000, Training Loss: 13.35662, Validation Loss: 1.91311\n",
      "Epoch 575/1000, Training Loss: 13.35484, Validation Loss: 1.91281\n",
      "Epoch 576/1000, Training Loss: 13.35305, Validation Loss: 1.91251\n",
      "Epoch 577/1000, Training Loss: 13.35127, Validation Loss: 1.91221\n",
      "Epoch 578/1000, Training Loss: 13.34949, Validation Loss: 1.91190\n",
      "Epoch 579/1000, Training Loss: 13.34771, Validation Loss: 1.91160\n",
      "Epoch 580/1000, Training Loss: 13.34593, Validation Loss: 1.91130\n",
      "Epoch 581/1000, Training Loss: 13.34415, Validation Loss: 1.91100\n",
      "Epoch 582/1000, Training Loss: 13.34238, Validation Loss: 1.91070\n",
      "Epoch 583/1000, Training Loss: 13.34061, Validation Loss: 1.91040\n",
      "Epoch 584/1000, Training Loss: 13.33883, Validation Loss: 1.91010\n",
      "Epoch 585/1000, Training Loss: 13.33706, Validation Loss: 1.90980\n",
      "Epoch 586/1000, Training Loss: 13.33529, Validation Loss: 1.90950\n",
      "Epoch 587/1000, Training Loss: 13.33353, Validation Loss: 1.90920\n",
      "Epoch 588/1000, Training Loss: 13.33176, Validation Loss: 1.90890\n",
      "Epoch 589/1000, Training Loss: 13.33000, Validation Loss: 1.90861\n",
      "Epoch 590/1000, Training Loss: 13.32823, Validation Loss: 1.90831\n",
      "Epoch 591/1000, Training Loss: 13.32647, Validation Loss: 1.90801\n",
      "Epoch 592/1000, Training Loss: 13.32471, Validation Loss: 1.90771\n",
      "Epoch 593/1000, Training Loss: 13.32295, Validation Loss: 1.90741\n",
      "Epoch 594/1000, Training Loss: 13.32120, Validation Loss: 1.90711\n",
      "Epoch 595/1000, Training Loss: 13.31944, Validation Loss: 1.90682\n",
      "Epoch 596/1000, Training Loss: 13.31769, Validation Loss: 1.90652\n",
      "Epoch 597/1000, Training Loss: 13.31594, Validation Loss: 1.90622\n",
      "Epoch 598/1000, Training Loss: 13.31419, Validation Loss: 1.90593\n",
      "Epoch 599/1000, Training Loss: 13.31244, Validation Loss: 1.90563\n",
      "Epoch 600/1000, Training Loss: 13.31069, Validation Loss: 1.90533\n",
      "Epoch 601/1000, Training Loss: 13.30894, Validation Loss: 1.90504\n",
      "Epoch 602/1000, Training Loss: 13.30720, Validation Loss: 1.90474\n",
      "Epoch 603/1000, Training Loss: 13.30546, Validation Loss: 1.90445\n",
      "Epoch 604/1000, Training Loss: 13.30372, Validation Loss: 1.90415\n",
      "Epoch 605/1000, Training Loss: 13.30198, Validation Loss: 1.90385\n",
      "Epoch 606/1000, Training Loss: 13.30024, Validation Loss: 1.90356\n",
      "Epoch 607/1000, Training Loss: 13.29851, Validation Loss: 1.90327\n",
      "Epoch 608/1000, Training Loss: 13.29677, Validation Loss: 1.90297\n",
      "Epoch 609/1000, Training Loss: 13.29504, Validation Loss: 1.90268\n",
      "Epoch 610/1000, Training Loss: 13.29331, Validation Loss: 1.90238\n",
      "Epoch 611/1000, Training Loss: 13.29158, Validation Loss: 1.90209\n",
      "Epoch 612/1000, Training Loss: 13.28985, Validation Loss: 1.90180\n",
      "Epoch 613/1000, Training Loss: 13.28812, Validation Loss: 1.90150\n",
      "Epoch 614/1000, Training Loss: 13.28640, Validation Loss: 1.90121\n",
      "Epoch 615/1000, Training Loss: 13.28467, Validation Loss: 1.90092\n",
      "Epoch 616/1000, Training Loss: 13.28295, Validation Loss: 1.90062\n",
      "Epoch 617/1000, Training Loss: 13.28123, Validation Loss: 1.90033\n",
      "Epoch 618/1000, Training Loss: 13.27950, Validation Loss: 1.90004\n",
      "Epoch 619/1000, Training Loss: 13.27778, Validation Loss: 1.89975\n",
      "Epoch 620/1000, Training Loss: 13.27606, Validation Loss: 1.89945\n",
      "Epoch 621/1000, Training Loss: 13.27434, Validation Loss: 1.89916\n",
      "Epoch 622/1000, Training Loss: 13.27263, Validation Loss: 1.89887\n",
      "Epoch 623/1000, Training Loss: 13.27091, Validation Loss: 1.89858\n",
      "Epoch 624/1000, Training Loss: 13.26920, Validation Loss: 1.89829\n",
      "Epoch 625/1000, Training Loss: 13.26749, Validation Loss: 1.89800\n",
      "Epoch 626/1000, Training Loss: 13.26577, Validation Loss: 1.89771\n",
      "Epoch 627/1000, Training Loss: 13.26407, Validation Loss: 1.89742\n",
      "Epoch 628/1000, Training Loss: 13.26236, Validation Loss: 1.89713\n",
      "Epoch 629/1000, Training Loss: 13.26065, Validation Loss: 1.89684\n",
      "Epoch 630/1000, Training Loss: 13.25894, Validation Loss: 1.89655\n",
      "Epoch 631/1000, Training Loss: 13.25724, Validation Loss: 1.89626\n",
      "Epoch 632/1000, Training Loss: 13.25554, Validation Loss: 1.89597\n",
      "Epoch 633/1000, Training Loss: 13.25383, Validation Loss: 1.89568\n",
      "Epoch 634/1000, Training Loss: 13.25213, Validation Loss: 1.89539\n",
      "Epoch 635/1000, Training Loss: 13.25043, Validation Loss: 1.89510\n",
      "Epoch 636/1000, Training Loss: 13.24873, Validation Loss: 1.89481\n",
      "Epoch 637/1000, Training Loss: 13.24704, Validation Loss: 1.89452\n",
      "Epoch 638/1000, Training Loss: 13.24534, Validation Loss: 1.89423\n",
      "Epoch 639/1000, Training Loss: 13.24365, Validation Loss: 1.89395\n",
      "Epoch 640/1000, Training Loss: 13.24195, Validation Loss: 1.89366\n",
      "Epoch 641/1000, Training Loss: 13.24026, Validation Loss: 1.89337\n",
      "Epoch 642/1000, Training Loss: 13.23857, Validation Loss: 1.89308\n",
      "Epoch 643/1000, Training Loss: 13.23688, Validation Loss: 1.89280\n",
      "Epoch 644/1000, Training Loss: 13.23519, Validation Loss: 1.89251\n",
      "Epoch 645/1000, Training Loss: 13.23350, Validation Loss: 1.89222\n",
      "Epoch 646/1000, Training Loss: 13.23182, Validation Loss: 1.89194\n",
      "Epoch 647/1000, Training Loss: 13.23013, Validation Loss: 1.89165\n",
      "Epoch 648/1000, Training Loss: 13.22845, Validation Loss: 1.89137\n",
      "Epoch 649/1000, Training Loss: 13.22677, Validation Loss: 1.89108\n",
      "Epoch 650/1000, Training Loss: 13.22509, Validation Loss: 1.89079\n",
      "Epoch 651/1000, Training Loss: 13.22341, Validation Loss: 1.89051\n",
      "Epoch 652/1000, Training Loss: 13.22173, Validation Loss: 1.89022\n",
      "Epoch 653/1000, Training Loss: 13.22005, Validation Loss: 1.88994\n",
      "Epoch 654/1000, Training Loss: 13.21837, Validation Loss: 1.88966\n",
      "Epoch 655/1000, Training Loss: 13.21670, Validation Loss: 1.88937\n",
      "Epoch 656/1000, Training Loss: 13.21502, Validation Loss: 1.88909\n",
      "Epoch 657/1000, Training Loss: 13.21335, Validation Loss: 1.88880\n",
      "Epoch 658/1000, Training Loss: 13.21168, Validation Loss: 1.88852\n",
      "Epoch 659/1000, Training Loss: 13.21001, Validation Loss: 1.88823\n",
      "Epoch 660/1000, Training Loss: 13.20834, Validation Loss: 1.88795\n",
      "Epoch 661/1000, Training Loss: 13.20667, Validation Loss: 1.88767\n",
      "Epoch 662/1000, Training Loss: 13.20500, Validation Loss: 1.88738\n",
      "Epoch 663/1000, Training Loss: 13.20334, Validation Loss: 1.88710\n",
      "Epoch 664/1000, Training Loss: 13.20167, Validation Loss: 1.88682\n",
      "Epoch 665/1000, Training Loss: 13.20001, Validation Loss: 1.88654\n",
      "Epoch 666/1000, Training Loss: 13.19835, Validation Loss: 1.88625\n",
      "Epoch 667/1000, Training Loss: 13.19668, Validation Loss: 1.88597\n",
      "Epoch 668/1000, Training Loss: 13.19502, Validation Loss: 1.88569\n",
      "Epoch 669/1000, Training Loss: 13.19336, Validation Loss: 1.88541\n",
      "Epoch 670/1000, Training Loss: 13.19170, Validation Loss: 1.88513\n",
      "Epoch 671/1000, Training Loss: 13.19005, Validation Loss: 1.88484\n",
      "Epoch 672/1000, Training Loss: 13.18839, Validation Loss: 1.88456\n",
      "Epoch 673/1000, Training Loss: 13.18674, Validation Loss: 1.88428\n",
      "Epoch 674/1000, Training Loss: 13.18509, Validation Loss: 1.88400\n",
      "Epoch 675/1000, Training Loss: 13.18344, Validation Loss: 1.88372\n",
      "Epoch 676/1000, Training Loss: 13.18178, Validation Loss: 1.88344\n",
      "Epoch 677/1000, Training Loss: 13.18013, Validation Loss: 1.88316\n",
      "Epoch 678/1000, Training Loss: 13.17849, Validation Loss: 1.88288\n",
      "Epoch 679/1000, Training Loss: 13.17684, Validation Loss: 1.88260\n",
      "Epoch 680/1000, Training Loss: 13.17519, Validation Loss: 1.88232\n",
      "Epoch 681/1000, Training Loss: 13.17355, Validation Loss: 1.88204\n",
      "Epoch 682/1000, Training Loss: 13.17191, Validation Loss: 1.88176\n",
      "Epoch 683/1000, Training Loss: 13.17026, Validation Loss: 1.88148\n",
      "Epoch 684/1000, Training Loss: 13.16862, Validation Loss: 1.88120\n",
      "Epoch 685/1000, Training Loss: 13.16698, Validation Loss: 1.88092\n",
      "Epoch 686/1000, Training Loss: 13.16534, Validation Loss: 1.88064\n",
      "Epoch 687/1000, Training Loss: 13.16370, Validation Loss: 1.88037\n",
      "Epoch 688/1000, Training Loss: 13.16207, Validation Loss: 1.88009\n",
      "Epoch 689/1000, Training Loss: 13.16043, Validation Loss: 1.87981\n",
      "Epoch 690/1000, Training Loss: 13.15880, Validation Loss: 1.87953\n",
      "Epoch 691/1000, Training Loss: 13.15716, Validation Loss: 1.87925\n",
      "Epoch 692/1000, Training Loss: 13.15553, Validation Loss: 1.87897\n",
      "Epoch 693/1000, Training Loss: 13.15390, Validation Loss: 1.87870\n",
      "Epoch 694/1000, Training Loss: 13.15227, Validation Loss: 1.87842\n",
      "Epoch 695/1000, Training Loss: 13.15065, Validation Loss: 1.87814\n",
      "Epoch 696/1000, Training Loss: 13.14902, Validation Loss: 1.87786\n",
      "Epoch 697/1000, Training Loss: 13.14740, Validation Loss: 1.87759\n",
      "Epoch 698/1000, Training Loss: 13.14577, Validation Loss: 1.87731\n",
      "Epoch 699/1000, Training Loss: 13.14415, Validation Loss: 1.87703\n",
      "Epoch 700/1000, Training Loss: 13.14253, Validation Loss: 1.87676\n",
      "Epoch 701/1000, Training Loss: 13.14091, Validation Loss: 1.87648\n",
      "Epoch 702/1000, Training Loss: 13.13929, Validation Loss: 1.87620\n",
      "Epoch 703/1000, Training Loss: 13.13767, Validation Loss: 1.87593\n",
      "Epoch 704/1000, Training Loss: 13.13606, Validation Loss: 1.87565\n",
      "Epoch 705/1000, Training Loss: 13.13444, Validation Loss: 1.87537\n",
      "Epoch 706/1000, Training Loss: 13.13282, Validation Loss: 1.87510\n",
      "Epoch 707/1000, Training Loss: 13.13121, Validation Loss: 1.87482\n",
      "Epoch 708/1000, Training Loss: 13.12960, Validation Loss: 1.87455\n",
      "Epoch 709/1000, Training Loss: 13.12799, Validation Loss: 1.87427\n",
      "Epoch 710/1000, Training Loss: 13.12637, Validation Loss: 1.87400\n",
      "Epoch 711/1000, Training Loss: 13.12477, Validation Loss: 1.87372\n",
      "Epoch 712/1000, Training Loss: 13.12316, Validation Loss: 1.87345\n",
      "Epoch 713/1000, Training Loss: 13.12155, Validation Loss: 1.87317\n",
      "Epoch 714/1000, Training Loss: 13.11994, Validation Loss: 1.87290\n",
      "Epoch 715/1000, Training Loss: 13.11834, Validation Loss: 1.87262\n",
      "Epoch 716/1000, Training Loss: 13.11673, Validation Loss: 1.87235\n",
      "Epoch 717/1000, Training Loss: 13.11513, Validation Loss: 1.87208\n",
      "Epoch 718/1000, Training Loss: 13.11352, Validation Loss: 1.87180\n",
      "Epoch 719/1000, Training Loss: 13.11192, Validation Loss: 1.87153\n",
      "Epoch 720/1000, Training Loss: 13.11032, Validation Loss: 1.87126\n",
      "Epoch 721/1000, Training Loss: 13.10872, Validation Loss: 1.87098\n",
      "Epoch 722/1000, Training Loss: 13.10712, Validation Loss: 1.87071\n",
      "Epoch 723/1000, Training Loss: 13.10552, Validation Loss: 1.87044\n",
      "Epoch 724/1000, Training Loss: 13.10393, Validation Loss: 1.87016\n",
      "Epoch 725/1000, Training Loss: 13.10233, Validation Loss: 1.86989\n",
      "Epoch 726/1000, Training Loss: 13.10074, Validation Loss: 1.86962\n",
      "Epoch 727/1000, Training Loss: 13.09914, Validation Loss: 1.86934\n",
      "Epoch 728/1000, Training Loss: 13.09755, Validation Loss: 1.86907\n",
      "Epoch 729/1000, Training Loss: 13.09596, Validation Loss: 1.86880\n",
      "Epoch 730/1000, Training Loss: 13.09437, Validation Loss: 1.86853\n",
      "Epoch 731/1000, Training Loss: 13.09278, Validation Loss: 1.86826\n",
      "Epoch 732/1000, Training Loss: 13.09119, Validation Loss: 1.86799\n",
      "Epoch 733/1000, Training Loss: 13.08960, Validation Loss: 1.86771\n",
      "Epoch 734/1000, Training Loss: 13.08802, Validation Loss: 1.86744\n",
      "Epoch 735/1000, Training Loss: 13.08643, Validation Loss: 1.86717\n",
      "Epoch 736/1000, Training Loss: 13.08484, Validation Loss: 1.86690\n",
      "Epoch 737/1000, Training Loss: 13.08326, Validation Loss: 1.86663\n",
      "Epoch 738/1000, Training Loss: 13.08168, Validation Loss: 1.86636\n",
      "Epoch 739/1000, Training Loss: 13.08010, Validation Loss: 1.86609\n",
      "Epoch 740/1000, Training Loss: 13.07852, Validation Loss: 1.86582\n",
      "Epoch 741/1000, Training Loss: 13.07694, Validation Loss: 1.86555\n",
      "Epoch 742/1000, Training Loss: 13.07536, Validation Loss: 1.86527\n",
      "Epoch 743/1000, Training Loss: 13.07378, Validation Loss: 1.86500\n",
      "Epoch 744/1000, Training Loss: 13.07221, Validation Loss: 1.86473\n",
      "Epoch 745/1000, Training Loss: 13.07063, Validation Loss: 1.86447\n",
      "Epoch 746/1000, Training Loss: 13.06906, Validation Loss: 1.86420\n",
      "Epoch 747/1000, Training Loss: 13.06748, Validation Loss: 1.86393\n",
      "Epoch 748/1000, Training Loss: 13.06591, Validation Loss: 1.86366\n",
      "Epoch 749/1000, Training Loss: 13.06434, Validation Loss: 1.86339\n",
      "Epoch 750/1000, Training Loss: 13.06277, Validation Loss: 1.86312\n",
      "Epoch 751/1000, Training Loss: 13.06120, Validation Loss: 1.86285\n",
      "Epoch 752/1000, Training Loss: 13.05963, Validation Loss: 1.86258\n",
      "Epoch 753/1000, Training Loss: 13.05807, Validation Loss: 1.86231\n",
      "Epoch 754/1000, Training Loss: 13.05650, Validation Loss: 1.86204\n",
      "Epoch 755/1000, Training Loss: 13.05493, Validation Loss: 1.86177\n",
      "Epoch 756/1000, Training Loss: 13.05337, Validation Loss: 1.86150\n",
      "Epoch 757/1000, Training Loss: 13.05181, Validation Loss: 1.86123\n",
      "Epoch 758/1000, Training Loss: 13.05024, Validation Loss: 1.86097\n",
      "Epoch 759/1000, Training Loss: 13.04868, Validation Loss: 1.86070\n",
      "Epoch 760/1000, Training Loss: 13.04712, Validation Loss: 1.86043\n",
      "Epoch 761/1000, Training Loss: 13.04556, Validation Loss: 1.86016\n",
      "Epoch 762/1000, Training Loss: 13.04400, Validation Loss: 1.85989\n",
      "Epoch 763/1000, Training Loss: 13.04244, Validation Loss: 1.85962\n",
      "Epoch 764/1000, Training Loss: 13.04088, Validation Loss: 1.85936\n",
      "Epoch 765/1000, Training Loss: 13.03932, Validation Loss: 1.85909\n",
      "Epoch 766/1000, Training Loss: 13.03776, Validation Loss: 1.85882\n",
      "Epoch 767/1000, Training Loss: 13.03621, Validation Loss: 1.85855\n",
      "Epoch 768/1000, Training Loss: 13.03465, Validation Loss: 1.85829\n",
      "Epoch 769/1000, Training Loss: 13.03310, Validation Loss: 1.85802\n",
      "Epoch 770/1000, Training Loss: 13.03154, Validation Loss: 1.85775\n",
      "Epoch 771/1000, Training Loss: 13.02999, Validation Loss: 1.85749\n",
      "Epoch 772/1000, Training Loss: 13.02844, Validation Loss: 1.85722\n",
      "Epoch 773/1000, Training Loss: 13.02689, Validation Loss: 1.85695\n",
      "Epoch 774/1000, Training Loss: 13.02534, Validation Loss: 1.85669\n",
      "Epoch 775/1000, Training Loss: 13.02379, Validation Loss: 1.85642\n",
      "Epoch 776/1000, Training Loss: 13.02225, Validation Loss: 1.85616\n",
      "Epoch 777/1000, Training Loss: 13.02070, Validation Loss: 1.85589\n",
      "Epoch 778/1000, Training Loss: 13.01915, Validation Loss: 1.85562\n",
      "Epoch 779/1000, Training Loss: 13.01761, Validation Loss: 1.85536\n",
      "Epoch 780/1000, Training Loss: 13.01607, Validation Loss: 1.85509\n",
      "Epoch 781/1000, Training Loss: 13.01452, Validation Loss: 1.85483\n",
      "Epoch 782/1000, Training Loss: 13.01298, Validation Loss: 1.85456\n",
      "Epoch 783/1000, Training Loss: 13.01144, Validation Loss: 1.85430\n",
      "Epoch 784/1000, Training Loss: 13.00990, Validation Loss: 1.85403\n",
      "Epoch 785/1000, Training Loss: 13.00836, Validation Loss: 1.85377\n",
      "Epoch 786/1000, Training Loss: 13.00682, Validation Loss: 1.85351\n",
      "Epoch 787/1000, Training Loss: 13.00529, Validation Loss: 1.85324\n",
      "Epoch 788/1000, Training Loss: 13.00375, Validation Loss: 1.85298\n",
      "Epoch 789/1000, Training Loss: 13.00221, Validation Loss: 1.85271\n",
      "Epoch 790/1000, Training Loss: 13.00068, Validation Loss: 1.85245\n",
      "Epoch 791/1000, Training Loss: 12.99914, Validation Loss: 1.85219\n",
      "Epoch 792/1000, Training Loss: 12.99761, Validation Loss: 1.85192\n",
      "Epoch 793/1000, Training Loss: 12.99608, Validation Loss: 1.85166\n",
      "Epoch 794/1000, Training Loss: 12.99454, Validation Loss: 1.85139\n",
      "Epoch 795/1000, Training Loss: 12.99301, Validation Loss: 1.85113\n",
      "Epoch 796/1000, Training Loss: 12.99148, Validation Loss: 1.85087\n",
      "Epoch 797/1000, Training Loss: 12.98995, Validation Loss: 1.85060\n",
      "Epoch 798/1000, Training Loss: 12.98842, Validation Loss: 1.85034\n",
      "Epoch 799/1000, Training Loss: 12.98689, Validation Loss: 1.85008\n",
      "Epoch 800/1000, Training Loss: 12.98537, Validation Loss: 1.84982\n",
      "Epoch 801/1000, Training Loss: 12.98384, Validation Loss: 1.84955\n",
      "Epoch 802/1000, Training Loss: 12.98231, Validation Loss: 1.84929\n",
      "Epoch 803/1000, Training Loss: 12.98079, Validation Loss: 1.84903\n",
      "Epoch 804/1000, Training Loss: 12.97927, Validation Loss: 1.84877\n",
      "Epoch 805/1000, Training Loss: 12.97774, Validation Loss: 1.84851\n",
      "Epoch 806/1000, Training Loss: 12.97622, Validation Loss: 1.84824\n",
      "Epoch 807/1000, Training Loss: 12.97470, Validation Loss: 1.84798\n",
      "Epoch 808/1000, Training Loss: 12.97318, Validation Loss: 1.84772\n",
      "Epoch 809/1000, Training Loss: 12.97166, Validation Loss: 1.84746\n",
      "Epoch 810/1000, Training Loss: 12.97014, Validation Loss: 1.84720\n",
      "Epoch 811/1000, Training Loss: 12.96862, Validation Loss: 1.84694\n",
      "Epoch 812/1000, Training Loss: 12.96711, Validation Loss: 1.84668\n",
      "Epoch 813/1000, Training Loss: 12.96559, Validation Loss: 1.84642\n",
      "Epoch 814/1000, Training Loss: 12.96407, Validation Loss: 1.84615\n",
      "Epoch 815/1000, Training Loss: 12.96256, Validation Loss: 1.84589\n",
      "Epoch 816/1000, Training Loss: 12.96105, Validation Loss: 1.84563\n",
      "Epoch 817/1000, Training Loss: 12.95954, Validation Loss: 1.84537\n",
      "Epoch 818/1000, Training Loss: 12.95803, Validation Loss: 1.84511\n",
      "Epoch 819/1000, Training Loss: 12.95652, Validation Loss: 1.84485\n",
      "Epoch 820/1000, Training Loss: 12.95501, Validation Loss: 1.84460\n",
      "Epoch 821/1000, Training Loss: 12.95350, Validation Loss: 1.84434\n",
      "Epoch 822/1000, Training Loss: 12.95199, Validation Loss: 1.84408\n",
      "Epoch 823/1000, Training Loss: 12.95049, Validation Loss: 1.84382\n",
      "Epoch 824/1000, Training Loss: 12.94898, Validation Loss: 1.84356\n",
      "Epoch 825/1000, Training Loss: 12.94747, Validation Loss: 1.84330\n",
      "Epoch 826/1000, Training Loss: 12.94597, Validation Loss: 1.84304\n",
      "Epoch 827/1000, Training Loss: 12.94446, Validation Loss: 1.84278\n",
      "Epoch 828/1000, Training Loss: 12.94296, Validation Loss: 1.84252\n",
      "Epoch 829/1000, Training Loss: 12.94146, Validation Loss: 1.84226\n",
      "Epoch 830/1000, Training Loss: 12.93995, Validation Loss: 1.84201\n",
      "Epoch 831/1000, Training Loss: 12.93845, Validation Loss: 1.84175\n",
      "Epoch 832/1000, Training Loss: 12.93695, Validation Loss: 1.84149\n",
      "Epoch 833/1000, Training Loss: 12.93545, Validation Loss: 1.84123\n",
      "Epoch 834/1000, Training Loss: 12.93395, Validation Loss: 1.84097\n",
      "Epoch 835/1000, Training Loss: 12.93245, Validation Loss: 1.84071\n",
      "Epoch 836/1000, Training Loss: 12.93096, Validation Loss: 1.84046\n",
      "Epoch 837/1000, Training Loss: 12.92946, Validation Loss: 1.84020\n",
      "Epoch 838/1000, Training Loss: 12.92797, Validation Loss: 1.83994\n",
      "Epoch 839/1000, Training Loss: 12.92647, Validation Loss: 1.83968\n",
      "Epoch 840/1000, Training Loss: 12.92498, Validation Loss: 1.83943\n",
      "Epoch 841/1000, Training Loss: 12.92349, Validation Loss: 1.83917\n",
      "Epoch 842/1000, Training Loss: 12.92199, Validation Loss: 1.83891\n",
      "Epoch 843/1000, Training Loss: 12.92050, Validation Loss: 1.83865\n",
      "Epoch 844/1000, Training Loss: 12.91901, Validation Loss: 1.83840\n",
      "Epoch 845/1000, Training Loss: 12.91752, Validation Loss: 1.83814\n",
      "Epoch 846/1000, Training Loss: 12.91603, Validation Loss: 1.83788\n",
      "Epoch 847/1000, Training Loss: 12.91454, Validation Loss: 1.83763\n",
      "Epoch 848/1000, Training Loss: 12.91305, Validation Loss: 1.83737\n",
      "Epoch 849/1000, Training Loss: 12.91156, Validation Loss: 1.83711\n",
      "Epoch 850/1000, Training Loss: 12.91008, Validation Loss: 1.83686\n",
      "Epoch 851/1000, Training Loss: 12.90859, Validation Loss: 1.83660\n",
      "Epoch 852/1000, Training Loss: 12.90711, Validation Loss: 1.83635\n",
      "Epoch 853/1000, Training Loss: 12.90562, Validation Loss: 1.83609\n",
      "Epoch 854/1000, Training Loss: 12.90414, Validation Loss: 1.83583\n",
      "Epoch 855/1000, Training Loss: 12.90265, Validation Loss: 1.83558\n",
      "Epoch 856/1000, Training Loss: 12.90117, Validation Loss: 1.83532\n",
      "Epoch 857/1000, Training Loss: 12.89969, Validation Loss: 1.83507\n",
      "Epoch 858/1000, Training Loss: 12.89820, Validation Loss: 1.83481\n",
      "Epoch 859/1000, Training Loss: 12.89672, Validation Loss: 1.83456\n",
      "Epoch 860/1000, Training Loss: 12.89524, Validation Loss: 1.83430\n",
      "Epoch 861/1000, Training Loss: 12.89376, Validation Loss: 1.83405\n",
      "Epoch 862/1000, Training Loss: 12.89228, Validation Loss: 1.83379\n",
      "Epoch 863/1000, Training Loss: 12.89080, Validation Loss: 1.83354\n",
      "Epoch 864/1000, Training Loss: 12.88933, Validation Loss: 1.83328\n",
      "Epoch 865/1000, Training Loss: 12.88785, Validation Loss: 1.83303\n",
      "Epoch 866/1000, Training Loss: 12.88637, Validation Loss: 1.83277\n",
      "Epoch 867/1000, Training Loss: 12.88489, Validation Loss: 1.83252\n",
      "Epoch 868/1000, Training Loss: 12.88342, Validation Loss: 1.83226\n",
      "Epoch 869/1000, Training Loss: 12.88194, Validation Loss: 1.83201\n",
      "Epoch 870/1000, Training Loss: 12.88046, Validation Loss: 1.83175\n",
      "Epoch 871/1000, Training Loss: 12.87899, Validation Loss: 1.83150\n",
      "Epoch 872/1000, Training Loss: 12.87751, Validation Loss: 1.83124\n",
      "Epoch 873/1000, Training Loss: 12.87603, Validation Loss: 1.83099\n",
      "Epoch 874/1000, Training Loss: 12.87456, Validation Loss: 1.83073\n",
      "Epoch 875/1000, Training Loss: 12.87309, Validation Loss: 1.83048\n",
      "Epoch 876/1000, Training Loss: 12.87161, Validation Loss: 1.83023\n",
      "Epoch 877/1000, Training Loss: 12.87014, Validation Loss: 1.82997\n",
      "Epoch 878/1000, Training Loss: 12.86867, Validation Loss: 1.82972\n",
      "Epoch 879/1000, Training Loss: 12.86719, Validation Loss: 1.82946\n",
      "Epoch 880/1000, Training Loss: 12.86572, Validation Loss: 1.82921\n",
      "Epoch 881/1000, Training Loss: 12.86425, Validation Loss: 1.82896\n",
      "Epoch 882/1000, Training Loss: 12.86278, Validation Loss: 1.82870\n",
      "Epoch 883/1000, Training Loss: 12.86132, Validation Loss: 1.82845\n",
      "Epoch 884/1000, Training Loss: 12.85985, Validation Loss: 1.82820\n",
      "Epoch 885/1000, Training Loss: 12.85838, Validation Loss: 1.82794\n",
      "Epoch 886/1000, Training Loss: 12.85691, Validation Loss: 1.82769\n",
      "Epoch 887/1000, Training Loss: 12.85545, Validation Loss: 1.82744\n",
      "Epoch 888/1000, Training Loss: 12.85398, Validation Loss: 1.82719\n",
      "Epoch 889/1000, Training Loss: 12.85252, Validation Loss: 1.82693\n",
      "Epoch 890/1000, Training Loss: 12.85106, Validation Loss: 1.82668\n",
      "Epoch 891/1000, Training Loss: 12.84959, Validation Loss: 1.82643\n",
      "Epoch 892/1000, Training Loss: 12.84813, Validation Loss: 1.82618\n",
      "Epoch 893/1000, Training Loss: 12.84667, Validation Loss: 1.82593\n",
      "Epoch 894/1000, Training Loss: 12.84521, Validation Loss: 1.82567\n",
      "Epoch 895/1000, Training Loss: 12.84375, Validation Loss: 1.82542\n",
      "Epoch 896/1000, Training Loss: 12.84229, Validation Loss: 1.82517\n",
      "Epoch 897/1000, Training Loss: 12.84083, Validation Loss: 1.82492\n",
      "Epoch 898/1000, Training Loss: 12.83937, Validation Loss: 1.82467\n",
      "Epoch 899/1000, Training Loss: 12.83791, Validation Loss: 1.82442\n",
      "Epoch 900/1000, Training Loss: 12.83645, Validation Loss: 1.82416\n",
      "Epoch 901/1000, Training Loss: 12.83500, Validation Loss: 1.82391\n",
      "Epoch 902/1000, Training Loss: 12.83354, Validation Loss: 1.82366\n",
      "Epoch 903/1000, Training Loss: 12.83208, Validation Loss: 1.82341\n",
      "Epoch 904/1000, Training Loss: 12.83063, Validation Loss: 1.82316\n",
      "Epoch 905/1000, Training Loss: 12.82917, Validation Loss: 1.82291\n",
      "Epoch 906/1000, Training Loss: 12.82772, Validation Loss: 1.82266\n",
      "Epoch 907/1000, Training Loss: 12.82626, Validation Loss: 1.82241\n",
      "Epoch 908/1000, Training Loss: 12.82481, Validation Loss: 1.82216\n",
      "Epoch 909/1000, Training Loss: 12.82335, Validation Loss: 1.82191\n",
      "Epoch 910/1000, Training Loss: 12.82190, Validation Loss: 1.82166\n",
      "Epoch 911/1000, Training Loss: 12.82045, Validation Loss: 1.82141\n",
      "Epoch 912/1000, Training Loss: 12.81899, Validation Loss: 1.82116\n",
      "Epoch 913/1000, Training Loss: 12.81754, Validation Loss: 1.82091\n",
      "Epoch 914/1000, Training Loss: 12.81609, Validation Loss: 1.82066\n",
      "Epoch 915/1000, Training Loss: 12.81464, Validation Loss: 1.82041\n",
      "Epoch 916/1000, Training Loss: 12.81319, Validation Loss: 1.82016\n",
      "Epoch 917/1000, Training Loss: 12.81174, Validation Loss: 1.81991\n",
      "Epoch 918/1000, Training Loss: 12.81029, Validation Loss: 1.81966\n",
      "Epoch 919/1000, Training Loss: 12.80884, Validation Loss: 1.81941\n",
      "Epoch 920/1000, Training Loss: 12.80739, Validation Loss: 1.81916\n",
      "Epoch 921/1000, Training Loss: 12.80594, Validation Loss: 1.81891\n",
      "Epoch 922/1000, Training Loss: 12.80449, Validation Loss: 1.81866\n",
      "Epoch 923/1000, Training Loss: 12.80304, Validation Loss: 1.81841\n",
      "Epoch 924/1000, Training Loss: 12.80159, Validation Loss: 1.81816\n",
      "Epoch 925/1000, Training Loss: 12.80014, Validation Loss: 1.81791\n",
      "Epoch 926/1000, Training Loss: 12.79870, Validation Loss: 1.81767\n",
      "Epoch 927/1000, Training Loss: 12.79725, Validation Loss: 1.81742\n",
      "Epoch 928/1000, Training Loss: 12.79580, Validation Loss: 1.81717\n",
      "Epoch 929/1000, Training Loss: 12.79436, Validation Loss: 1.81692\n",
      "Epoch 930/1000, Training Loss: 12.79291, Validation Loss: 1.81667\n",
      "Epoch 931/1000, Training Loss: 12.79147, Validation Loss: 1.81642\n",
      "Epoch 932/1000, Training Loss: 12.79003, Validation Loss: 1.81617\n",
      "Epoch 933/1000, Training Loss: 12.78858, Validation Loss: 1.81593\n",
      "Epoch 934/1000, Training Loss: 12.78714, Validation Loss: 1.81568\n",
      "Epoch 935/1000, Training Loss: 12.78570, Validation Loss: 1.81543\n",
      "Epoch 936/1000, Training Loss: 12.78425, Validation Loss: 1.81518\n",
      "Epoch 937/1000, Training Loss: 12.78281, Validation Loss: 1.81493\n",
      "Epoch 938/1000, Training Loss: 12.78137, Validation Loss: 1.81469\n",
      "Epoch 939/1000, Training Loss: 12.77993, Validation Loss: 1.81444\n",
      "Epoch 940/1000, Training Loss: 12.77849, Validation Loss: 1.81419\n",
      "Epoch 941/1000, Training Loss: 12.77705, Validation Loss: 1.81394\n",
      "Epoch 942/1000, Training Loss: 12.77561, Validation Loss: 1.81369\n",
      "Epoch 943/1000, Training Loss: 12.77417, Validation Loss: 1.81345\n",
      "Epoch 944/1000, Training Loss: 12.77273, Validation Loss: 1.81320\n",
      "Epoch 945/1000, Training Loss: 12.77129, Validation Loss: 1.81295\n",
      "Epoch 946/1000, Training Loss: 12.76985, Validation Loss: 1.81271\n",
      "Epoch 947/1000, Training Loss: 12.76842, Validation Loss: 1.81246\n",
      "Epoch 948/1000, Training Loss: 12.76698, Validation Loss: 1.81221\n",
      "Epoch 949/1000, Training Loss: 12.76554, Validation Loss: 1.81197\n",
      "Epoch 950/1000, Training Loss: 12.76411, Validation Loss: 1.81172\n",
      "Epoch 951/1000, Training Loss: 12.76267, Validation Loss: 1.81147\n",
      "Epoch 952/1000, Training Loss: 12.76124, Validation Loss: 1.81122\n",
      "Epoch 953/1000, Training Loss: 12.75980, Validation Loss: 1.81098\n",
      "Epoch 954/1000, Training Loss: 12.75837, Validation Loss: 1.81073\n",
      "Epoch 955/1000, Training Loss: 12.75694, Validation Loss: 1.81048\n",
      "Epoch 956/1000, Training Loss: 12.75550, Validation Loss: 1.81024\n",
      "Epoch 957/1000, Training Loss: 12.75407, Validation Loss: 1.80999\n",
      "Epoch 958/1000, Training Loss: 12.75264, Validation Loss: 1.80975\n",
      "Epoch 959/1000, Training Loss: 12.75121, Validation Loss: 1.80950\n",
      "Epoch 960/1000, Training Loss: 12.74978, Validation Loss: 1.80925\n",
      "Epoch 961/1000, Training Loss: 12.74835, Validation Loss: 1.80901\n",
      "Epoch 962/1000, Training Loss: 12.74692, Validation Loss: 1.80876\n",
      "Epoch 963/1000, Training Loss: 12.74549, Validation Loss: 1.80852\n",
      "Epoch 964/1000, Training Loss: 12.74406, Validation Loss: 1.80827\n",
      "Epoch 965/1000, Training Loss: 12.74263, Validation Loss: 1.80802\n",
      "Epoch 966/1000, Training Loss: 12.74120, Validation Loss: 1.80778\n",
      "Epoch 967/1000, Training Loss: 12.73977, Validation Loss: 1.80753\n",
      "Epoch 968/1000, Training Loss: 12.73834, Validation Loss: 1.80729\n",
      "Epoch 969/1000, Training Loss: 12.73692, Validation Loss: 1.80704\n",
      "Epoch 970/1000, Training Loss: 12.73549, Validation Loss: 1.80680\n",
      "Epoch 971/1000, Training Loss: 12.73407, Validation Loss: 1.80655\n",
      "Epoch 972/1000, Training Loss: 12.73265, Validation Loss: 1.80631\n",
      "Epoch 973/1000, Training Loss: 12.73123, Validation Loss: 1.80606\n",
      "Epoch 974/1000, Training Loss: 12.72980, Validation Loss: 1.80582\n",
      "Epoch 975/1000, Training Loss: 12.72838, Validation Loss: 1.80557\n",
      "Epoch 976/1000, Training Loss: 12.72696, Validation Loss: 1.80533\n",
      "Epoch 977/1000, Training Loss: 12.72554, Validation Loss: 1.80508\n",
      "Epoch 978/1000, Training Loss: 12.72412, Validation Loss: 1.80484\n",
      "Epoch 979/1000, Training Loss: 12.72270, Validation Loss: 1.80459\n",
      "Epoch 980/1000, Training Loss: 12.72127, Validation Loss: 1.80435\n",
      "Epoch 981/1000, Training Loss: 12.71985, Validation Loss: 1.80411\n",
      "Epoch 982/1000, Training Loss: 12.71843, Validation Loss: 1.80386\n",
      "Epoch 983/1000, Training Loss: 12.71701, Validation Loss: 1.80362\n",
      "Epoch 984/1000, Training Loss: 12.71560, Validation Loss: 1.80337\n",
      "Epoch 985/1000, Training Loss: 12.71418, Validation Loss: 1.80313\n",
      "Epoch 986/1000, Training Loss: 12.71276, Validation Loss: 1.80288\n",
      "Epoch 987/1000, Training Loss: 12.71134, Validation Loss: 1.80264\n",
      "Epoch 988/1000, Training Loss: 12.70992, Validation Loss: 1.80240\n",
      "Epoch 989/1000, Training Loss: 12.70851, Validation Loss: 1.80215\n",
      "Epoch 990/1000, Training Loss: 12.70709, Validation Loss: 1.80191\n",
      "Epoch 991/1000, Training Loss: 12.70567, Validation Loss: 1.80166\n",
      "Epoch 992/1000, Training Loss: 12.70425, Validation Loss: 1.80142\n",
      "Epoch 993/1000, Training Loss: 12.70284, Validation Loss: 1.80118\n",
      "Epoch 994/1000, Training Loss: 12.70142, Validation Loss: 1.80093\n",
      "Epoch 995/1000, Training Loss: 12.70001, Validation Loss: 1.80069\n",
      "Epoch 996/1000, Training Loss: 12.69859, Validation Loss: 1.80045\n",
      "Epoch 997/1000, Training Loss: 12.69718, Validation Loss: 1.80020\n",
      "Epoch 998/1000, Training Loss: 12.69576, Validation Loss: 1.79996\n",
      "Epoch 999/1000, Training Loss: 12.69435, Validation Loss: 1.79972\n",
      "Epoch 1000/1000, Training Loss: 12.69294, Validation Loss: 1.79947\n",
      "Training took: 36.52 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_batch_30_3 = NeuralNetwork().to(device)\n",
    "summary(model_batch_30_3, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 30\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_batch_30_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_batch_30_3=[]\n",
    "val_loss_list_batch_30_3=[]\n",
    "train_accuracy_list_batch_30_3=[]\n",
    "val_accuracy_list_batch_30_3=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_batch_30_3.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_batch_30_3(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_batch_30_3.append(train_accuracy)\n",
    "    train_loss_list_batch_30_3.append(train_loss)\n",
    "\n",
    "    model_batch_30_3.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_batch_30_3(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_batch_30_3.append(val_accuracy)\n",
    "    val_accuracy_list_batch_30_3.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-k5F-CLpK-n",
    "outputId": "661b48d1-bae3-4d5f-84ae-3689cef89521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable batch size with batch size as 30: 0.6494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_batch_30_3.eval()\n",
    "test_predictions_batch_30_3 = model_batch_30_3(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_batch_30_3 = torch.round(test_predictions_batch_30_3)\n",
    "\n",
    "test_predictions_rounded_numpy_batch_30_3 = test_predictions_rounded_batch_30_3.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_batch_30_3 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_batch_30_3)\n",
    "\n",
    "print(f\"Accuracy for variable batch size with batch size as 30: {accuracy_batch_30_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFbqAKs9pOy4",
    "outputId": "5e7cec28-15b2-47e1-f43c-d759dd6bbbe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable batch size with batch size as 30: 0.59580\n"
     ]
    }
   ],
   "source": [
    "model_batch_30_3.eval()\n",
    "test_loss_batch_30_3=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_batch_30_3 = model_batch_30_3(X_test_tensor)\n",
    "    test_loss_batch_30_3 = loss_function(test_outputs_batch_30_3, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable batch size with batch size as 30: {test_loss_batch_30_3.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1fsLcuVpueS"
   },
   "source": [
    "Taking Epochs as the Hyperparameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFxmastVp6vT"
   },
   "source": [
    "Epochs = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnsZCxhvqGoi",
    "outputId": "ffc2f5b4-e3a7-46b3-b2d4-3dc268aefcae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200, Training Loss: 42.75400, Validation Loss: 6.28222\n",
      "Epoch 2/1200, Training Loss: 42.72883, Validation Loss: 6.27796\n",
      "Epoch 3/1200, Training Loss: 42.70382, Validation Loss: 6.27371\n",
      "Epoch 4/1200, Training Loss: 42.67898, Validation Loss: 6.26950\n",
      "Epoch 5/1200, Training Loss: 42.65431, Validation Loss: 6.26531\n",
      "Epoch 6/1200, Training Loss: 42.62980, Validation Loss: 6.26115\n",
      "Epoch 7/1200, Training Loss: 42.60544, Validation Loss: 6.25701\n",
      "Epoch 8/1200, Training Loss: 42.58123, Validation Loss: 6.25290\n",
      "Epoch 9/1200, Training Loss: 42.55717, Validation Loss: 6.24881\n",
      "Epoch 10/1200, Training Loss: 42.53326, Validation Loss: 6.24474\n",
      "Epoch 11/1200, Training Loss: 42.50949, Validation Loss: 6.24069\n",
      "Epoch 12/1200, Training Loss: 42.48585, Validation Loss: 6.23666\n",
      "Epoch 13/1200, Training Loss: 42.46234, Validation Loss: 6.23265\n",
      "Epoch 14/1200, Training Loss: 42.43896, Validation Loss: 6.22866\n",
      "Epoch 15/1200, Training Loss: 42.41571, Validation Loss: 6.22469\n",
      "Epoch 16/1200, Training Loss: 42.39260, Validation Loss: 6.22074\n",
      "Epoch 17/1200, Training Loss: 42.36961, Validation Loss: 6.21681\n",
      "Epoch 18/1200, Training Loss: 42.34676, Validation Loss: 6.21289\n",
      "Epoch 19/1200, Training Loss: 42.32403, Validation Loss: 6.20899\n",
      "Epoch 20/1200, Training Loss: 42.30142, Validation Loss: 6.20510\n",
      "Epoch 21/1200, Training Loss: 42.27892, Validation Loss: 6.20123\n",
      "Epoch 22/1200, Training Loss: 42.25655, Validation Loss: 6.19738\n",
      "Epoch 23/1200, Training Loss: 42.23432, Validation Loss: 6.19355\n",
      "Epoch 24/1200, Training Loss: 42.21221, Validation Loss: 6.18975\n",
      "Epoch 25/1200, Training Loss: 42.19023, Validation Loss: 6.18596\n",
      "Epoch 26/1200, Training Loss: 42.16836, Validation Loss: 6.18219\n",
      "Epoch 27/1200, Training Loss: 42.14661, Validation Loss: 6.17844\n",
      "Epoch 28/1200, Training Loss: 42.12496, Validation Loss: 6.17471\n",
      "Epoch 29/1200, Training Loss: 42.10344, Validation Loss: 6.17100\n",
      "Epoch 30/1200, Training Loss: 42.08204, Validation Loss: 6.16731\n",
      "Epoch 31/1200, Training Loss: 42.06075, Validation Loss: 6.16363\n",
      "Epoch 32/1200, Training Loss: 42.03957, Validation Loss: 6.15997\n",
      "Epoch 33/1200, Training Loss: 42.01849, Validation Loss: 6.15633\n",
      "Epoch 34/1200, Training Loss: 41.99750, Validation Loss: 6.15270\n",
      "Epoch 35/1200, Training Loss: 41.97660, Validation Loss: 6.14909\n",
      "Epoch 36/1200, Training Loss: 41.95581, Validation Loss: 6.14549\n",
      "Epoch 37/1200, Training Loss: 41.93515, Validation Loss: 6.14192\n",
      "Epoch 38/1200, Training Loss: 41.91460, Validation Loss: 6.13836\n",
      "Epoch 39/1200, Training Loss: 41.89415, Validation Loss: 6.13482\n",
      "Epoch 40/1200, Training Loss: 41.87381, Validation Loss: 6.13129\n",
      "Epoch 41/1200, Training Loss: 41.85357, Validation Loss: 6.12778\n",
      "Epoch 42/1200, Training Loss: 41.83344, Validation Loss: 6.12428\n",
      "Epoch 43/1200, Training Loss: 41.81341, Validation Loss: 6.12080\n",
      "Epoch 44/1200, Training Loss: 41.79348, Validation Loss: 6.11733\n",
      "Epoch 45/1200, Training Loss: 41.77365, Validation Loss: 6.11388\n",
      "Epoch 46/1200, Training Loss: 41.75394, Validation Loss: 6.11045\n",
      "Epoch 47/1200, Training Loss: 41.73432, Validation Loss: 6.10703\n",
      "Epoch 48/1200, Training Loss: 41.71481, Validation Loss: 6.10363\n",
      "Epoch 49/1200, Training Loss: 41.69539, Validation Loss: 6.10024\n",
      "Epoch 50/1200, Training Loss: 41.67607, Validation Loss: 6.09687\n",
      "Epoch 51/1200, Training Loss: 41.65686, Validation Loss: 6.09350\n",
      "Epoch 52/1200, Training Loss: 41.63773, Validation Loss: 6.09016\n",
      "Epoch 53/1200, Training Loss: 41.61870, Validation Loss: 6.08682\n",
      "Epoch 54/1200, Training Loss: 41.59975, Validation Loss: 6.08350\n",
      "Epoch 55/1200, Training Loss: 41.58089, Validation Loss: 6.08019\n",
      "Epoch 56/1200, Training Loss: 41.56213, Validation Loss: 6.07690\n",
      "Epoch 57/1200, Training Loss: 41.54345, Validation Loss: 6.07362\n",
      "Epoch 58/1200, Training Loss: 41.52485, Validation Loss: 6.07036\n",
      "Epoch 59/1200, Training Loss: 41.50633, Validation Loss: 6.06710\n",
      "Epoch 60/1200, Training Loss: 41.48790, Validation Loss: 6.06386\n",
      "Epoch 61/1200, Training Loss: 41.46956, Validation Loss: 6.06064\n",
      "Epoch 62/1200, Training Loss: 41.45130, Validation Loss: 6.05743\n",
      "Epoch 63/1200, Training Loss: 41.43311, Validation Loss: 6.05423\n",
      "Epoch 64/1200, Training Loss: 41.41500, Validation Loss: 6.05105\n",
      "Epoch 65/1200, Training Loss: 41.39696, Validation Loss: 6.04787\n",
      "Epoch 66/1200, Training Loss: 41.37900, Validation Loss: 6.04471\n",
      "Epoch 67/1200, Training Loss: 41.36111, Validation Loss: 6.04156\n",
      "Epoch 68/1200, Training Loss: 41.34330, Validation Loss: 6.03841\n",
      "Epoch 69/1200, Training Loss: 41.32557, Validation Loss: 6.03527\n",
      "Epoch 70/1200, Training Loss: 41.30792, Validation Loss: 6.03214\n",
      "Epoch 71/1200, Training Loss: 41.29035, Validation Loss: 6.02903\n",
      "Epoch 72/1200, Training Loss: 41.27285, Validation Loss: 6.02593\n",
      "Epoch 73/1200, Training Loss: 41.25542, Validation Loss: 6.02284\n",
      "Epoch 74/1200, Training Loss: 41.23807, Validation Loss: 6.01975\n",
      "Epoch 75/1200, Training Loss: 41.22079, Validation Loss: 6.01669\n",
      "Epoch 76/1200, Training Loss: 41.20361, Validation Loss: 6.01363\n",
      "Epoch 77/1200, Training Loss: 41.18650, Validation Loss: 6.01059\n",
      "Epoch 78/1200, Training Loss: 41.16945, Validation Loss: 6.00756\n",
      "Epoch 79/1200, Training Loss: 41.15246, Validation Loss: 6.00454\n",
      "Epoch 80/1200, Training Loss: 41.13554, Validation Loss: 6.00153\n",
      "Epoch 81/1200, Training Loss: 41.11866, Validation Loss: 5.99853\n",
      "Epoch 82/1200, Training Loss: 41.10183, Validation Loss: 5.99553\n",
      "Epoch 83/1200, Training Loss: 41.08504, Validation Loss: 5.99255\n",
      "Epoch 84/1200, Training Loss: 41.06832, Validation Loss: 5.98958\n",
      "Epoch 85/1200, Training Loss: 41.05169, Validation Loss: 5.98662\n",
      "Epoch 86/1200, Training Loss: 41.03513, Validation Loss: 5.98367\n",
      "Epoch 87/1200, Training Loss: 41.01864, Validation Loss: 5.98073\n",
      "Epoch 88/1200, Training Loss: 41.00222, Validation Loss: 5.97780\n",
      "Epoch 89/1200, Training Loss: 40.98587, Validation Loss: 5.97488\n",
      "Epoch 90/1200, Training Loss: 40.96958, Validation Loss: 5.97196\n",
      "Epoch 91/1200, Training Loss: 40.95337, Validation Loss: 5.96906\n",
      "Epoch 92/1200, Training Loss: 40.93722, Validation Loss: 5.96617\n",
      "Epoch 93/1200, Training Loss: 40.92114, Validation Loss: 5.96329\n",
      "Epoch 94/1200, Training Loss: 40.90514, Validation Loss: 5.96042\n",
      "Epoch 95/1200, Training Loss: 40.88919, Validation Loss: 5.95756\n",
      "Epoch 96/1200, Training Loss: 40.87329, Validation Loss: 5.95471\n",
      "Epoch 97/1200, Training Loss: 40.85744, Validation Loss: 5.95186\n",
      "Epoch 98/1200, Training Loss: 40.84165, Validation Loss: 5.94902\n",
      "Epoch 99/1200, Training Loss: 40.82590, Validation Loss: 5.94619\n",
      "Epoch 100/1200, Training Loss: 40.81020, Validation Loss: 5.94337\n",
      "Epoch 101/1200, Training Loss: 40.79456, Validation Loss: 5.94055\n",
      "Epoch 102/1200, Training Loss: 40.77898, Validation Loss: 5.93775\n",
      "Epoch 103/1200, Training Loss: 40.76345, Validation Loss: 5.93496\n",
      "Epoch 104/1200, Training Loss: 40.74798, Validation Loss: 5.93217\n",
      "Epoch 105/1200, Training Loss: 40.73255, Validation Loss: 5.92939\n",
      "Epoch 106/1200, Training Loss: 40.71718, Validation Loss: 5.92663\n",
      "Epoch 107/1200, Training Loss: 40.70186, Validation Loss: 5.92387\n",
      "Epoch 108/1200, Training Loss: 40.68657, Validation Loss: 5.92111\n",
      "Epoch 109/1200, Training Loss: 40.67134, Validation Loss: 5.91836\n",
      "Epoch 110/1200, Training Loss: 40.65615, Validation Loss: 5.91562\n",
      "Epoch 111/1200, Training Loss: 40.64102, Validation Loss: 5.91289\n",
      "Epoch 112/1200, Training Loss: 40.62594, Validation Loss: 5.91016\n",
      "Epoch 113/1200, Training Loss: 40.61092, Validation Loss: 5.90744\n",
      "Epoch 114/1200, Training Loss: 40.59594, Validation Loss: 5.90473\n",
      "Epoch 115/1200, Training Loss: 40.58101, Validation Loss: 5.90203\n",
      "Epoch 116/1200, Training Loss: 40.56613, Validation Loss: 5.89933\n",
      "Epoch 117/1200, Training Loss: 40.55130, Validation Loss: 5.89665\n",
      "Epoch 118/1200, Training Loss: 40.53652, Validation Loss: 5.89397\n",
      "Epoch 119/1200, Training Loss: 40.52179, Validation Loss: 5.89130\n",
      "Epoch 120/1200, Training Loss: 40.50711, Validation Loss: 5.88864\n",
      "Epoch 121/1200, Training Loss: 40.49248, Validation Loss: 5.88598\n",
      "Epoch 122/1200, Training Loss: 40.47789, Validation Loss: 5.88333\n",
      "Epoch 123/1200, Training Loss: 40.46334, Validation Loss: 5.88069\n",
      "Epoch 124/1200, Training Loss: 40.44884, Validation Loss: 5.87805\n",
      "Epoch 125/1200, Training Loss: 40.43438, Validation Loss: 5.87542\n",
      "Epoch 126/1200, Training Loss: 40.41997, Validation Loss: 5.87280\n",
      "Epoch 127/1200, Training Loss: 40.40560, Validation Loss: 5.87018\n",
      "Epoch 128/1200, Training Loss: 40.39127, Validation Loss: 5.86758\n",
      "Epoch 129/1200, Training Loss: 40.37700, Validation Loss: 5.86498\n",
      "Epoch 130/1200, Training Loss: 40.36277, Validation Loss: 5.86238\n",
      "Epoch 131/1200, Training Loss: 40.34857, Validation Loss: 5.85979\n",
      "Epoch 132/1200, Training Loss: 40.33443, Validation Loss: 5.85721\n",
      "Epoch 133/1200, Training Loss: 40.32034, Validation Loss: 5.85463\n",
      "Epoch 134/1200, Training Loss: 40.30629, Validation Loss: 5.85206\n",
      "Epoch 135/1200, Training Loss: 40.29230, Validation Loss: 5.84950\n",
      "Epoch 136/1200, Training Loss: 40.27834, Validation Loss: 5.84694\n",
      "Epoch 137/1200, Training Loss: 40.26444, Validation Loss: 5.84439\n",
      "Epoch 138/1200, Training Loss: 40.25058, Validation Loss: 5.84185\n",
      "Epoch 139/1200, Training Loss: 40.23676, Validation Loss: 5.83931\n",
      "Epoch 140/1200, Training Loss: 40.22298, Validation Loss: 5.83678\n",
      "Epoch 141/1200, Training Loss: 40.20924, Validation Loss: 5.83426\n",
      "Epoch 142/1200, Training Loss: 40.19554, Validation Loss: 5.83174\n",
      "Epoch 143/1200, Training Loss: 40.18188, Validation Loss: 5.82922\n",
      "Epoch 144/1200, Training Loss: 40.16826, Validation Loss: 5.82670\n",
      "Epoch 145/1200, Training Loss: 40.15466, Validation Loss: 5.82420\n",
      "Epoch 146/1200, Training Loss: 40.14112, Validation Loss: 5.82170\n",
      "Epoch 147/1200, Training Loss: 40.12761, Validation Loss: 5.81920\n",
      "Epoch 148/1200, Training Loss: 40.11413, Validation Loss: 5.81672\n",
      "Epoch 149/1200, Training Loss: 40.10068, Validation Loss: 5.81423\n",
      "Epoch 150/1200, Training Loss: 40.08727, Validation Loss: 5.81175\n",
      "Epoch 151/1200, Training Loss: 40.07390, Validation Loss: 5.80928\n",
      "Epoch 152/1200, Training Loss: 40.06056, Validation Loss: 5.80681\n",
      "Epoch 153/1200, Training Loss: 40.04726, Validation Loss: 5.80434\n",
      "Epoch 154/1200, Training Loss: 40.03399, Validation Loss: 5.80189\n",
      "Epoch 155/1200, Training Loss: 40.02076, Validation Loss: 5.79943\n",
      "Epoch 156/1200, Training Loss: 40.00755, Validation Loss: 5.79699\n",
      "Epoch 157/1200, Training Loss: 39.99438, Validation Loss: 5.79454\n",
      "Epoch 158/1200, Training Loss: 39.98124, Validation Loss: 5.79211\n",
      "Epoch 159/1200, Training Loss: 39.96813, Validation Loss: 5.78967\n",
      "Epoch 160/1200, Training Loss: 39.95505, Validation Loss: 5.78724\n",
      "Epoch 161/1200, Training Loss: 39.94199, Validation Loss: 5.78481\n",
      "Epoch 162/1200, Training Loss: 39.92896, Validation Loss: 5.78238\n",
      "Epoch 163/1200, Training Loss: 39.91597, Validation Loss: 5.77997\n",
      "Epoch 164/1200, Training Loss: 39.90302, Validation Loss: 5.77755\n",
      "Epoch 165/1200, Training Loss: 39.89010, Validation Loss: 5.77514\n",
      "Epoch 166/1200, Training Loss: 39.87720, Validation Loss: 5.77273\n",
      "Epoch 167/1200, Training Loss: 39.86434, Validation Loss: 5.77033\n",
      "Epoch 168/1200, Training Loss: 39.85151, Validation Loss: 5.76794\n",
      "Epoch 169/1200, Training Loss: 39.83870, Validation Loss: 5.76554\n",
      "Epoch 170/1200, Training Loss: 39.82593, Validation Loss: 5.76316\n",
      "Epoch 171/1200, Training Loss: 39.81319, Validation Loss: 5.76077\n",
      "Epoch 172/1200, Training Loss: 39.80048, Validation Loss: 5.75840\n",
      "Epoch 173/1200, Training Loss: 39.78781, Validation Loss: 5.75602\n",
      "Epoch 174/1200, Training Loss: 39.77517, Validation Loss: 5.75365\n",
      "Epoch 175/1200, Training Loss: 39.76255, Validation Loss: 5.75129\n",
      "Epoch 176/1200, Training Loss: 39.74997, Validation Loss: 5.74893\n",
      "Epoch 177/1200, Training Loss: 39.73742, Validation Loss: 5.74657\n",
      "Epoch 178/1200, Training Loss: 39.72489, Validation Loss: 5.74422\n",
      "Epoch 179/1200, Training Loss: 39.71239, Validation Loss: 5.74187\n",
      "Epoch 180/1200, Training Loss: 39.69992, Validation Loss: 5.73953\n",
      "Epoch 181/1200, Training Loss: 39.68748, Validation Loss: 5.73719\n",
      "Epoch 182/1200, Training Loss: 39.67508, Validation Loss: 5.73486\n",
      "Epoch 183/1200, Training Loss: 39.66269, Validation Loss: 5.73252\n",
      "Epoch 184/1200, Training Loss: 39.65034, Validation Loss: 5.73020\n",
      "Epoch 185/1200, Training Loss: 39.63802, Validation Loss: 5.72787\n",
      "Epoch 186/1200, Training Loss: 39.62573, Validation Loss: 5.72555\n",
      "Epoch 187/1200, Training Loss: 39.61345, Validation Loss: 5.72323\n",
      "Epoch 188/1200, Training Loss: 39.60120, Validation Loss: 5.72092\n",
      "Epoch 189/1200, Training Loss: 39.58896, Validation Loss: 5.71860\n",
      "Epoch 190/1200, Training Loss: 39.57676, Validation Loss: 5.71629\n",
      "Epoch 191/1200, Training Loss: 39.56458, Validation Loss: 5.71399\n",
      "Epoch 192/1200, Training Loss: 39.55243, Validation Loss: 5.71169\n",
      "Epoch 193/1200, Training Loss: 39.54030, Validation Loss: 5.70939\n",
      "Epoch 194/1200, Training Loss: 39.52821, Validation Loss: 5.70710\n",
      "Epoch 195/1200, Training Loss: 39.51616, Validation Loss: 5.70481\n",
      "Epoch 196/1200, Training Loss: 39.50414, Validation Loss: 5.70253\n",
      "Epoch 197/1200, Training Loss: 39.49213, Validation Loss: 5.70025\n",
      "Epoch 198/1200, Training Loss: 39.48014, Validation Loss: 5.69798\n",
      "Epoch 199/1200, Training Loss: 39.46817, Validation Loss: 5.69570\n",
      "Epoch 200/1200, Training Loss: 39.45622, Validation Loss: 5.69344\n",
      "Epoch 201/1200, Training Loss: 39.44429, Validation Loss: 5.69117\n",
      "Epoch 202/1200, Training Loss: 39.43239, Validation Loss: 5.68891\n",
      "Epoch 203/1200, Training Loss: 39.42051, Validation Loss: 5.68665\n",
      "Epoch 204/1200, Training Loss: 39.40866, Validation Loss: 5.68440\n",
      "Epoch 205/1200, Training Loss: 39.39683, Validation Loss: 5.68215\n",
      "Epoch 206/1200, Training Loss: 39.38501, Validation Loss: 5.67989\n",
      "Epoch 207/1200, Training Loss: 39.37321, Validation Loss: 5.67764\n",
      "Epoch 208/1200, Training Loss: 39.36143, Validation Loss: 5.67540\n",
      "Epoch 209/1200, Training Loss: 39.34968, Validation Loss: 5.67315\n",
      "Epoch 210/1200, Training Loss: 39.33796, Validation Loss: 5.67091\n",
      "Epoch 211/1200, Training Loss: 39.32625, Validation Loss: 5.66867\n",
      "Epoch 212/1200, Training Loss: 39.31458, Validation Loss: 5.66643\n",
      "Epoch 213/1200, Training Loss: 39.30292, Validation Loss: 5.66420\n",
      "Epoch 214/1200, Training Loss: 39.29129, Validation Loss: 5.66197\n",
      "Epoch 215/1200, Training Loss: 39.27968, Validation Loss: 5.65974\n",
      "Epoch 216/1200, Training Loss: 39.26809, Validation Loss: 5.65752\n",
      "Epoch 217/1200, Training Loss: 39.25652, Validation Loss: 5.65530\n",
      "Epoch 218/1200, Training Loss: 39.24497, Validation Loss: 5.65308\n",
      "Epoch 219/1200, Training Loss: 39.23344, Validation Loss: 5.65087\n",
      "Epoch 220/1200, Training Loss: 39.22194, Validation Loss: 5.64866\n",
      "Epoch 221/1200, Training Loss: 39.21046, Validation Loss: 5.64645\n",
      "Epoch 222/1200, Training Loss: 39.19900, Validation Loss: 5.64424\n",
      "Epoch 223/1200, Training Loss: 39.18754, Validation Loss: 5.64204\n",
      "Epoch 224/1200, Training Loss: 39.17611, Validation Loss: 5.63983\n",
      "Epoch 225/1200, Training Loss: 39.16470, Validation Loss: 5.63763\n",
      "Epoch 226/1200, Training Loss: 39.15331, Validation Loss: 5.63542\n",
      "Epoch 227/1200, Training Loss: 39.14193, Validation Loss: 5.63322\n",
      "Epoch 228/1200, Training Loss: 39.13058, Validation Loss: 5.63102\n",
      "Epoch 229/1200, Training Loss: 39.11926, Validation Loss: 5.62882\n",
      "Epoch 230/1200, Training Loss: 39.10795, Validation Loss: 5.62663\n",
      "Epoch 231/1200, Training Loss: 39.09665, Validation Loss: 5.62444\n",
      "Epoch 232/1200, Training Loss: 39.08538, Validation Loss: 5.62225\n",
      "Epoch 233/1200, Training Loss: 39.07411, Validation Loss: 5.62006\n",
      "Epoch 234/1200, Training Loss: 39.06287, Validation Loss: 5.61787\n",
      "Epoch 235/1200, Training Loss: 39.05164, Validation Loss: 5.61569\n",
      "Epoch 236/1200, Training Loss: 39.04043, Validation Loss: 5.61351\n",
      "Epoch 237/1200, Training Loss: 39.02923, Validation Loss: 5.61133\n",
      "Epoch 238/1200, Training Loss: 39.01805, Validation Loss: 5.60915\n",
      "Epoch 239/1200, Training Loss: 39.00690, Validation Loss: 5.60697\n",
      "Epoch 240/1200, Training Loss: 38.99575, Validation Loss: 5.60480\n",
      "Epoch 241/1200, Training Loss: 38.98463, Validation Loss: 5.60262\n",
      "Epoch 242/1200, Training Loss: 38.97352, Validation Loss: 5.60045\n",
      "Epoch 243/1200, Training Loss: 38.96242, Validation Loss: 5.59828\n",
      "Epoch 244/1200, Training Loss: 38.95134, Validation Loss: 5.59611\n",
      "Epoch 245/1200, Training Loss: 38.94028, Validation Loss: 5.59394\n",
      "Epoch 246/1200, Training Loss: 38.92923, Validation Loss: 5.59177\n",
      "Epoch 247/1200, Training Loss: 38.91819, Validation Loss: 5.58961\n",
      "Epoch 248/1200, Training Loss: 38.90717, Validation Loss: 5.58744\n",
      "Epoch 249/1200, Training Loss: 38.89616, Validation Loss: 5.58528\n",
      "Epoch 250/1200, Training Loss: 38.88518, Validation Loss: 5.58312\n",
      "Epoch 251/1200, Training Loss: 38.87423, Validation Loss: 5.58096\n",
      "Epoch 252/1200, Training Loss: 38.86329, Validation Loss: 5.57880\n",
      "Epoch 253/1200, Training Loss: 38.85237, Validation Loss: 5.57664\n",
      "Epoch 254/1200, Training Loss: 38.84147, Validation Loss: 5.57448\n",
      "Epoch 255/1200, Training Loss: 38.83058, Validation Loss: 5.57233\n",
      "Epoch 256/1200, Training Loss: 38.81971, Validation Loss: 5.57018\n",
      "Epoch 257/1200, Training Loss: 38.80885, Validation Loss: 5.56803\n",
      "Epoch 258/1200, Training Loss: 38.79800, Validation Loss: 5.56588\n",
      "Epoch 259/1200, Training Loss: 38.78717, Validation Loss: 5.56374\n",
      "Epoch 260/1200, Training Loss: 38.77634, Validation Loss: 5.56160\n",
      "Epoch 261/1200, Training Loss: 38.76553, Validation Loss: 5.55945\n",
      "Epoch 262/1200, Training Loss: 38.75473, Validation Loss: 5.55732\n",
      "Epoch 263/1200, Training Loss: 38.74393, Validation Loss: 5.55518\n",
      "Epoch 264/1200, Training Loss: 38.73315, Validation Loss: 5.55304\n",
      "Epoch 265/1200, Training Loss: 38.72238, Validation Loss: 5.55091\n",
      "Epoch 266/1200, Training Loss: 38.71162, Validation Loss: 5.54878\n",
      "Epoch 267/1200, Training Loss: 38.70088, Validation Loss: 5.54665\n",
      "Epoch 268/1200, Training Loss: 38.69014, Validation Loss: 5.54452\n",
      "Epoch 269/1200, Training Loss: 38.67942, Validation Loss: 5.54240\n",
      "Epoch 270/1200, Training Loss: 38.66869, Validation Loss: 5.54027\n",
      "Epoch 271/1200, Training Loss: 38.65797, Validation Loss: 5.53815\n",
      "Epoch 272/1200, Training Loss: 38.64725, Validation Loss: 5.53603\n",
      "Epoch 273/1200, Training Loss: 38.63653, Validation Loss: 5.53391\n",
      "Epoch 274/1200, Training Loss: 38.62582, Validation Loss: 5.53178\n",
      "Epoch 275/1200, Training Loss: 38.61512, Validation Loss: 5.52966\n",
      "Epoch 276/1200, Training Loss: 38.60443, Validation Loss: 5.52755\n",
      "Epoch 277/1200, Training Loss: 38.59375, Validation Loss: 5.52543\n",
      "Epoch 278/1200, Training Loss: 38.58309, Validation Loss: 5.52332\n",
      "Epoch 279/1200, Training Loss: 38.57245, Validation Loss: 5.52121\n",
      "Epoch 280/1200, Training Loss: 38.56183, Validation Loss: 5.51909\n",
      "Epoch 281/1200, Training Loss: 38.55121, Validation Loss: 5.51699\n",
      "Epoch 282/1200, Training Loss: 38.54060, Validation Loss: 5.51488\n",
      "Epoch 283/1200, Training Loss: 38.53000, Validation Loss: 5.51278\n",
      "Epoch 284/1200, Training Loss: 38.51941, Validation Loss: 5.51068\n",
      "Epoch 285/1200, Training Loss: 38.50883, Validation Loss: 5.50858\n",
      "Epoch 286/1200, Training Loss: 38.49827, Validation Loss: 5.50648\n",
      "Epoch 287/1200, Training Loss: 38.48771, Validation Loss: 5.50438\n",
      "Epoch 288/1200, Training Loss: 38.47717, Validation Loss: 5.50229\n",
      "Epoch 289/1200, Training Loss: 38.46663, Validation Loss: 5.50019\n",
      "Epoch 290/1200, Training Loss: 38.45611, Validation Loss: 5.49810\n",
      "Epoch 291/1200, Training Loss: 38.44560, Validation Loss: 5.49601\n",
      "Epoch 292/1200, Training Loss: 38.43510, Validation Loss: 5.49393\n",
      "Epoch 293/1200, Training Loss: 38.42461, Validation Loss: 5.49184\n",
      "Epoch 294/1200, Training Loss: 38.41413, Validation Loss: 5.48976\n",
      "Epoch 295/1200, Training Loss: 38.40366, Validation Loss: 5.48767\n",
      "Epoch 296/1200, Training Loss: 38.39321, Validation Loss: 5.48559\n",
      "Epoch 297/1200, Training Loss: 38.38277, Validation Loss: 5.48351\n",
      "Epoch 298/1200, Training Loss: 38.37234, Validation Loss: 5.48143\n",
      "Epoch 299/1200, Training Loss: 38.36192, Validation Loss: 5.47936\n",
      "Epoch 300/1200, Training Loss: 38.35152, Validation Loss: 5.47728\n",
      "Epoch 301/1200, Training Loss: 38.34113, Validation Loss: 5.47521\n",
      "Epoch 302/1200, Training Loss: 38.33076, Validation Loss: 5.47314\n",
      "Epoch 303/1200, Training Loss: 38.32039, Validation Loss: 5.47108\n",
      "Epoch 304/1200, Training Loss: 38.31004, Validation Loss: 5.46901\n",
      "Epoch 305/1200, Training Loss: 38.29969, Validation Loss: 5.46694\n",
      "Epoch 306/1200, Training Loss: 38.28935, Validation Loss: 5.46488\n",
      "Epoch 307/1200, Training Loss: 38.27902, Validation Loss: 5.46282\n",
      "Epoch 308/1200, Training Loss: 38.26869, Validation Loss: 5.46076\n",
      "Epoch 309/1200, Training Loss: 38.25837, Validation Loss: 5.45869\n",
      "Epoch 310/1200, Training Loss: 38.24805, Validation Loss: 5.45663\n",
      "Epoch 311/1200, Training Loss: 38.23774, Validation Loss: 5.45457\n",
      "Epoch 312/1200, Training Loss: 38.22744, Validation Loss: 5.45251\n",
      "Epoch 313/1200, Training Loss: 38.21714, Validation Loss: 5.45045\n",
      "Epoch 314/1200, Training Loss: 38.20686, Validation Loss: 5.44840\n",
      "Epoch 315/1200, Training Loss: 38.19658, Validation Loss: 5.44634\n",
      "Epoch 316/1200, Training Loss: 38.18631, Validation Loss: 5.44429\n",
      "Epoch 317/1200, Training Loss: 38.17605, Validation Loss: 5.44223\n",
      "Epoch 318/1200, Training Loss: 38.16579, Validation Loss: 5.44018\n",
      "Epoch 319/1200, Training Loss: 38.15555, Validation Loss: 5.43813\n",
      "Epoch 320/1200, Training Loss: 38.14531, Validation Loss: 5.43608\n",
      "Epoch 321/1200, Training Loss: 38.13509, Validation Loss: 5.43403\n",
      "Epoch 322/1200, Training Loss: 38.12487, Validation Loss: 5.43198\n",
      "Epoch 323/1200, Training Loss: 38.11466, Validation Loss: 5.42994\n",
      "Epoch 324/1200, Training Loss: 38.10445, Validation Loss: 5.42789\n",
      "Epoch 325/1200, Training Loss: 38.09426, Validation Loss: 5.42585\n",
      "Epoch 326/1200, Training Loss: 38.08407, Validation Loss: 5.42381\n",
      "Epoch 327/1200, Training Loss: 38.07389, Validation Loss: 5.42177\n",
      "Epoch 328/1200, Training Loss: 38.06371, Validation Loss: 5.41973\n",
      "Epoch 329/1200, Training Loss: 38.05353, Validation Loss: 5.41769\n",
      "Epoch 330/1200, Training Loss: 38.04336, Validation Loss: 5.41565\n",
      "Epoch 331/1200, Training Loss: 38.03319, Validation Loss: 5.41362\n",
      "Epoch 332/1200, Training Loss: 38.02302, Validation Loss: 5.41158\n",
      "Epoch 333/1200, Training Loss: 38.01286, Validation Loss: 5.40955\n",
      "Epoch 334/1200, Training Loss: 38.00270, Validation Loss: 5.40751\n",
      "Epoch 335/1200, Training Loss: 37.99254, Validation Loss: 5.40548\n",
      "Epoch 336/1200, Training Loss: 37.98238, Validation Loss: 5.40344\n",
      "Epoch 337/1200, Training Loss: 37.97223, Validation Loss: 5.40141\n",
      "Epoch 338/1200, Training Loss: 37.96209, Validation Loss: 5.39937\n",
      "Epoch 339/1200, Training Loss: 37.95195, Validation Loss: 5.39734\n",
      "Epoch 340/1200, Training Loss: 37.94182, Validation Loss: 5.39531\n",
      "Epoch 341/1200, Training Loss: 37.93170, Validation Loss: 5.39328\n",
      "Epoch 342/1200, Training Loss: 37.92159, Validation Loss: 5.39124\n",
      "Epoch 343/1200, Training Loss: 37.91149, Validation Loss: 5.38921\n",
      "Epoch 344/1200, Training Loss: 37.90139, Validation Loss: 5.38718\n",
      "Epoch 345/1200, Training Loss: 37.89130, Validation Loss: 5.38516\n",
      "Epoch 346/1200, Training Loss: 37.88120, Validation Loss: 5.38313\n",
      "Epoch 347/1200, Training Loss: 37.87112, Validation Loss: 5.38111\n",
      "Epoch 348/1200, Training Loss: 37.86106, Validation Loss: 5.37908\n",
      "Epoch 349/1200, Training Loss: 37.85098, Validation Loss: 5.37706\n",
      "Epoch 350/1200, Training Loss: 37.84091, Validation Loss: 5.37504\n",
      "Epoch 351/1200, Training Loss: 37.83084, Validation Loss: 5.37302\n",
      "Epoch 352/1200, Training Loss: 37.82078, Validation Loss: 5.37101\n",
      "Epoch 353/1200, Training Loss: 37.81072, Validation Loss: 5.36899\n",
      "Epoch 354/1200, Training Loss: 37.80067, Validation Loss: 5.36697\n",
      "Epoch 355/1200, Training Loss: 37.79063, Validation Loss: 5.36496\n",
      "Epoch 356/1200, Training Loss: 37.78058, Validation Loss: 5.36294\n",
      "Epoch 357/1200, Training Loss: 37.77053, Validation Loss: 5.36092\n",
      "Epoch 358/1200, Training Loss: 37.76049, Validation Loss: 5.35891\n",
      "Epoch 359/1200, Training Loss: 37.75044, Validation Loss: 5.35690\n",
      "Epoch 360/1200, Training Loss: 37.74040, Validation Loss: 5.35488\n",
      "Epoch 361/1200, Training Loss: 37.73037, Validation Loss: 5.35287\n",
      "Epoch 362/1200, Training Loss: 37.72035, Validation Loss: 5.35086\n",
      "Epoch 363/1200, Training Loss: 37.71034, Validation Loss: 5.34885\n",
      "Epoch 364/1200, Training Loss: 37.70034, Validation Loss: 5.34684\n",
      "Epoch 365/1200, Training Loss: 37.69033, Validation Loss: 5.34484\n",
      "Epoch 366/1200, Training Loss: 37.68033, Validation Loss: 5.34283\n",
      "Epoch 367/1200, Training Loss: 37.67033, Validation Loss: 5.34082\n",
      "Epoch 368/1200, Training Loss: 37.66034, Validation Loss: 5.33881\n",
      "Epoch 369/1200, Training Loss: 37.65035, Validation Loss: 5.33681\n",
      "Epoch 370/1200, Training Loss: 37.64035, Validation Loss: 5.33480\n",
      "Epoch 371/1200, Training Loss: 37.63037, Validation Loss: 5.33280\n",
      "Epoch 372/1200, Training Loss: 37.62039, Validation Loss: 5.33079\n",
      "Epoch 373/1200, Training Loss: 37.61040, Validation Loss: 5.32879\n",
      "Epoch 374/1200, Training Loss: 37.60041, Validation Loss: 5.32678\n",
      "Epoch 375/1200, Training Loss: 37.59043, Validation Loss: 5.32478\n",
      "Epoch 376/1200, Training Loss: 37.58046, Validation Loss: 5.32277\n",
      "Epoch 377/1200, Training Loss: 37.57048, Validation Loss: 5.32076\n",
      "Epoch 378/1200, Training Loss: 37.56051, Validation Loss: 5.31876\n",
      "Epoch 379/1200, Training Loss: 37.55054, Validation Loss: 5.31675\n",
      "Epoch 380/1200, Training Loss: 37.54056, Validation Loss: 5.31475\n",
      "Epoch 381/1200, Training Loss: 37.53060, Validation Loss: 5.31275\n",
      "Epoch 382/1200, Training Loss: 37.52063, Validation Loss: 5.31074\n",
      "Epoch 383/1200, Training Loss: 37.51067, Validation Loss: 5.30874\n",
      "Epoch 384/1200, Training Loss: 37.50070, Validation Loss: 5.30673\n",
      "Epoch 385/1200, Training Loss: 37.49073, Validation Loss: 5.30472\n",
      "Epoch 386/1200, Training Loss: 37.48076, Validation Loss: 5.30272\n",
      "Epoch 387/1200, Training Loss: 37.47080, Validation Loss: 5.30071\n",
      "Epoch 388/1200, Training Loss: 37.46084, Validation Loss: 5.29870\n",
      "Epoch 389/1200, Training Loss: 37.45089, Validation Loss: 5.29670\n",
      "Epoch 390/1200, Training Loss: 37.44094, Validation Loss: 5.29470\n",
      "Epoch 391/1200, Training Loss: 37.43100, Validation Loss: 5.29270\n",
      "Epoch 392/1200, Training Loss: 37.42107, Validation Loss: 5.29070\n",
      "Epoch 393/1200, Training Loss: 37.41114, Validation Loss: 5.28870\n",
      "Epoch 394/1200, Training Loss: 37.40120, Validation Loss: 5.28670\n",
      "Epoch 395/1200, Training Loss: 37.39126, Validation Loss: 5.28469\n",
      "Epoch 396/1200, Training Loss: 37.38133, Validation Loss: 5.28269\n",
      "Epoch 397/1200, Training Loss: 37.37139, Validation Loss: 5.28069\n",
      "Epoch 398/1200, Training Loss: 37.36146, Validation Loss: 5.27869\n",
      "Epoch 399/1200, Training Loss: 37.35153, Validation Loss: 5.27669\n",
      "Epoch 400/1200, Training Loss: 37.34161, Validation Loss: 5.27469\n",
      "Epoch 401/1200, Training Loss: 37.33169, Validation Loss: 5.27269\n",
      "Epoch 402/1200, Training Loss: 37.32178, Validation Loss: 5.27069\n",
      "Epoch 403/1200, Training Loss: 37.31187, Validation Loss: 5.26869\n",
      "Epoch 404/1200, Training Loss: 37.30197, Validation Loss: 5.26669\n",
      "Epoch 405/1200, Training Loss: 37.29208, Validation Loss: 5.26469\n",
      "Epoch 406/1200, Training Loss: 37.28219, Validation Loss: 5.26270\n",
      "Epoch 407/1200, Training Loss: 37.27231, Validation Loss: 5.26070\n",
      "Epoch 408/1200, Training Loss: 37.26243, Validation Loss: 5.25870\n",
      "Epoch 409/1200, Training Loss: 37.25256, Validation Loss: 5.25670\n",
      "Epoch 410/1200, Training Loss: 37.24270, Validation Loss: 5.25470\n",
      "Epoch 411/1200, Training Loss: 37.23284, Validation Loss: 5.25271\n",
      "Epoch 412/1200, Training Loss: 37.22298, Validation Loss: 5.25071\n",
      "Epoch 413/1200, Training Loss: 37.21311, Validation Loss: 5.24871\n",
      "Epoch 414/1200, Training Loss: 37.20325, Validation Loss: 5.24672\n",
      "Epoch 415/1200, Training Loss: 37.19339, Validation Loss: 5.24473\n",
      "Epoch 416/1200, Training Loss: 37.18352, Validation Loss: 5.24273\n",
      "Epoch 417/1200, Training Loss: 37.17366, Validation Loss: 5.24074\n",
      "Epoch 418/1200, Training Loss: 37.16381, Validation Loss: 5.23874\n",
      "Epoch 419/1200, Training Loss: 37.15395, Validation Loss: 5.23674\n",
      "Epoch 420/1200, Training Loss: 37.14410, Validation Loss: 5.23475\n",
      "Epoch 421/1200, Training Loss: 37.13425, Validation Loss: 5.23275\n",
      "Epoch 422/1200, Training Loss: 37.12441, Validation Loss: 5.23076\n",
      "Epoch 423/1200, Training Loss: 37.11457, Validation Loss: 5.22876\n",
      "Epoch 424/1200, Training Loss: 37.10473, Validation Loss: 5.22677\n",
      "Epoch 425/1200, Training Loss: 37.09489, Validation Loss: 5.22477\n",
      "Epoch 426/1200, Training Loss: 37.08505, Validation Loss: 5.22278\n",
      "Epoch 427/1200, Training Loss: 37.07522, Validation Loss: 5.22078\n",
      "Epoch 428/1200, Training Loss: 37.06539, Validation Loss: 5.21879\n",
      "Epoch 429/1200, Training Loss: 37.05555, Validation Loss: 5.21680\n",
      "Epoch 430/1200, Training Loss: 37.04573, Validation Loss: 5.21481\n",
      "Epoch 431/1200, Training Loss: 37.03590, Validation Loss: 5.21281\n",
      "Epoch 432/1200, Training Loss: 37.02607, Validation Loss: 5.21082\n",
      "Epoch 433/1200, Training Loss: 37.01625, Validation Loss: 5.20883\n",
      "Epoch 434/1200, Training Loss: 37.00642, Validation Loss: 5.20684\n",
      "Epoch 435/1200, Training Loss: 36.99660, Validation Loss: 5.20485\n",
      "Epoch 436/1200, Training Loss: 36.98678, Validation Loss: 5.20287\n",
      "Epoch 437/1200, Training Loss: 36.97696, Validation Loss: 5.20088\n",
      "Epoch 438/1200, Training Loss: 36.96715, Validation Loss: 5.19889\n",
      "Epoch 439/1200, Training Loss: 36.95733, Validation Loss: 5.19690\n",
      "Epoch 440/1200, Training Loss: 36.94753, Validation Loss: 5.19492\n",
      "Epoch 441/1200, Training Loss: 36.93774, Validation Loss: 5.19293\n",
      "Epoch 442/1200, Training Loss: 36.92794, Validation Loss: 5.19095\n",
      "Epoch 443/1200, Training Loss: 36.91814, Validation Loss: 5.18896\n",
      "Epoch 444/1200, Training Loss: 36.90834, Validation Loss: 5.18698\n",
      "Epoch 445/1200, Training Loss: 36.89854, Validation Loss: 5.18500\n",
      "Epoch 446/1200, Training Loss: 36.88875, Validation Loss: 5.18301\n",
      "Epoch 447/1200, Training Loss: 36.87896, Validation Loss: 5.18103\n",
      "Epoch 448/1200, Training Loss: 36.86918, Validation Loss: 5.17905\n",
      "Epoch 449/1200, Training Loss: 36.85941, Validation Loss: 5.17707\n",
      "Epoch 450/1200, Training Loss: 36.84965, Validation Loss: 5.17510\n",
      "Epoch 451/1200, Training Loss: 36.83988, Validation Loss: 5.17312\n",
      "Epoch 452/1200, Training Loss: 36.83012, Validation Loss: 5.17114\n",
      "Epoch 453/1200, Training Loss: 36.82036, Validation Loss: 5.16916\n",
      "Epoch 454/1200, Training Loss: 36.81060, Validation Loss: 5.16719\n",
      "Epoch 455/1200, Training Loss: 36.80083, Validation Loss: 5.16521\n",
      "Epoch 456/1200, Training Loss: 36.79106, Validation Loss: 5.16323\n",
      "Epoch 457/1200, Training Loss: 36.78130, Validation Loss: 5.16126\n",
      "Epoch 458/1200, Training Loss: 36.77154, Validation Loss: 5.15928\n",
      "Epoch 459/1200, Training Loss: 36.76179, Validation Loss: 5.15731\n",
      "Epoch 460/1200, Training Loss: 36.75204, Validation Loss: 5.15533\n",
      "Epoch 461/1200, Training Loss: 36.74228, Validation Loss: 5.15336\n",
      "Epoch 462/1200, Training Loss: 36.73253, Validation Loss: 5.15138\n",
      "Epoch 463/1200, Training Loss: 36.72277, Validation Loss: 5.14941\n",
      "Epoch 464/1200, Training Loss: 36.71301, Validation Loss: 5.14743\n",
      "Epoch 465/1200, Training Loss: 36.70325, Validation Loss: 5.14545\n",
      "Epoch 466/1200, Training Loss: 36.69350, Validation Loss: 5.14348\n",
      "Epoch 467/1200, Training Loss: 36.68375, Validation Loss: 5.14150\n",
      "Epoch 468/1200, Training Loss: 36.67400, Validation Loss: 5.13952\n",
      "Epoch 469/1200, Training Loss: 36.66426, Validation Loss: 5.13755\n",
      "Epoch 470/1200, Training Loss: 36.65452, Validation Loss: 5.13557\n",
      "Epoch 471/1200, Training Loss: 36.64478, Validation Loss: 5.13359\n",
      "Epoch 472/1200, Training Loss: 36.63503, Validation Loss: 5.13162\n",
      "Epoch 473/1200, Training Loss: 36.62529, Validation Loss: 5.12964\n",
      "Epoch 474/1200, Training Loss: 36.61555, Validation Loss: 5.12767\n",
      "Epoch 475/1200, Training Loss: 36.60581, Validation Loss: 5.12569\n",
      "Epoch 476/1200, Training Loss: 36.59606, Validation Loss: 5.12372\n",
      "Epoch 477/1200, Training Loss: 36.58630, Validation Loss: 5.12174\n",
      "Epoch 478/1200, Training Loss: 36.57655, Validation Loss: 5.11977\n",
      "Epoch 479/1200, Training Loss: 36.56680, Validation Loss: 5.11779\n",
      "Epoch 480/1200, Training Loss: 36.55705, Validation Loss: 5.11582\n",
      "Epoch 481/1200, Training Loss: 36.54730, Validation Loss: 5.11385\n",
      "Epoch 482/1200, Training Loss: 36.53755, Validation Loss: 5.11188\n",
      "Epoch 483/1200, Training Loss: 36.52780, Validation Loss: 5.10991\n",
      "Epoch 484/1200, Training Loss: 36.51807, Validation Loss: 5.10794\n",
      "Epoch 485/1200, Training Loss: 36.50833, Validation Loss: 5.10598\n",
      "Epoch 486/1200, Training Loss: 36.49860, Validation Loss: 5.10401\n",
      "Epoch 487/1200, Training Loss: 36.48886, Validation Loss: 5.10205\n",
      "Epoch 488/1200, Training Loss: 36.47914, Validation Loss: 5.10008\n",
      "Epoch 489/1200, Training Loss: 36.46942, Validation Loss: 5.09812\n",
      "Epoch 490/1200, Training Loss: 36.45970, Validation Loss: 5.09616\n",
      "Epoch 491/1200, Training Loss: 36.44999, Validation Loss: 5.09419\n",
      "Epoch 492/1200, Training Loss: 36.44028, Validation Loss: 5.09223\n",
      "Epoch 493/1200, Training Loss: 36.43057, Validation Loss: 5.09027\n",
      "Epoch 494/1200, Training Loss: 36.42086, Validation Loss: 5.08831\n",
      "Epoch 495/1200, Training Loss: 36.41115, Validation Loss: 5.08634\n",
      "Epoch 496/1200, Training Loss: 36.40144, Validation Loss: 5.08438\n",
      "Epoch 497/1200, Training Loss: 36.39174, Validation Loss: 5.08242\n",
      "Epoch 498/1200, Training Loss: 36.38204, Validation Loss: 5.08046\n",
      "Epoch 499/1200, Training Loss: 36.37233, Validation Loss: 5.07850\n",
      "Epoch 500/1200, Training Loss: 36.36263, Validation Loss: 5.07655\n",
      "Epoch 501/1200, Training Loss: 36.35292, Validation Loss: 5.07459\n",
      "Epoch 502/1200, Training Loss: 36.34322, Validation Loss: 5.07263\n",
      "Epoch 503/1200, Training Loss: 36.33352, Validation Loss: 5.07067\n",
      "Epoch 504/1200, Training Loss: 36.32382, Validation Loss: 5.06872\n",
      "Epoch 505/1200, Training Loss: 36.31412, Validation Loss: 5.06676\n",
      "Epoch 506/1200, Training Loss: 36.30443, Validation Loss: 5.06481\n",
      "Epoch 507/1200, Training Loss: 36.29473, Validation Loss: 5.06285\n",
      "Epoch 508/1200, Training Loss: 36.28505, Validation Loss: 5.06090\n",
      "Epoch 509/1200, Training Loss: 36.27536, Validation Loss: 5.05895\n",
      "Epoch 510/1200, Training Loss: 36.26567, Validation Loss: 5.05699\n",
      "Epoch 511/1200, Training Loss: 36.25598, Validation Loss: 5.05504\n",
      "Epoch 512/1200, Training Loss: 36.24629, Validation Loss: 5.05309\n",
      "Epoch 513/1200, Training Loss: 36.23660, Validation Loss: 5.05114\n",
      "Epoch 514/1200, Training Loss: 36.22691, Validation Loss: 5.04918\n",
      "Epoch 515/1200, Training Loss: 36.21721, Validation Loss: 5.04723\n",
      "Epoch 516/1200, Training Loss: 36.20752, Validation Loss: 5.04528\n",
      "Epoch 517/1200, Training Loss: 36.19781, Validation Loss: 5.04333\n",
      "Epoch 518/1200, Training Loss: 36.18810, Validation Loss: 5.04138\n",
      "Epoch 519/1200, Training Loss: 36.17839, Validation Loss: 5.03943\n",
      "Epoch 520/1200, Training Loss: 36.16869, Validation Loss: 5.03748\n",
      "Epoch 521/1200, Training Loss: 36.15899, Validation Loss: 5.03553\n",
      "Epoch 522/1200, Training Loss: 36.14928, Validation Loss: 5.03358\n",
      "Epoch 523/1200, Training Loss: 36.13958, Validation Loss: 5.03163\n",
      "Epoch 524/1200, Training Loss: 36.12987, Validation Loss: 5.02969\n",
      "Epoch 525/1200, Training Loss: 36.12017, Validation Loss: 5.02774\n",
      "Epoch 526/1200, Training Loss: 36.11047, Validation Loss: 5.02580\n",
      "Epoch 527/1200, Training Loss: 36.10077, Validation Loss: 5.02385\n",
      "Epoch 528/1200, Training Loss: 36.09108, Validation Loss: 5.02190\n",
      "Epoch 529/1200, Training Loss: 36.08138, Validation Loss: 5.01996\n",
      "Epoch 530/1200, Training Loss: 36.07168, Validation Loss: 5.01801\n",
      "Epoch 531/1200, Training Loss: 36.06197, Validation Loss: 5.01607\n",
      "Epoch 532/1200, Training Loss: 36.05226, Validation Loss: 5.01412\n",
      "Epoch 533/1200, Training Loss: 36.04255, Validation Loss: 5.01218\n",
      "Epoch 534/1200, Training Loss: 36.03284, Validation Loss: 5.01023\n",
      "Epoch 535/1200, Training Loss: 36.02312, Validation Loss: 5.00829\n",
      "Epoch 536/1200, Training Loss: 36.01341, Validation Loss: 5.00635\n",
      "Epoch 537/1200, Training Loss: 36.00369, Validation Loss: 5.00440\n",
      "Epoch 538/1200, Training Loss: 35.99398, Validation Loss: 5.00246\n",
      "Epoch 539/1200, Training Loss: 35.98426, Validation Loss: 5.00051\n",
      "Epoch 540/1200, Training Loss: 35.97454, Validation Loss: 4.99857\n",
      "Epoch 541/1200, Training Loss: 35.96483, Validation Loss: 4.99662\n",
      "Epoch 542/1200, Training Loss: 35.95512, Validation Loss: 4.99468\n",
      "Epoch 543/1200, Training Loss: 35.94542, Validation Loss: 4.99274\n",
      "Epoch 544/1200, Training Loss: 35.93571, Validation Loss: 4.99081\n",
      "Epoch 545/1200, Training Loss: 35.92601, Validation Loss: 4.98887\n",
      "Epoch 546/1200, Training Loss: 35.91630, Validation Loss: 4.98693\n",
      "Epoch 547/1200, Training Loss: 35.90660, Validation Loss: 4.98500\n",
      "Epoch 548/1200, Training Loss: 35.89690, Validation Loss: 4.98306\n",
      "Epoch 549/1200, Training Loss: 35.88720, Validation Loss: 4.98112\n",
      "Epoch 550/1200, Training Loss: 35.87750, Validation Loss: 4.97919\n",
      "Epoch 551/1200, Training Loss: 35.86781, Validation Loss: 4.97726\n",
      "Epoch 552/1200, Training Loss: 35.85812, Validation Loss: 4.97532\n",
      "Epoch 553/1200, Training Loss: 35.84844, Validation Loss: 4.97339\n",
      "Epoch 554/1200, Training Loss: 35.83876, Validation Loss: 4.97146\n",
      "Epoch 555/1200, Training Loss: 35.82908, Validation Loss: 4.96952\n",
      "Epoch 556/1200, Training Loss: 35.81941, Validation Loss: 4.96759\n",
      "Epoch 557/1200, Training Loss: 35.80974, Validation Loss: 4.96566\n",
      "Epoch 558/1200, Training Loss: 35.80006, Validation Loss: 4.96373\n",
      "Epoch 559/1200, Training Loss: 35.79039, Validation Loss: 4.96180\n",
      "Epoch 560/1200, Training Loss: 35.78073, Validation Loss: 4.95987\n",
      "Epoch 561/1200, Training Loss: 35.77105, Validation Loss: 4.95794\n",
      "Epoch 562/1200, Training Loss: 35.76137, Validation Loss: 4.95601\n",
      "Epoch 563/1200, Training Loss: 35.75168, Validation Loss: 4.95408\n",
      "Epoch 564/1200, Training Loss: 35.74198, Validation Loss: 4.95215\n",
      "Epoch 565/1200, Training Loss: 35.73229, Validation Loss: 4.95022\n",
      "Epoch 566/1200, Training Loss: 35.72260, Validation Loss: 4.94830\n",
      "Epoch 567/1200, Training Loss: 35.71292, Validation Loss: 4.94637\n",
      "Epoch 568/1200, Training Loss: 35.70324, Validation Loss: 4.94445\n",
      "Epoch 569/1200, Training Loss: 35.69356, Validation Loss: 4.94253\n",
      "Epoch 570/1200, Training Loss: 35.68389, Validation Loss: 4.94061\n",
      "Epoch 571/1200, Training Loss: 35.67422, Validation Loss: 4.93869\n",
      "Epoch 572/1200, Training Loss: 35.66455, Validation Loss: 4.93676\n",
      "Epoch 573/1200, Training Loss: 35.65488, Validation Loss: 4.93484\n",
      "Epoch 574/1200, Training Loss: 35.64521, Validation Loss: 4.93292\n",
      "Epoch 575/1200, Training Loss: 35.63554, Validation Loss: 4.93100\n",
      "Epoch 576/1200, Training Loss: 35.62587, Validation Loss: 4.92908\n",
      "Epoch 577/1200, Training Loss: 35.61621, Validation Loss: 4.92715\n",
      "Epoch 578/1200, Training Loss: 35.60655, Validation Loss: 4.92523\n",
      "Epoch 579/1200, Training Loss: 35.59690, Validation Loss: 4.92331\n",
      "Epoch 580/1200, Training Loss: 35.58724, Validation Loss: 4.92139\n",
      "Epoch 581/1200, Training Loss: 35.57759, Validation Loss: 4.91947\n",
      "Epoch 582/1200, Training Loss: 35.56795, Validation Loss: 4.91755\n",
      "Epoch 583/1200, Training Loss: 35.55830, Validation Loss: 4.91564\n",
      "Epoch 584/1200, Training Loss: 35.54865, Validation Loss: 4.91372\n",
      "Epoch 585/1200, Training Loss: 35.53899, Validation Loss: 4.91181\n",
      "Epoch 586/1200, Training Loss: 35.52934, Validation Loss: 4.90990\n",
      "Epoch 587/1200, Training Loss: 35.51969, Validation Loss: 4.90798\n",
      "Epoch 588/1200, Training Loss: 35.51005, Validation Loss: 4.90607\n",
      "Epoch 589/1200, Training Loss: 35.50040, Validation Loss: 4.90416\n",
      "Epoch 590/1200, Training Loss: 35.49077, Validation Loss: 4.90225\n",
      "Epoch 591/1200, Training Loss: 35.48114, Validation Loss: 4.90034\n",
      "Epoch 592/1200, Training Loss: 35.47153, Validation Loss: 4.89843\n",
      "Epoch 593/1200, Training Loss: 35.46193, Validation Loss: 4.89652\n",
      "Epoch 594/1200, Training Loss: 35.45233, Validation Loss: 4.89462\n",
      "Epoch 595/1200, Training Loss: 35.44273, Validation Loss: 4.89271\n",
      "Epoch 596/1200, Training Loss: 35.43314, Validation Loss: 4.89081\n",
      "Epoch 597/1200, Training Loss: 35.42355, Validation Loss: 4.88891\n",
      "Epoch 598/1200, Training Loss: 35.41396, Validation Loss: 4.88700\n",
      "Epoch 599/1200, Training Loss: 35.40438, Validation Loss: 4.88510\n",
      "Epoch 600/1200, Training Loss: 35.39482, Validation Loss: 4.88320\n",
      "Epoch 601/1200, Training Loss: 35.38526, Validation Loss: 4.88130\n",
      "Epoch 602/1200, Training Loss: 35.37570, Validation Loss: 4.87940\n",
      "Epoch 603/1200, Training Loss: 35.36614, Validation Loss: 4.87749\n",
      "Epoch 604/1200, Training Loss: 35.35659, Validation Loss: 4.87559\n",
      "Epoch 605/1200, Training Loss: 35.34707, Validation Loss: 4.87369\n",
      "Epoch 606/1200, Training Loss: 35.33755, Validation Loss: 4.87179\n",
      "Epoch 607/1200, Training Loss: 35.32803, Validation Loss: 4.86989\n",
      "Epoch 608/1200, Training Loss: 35.31850, Validation Loss: 4.86800\n",
      "Epoch 609/1200, Training Loss: 35.30898, Validation Loss: 4.86610\n",
      "Epoch 610/1200, Training Loss: 35.29945, Validation Loss: 4.86420\n",
      "Epoch 611/1200, Training Loss: 35.28993, Validation Loss: 4.86231\n",
      "Epoch 612/1200, Training Loss: 35.28041, Validation Loss: 4.86041\n",
      "Epoch 613/1200, Training Loss: 35.27090, Validation Loss: 4.85852\n",
      "Epoch 614/1200, Training Loss: 35.26138, Validation Loss: 4.85663\n",
      "Epoch 615/1200, Training Loss: 35.25187, Validation Loss: 4.85474\n",
      "Epoch 616/1200, Training Loss: 35.24236, Validation Loss: 4.85285\n",
      "Epoch 617/1200, Training Loss: 35.23285, Validation Loss: 4.85096\n",
      "Epoch 618/1200, Training Loss: 35.22334, Validation Loss: 4.84907\n",
      "Epoch 619/1200, Training Loss: 35.21384, Validation Loss: 4.84718\n",
      "Epoch 620/1200, Training Loss: 35.20434, Validation Loss: 4.84530\n",
      "Epoch 621/1200, Training Loss: 35.19485, Validation Loss: 4.84341\n",
      "Epoch 622/1200, Training Loss: 35.18537, Validation Loss: 4.84152\n",
      "Epoch 623/1200, Training Loss: 35.17589, Validation Loss: 4.83964\n",
      "Epoch 624/1200, Training Loss: 35.16642, Validation Loss: 4.83776\n",
      "Epoch 625/1200, Training Loss: 35.15695, Validation Loss: 4.83587\n",
      "Epoch 626/1200, Training Loss: 35.14748, Validation Loss: 4.83399\n",
      "Epoch 627/1200, Training Loss: 35.13802, Validation Loss: 4.83212\n",
      "Epoch 628/1200, Training Loss: 35.12857, Validation Loss: 4.83024\n",
      "Epoch 629/1200, Training Loss: 35.11912, Validation Loss: 4.82836\n",
      "Epoch 630/1200, Training Loss: 35.10967, Validation Loss: 4.82649\n",
      "Epoch 631/1200, Training Loss: 35.10023, Validation Loss: 4.82461\n",
      "Epoch 632/1200, Training Loss: 35.09080, Validation Loss: 4.82274\n",
      "Epoch 633/1200, Training Loss: 35.08138, Validation Loss: 4.82087\n",
      "Epoch 634/1200, Training Loss: 35.07195, Validation Loss: 4.81900\n",
      "Epoch 635/1200, Training Loss: 35.06254, Validation Loss: 4.81713\n",
      "Epoch 636/1200, Training Loss: 35.05312, Validation Loss: 4.81527\n",
      "Epoch 637/1200, Training Loss: 35.04371, Validation Loss: 4.81340\n",
      "Epoch 638/1200, Training Loss: 35.03430, Validation Loss: 4.81153\n",
      "Epoch 639/1200, Training Loss: 35.02490, Validation Loss: 4.80967\n",
      "Epoch 640/1200, Training Loss: 35.01551, Validation Loss: 4.80781\n",
      "Epoch 641/1200, Training Loss: 35.00611, Validation Loss: 4.80595\n",
      "Epoch 642/1200, Training Loss: 34.99672, Validation Loss: 4.80410\n",
      "Epoch 643/1200, Training Loss: 34.98734, Validation Loss: 4.80224\n",
      "Epoch 644/1200, Training Loss: 34.97796, Validation Loss: 4.80039\n",
      "Epoch 645/1200, Training Loss: 34.96859, Validation Loss: 4.79854\n",
      "Epoch 646/1200, Training Loss: 34.95924, Validation Loss: 4.79670\n",
      "Epoch 647/1200, Training Loss: 34.94990, Validation Loss: 4.79485\n",
      "Epoch 648/1200, Training Loss: 34.94056, Validation Loss: 4.79300\n",
      "Epoch 649/1200, Training Loss: 34.93123, Validation Loss: 4.79116\n",
      "Epoch 650/1200, Training Loss: 34.92191, Validation Loss: 4.78932\n",
      "Epoch 651/1200, Training Loss: 34.91259, Validation Loss: 4.78748\n",
      "Epoch 652/1200, Training Loss: 34.90329, Validation Loss: 4.78564\n",
      "Epoch 653/1200, Training Loss: 34.89399, Validation Loss: 4.78381\n",
      "Epoch 654/1200, Training Loss: 34.88469, Validation Loss: 4.78197\n",
      "Epoch 655/1200, Training Loss: 34.87540, Validation Loss: 4.78015\n",
      "Epoch 656/1200, Training Loss: 34.86610, Validation Loss: 4.77832\n",
      "Epoch 657/1200, Training Loss: 34.85682, Validation Loss: 4.77650\n",
      "Epoch 658/1200, Training Loss: 34.84755, Validation Loss: 4.77468\n",
      "Epoch 659/1200, Training Loss: 34.83830, Validation Loss: 4.77286\n",
      "Epoch 660/1200, Training Loss: 34.82906, Validation Loss: 4.77104\n",
      "Epoch 661/1200, Training Loss: 34.81982, Validation Loss: 4.76923\n",
      "Epoch 662/1200, Training Loss: 34.81060, Validation Loss: 4.76741\n",
      "Epoch 663/1200, Training Loss: 34.80138, Validation Loss: 4.76560\n",
      "Epoch 664/1200, Training Loss: 34.79216, Validation Loss: 4.76378\n",
      "Epoch 665/1200, Training Loss: 34.78295, Validation Loss: 4.76197\n",
      "Epoch 666/1200, Training Loss: 34.77373, Validation Loss: 4.76016\n",
      "Epoch 667/1200, Training Loss: 34.76451, Validation Loss: 4.75835\n",
      "Epoch 668/1200, Training Loss: 34.75529, Validation Loss: 4.75654\n",
      "Epoch 669/1200, Training Loss: 34.74608, Validation Loss: 4.75474\n",
      "Epoch 670/1200, Training Loss: 34.73689, Validation Loss: 4.75293\n",
      "Epoch 671/1200, Training Loss: 34.72770, Validation Loss: 4.75113\n",
      "Epoch 672/1200, Training Loss: 34.71852, Validation Loss: 4.74933\n",
      "Epoch 673/1200, Training Loss: 34.70936, Validation Loss: 4.74753\n",
      "Epoch 674/1200, Training Loss: 34.70021, Validation Loss: 4.74574\n",
      "Epoch 675/1200, Training Loss: 34.69106, Validation Loss: 4.74394\n",
      "Epoch 676/1200, Training Loss: 34.68193, Validation Loss: 4.74215\n",
      "Epoch 677/1200, Training Loss: 34.67281, Validation Loss: 4.74035\n",
      "Epoch 678/1200, Training Loss: 34.66370, Validation Loss: 4.73856\n",
      "Epoch 679/1200, Training Loss: 34.65460, Validation Loss: 4.73677\n",
      "Epoch 680/1200, Training Loss: 34.64550, Validation Loss: 4.73498\n",
      "Epoch 681/1200, Training Loss: 34.63641, Validation Loss: 4.73319\n",
      "Epoch 682/1200, Training Loss: 34.62732, Validation Loss: 4.73141\n",
      "Epoch 683/1200, Training Loss: 34.61823, Validation Loss: 4.72962\n",
      "Epoch 684/1200, Training Loss: 34.60916, Validation Loss: 4.72784\n",
      "Epoch 685/1200, Training Loss: 34.60008, Validation Loss: 4.72606\n",
      "Epoch 686/1200, Training Loss: 34.59100, Validation Loss: 4.72428\n",
      "Epoch 687/1200, Training Loss: 34.58194, Validation Loss: 4.72250\n",
      "Epoch 688/1200, Training Loss: 34.57289, Validation Loss: 4.72072\n",
      "Epoch 689/1200, Training Loss: 34.56385, Validation Loss: 4.71894\n",
      "Epoch 690/1200, Training Loss: 34.55482, Validation Loss: 4.71717\n",
      "Epoch 691/1200, Training Loss: 34.54581, Validation Loss: 4.71539\n",
      "Epoch 692/1200, Training Loss: 34.53680, Validation Loss: 4.71362\n",
      "Epoch 693/1200, Training Loss: 34.52780, Validation Loss: 4.71185\n",
      "Epoch 694/1200, Training Loss: 34.51880, Validation Loss: 4.71008\n",
      "Epoch 695/1200, Training Loss: 34.50981, Validation Loss: 4.70832\n",
      "Epoch 696/1200, Training Loss: 34.50082, Validation Loss: 4.70655\n",
      "Epoch 697/1200, Training Loss: 34.49184, Validation Loss: 4.70479\n",
      "Epoch 698/1200, Training Loss: 34.48286, Validation Loss: 4.70304\n",
      "Epoch 699/1200, Training Loss: 34.47389, Validation Loss: 4.70128\n",
      "Epoch 700/1200, Training Loss: 34.46493, Validation Loss: 4.69953\n",
      "Epoch 701/1200, Training Loss: 34.45597, Validation Loss: 4.69778\n",
      "Epoch 702/1200, Training Loss: 34.44702, Validation Loss: 4.69603\n",
      "Epoch 703/1200, Training Loss: 34.43807, Validation Loss: 4.69429\n",
      "Epoch 704/1200, Training Loss: 34.42912, Validation Loss: 4.69254\n",
      "Epoch 705/1200, Training Loss: 34.42018, Validation Loss: 4.69080\n",
      "Epoch 706/1200, Training Loss: 34.41124, Validation Loss: 4.68906\n",
      "Epoch 707/1200, Training Loss: 34.40231, Validation Loss: 4.68732\n",
      "Epoch 708/1200, Training Loss: 34.39339, Validation Loss: 4.68558\n",
      "Epoch 709/1200, Training Loss: 34.38448, Validation Loss: 4.68384\n",
      "Epoch 710/1200, Training Loss: 34.37557, Validation Loss: 4.68211\n",
      "Epoch 711/1200, Training Loss: 34.36667, Validation Loss: 4.68038\n",
      "Epoch 712/1200, Training Loss: 34.35778, Validation Loss: 4.67865\n",
      "Epoch 713/1200, Training Loss: 34.34890, Validation Loss: 4.67691\n",
      "Epoch 714/1200, Training Loss: 34.34003, Validation Loss: 4.67518\n",
      "Epoch 715/1200, Training Loss: 34.33116, Validation Loss: 4.67346\n",
      "Epoch 716/1200, Training Loss: 34.32230, Validation Loss: 4.67173\n",
      "Epoch 717/1200, Training Loss: 34.31344, Validation Loss: 4.67001\n",
      "Epoch 718/1200, Training Loss: 34.30459, Validation Loss: 4.66828\n",
      "Epoch 719/1200, Training Loss: 34.29574, Validation Loss: 4.66657\n",
      "Epoch 720/1200, Training Loss: 34.28690, Validation Loss: 4.66485\n",
      "Epoch 721/1200, Training Loss: 34.27807, Validation Loss: 4.66313\n",
      "Epoch 722/1200, Training Loss: 34.26924, Validation Loss: 4.66142\n",
      "Epoch 723/1200, Training Loss: 34.26041, Validation Loss: 4.65971\n",
      "Epoch 724/1200, Training Loss: 34.25159, Validation Loss: 4.65800\n",
      "Epoch 725/1200, Training Loss: 34.24278, Validation Loss: 4.65629\n",
      "Epoch 726/1200, Training Loss: 34.23397, Validation Loss: 4.65458\n",
      "Epoch 727/1200, Training Loss: 34.22517, Validation Loss: 4.65287\n",
      "Epoch 728/1200, Training Loss: 34.21636, Validation Loss: 4.65117\n",
      "Epoch 729/1200, Training Loss: 34.20757, Validation Loss: 4.64947\n",
      "Epoch 730/1200, Training Loss: 34.19877, Validation Loss: 4.64777\n",
      "Epoch 731/1200, Training Loss: 34.18999, Validation Loss: 4.64607\n",
      "Epoch 732/1200, Training Loss: 34.18120, Validation Loss: 4.64437\n",
      "Epoch 733/1200, Training Loss: 34.17242, Validation Loss: 4.64268\n",
      "Epoch 734/1200, Training Loss: 34.16365, Validation Loss: 4.64098\n",
      "Epoch 735/1200, Training Loss: 34.15489, Validation Loss: 4.63929\n",
      "Epoch 736/1200, Training Loss: 34.14613, Validation Loss: 4.63760\n",
      "Epoch 737/1200, Training Loss: 34.13737, Validation Loss: 4.63591\n",
      "Epoch 738/1200, Training Loss: 34.12862, Validation Loss: 4.63422\n",
      "Epoch 739/1200, Training Loss: 34.11989, Validation Loss: 4.63253\n",
      "Epoch 740/1200, Training Loss: 34.11116, Validation Loss: 4.63085\n",
      "Epoch 741/1200, Training Loss: 34.10243, Validation Loss: 4.62917\n",
      "Epoch 742/1200, Training Loss: 34.09371, Validation Loss: 4.62749\n",
      "Epoch 743/1200, Training Loss: 34.08499, Validation Loss: 4.62582\n",
      "Epoch 744/1200, Training Loss: 34.07628, Validation Loss: 4.62414\n",
      "Epoch 745/1200, Training Loss: 34.06758, Validation Loss: 4.62247\n",
      "Epoch 746/1200, Training Loss: 34.05888, Validation Loss: 4.62079\n",
      "Epoch 747/1200, Training Loss: 34.05020, Validation Loss: 4.61912\n",
      "Epoch 748/1200, Training Loss: 34.04152, Validation Loss: 4.61745\n",
      "Epoch 749/1200, Training Loss: 34.03286, Validation Loss: 4.61578\n",
      "Epoch 750/1200, Training Loss: 34.02420, Validation Loss: 4.61412\n",
      "Epoch 751/1200, Training Loss: 34.01555, Validation Loss: 4.61246\n",
      "Epoch 752/1200, Training Loss: 34.00691, Validation Loss: 4.61080\n",
      "Epoch 753/1200, Training Loss: 33.99828, Validation Loss: 4.60915\n",
      "Epoch 754/1200, Training Loss: 33.98965, Validation Loss: 4.60750\n",
      "Epoch 755/1200, Training Loss: 33.98103, Validation Loss: 4.60586\n",
      "Epoch 756/1200, Training Loss: 33.97242, Validation Loss: 4.60421\n",
      "Epoch 757/1200, Training Loss: 33.96381, Validation Loss: 4.60256\n",
      "Epoch 758/1200, Training Loss: 33.95521, Validation Loss: 4.60092\n",
      "Epoch 759/1200, Training Loss: 33.94663, Validation Loss: 4.59928\n",
      "Epoch 760/1200, Training Loss: 33.93803, Validation Loss: 4.59764\n",
      "Epoch 761/1200, Training Loss: 33.92944, Validation Loss: 4.59600\n",
      "Epoch 762/1200, Training Loss: 33.92086, Validation Loss: 4.59437\n",
      "Epoch 763/1200, Training Loss: 33.91229, Validation Loss: 4.59273\n",
      "Epoch 764/1200, Training Loss: 33.90373, Validation Loss: 4.59110\n",
      "Epoch 765/1200, Training Loss: 33.89517, Validation Loss: 4.58947\n",
      "Epoch 766/1200, Training Loss: 33.88662, Validation Loss: 4.58784\n",
      "Epoch 767/1200, Training Loss: 33.87808, Validation Loss: 4.58621\n",
      "Epoch 768/1200, Training Loss: 33.86954, Validation Loss: 4.58458\n",
      "Epoch 769/1200, Training Loss: 33.86101, Validation Loss: 4.58295\n",
      "Epoch 770/1200, Training Loss: 33.85248, Validation Loss: 4.58133\n",
      "Epoch 771/1200, Training Loss: 33.84397, Validation Loss: 4.57970\n",
      "Epoch 772/1200, Training Loss: 33.83547, Validation Loss: 4.57808\n",
      "Epoch 773/1200, Training Loss: 33.82699, Validation Loss: 4.57646\n",
      "Epoch 774/1200, Training Loss: 33.81851, Validation Loss: 4.57484\n",
      "Epoch 775/1200, Training Loss: 33.81003, Validation Loss: 4.57323\n",
      "Epoch 776/1200, Training Loss: 33.80156, Validation Loss: 4.57161\n",
      "Epoch 777/1200, Training Loss: 33.79309, Validation Loss: 4.57000\n",
      "Epoch 778/1200, Training Loss: 33.78464, Validation Loss: 4.56839\n",
      "Epoch 779/1200, Training Loss: 33.77619, Validation Loss: 4.56678\n",
      "Epoch 780/1200, Training Loss: 33.76775, Validation Loss: 4.56518\n",
      "Epoch 781/1200, Training Loss: 33.75932, Validation Loss: 4.56357\n",
      "Epoch 782/1200, Training Loss: 33.75090, Validation Loss: 4.56197\n",
      "Epoch 783/1200, Training Loss: 33.74249, Validation Loss: 4.56037\n",
      "Epoch 784/1200, Training Loss: 33.73409, Validation Loss: 4.55877\n",
      "Epoch 785/1200, Training Loss: 33.72571, Validation Loss: 4.55718\n",
      "Epoch 786/1200, Training Loss: 33.71734, Validation Loss: 4.55559\n",
      "Epoch 787/1200, Training Loss: 33.70898, Validation Loss: 4.55400\n",
      "Epoch 788/1200, Training Loss: 33.70063, Validation Loss: 4.55241\n",
      "Epoch 789/1200, Training Loss: 33.69229, Validation Loss: 4.55082\n",
      "Epoch 790/1200, Training Loss: 33.68396, Validation Loss: 4.54924\n",
      "Epoch 791/1200, Training Loss: 33.67564, Validation Loss: 4.54765\n",
      "Epoch 792/1200, Training Loss: 33.66732, Validation Loss: 4.54607\n",
      "Epoch 793/1200, Training Loss: 33.65901, Validation Loss: 4.54449\n",
      "Epoch 794/1200, Training Loss: 33.65070, Validation Loss: 4.54292\n",
      "Epoch 795/1200, Training Loss: 33.64240, Validation Loss: 4.54134\n",
      "Epoch 796/1200, Training Loss: 33.63412, Validation Loss: 4.53977\n",
      "Epoch 797/1200, Training Loss: 33.62584, Validation Loss: 4.53820\n",
      "Epoch 798/1200, Training Loss: 33.61758, Validation Loss: 4.53663\n",
      "Epoch 799/1200, Training Loss: 33.60932, Validation Loss: 4.53506\n",
      "Epoch 800/1200, Training Loss: 33.60107, Validation Loss: 4.53350\n",
      "Epoch 801/1200, Training Loss: 33.59283, Validation Loss: 4.53194\n",
      "Epoch 802/1200, Training Loss: 33.58459, Validation Loss: 4.53038\n",
      "Epoch 803/1200, Training Loss: 33.57636, Validation Loss: 4.52883\n",
      "Epoch 804/1200, Training Loss: 33.56814, Validation Loss: 4.52727\n",
      "Epoch 805/1200, Training Loss: 33.55992, Validation Loss: 4.52572\n",
      "Epoch 806/1200, Training Loss: 33.55171, Validation Loss: 4.52416\n",
      "Epoch 807/1200, Training Loss: 33.54351, Validation Loss: 4.52261\n",
      "Epoch 808/1200, Training Loss: 33.53533, Validation Loss: 4.52107\n",
      "Epoch 809/1200, Training Loss: 33.52716, Validation Loss: 4.51952\n",
      "Epoch 810/1200, Training Loss: 33.51900, Validation Loss: 4.51798\n",
      "Epoch 811/1200, Training Loss: 33.51084, Validation Loss: 4.51644\n",
      "Epoch 812/1200, Training Loss: 33.50270, Validation Loss: 4.51490\n",
      "Epoch 813/1200, Training Loss: 33.49457, Validation Loss: 4.51336\n",
      "Epoch 814/1200, Training Loss: 33.48644, Validation Loss: 4.51182\n",
      "Epoch 815/1200, Training Loss: 33.47833, Validation Loss: 4.51029\n",
      "Epoch 816/1200, Training Loss: 33.47022, Validation Loss: 4.50876\n",
      "Epoch 817/1200, Training Loss: 33.46212, Validation Loss: 4.50723\n",
      "Epoch 818/1200, Training Loss: 33.45403, Validation Loss: 4.50571\n",
      "Epoch 819/1200, Training Loss: 33.44594, Validation Loss: 4.50419\n",
      "Epoch 820/1200, Training Loss: 33.43787, Validation Loss: 4.50267\n",
      "Epoch 821/1200, Training Loss: 33.42981, Validation Loss: 4.50115\n",
      "Epoch 822/1200, Training Loss: 33.42174, Validation Loss: 4.49964\n",
      "Epoch 823/1200, Training Loss: 33.41369, Validation Loss: 4.49813\n",
      "Epoch 824/1200, Training Loss: 33.40564, Validation Loss: 4.49662\n",
      "Epoch 825/1200, Training Loss: 33.39760, Validation Loss: 4.49511\n",
      "Epoch 826/1200, Training Loss: 33.38956, Validation Loss: 4.49361\n",
      "Epoch 827/1200, Training Loss: 33.38154, Validation Loss: 4.49211\n",
      "Epoch 828/1200, Training Loss: 33.37351, Validation Loss: 4.49061\n",
      "Epoch 829/1200, Training Loss: 33.36550, Validation Loss: 4.48912\n",
      "Epoch 830/1200, Training Loss: 33.35749, Validation Loss: 4.48762\n",
      "Epoch 831/1200, Training Loss: 33.34950, Validation Loss: 4.48613\n",
      "Epoch 832/1200, Training Loss: 33.34151, Validation Loss: 4.48464\n",
      "Epoch 833/1200, Training Loss: 33.33355, Validation Loss: 4.48316\n",
      "Epoch 834/1200, Training Loss: 33.32559, Validation Loss: 4.48167\n",
      "Epoch 835/1200, Training Loss: 33.31765, Validation Loss: 4.48019\n",
      "Epoch 836/1200, Training Loss: 33.30972, Validation Loss: 4.47871\n",
      "Epoch 837/1200, Training Loss: 33.30179, Validation Loss: 4.47723\n",
      "Epoch 838/1200, Training Loss: 33.29388, Validation Loss: 4.47575\n",
      "Epoch 839/1200, Training Loss: 33.28599, Validation Loss: 4.47428\n",
      "Epoch 840/1200, Training Loss: 33.27810, Validation Loss: 4.47280\n",
      "Epoch 841/1200, Training Loss: 33.27021, Validation Loss: 4.47133\n",
      "Epoch 842/1200, Training Loss: 33.26233, Validation Loss: 4.46986\n",
      "Epoch 843/1200, Training Loss: 33.25447, Validation Loss: 4.46840\n",
      "Epoch 844/1200, Training Loss: 33.24661, Validation Loss: 4.46693\n",
      "Epoch 845/1200, Training Loss: 33.23876, Validation Loss: 4.46547\n",
      "Epoch 846/1200, Training Loss: 33.23092, Validation Loss: 4.46401\n",
      "Epoch 847/1200, Training Loss: 33.22309, Validation Loss: 4.46256\n",
      "Epoch 848/1200, Training Loss: 33.21527, Validation Loss: 4.46110\n",
      "Epoch 849/1200, Training Loss: 33.20745, Validation Loss: 4.45965\n",
      "Epoch 850/1200, Training Loss: 33.19964, Validation Loss: 4.45820\n",
      "Epoch 851/1200, Training Loss: 33.19185, Validation Loss: 4.45675\n",
      "Epoch 852/1200, Training Loss: 33.18405, Validation Loss: 4.45531\n",
      "Epoch 853/1200, Training Loss: 33.17627, Validation Loss: 4.45386\n",
      "Epoch 854/1200, Training Loss: 33.16850, Validation Loss: 4.45242\n",
      "Epoch 855/1200, Training Loss: 33.16073, Validation Loss: 4.45099\n",
      "Epoch 856/1200, Training Loss: 33.15297, Validation Loss: 4.44955\n",
      "Epoch 857/1200, Training Loss: 33.14522, Validation Loss: 4.44811\n",
      "Epoch 858/1200, Training Loss: 33.13748, Validation Loss: 4.44668\n",
      "Epoch 859/1200, Training Loss: 33.12974, Validation Loss: 4.44525\n",
      "Epoch 860/1200, Training Loss: 33.12202, Validation Loss: 4.44382\n",
      "Epoch 861/1200, Training Loss: 33.11429, Validation Loss: 4.44239\n",
      "Epoch 862/1200, Training Loss: 33.10658, Validation Loss: 4.44097\n",
      "Epoch 863/1200, Training Loss: 33.09887, Validation Loss: 4.43955\n",
      "Epoch 864/1200, Training Loss: 33.09117, Validation Loss: 4.43813\n",
      "Epoch 865/1200, Training Loss: 33.08348, Validation Loss: 4.43671\n",
      "Epoch 866/1200, Training Loss: 33.07580, Validation Loss: 4.43529\n",
      "Epoch 867/1200, Training Loss: 33.06813, Validation Loss: 4.43388\n",
      "Epoch 868/1200, Training Loss: 33.06048, Validation Loss: 4.43246\n",
      "Epoch 869/1200, Training Loss: 33.05283, Validation Loss: 4.43105\n",
      "Epoch 870/1200, Training Loss: 33.04519, Validation Loss: 4.42964\n",
      "Epoch 871/1200, Training Loss: 33.03756, Validation Loss: 4.42823\n",
      "Epoch 872/1200, Training Loss: 33.02995, Validation Loss: 4.42683\n",
      "Epoch 873/1200, Training Loss: 33.02235, Validation Loss: 4.42542\n",
      "Epoch 874/1200, Training Loss: 33.01475, Validation Loss: 4.42402\n",
      "Epoch 875/1200, Training Loss: 33.00717, Validation Loss: 4.42262\n",
      "Epoch 876/1200, Training Loss: 32.99959, Validation Loss: 4.42122\n",
      "Epoch 877/1200, Training Loss: 32.99203, Validation Loss: 4.41982\n",
      "Epoch 878/1200, Training Loss: 32.98447, Validation Loss: 4.41842\n",
      "Epoch 879/1200, Training Loss: 32.97693, Validation Loss: 4.41703\n",
      "Epoch 880/1200, Training Loss: 32.96939, Validation Loss: 4.41563\n",
      "Epoch 881/1200, Training Loss: 32.96186, Validation Loss: 4.41424\n",
      "Epoch 882/1200, Training Loss: 32.95435, Validation Loss: 4.41285\n",
      "Epoch 883/1200, Training Loss: 32.94684, Validation Loss: 4.41147\n",
      "Epoch 884/1200, Training Loss: 32.93934, Validation Loss: 4.41008\n",
      "Epoch 885/1200, Training Loss: 32.93186, Validation Loss: 4.40870\n",
      "Epoch 886/1200, Training Loss: 32.92438, Validation Loss: 4.40732\n",
      "Epoch 887/1200, Training Loss: 32.91691, Validation Loss: 4.40594\n",
      "Epoch 888/1200, Training Loss: 32.90945, Validation Loss: 4.40456\n",
      "Epoch 889/1200, Training Loss: 32.90200, Validation Loss: 4.40319\n",
      "Epoch 890/1200, Training Loss: 32.89456, Validation Loss: 4.40182\n",
      "Epoch 891/1200, Training Loss: 32.88713, Validation Loss: 4.40045\n",
      "Epoch 892/1200, Training Loss: 32.87971, Validation Loss: 4.39908\n",
      "Epoch 893/1200, Training Loss: 32.87229, Validation Loss: 4.39771\n",
      "Epoch 894/1200, Training Loss: 32.86489, Validation Loss: 4.39635\n",
      "Epoch 895/1200, Training Loss: 32.85749, Validation Loss: 4.39498\n",
      "Epoch 896/1200, Training Loss: 32.85010, Validation Loss: 4.39362\n",
      "Epoch 897/1200, Training Loss: 32.84272, Validation Loss: 4.39226\n",
      "Epoch 898/1200, Training Loss: 32.83535, Validation Loss: 4.39091\n",
      "Epoch 899/1200, Training Loss: 32.82799, Validation Loss: 4.38955\n",
      "Epoch 900/1200, Training Loss: 32.82063, Validation Loss: 4.38820\n",
      "Epoch 901/1200, Training Loss: 32.81327, Validation Loss: 4.38685\n",
      "Epoch 902/1200, Training Loss: 32.80593, Validation Loss: 4.38551\n",
      "Epoch 903/1200, Training Loss: 32.79859, Validation Loss: 4.38416\n",
      "Epoch 904/1200, Training Loss: 32.79126, Validation Loss: 4.38282\n",
      "Epoch 905/1200, Training Loss: 32.78394, Validation Loss: 4.38147\n",
      "Epoch 906/1200, Training Loss: 32.77662, Validation Loss: 4.38013\n",
      "Epoch 907/1200, Training Loss: 32.76931, Validation Loss: 4.37880\n",
      "Epoch 908/1200, Training Loss: 32.76202, Validation Loss: 4.37746\n",
      "Epoch 909/1200, Training Loss: 32.75473, Validation Loss: 4.37613\n",
      "Epoch 910/1200, Training Loss: 32.74745, Validation Loss: 4.37480\n",
      "Epoch 911/1200, Training Loss: 32.74017, Validation Loss: 4.37347\n",
      "Epoch 912/1200, Training Loss: 32.73291, Validation Loss: 4.37214\n",
      "Epoch 913/1200, Training Loss: 32.72565, Validation Loss: 4.37082\n",
      "Epoch 914/1200, Training Loss: 32.71840, Validation Loss: 4.36950\n",
      "Epoch 915/1200, Training Loss: 32.71117, Validation Loss: 4.36818\n",
      "Epoch 916/1200, Training Loss: 32.70393, Validation Loss: 4.36686\n",
      "Epoch 917/1200, Training Loss: 32.69671, Validation Loss: 4.36554\n",
      "Epoch 918/1200, Training Loss: 32.68949, Validation Loss: 4.36423\n",
      "Epoch 919/1200, Training Loss: 32.68229, Validation Loss: 4.36291\n",
      "Epoch 920/1200, Training Loss: 32.67509, Validation Loss: 4.36160\n",
      "Epoch 921/1200, Training Loss: 32.66791, Validation Loss: 4.36029\n",
      "Epoch 922/1200, Training Loss: 32.66074, Validation Loss: 4.35898\n",
      "Epoch 923/1200, Training Loss: 32.65358, Validation Loss: 4.35768\n",
      "Epoch 924/1200, Training Loss: 32.64642, Validation Loss: 4.35637\n",
      "Epoch 925/1200, Training Loss: 32.63927, Validation Loss: 4.35507\n",
      "Epoch 926/1200, Training Loss: 32.63214, Validation Loss: 4.35377\n",
      "Epoch 927/1200, Training Loss: 32.62502, Validation Loss: 4.35248\n",
      "Epoch 928/1200, Training Loss: 32.61791, Validation Loss: 4.35118\n",
      "Epoch 929/1200, Training Loss: 32.61081, Validation Loss: 4.34989\n",
      "Epoch 930/1200, Training Loss: 32.60372, Validation Loss: 4.34860\n",
      "Epoch 931/1200, Training Loss: 32.59665, Validation Loss: 4.34731\n",
      "Epoch 932/1200, Training Loss: 32.58958, Validation Loss: 4.34603\n",
      "Epoch 933/1200, Training Loss: 32.58252, Validation Loss: 4.34474\n",
      "Epoch 934/1200, Training Loss: 32.57548, Validation Loss: 4.34346\n",
      "Epoch 935/1200, Training Loss: 32.56845, Validation Loss: 4.34219\n",
      "Epoch 936/1200, Training Loss: 32.56143, Validation Loss: 4.34091\n",
      "Epoch 937/1200, Training Loss: 32.55441, Validation Loss: 4.33964\n",
      "Epoch 938/1200, Training Loss: 32.54741, Validation Loss: 4.33837\n",
      "Epoch 939/1200, Training Loss: 32.54042, Validation Loss: 4.33710\n",
      "Epoch 940/1200, Training Loss: 32.53344, Validation Loss: 4.33583\n",
      "Epoch 941/1200, Training Loss: 32.52646, Validation Loss: 4.33456\n",
      "Epoch 942/1200, Training Loss: 32.51950, Validation Loss: 4.33330\n",
      "Epoch 943/1200, Training Loss: 32.51254, Validation Loss: 4.33204\n",
      "Epoch 944/1200, Training Loss: 32.50559, Validation Loss: 4.33078\n",
      "Epoch 945/1200, Training Loss: 32.49864, Validation Loss: 4.32952\n",
      "Epoch 946/1200, Training Loss: 32.49172, Validation Loss: 4.32826\n",
      "Epoch 947/1200, Training Loss: 32.48479, Validation Loss: 4.32701\n",
      "Epoch 948/1200, Training Loss: 32.47788, Validation Loss: 4.32576\n",
      "Epoch 949/1200, Training Loss: 32.47097, Validation Loss: 4.32451\n",
      "Epoch 950/1200, Training Loss: 32.46407, Validation Loss: 4.32326\n",
      "Epoch 951/1200, Training Loss: 32.45718, Validation Loss: 4.32202\n",
      "Epoch 952/1200, Training Loss: 32.45030, Validation Loss: 4.32077\n",
      "Epoch 953/1200, Training Loss: 32.44343, Validation Loss: 4.31953\n",
      "Epoch 954/1200, Training Loss: 32.43658, Validation Loss: 4.31830\n",
      "Epoch 955/1200, Training Loss: 32.42973, Validation Loss: 4.31706\n",
      "Epoch 956/1200, Training Loss: 32.42289, Validation Loss: 4.31583\n",
      "Epoch 957/1200, Training Loss: 32.41606, Validation Loss: 4.31460\n",
      "Epoch 958/1200, Training Loss: 32.40924, Validation Loss: 4.31337\n",
      "Epoch 959/1200, Training Loss: 32.40243, Validation Loss: 4.31215\n",
      "Epoch 960/1200, Training Loss: 32.39562, Validation Loss: 4.31092\n",
      "Epoch 961/1200, Training Loss: 32.38882, Validation Loss: 4.30970\n",
      "Epoch 962/1200, Training Loss: 32.38203, Validation Loss: 4.30848\n",
      "Epoch 963/1200, Training Loss: 32.37525, Validation Loss: 4.30726\n",
      "Epoch 964/1200, Training Loss: 32.36848, Validation Loss: 4.30604\n",
      "Epoch 965/1200, Training Loss: 32.36171, Validation Loss: 4.30483\n",
      "Epoch 966/1200, Training Loss: 32.35495, Validation Loss: 4.30362\n",
      "Epoch 967/1200, Training Loss: 32.34820, Validation Loss: 4.30242\n",
      "Epoch 968/1200, Training Loss: 32.34145, Validation Loss: 4.30121\n",
      "Epoch 969/1200, Training Loss: 32.33472, Validation Loss: 4.30001\n",
      "Epoch 970/1200, Training Loss: 32.32800, Validation Loss: 4.29881\n",
      "Epoch 971/1200, Training Loss: 32.32129, Validation Loss: 4.29761\n",
      "Epoch 972/1200, Training Loss: 32.31458, Validation Loss: 4.29642\n",
      "Epoch 973/1200, Training Loss: 32.30789, Validation Loss: 4.29523\n",
      "Epoch 974/1200, Training Loss: 32.30122, Validation Loss: 4.29403\n",
      "Epoch 975/1200, Training Loss: 32.29455, Validation Loss: 4.29284\n",
      "Epoch 976/1200, Training Loss: 32.28790, Validation Loss: 4.29166\n",
      "Epoch 977/1200, Training Loss: 32.28126, Validation Loss: 4.29047\n",
      "Epoch 978/1200, Training Loss: 32.27463, Validation Loss: 4.28929\n",
      "Epoch 979/1200, Training Loss: 32.26800, Validation Loss: 4.28811\n",
      "Epoch 980/1200, Training Loss: 32.26138, Validation Loss: 4.28693\n",
      "Epoch 981/1200, Training Loss: 32.25476, Validation Loss: 4.28576\n",
      "Epoch 982/1200, Training Loss: 32.24815, Validation Loss: 4.28458\n",
      "Epoch 983/1200, Training Loss: 32.24156, Validation Loss: 4.28341\n",
      "Epoch 984/1200, Training Loss: 32.23498, Validation Loss: 4.28224\n",
      "Epoch 985/1200, Training Loss: 32.22840, Validation Loss: 4.28106\n",
      "Epoch 986/1200, Training Loss: 32.22184, Validation Loss: 4.27990\n",
      "Epoch 987/1200, Training Loss: 32.21529, Validation Loss: 4.27872\n",
      "Epoch 988/1200, Training Loss: 32.20875, Validation Loss: 4.27755\n",
      "Epoch 989/1200, Training Loss: 32.20222, Validation Loss: 4.27638\n",
      "Epoch 990/1200, Training Loss: 32.19570, Validation Loss: 4.27522\n",
      "Epoch 991/1200, Training Loss: 32.18917, Validation Loss: 4.27406\n",
      "Epoch 992/1200, Training Loss: 32.18266, Validation Loss: 4.27290\n",
      "Epoch 993/1200, Training Loss: 32.17616, Validation Loss: 4.27174\n",
      "Epoch 994/1200, Training Loss: 32.16967, Validation Loss: 4.27059\n",
      "Epoch 995/1200, Training Loss: 32.16319, Validation Loss: 4.26943\n",
      "Epoch 996/1200, Training Loss: 32.15672, Validation Loss: 4.26828\n",
      "Epoch 997/1200, Training Loss: 32.15026, Validation Loss: 4.26713\n",
      "Epoch 998/1200, Training Loss: 32.14382, Validation Loss: 4.26599\n",
      "Epoch 999/1200, Training Loss: 32.13738, Validation Loss: 4.26484\n",
      "Epoch 1000/1200, Training Loss: 32.13095, Validation Loss: 4.26369\n",
      "Epoch 1001/1200, Training Loss: 32.12453, Validation Loss: 4.26255\n",
      "Epoch 1002/1200, Training Loss: 32.11812, Validation Loss: 4.26142\n",
      "Epoch 1003/1200, Training Loss: 32.11173, Validation Loss: 4.26028\n",
      "Epoch 1004/1200, Training Loss: 32.10534, Validation Loss: 4.25914\n",
      "Epoch 1005/1200, Training Loss: 32.09896, Validation Loss: 4.25800\n",
      "Epoch 1006/1200, Training Loss: 32.09259, Validation Loss: 4.25687\n",
      "Epoch 1007/1200, Training Loss: 32.08624, Validation Loss: 4.25574\n",
      "Epoch 1008/1200, Training Loss: 32.07989, Validation Loss: 4.25461\n",
      "Epoch 1009/1200, Training Loss: 32.07355, Validation Loss: 4.25349\n",
      "Epoch 1010/1200, Training Loss: 32.06722, Validation Loss: 4.25237\n",
      "Epoch 1011/1200, Training Loss: 32.06091, Validation Loss: 4.25124\n",
      "Epoch 1012/1200, Training Loss: 32.05460, Validation Loss: 4.25012\n",
      "Epoch 1013/1200, Training Loss: 32.04829, Validation Loss: 4.24901\n",
      "Epoch 1014/1200, Training Loss: 32.04200, Validation Loss: 4.24790\n",
      "Epoch 1015/1200, Training Loss: 32.03571, Validation Loss: 4.24679\n",
      "Epoch 1016/1200, Training Loss: 32.02944, Validation Loss: 4.24568\n",
      "Epoch 1017/1200, Training Loss: 32.02318, Validation Loss: 4.24457\n",
      "Epoch 1018/1200, Training Loss: 32.01693, Validation Loss: 4.24347\n",
      "Epoch 1019/1200, Training Loss: 32.01069, Validation Loss: 4.24237\n",
      "Epoch 1020/1200, Training Loss: 32.00446, Validation Loss: 4.24127\n",
      "Epoch 1021/1200, Training Loss: 31.99823, Validation Loss: 4.24017\n",
      "Epoch 1022/1200, Training Loss: 31.99202, Validation Loss: 4.23908\n",
      "Epoch 1023/1200, Training Loss: 31.98581, Validation Loss: 4.23799\n",
      "Epoch 1024/1200, Training Loss: 31.97962, Validation Loss: 4.23690\n",
      "Epoch 1025/1200, Training Loss: 31.97344, Validation Loss: 4.23582\n",
      "Epoch 1026/1200, Training Loss: 31.96728, Validation Loss: 4.23474\n",
      "Epoch 1027/1200, Training Loss: 31.96112, Validation Loss: 4.23365\n",
      "Epoch 1028/1200, Training Loss: 31.95497, Validation Loss: 4.23257\n",
      "Epoch 1029/1200, Training Loss: 31.94883, Validation Loss: 4.23150\n",
      "Epoch 1030/1200, Training Loss: 31.94270, Validation Loss: 4.23043\n",
      "Epoch 1031/1200, Training Loss: 31.93658, Validation Loss: 4.22935\n",
      "Epoch 1032/1200, Training Loss: 31.93047, Validation Loss: 4.22829\n",
      "Epoch 1033/1200, Training Loss: 31.92437, Validation Loss: 4.22722\n",
      "Epoch 1034/1200, Training Loss: 31.91828, Validation Loss: 4.22615\n",
      "Epoch 1035/1200, Training Loss: 31.91220, Validation Loss: 4.22508\n",
      "Epoch 1036/1200, Training Loss: 31.90612, Validation Loss: 4.22402\n",
      "Epoch 1037/1200, Training Loss: 31.90005, Validation Loss: 4.22296\n",
      "Epoch 1038/1200, Training Loss: 31.89399, Validation Loss: 4.22190\n",
      "Epoch 1039/1200, Training Loss: 31.88794, Validation Loss: 4.22084\n",
      "Epoch 1040/1200, Training Loss: 31.88190, Validation Loss: 4.21979\n",
      "Epoch 1041/1200, Training Loss: 31.87587, Validation Loss: 4.21874\n",
      "Epoch 1042/1200, Training Loss: 31.86985, Validation Loss: 4.21769\n",
      "Epoch 1043/1200, Training Loss: 31.86383, Validation Loss: 4.21663\n",
      "Epoch 1044/1200, Training Loss: 31.85783, Validation Loss: 4.21559\n",
      "Epoch 1045/1200, Training Loss: 31.85183, Validation Loss: 4.21454\n",
      "Epoch 1046/1200, Training Loss: 31.84585, Validation Loss: 4.21349\n",
      "Epoch 1047/1200, Training Loss: 31.83987, Validation Loss: 4.21245\n",
      "Epoch 1048/1200, Training Loss: 31.83391, Validation Loss: 4.21141\n",
      "Epoch 1049/1200, Training Loss: 31.82795, Validation Loss: 4.21037\n",
      "Epoch 1050/1200, Training Loss: 31.82201, Validation Loss: 4.20933\n",
      "Epoch 1051/1200, Training Loss: 31.81607, Validation Loss: 4.20830\n",
      "Epoch 1052/1200, Training Loss: 31.81015, Validation Loss: 4.20726\n",
      "Epoch 1053/1200, Training Loss: 31.80424, Validation Loss: 4.20623\n",
      "Epoch 1054/1200, Training Loss: 31.79833, Validation Loss: 4.20520\n",
      "Epoch 1055/1200, Training Loss: 31.79244, Validation Loss: 4.20417\n",
      "Epoch 1056/1200, Training Loss: 31.78656, Validation Loss: 4.20314\n",
      "Epoch 1057/1200, Training Loss: 31.78068, Validation Loss: 4.20212\n",
      "Epoch 1058/1200, Training Loss: 31.77482, Validation Loss: 4.20110\n",
      "Epoch 1059/1200, Training Loss: 31.76897, Validation Loss: 4.20008\n",
      "Epoch 1060/1200, Training Loss: 31.76312, Validation Loss: 4.19906\n",
      "Epoch 1061/1200, Training Loss: 31.75728, Validation Loss: 4.19804\n",
      "Epoch 1062/1200, Training Loss: 31.75145, Validation Loss: 4.19703\n",
      "Epoch 1063/1200, Training Loss: 31.74564, Validation Loss: 4.19602\n",
      "Epoch 1064/1200, Training Loss: 31.73984, Validation Loss: 4.19500\n",
      "Epoch 1065/1200, Training Loss: 31.73405, Validation Loss: 4.19399\n",
      "Epoch 1066/1200, Training Loss: 31.72827, Validation Loss: 4.19297\n",
      "Epoch 1067/1200, Training Loss: 31.72250, Validation Loss: 4.19196\n",
      "Epoch 1068/1200, Training Loss: 31.71674, Validation Loss: 4.19096\n",
      "Epoch 1069/1200, Training Loss: 31.71099, Validation Loss: 4.18995\n",
      "Epoch 1070/1200, Training Loss: 31.70525, Validation Loss: 4.18894\n",
      "Epoch 1071/1200, Training Loss: 31.69952, Validation Loss: 4.18794\n",
      "Epoch 1072/1200, Training Loss: 31.69380, Validation Loss: 4.18694\n",
      "Epoch 1073/1200, Training Loss: 31.68809, Validation Loss: 4.18594\n",
      "Epoch 1074/1200, Training Loss: 31.68239, Validation Loss: 4.18494\n",
      "Epoch 1075/1200, Training Loss: 31.67670, Validation Loss: 4.18394\n",
      "Epoch 1076/1200, Training Loss: 31.67103, Validation Loss: 4.18294\n",
      "Epoch 1077/1200, Training Loss: 31.66536, Validation Loss: 4.18195\n",
      "Epoch 1078/1200, Training Loss: 31.65970, Validation Loss: 4.18095\n",
      "Epoch 1079/1200, Training Loss: 31.65405, Validation Loss: 4.17996\n",
      "Epoch 1080/1200, Training Loss: 31.64841, Validation Loss: 4.17897\n",
      "Epoch 1081/1200, Training Loss: 31.64278, Validation Loss: 4.17798\n",
      "Epoch 1082/1200, Training Loss: 31.63717, Validation Loss: 4.17699\n",
      "Epoch 1083/1200, Training Loss: 31.63157, Validation Loss: 4.17601\n",
      "Epoch 1084/1200, Training Loss: 31.62598, Validation Loss: 4.17502\n",
      "Epoch 1085/1200, Training Loss: 31.62039, Validation Loss: 4.17404\n",
      "Epoch 1086/1200, Training Loss: 31.61482, Validation Loss: 4.17306\n",
      "Epoch 1087/1200, Training Loss: 31.60926, Validation Loss: 4.17208\n",
      "Epoch 1088/1200, Training Loss: 31.60371, Validation Loss: 4.17110\n",
      "Epoch 1089/1200, Training Loss: 31.59818, Validation Loss: 4.17012\n",
      "Epoch 1090/1200, Training Loss: 31.59265, Validation Loss: 4.16914\n",
      "Epoch 1091/1200, Training Loss: 31.58713, Validation Loss: 4.16818\n",
      "Epoch 1092/1200, Training Loss: 31.58163, Validation Loss: 4.16720\n",
      "Epoch 1093/1200, Training Loss: 31.57614, Validation Loss: 4.16623\n",
      "Epoch 1094/1200, Training Loss: 31.57067, Validation Loss: 4.16527\n",
      "Epoch 1095/1200, Training Loss: 31.56520, Validation Loss: 4.16430\n",
      "Epoch 1096/1200, Training Loss: 31.55974, Validation Loss: 4.16334\n",
      "Epoch 1097/1200, Training Loss: 31.55428, Validation Loss: 4.16238\n",
      "Epoch 1098/1200, Training Loss: 31.54883, Validation Loss: 4.16142\n",
      "Epoch 1099/1200, Training Loss: 31.54339, Validation Loss: 4.16047\n",
      "Epoch 1100/1200, Training Loss: 31.53796, Validation Loss: 4.15952\n",
      "Epoch 1101/1200, Training Loss: 31.53254, Validation Loss: 4.15856\n",
      "Epoch 1102/1200, Training Loss: 31.52712, Validation Loss: 4.15761\n",
      "Epoch 1103/1200, Training Loss: 31.52172, Validation Loss: 4.15666\n",
      "Epoch 1104/1200, Training Loss: 31.51632, Validation Loss: 4.15571\n",
      "Epoch 1105/1200, Training Loss: 31.51092, Validation Loss: 4.15477\n",
      "Epoch 1106/1200, Training Loss: 31.50553, Validation Loss: 4.15383\n",
      "Epoch 1107/1200, Training Loss: 31.50014, Validation Loss: 4.15289\n",
      "Epoch 1108/1200, Training Loss: 31.49477, Validation Loss: 4.15194\n",
      "Epoch 1109/1200, Training Loss: 31.48940, Validation Loss: 4.15101\n",
      "Epoch 1110/1200, Training Loss: 31.48405, Validation Loss: 4.15008\n",
      "Epoch 1111/1200, Training Loss: 31.47871, Validation Loss: 4.14914\n",
      "Epoch 1112/1200, Training Loss: 31.47338, Validation Loss: 4.14821\n",
      "Epoch 1113/1200, Training Loss: 31.46807, Validation Loss: 4.14727\n",
      "Epoch 1114/1200, Training Loss: 31.46276, Validation Loss: 4.14635\n",
      "Epoch 1115/1200, Training Loss: 31.45747, Validation Loss: 4.14542\n",
      "Epoch 1116/1200, Training Loss: 31.45218, Validation Loss: 4.14449\n",
      "Epoch 1117/1200, Training Loss: 31.44691, Validation Loss: 4.14356\n",
      "Epoch 1118/1200, Training Loss: 31.44164, Validation Loss: 4.14265\n",
      "Epoch 1119/1200, Training Loss: 31.43638, Validation Loss: 4.14173\n",
      "Epoch 1120/1200, Training Loss: 31.43112, Validation Loss: 4.14081\n",
      "Epoch 1121/1200, Training Loss: 31.42588, Validation Loss: 4.13990\n",
      "Epoch 1122/1200, Training Loss: 31.42065, Validation Loss: 4.13900\n",
      "Epoch 1123/1200, Training Loss: 31.41542, Validation Loss: 4.13809\n",
      "Epoch 1124/1200, Training Loss: 31.41020, Validation Loss: 4.13718\n",
      "Epoch 1125/1200, Training Loss: 31.40499, Validation Loss: 4.13627\n",
      "Epoch 1126/1200, Training Loss: 31.39979, Validation Loss: 4.13537\n",
      "Epoch 1127/1200, Training Loss: 31.39460, Validation Loss: 4.13447\n",
      "Epoch 1128/1200, Training Loss: 31.38942, Validation Loss: 4.13357\n",
      "Epoch 1129/1200, Training Loss: 31.38424, Validation Loss: 4.13267\n",
      "Epoch 1130/1200, Training Loss: 31.37908, Validation Loss: 4.13178\n",
      "Epoch 1131/1200, Training Loss: 31.37392, Validation Loss: 4.13089\n",
      "Epoch 1132/1200, Training Loss: 31.36877, Validation Loss: 4.13000\n",
      "Epoch 1133/1200, Training Loss: 31.36363, Validation Loss: 4.12910\n",
      "Epoch 1134/1200, Training Loss: 31.35850, Validation Loss: 4.12821\n",
      "Epoch 1135/1200, Training Loss: 31.35339, Validation Loss: 4.12733\n",
      "Epoch 1136/1200, Training Loss: 31.34827, Validation Loss: 4.12644\n",
      "Epoch 1137/1200, Training Loss: 31.34317, Validation Loss: 4.12556\n",
      "Epoch 1138/1200, Training Loss: 31.33808, Validation Loss: 4.12468\n",
      "Epoch 1139/1200, Training Loss: 31.33300, Validation Loss: 4.12380\n",
      "Epoch 1140/1200, Training Loss: 31.32792, Validation Loss: 4.12292\n",
      "Epoch 1141/1200, Training Loss: 31.32286, Validation Loss: 4.12204\n",
      "Epoch 1142/1200, Training Loss: 31.31781, Validation Loss: 4.12117\n",
      "Epoch 1143/1200, Training Loss: 31.31277, Validation Loss: 4.12029\n",
      "Epoch 1144/1200, Training Loss: 31.30773, Validation Loss: 4.11943\n",
      "Epoch 1145/1200, Training Loss: 31.30271, Validation Loss: 4.11855\n",
      "Epoch 1146/1200, Training Loss: 31.29769, Validation Loss: 4.11768\n",
      "Epoch 1147/1200, Training Loss: 31.29268, Validation Loss: 4.11682\n",
      "Epoch 1148/1200, Training Loss: 31.28768, Validation Loss: 4.11596\n",
      "Epoch 1149/1200, Training Loss: 31.28269, Validation Loss: 4.11509\n",
      "Epoch 1150/1200, Training Loss: 31.27770, Validation Loss: 4.11422\n",
      "Epoch 1151/1200, Training Loss: 31.27273, Validation Loss: 4.11336\n",
      "Epoch 1152/1200, Training Loss: 31.26776, Validation Loss: 4.11250\n",
      "Epoch 1153/1200, Training Loss: 31.26280, Validation Loss: 4.11164\n",
      "Epoch 1154/1200, Training Loss: 31.25785, Validation Loss: 4.11078\n",
      "Epoch 1155/1200, Training Loss: 31.25290, Validation Loss: 4.10992\n",
      "Epoch 1156/1200, Training Loss: 31.24796, Validation Loss: 4.10907\n",
      "Epoch 1157/1200, Training Loss: 31.24303, Validation Loss: 4.10822\n",
      "Epoch 1158/1200, Training Loss: 31.23810, Validation Loss: 4.10736\n",
      "Epoch 1159/1200, Training Loss: 31.23318, Validation Loss: 4.10651\n",
      "Epoch 1160/1200, Training Loss: 31.22826, Validation Loss: 4.10567\n",
      "Epoch 1161/1200, Training Loss: 31.22334, Validation Loss: 4.10482\n",
      "Epoch 1162/1200, Training Loss: 31.21844, Validation Loss: 4.10397\n",
      "Epoch 1163/1200, Training Loss: 31.21355, Validation Loss: 4.10312\n",
      "Epoch 1164/1200, Training Loss: 31.20866, Validation Loss: 4.10229\n",
      "Epoch 1165/1200, Training Loss: 31.20379, Validation Loss: 4.10144\n",
      "Epoch 1166/1200, Training Loss: 31.19892, Validation Loss: 4.10060\n",
      "Epoch 1167/1200, Training Loss: 31.19406, Validation Loss: 4.09976\n",
      "Epoch 1168/1200, Training Loss: 31.18921, Validation Loss: 4.09893\n",
      "Epoch 1169/1200, Training Loss: 31.18437, Validation Loss: 4.09809\n",
      "Epoch 1170/1200, Training Loss: 31.17954, Validation Loss: 4.09726\n",
      "Epoch 1171/1200, Training Loss: 31.17472, Validation Loss: 4.09642\n",
      "Epoch 1172/1200, Training Loss: 31.16991, Validation Loss: 4.09560\n",
      "Epoch 1173/1200, Training Loss: 31.16510, Validation Loss: 4.09477\n",
      "Epoch 1174/1200, Training Loss: 31.16031, Validation Loss: 4.09395\n",
      "Epoch 1175/1200, Training Loss: 31.15552, Validation Loss: 4.09312\n",
      "Epoch 1176/1200, Training Loss: 31.15075, Validation Loss: 4.09230\n",
      "Epoch 1177/1200, Training Loss: 31.14598, Validation Loss: 4.09149\n",
      "Epoch 1178/1200, Training Loss: 31.14122, Validation Loss: 4.09066\n",
      "Epoch 1179/1200, Training Loss: 31.13647, Validation Loss: 4.08985\n",
      "Epoch 1180/1200, Training Loss: 31.13173, Validation Loss: 4.08904\n",
      "Epoch 1181/1200, Training Loss: 31.12699, Validation Loss: 4.08822\n",
      "Epoch 1182/1200, Training Loss: 31.12226, Validation Loss: 4.08742\n",
      "Epoch 1183/1200, Training Loss: 31.11754, Validation Loss: 4.08661\n",
      "Epoch 1184/1200, Training Loss: 31.11283, Validation Loss: 4.08580\n",
      "Epoch 1185/1200, Training Loss: 31.10813, Validation Loss: 4.08499\n",
      "Epoch 1186/1200, Training Loss: 31.10344, Validation Loss: 4.08418\n",
      "Epoch 1187/1200, Training Loss: 31.09876, Validation Loss: 4.08339\n",
      "Epoch 1188/1200, Training Loss: 31.09408, Validation Loss: 4.08258\n",
      "Epoch 1189/1200, Training Loss: 31.08942, Validation Loss: 4.08178\n",
      "Epoch 1190/1200, Training Loss: 31.08476, Validation Loss: 4.08098\n",
      "Epoch 1191/1200, Training Loss: 31.08012, Validation Loss: 4.08019\n",
      "Epoch 1192/1200, Training Loss: 31.07548, Validation Loss: 4.07939\n",
      "Epoch 1193/1200, Training Loss: 31.07085, Validation Loss: 4.07860\n",
      "Epoch 1194/1200, Training Loss: 31.06623, Validation Loss: 4.07781\n",
      "Epoch 1195/1200, Training Loss: 31.06161, Validation Loss: 4.07702\n",
      "Epoch 1196/1200, Training Loss: 31.05700, Validation Loss: 4.07624\n",
      "Epoch 1197/1200, Training Loss: 31.05240, Validation Loss: 4.07546\n",
      "Epoch 1198/1200, Training Loss: 31.04781, Validation Loss: 4.07468\n",
      "Epoch 1199/1200, Training Loss: 31.04322, Validation Loss: 4.07389\n",
      "Epoch 1200/1200, Training Loss: 31.03865, Validation Loss: 4.07312\n",
      "Training took: 100.38 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_epoch_1200_1 = NeuralNetwork().to(device)\n",
    "summary(model_epoch_1200_1, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1200\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_epoch_1200_1.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_epoch_1200_1=[]\n",
    "val_loss_list_epoch_1200_1=[]\n",
    "train_accuracy_list_epoch_1200_1=[]\n",
    "val_accuracy_list_epoch_1200_1=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_epoch_1200_1.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_epoch_1200_1(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_epoch_1200_1.append(train_accuracy)\n",
    "    train_loss_list_epoch_1200_1.append(train_loss)\n",
    "\n",
    "    model_epoch_1200_1.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_epoch_1200_1(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_epoch_1200_1.append(val_accuracy)\n",
    "    val_accuracy_list_epoch_1200_1.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHuLXur5thGT",
    "outputId": "69622821-3e9b-4389-aa57-40385a9ca561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable epoch size with epoch size as 1200: 0.7403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_epoch_1200_1.eval()\n",
    "test_predictions_epoch_1200_1 = model_epoch_1200_1(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_epoch_1200_1 = torch.round(test_predictions_epoch_1200_1)\n",
    "\n",
    "test_predictions_rounded_numpy_epoch_1200_1 = test_predictions_rounded_epoch_1200_1.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_epoch_1200_1 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_epoch_1200_1)\n",
    "\n",
    "print(f\"Accuracy for variable epoch size with epoch size as 1200: {accuracy_epoch_1200_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0B9E_sdtoB4",
    "outputId": "111089d8-a155-4238-9063-44da6a7e0534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable epoch size with epoch size as 1200: 0.53790\n"
     ]
    }
   ],
   "source": [
    "model_epoch_1200_1.eval()\n",
    "test_loss_epoch_1200_1=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_epoch_1200_1 = model_epoch_1200_1(X_test_tensor)\n",
    "    test_loss_epoch_1200_1 = loss_function(test_outputs_epoch_1200_1, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable epoch size with epoch size as 1200: {test_loss_epoch_1200_1.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjYIsBmauHnk"
   },
   "source": [
    "Epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yk74qi8puKHk",
    "outputId": "5dc95d46-18a7-4d26-c83e-b7900948caca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Training Loss: 41.17896, Validation Loss: 6.03281\n",
      "Epoch 2/2000, Training Loss: 41.16479, Validation Loss: 6.03030\n",
      "Epoch 3/2000, Training Loss: 41.15070, Validation Loss: 6.02781\n",
      "Epoch 4/2000, Training Loss: 41.13667, Validation Loss: 6.02532\n",
      "Epoch 5/2000, Training Loss: 41.12269, Validation Loss: 6.02284\n",
      "Epoch 6/2000, Training Loss: 41.10876, Validation Loss: 6.02036\n",
      "Epoch 7/2000, Training Loss: 41.09489, Validation Loss: 6.01790\n",
      "Epoch 8/2000, Training Loss: 41.08109, Validation Loss: 6.01545\n",
      "Epoch 9/2000, Training Loss: 41.06735, Validation Loss: 6.01301\n",
      "Epoch 10/2000, Training Loss: 41.05367, Validation Loss: 6.01057\n",
      "Epoch 11/2000, Training Loss: 41.04005, Validation Loss: 6.00815\n",
      "Epoch 12/2000, Training Loss: 41.02649, Validation Loss: 6.00573\n",
      "Epoch 13/2000, Training Loss: 41.01300, Validation Loss: 6.00333\n",
      "Epoch 14/2000, Training Loss: 40.99955, Validation Loss: 6.00094\n",
      "Epoch 15/2000, Training Loss: 40.98615, Validation Loss: 5.99855\n",
      "Epoch 16/2000, Training Loss: 40.97280, Validation Loss: 5.99617\n",
      "Epoch 17/2000, Training Loss: 40.95950, Validation Loss: 5.99380\n",
      "Epoch 18/2000, Training Loss: 40.94626, Validation Loss: 5.99143\n",
      "Epoch 19/2000, Training Loss: 40.93308, Validation Loss: 5.98908\n",
      "Epoch 20/2000, Training Loss: 40.91995, Validation Loss: 5.98674\n",
      "Epoch 21/2000, Training Loss: 40.90687, Validation Loss: 5.98440\n",
      "Epoch 22/2000, Training Loss: 40.89386, Validation Loss: 5.98207\n",
      "Epoch 23/2000, Training Loss: 40.88090, Validation Loss: 5.97974\n",
      "Epoch 24/2000, Training Loss: 40.86800, Validation Loss: 5.97743\n",
      "Epoch 25/2000, Training Loss: 40.85514, Validation Loss: 5.97512\n",
      "Epoch 26/2000, Training Loss: 40.84233, Validation Loss: 5.97282\n",
      "Epoch 27/2000, Training Loss: 40.82958, Validation Loss: 5.97053\n",
      "Epoch 28/2000, Training Loss: 40.81688, Validation Loss: 5.96825\n",
      "Epoch 29/2000, Training Loss: 40.80422, Validation Loss: 5.96598\n",
      "Epoch 30/2000, Training Loss: 40.79162, Validation Loss: 5.96372\n",
      "Epoch 31/2000, Training Loss: 40.77906, Validation Loss: 5.96146\n",
      "Epoch 32/2000, Training Loss: 40.76656, Validation Loss: 5.95921\n",
      "Epoch 33/2000, Training Loss: 40.75411, Validation Loss: 5.95697\n",
      "Epoch 34/2000, Training Loss: 40.74170, Validation Loss: 5.95474\n",
      "Epoch 35/2000, Training Loss: 40.72935, Validation Loss: 5.95252\n",
      "Epoch 36/2000, Training Loss: 40.71706, Validation Loss: 5.95031\n",
      "Epoch 37/2000, Training Loss: 40.70482, Validation Loss: 5.94810\n",
      "Epoch 38/2000, Training Loss: 40.69264, Validation Loss: 5.94590\n",
      "Epoch 39/2000, Training Loss: 40.68051, Validation Loss: 5.94371\n",
      "Epoch 40/2000, Training Loss: 40.66842, Validation Loss: 5.94153\n",
      "Epoch 41/2000, Training Loss: 40.65638, Validation Loss: 5.93935\n",
      "Epoch 42/2000, Training Loss: 40.64438, Validation Loss: 5.93718\n",
      "Epoch 43/2000, Training Loss: 40.63243, Validation Loss: 5.93502\n",
      "Epoch 44/2000, Training Loss: 40.62052, Validation Loss: 5.93287\n",
      "Epoch 45/2000, Training Loss: 40.60867, Validation Loss: 5.93072\n",
      "Epoch 46/2000, Training Loss: 40.59686, Validation Loss: 5.92859\n",
      "Epoch 47/2000, Training Loss: 40.58510, Validation Loss: 5.92645\n",
      "Epoch 48/2000, Training Loss: 40.57336, Validation Loss: 5.92432\n",
      "Epoch 49/2000, Training Loss: 40.56167, Validation Loss: 5.92219\n",
      "Epoch 50/2000, Training Loss: 40.55003, Validation Loss: 5.92007\n",
      "Epoch 51/2000, Training Loss: 40.53844, Validation Loss: 5.91795\n",
      "Epoch 52/2000, Training Loss: 40.52690, Validation Loss: 5.91584\n",
      "Epoch 53/2000, Training Loss: 40.51542, Validation Loss: 5.91375\n",
      "Epoch 54/2000, Training Loss: 40.50398, Validation Loss: 5.91165\n",
      "Epoch 55/2000, Training Loss: 40.49258, Validation Loss: 5.90957\n",
      "Epoch 56/2000, Training Loss: 40.48123, Validation Loss: 5.90749\n",
      "Epoch 57/2000, Training Loss: 40.46992, Validation Loss: 5.90541\n",
      "Epoch 58/2000, Training Loss: 40.45866, Validation Loss: 5.90335\n",
      "Epoch 59/2000, Training Loss: 40.44745, Validation Loss: 5.90129\n",
      "Epoch 60/2000, Training Loss: 40.43627, Validation Loss: 5.89924\n",
      "Epoch 61/2000, Training Loss: 40.42512, Validation Loss: 5.89719\n",
      "Epoch 62/2000, Training Loss: 40.41399, Validation Loss: 5.89514\n",
      "Epoch 63/2000, Training Loss: 40.40289, Validation Loss: 5.89310\n",
      "Epoch 64/2000, Training Loss: 40.39181, Validation Loss: 5.89107\n",
      "Epoch 65/2000, Training Loss: 40.38077, Validation Loss: 5.88904\n",
      "Epoch 66/2000, Training Loss: 40.36976, Validation Loss: 5.88702\n",
      "Epoch 67/2000, Training Loss: 40.35878, Validation Loss: 5.88500\n",
      "Epoch 68/2000, Training Loss: 40.34784, Validation Loss: 5.88299\n",
      "Epoch 69/2000, Training Loss: 40.33692, Validation Loss: 5.88099\n",
      "Epoch 70/2000, Training Loss: 40.32606, Validation Loss: 5.87899\n",
      "Epoch 71/2000, Training Loss: 40.31525, Validation Loss: 5.87700\n",
      "Epoch 72/2000, Training Loss: 40.30447, Validation Loss: 5.87501\n",
      "Epoch 73/2000, Training Loss: 40.29373, Validation Loss: 5.87303\n",
      "Epoch 74/2000, Training Loss: 40.28301, Validation Loss: 5.87105\n",
      "Epoch 75/2000, Training Loss: 40.27233, Validation Loss: 5.86908\n",
      "Epoch 76/2000, Training Loss: 40.26170, Validation Loss: 5.86711\n",
      "Epoch 77/2000, Training Loss: 40.25111, Validation Loss: 5.86515\n",
      "Epoch 78/2000, Training Loss: 40.24054, Validation Loss: 5.86319\n",
      "Epoch 79/2000, Training Loss: 40.23001, Validation Loss: 5.86124\n",
      "Epoch 80/2000, Training Loss: 40.21951, Validation Loss: 5.85929\n",
      "Epoch 81/2000, Training Loss: 40.20905, Validation Loss: 5.85734\n",
      "Epoch 82/2000, Training Loss: 40.19862, Validation Loss: 5.85539\n",
      "Epoch 83/2000, Training Loss: 40.18823, Validation Loss: 5.85345\n",
      "Epoch 84/2000, Training Loss: 40.17788, Validation Loss: 5.85152\n",
      "Epoch 85/2000, Training Loss: 40.16755, Validation Loss: 5.84959\n",
      "Epoch 86/2000, Training Loss: 40.15727, Validation Loss: 5.84767\n",
      "Epoch 87/2000, Training Loss: 40.14703, Validation Loss: 5.84576\n",
      "Epoch 88/2000, Training Loss: 40.13681, Validation Loss: 5.84385\n",
      "Epoch 89/2000, Training Loss: 40.12662, Validation Loss: 5.84194\n",
      "Epoch 90/2000, Training Loss: 40.11649, Validation Loss: 5.84004\n",
      "Epoch 91/2000, Training Loss: 40.10639, Validation Loss: 5.83815\n",
      "Epoch 92/2000, Training Loss: 40.09633, Validation Loss: 5.83627\n",
      "Epoch 93/2000, Training Loss: 40.08631, Validation Loss: 5.83438\n",
      "Epoch 94/2000, Training Loss: 40.07631, Validation Loss: 5.83250\n",
      "Epoch 95/2000, Training Loss: 40.06634, Validation Loss: 5.83063\n",
      "Epoch 96/2000, Training Loss: 40.05638, Validation Loss: 5.82876\n",
      "Epoch 97/2000, Training Loss: 40.04646, Validation Loss: 5.82689\n",
      "Epoch 98/2000, Training Loss: 40.03656, Validation Loss: 5.82502\n",
      "Epoch 99/2000, Training Loss: 40.02671, Validation Loss: 5.82315\n",
      "Epoch 100/2000, Training Loss: 40.01688, Validation Loss: 5.82129\n",
      "Epoch 101/2000, Training Loss: 40.00709, Validation Loss: 5.81944\n",
      "Epoch 102/2000, Training Loss: 39.99733, Validation Loss: 5.81759\n",
      "Epoch 103/2000, Training Loss: 39.98760, Validation Loss: 5.81575\n",
      "Epoch 104/2000, Training Loss: 39.97791, Validation Loss: 5.81391\n",
      "Epoch 105/2000, Training Loss: 39.96826, Validation Loss: 5.81208\n",
      "Epoch 106/2000, Training Loss: 39.95864, Validation Loss: 5.81025\n",
      "Epoch 107/2000, Training Loss: 39.94905, Validation Loss: 5.80843\n",
      "Epoch 108/2000, Training Loss: 39.93948, Validation Loss: 5.80661\n",
      "Epoch 109/2000, Training Loss: 39.92994, Validation Loss: 5.80479\n",
      "Epoch 110/2000, Training Loss: 39.92043, Validation Loss: 5.80298\n",
      "Epoch 111/2000, Training Loss: 39.91093, Validation Loss: 5.80117\n",
      "Epoch 112/2000, Training Loss: 39.90147, Validation Loss: 5.79937\n",
      "Epoch 113/2000, Training Loss: 39.89204, Validation Loss: 5.79757\n",
      "Epoch 114/2000, Training Loss: 39.88263, Validation Loss: 5.79578\n",
      "Epoch 115/2000, Training Loss: 39.87325, Validation Loss: 5.79399\n",
      "Epoch 116/2000, Training Loss: 39.86390, Validation Loss: 5.79220\n",
      "Epoch 117/2000, Training Loss: 39.85457, Validation Loss: 5.79042\n",
      "Epoch 118/2000, Training Loss: 39.84527, Validation Loss: 5.78864\n",
      "Epoch 119/2000, Training Loss: 39.83599, Validation Loss: 5.78687\n",
      "Epoch 120/2000, Training Loss: 39.82674, Validation Loss: 5.78510\n",
      "Epoch 121/2000, Training Loss: 39.81753, Validation Loss: 5.78333\n",
      "Epoch 122/2000, Training Loss: 39.80833, Validation Loss: 5.78157\n",
      "Epoch 123/2000, Training Loss: 39.79916, Validation Loss: 5.77981\n",
      "Epoch 124/2000, Training Loss: 39.79002, Validation Loss: 5.77805\n",
      "Epoch 125/2000, Training Loss: 39.78089, Validation Loss: 5.77630\n",
      "Epoch 126/2000, Training Loss: 39.77179, Validation Loss: 5.77455\n",
      "Epoch 127/2000, Training Loss: 39.76271, Validation Loss: 5.77280\n",
      "Epoch 128/2000, Training Loss: 39.75363, Validation Loss: 5.77105\n",
      "Epoch 129/2000, Training Loss: 39.74458, Validation Loss: 5.76931\n",
      "Epoch 130/2000, Training Loss: 39.73556, Validation Loss: 5.76758\n",
      "Epoch 131/2000, Training Loss: 39.72656, Validation Loss: 5.76584\n",
      "Epoch 132/2000, Training Loss: 39.71759, Validation Loss: 5.76411\n",
      "Epoch 133/2000, Training Loss: 39.70865, Validation Loss: 5.76238\n",
      "Epoch 134/2000, Training Loss: 39.69972, Validation Loss: 5.76066\n",
      "Epoch 135/2000, Training Loss: 39.69082, Validation Loss: 5.75894\n",
      "Epoch 136/2000, Training Loss: 39.68195, Validation Loss: 5.75722\n",
      "Epoch 137/2000, Training Loss: 39.67309, Validation Loss: 5.75550\n",
      "Epoch 138/2000, Training Loss: 39.66425, Validation Loss: 5.75379\n",
      "Epoch 139/2000, Training Loss: 39.65545, Validation Loss: 5.75208\n",
      "Epoch 140/2000, Training Loss: 39.64666, Validation Loss: 5.75037\n",
      "Epoch 141/2000, Training Loss: 39.63791, Validation Loss: 5.74866\n",
      "Epoch 142/2000, Training Loss: 39.62918, Validation Loss: 5.74697\n",
      "Epoch 143/2000, Training Loss: 39.62048, Validation Loss: 5.74527\n",
      "Epoch 144/2000, Training Loss: 39.61180, Validation Loss: 5.74358\n",
      "Epoch 145/2000, Training Loss: 39.60313, Validation Loss: 5.74189\n",
      "Epoch 146/2000, Training Loss: 39.59448, Validation Loss: 5.74020\n",
      "Epoch 147/2000, Training Loss: 39.58585, Validation Loss: 5.73852\n",
      "Epoch 148/2000, Training Loss: 39.57724, Validation Loss: 5.73684\n",
      "Epoch 149/2000, Training Loss: 39.56865, Validation Loss: 5.73516\n",
      "Epoch 150/2000, Training Loss: 39.56009, Validation Loss: 5.73349\n",
      "Epoch 151/2000, Training Loss: 39.55154, Validation Loss: 5.73183\n",
      "Epoch 152/2000, Training Loss: 39.54301, Validation Loss: 5.73016\n",
      "Epoch 153/2000, Training Loss: 39.53450, Validation Loss: 5.72851\n",
      "Epoch 154/2000, Training Loss: 39.52601, Validation Loss: 5.72685\n",
      "Epoch 155/2000, Training Loss: 39.51756, Validation Loss: 5.72520\n",
      "Epoch 156/2000, Training Loss: 39.50911, Validation Loss: 5.72355\n",
      "Epoch 157/2000, Training Loss: 39.50068, Validation Loss: 5.72190\n",
      "Epoch 158/2000, Training Loss: 39.49226, Validation Loss: 5.72025\n",
      "Epoch 159/2000, Training Loss: 39.48385, Validation Loss: 5.71860\n",
      "Epoch 160/2000, Training Loss: 39.47546, Validation Loss: 5.71696\n",
      "Epoch 161/2000, Training Loss: 39.46708, Validation Loss: 5.71532\n",
      "Epoch 162/2000, Training Loss: 39.45872, Validation Loss: 5.71368\n",
      "Epoch 163/2000, Training Loss: 39.45039, Validation Loss: 5.71205\n",
      "Epoch 164/2000, Training Loss: 39.44207, Validation Loss: 5.71042\n",
      "Epoch 165/2000, Training Loss: 39.43377, Validation Loss: 5.70879\n",
      "Epoch 166/2000, Training Loss: 39.42550, Validation Loss: 5.70716\n",
      "Epoch 167/2000, Training Loss: 39.41724, Validation Loss: 5.70554\n",
      "Epoch 168/2000, Training Loss: 39.40901, Validation Loss: 5.70392\n",
      "Epoch 169/2000, Training Loss: 39.40079, Validation Loss: 5.70230\n",
      "Epoch 170/2000, Training Loss: 39.39260, Validation Loss: 5.70069\n",
      "Epoch 171/2000, Training Loss: 39.38443, Validation Loss: 5.69908\n",
      "Epoch 172/2000, Training Loss: 39.37625, Validation Loss: 5.69746\n",
      "Epoch 173/2000, Training Loss: 39.36808, Validation Loss: 5.69586\n",
      "Epoch 174/2000, Training Loss: 39.35993, Validation Loss: 5.69425\n",
      "Epoch 175/2000, Training Loss: 39.35180, Validation Loss: 5.69264\n",
      "Epoch 176/2000, Training Loss: 39.34369, Validation Loss: 5.69103\n",
      "Epoch 177/2000, Training Loss: 39.33560, Validation Loss: 5.68943\n",
      "Epoch 178/2000, Training Loss: 39.32751, Validation Loss: 5.68782\n",
      "Epoch 179/2000, Training Loss: 39.31944, Validation Loss: 5.68622\n",
      "Epoch 180/2000, Training Loss: 39.31139, Validation Loss: 5.68462\n",
      "Epoch 181/2000, Training Loss: 39.30335, Validation Loss: 5.68302\n",
      "Epoch 182/2000, Training Loss: 39.29533, Validation Loss: 5.68142\n",
      "Epoch 183/2000, Training Loss: 39.28731, Validation Loss: 5.67982\n",
      "Epoch 184/2000, Training Loss: 39.27932, Validation Loss: 5.67823\n",
      "Epoch 185/2000, Training Loss: 39.27133, Validation Loss: 5.67663\n",
      "Epoch 186/2000, Training Loss: 39.26335, Validation Loss: 5.67504\n",
      "Epoch 187/2000, Training Loss: 39.25539, Validation Loss: 5.67345\n",
      "Epoch 188/2000, Training Loss: 39.24743, Validation Loss: 5.67186\n",
      "Epoch 189/2000, Training Loss: 39.23949, Validation Loss: 5.67028\n",
      "Epoch 190/2000, Training Loss: 39.23155, Validation Loss: 5.66869\n",
      "Epoch 191/2000, Training Loss: 39.22362, Validation Loss: 5.66711\n",
      "Epoch 192/2000, Training Loss: 39.21571, Validation Loss: 5.66553\n",
      "Epoch 193/2000, Training Loss: 39.20781, Validation Loss: 5.66395\n",
      "Epoch 194/2000, Training Loss: 39.19993, Validation Loss: 5.66237\n",
      "Epoch 195/2000, Training Loss: 39.19208, Validation Loss: 5.66080\n",
      "Epoch 196/2000, Training Loss: 39.18423, Validation Loss: 5.65923\n",
      "Epoch 197/2000, Training Loss: 39.17639, Validation Loss: 5.65766\n",
      "Epoch 198/2000, Training Loss: 39.16855, Validation Loss: 5.65609\n",
      "Epoch 199/2000, Training Loss: 39.16073, Validation Loss: 5.65452\n",
      "Epoch 200/2000, Training Loss: 39.15291, Validation Loss: 5.65296\n",
      "Epoch 201/2000, Training Loss: 39.14512, Validation Loss: 5.65139\n",
      "Epoch 202/2000, Training Loss: 39.13734, Validation Loss: 5.64983\n",
      "Epoch 203/2000, Training Loss: 39.12958, Validation Loss: 5.64828\n",
      "Epoch 204/2000, Training Loss: 39.12182, Validation Loss: 5.64672\n",
      "Epoch 205/2000, Training Loss: 39.11408, Validation Loss: 5.64517\n",
      "Epoch 206/2000, Training Loss: 39.10635, Validation Loss: 5.64361\n",
      "Epoch 207/2000, Training Loss: 39.09864, Validation Loss: 5.64206\n",
      "Epoch 208/2000, Training Loss: 39.09094, Validation Loss: 5.64051\n",
      "Epoch 209/2000, Training Loss: 39.08325, Validation Loss: 5.63897\n",
      "Epoch 210/2000, Training Loss: 39.07557, Validation Loss: 5.63742\n",
      "Epoch 211/2000, Training Loss: 39.06791, Validation Loss: 5.63588\n",
      "Epoch 212/2000, Training Loss: 39.06025, Validation Loss: 5.63433\n",
      "Epoch 213/2000, Training Loss: 39.05261, Validation Loss: 5.63279\n",
      "Epoch 214/2000, Training Loss: 39.04499, Validation Loss: 5.63126\n",
      "Epoch 215/2000, Training Loss: 39.03739, Validation Loss: 5.62972\n",
      "Epoch 216/2000, Training Loss: 39.02980, Validation Loss: 5.62819\n",
      "Epoch 217/2000, Training Loss: 39.02223, Validation Loss: 5.62666\n",
      "Epoch 218/2000, Training Loss: 39.01467, Validation Loss: 5.62513\n",
      "Epoch 219/2000, Training Loss: 39.00712, Validation Loss: 5.62360\n",
      "Epoch 220/2000, Training Loss: 38.99958, Validation Loss: 5.62207\n",
      "Epoch 221/2000, Training Loss: 38.99205, Validation Loss: 5.62054\n",
      "Epoch 222/2000, Training Loss: 38.98452, Validation Loss: 5.61901\n",
      "Epoch 223/2000, Training Loss: 38.97700, Validation Loss: 5.61749\n",
      "Epoch 224/2000, Training Loss: 38.96949, Validation Loss: 5.61596\n",
      "Epoch 225/2000, Training Loss: 38.96200, Validation Loss: 5.61444\n",
      "Epoch 226/2000, Training Loss: 38.95452, Validation Loss: 5.61291\n",
      "Epoch 227/2000, Training Loss: 38.94706, Validation Loss: 5.61139\n",
      "Epoch 228/2000, Training Loss: 38.93961, Validation Loss: 5.60987\n",
      "Epoch 229/2000, Training Loss: 38.93215, Validation Loss: 5.60835\n",
      "Epoch 230/2000, Training Loss: 38.92471, Validation Loss: 5.60683\n",
      "Epoch 231/2000, Training Loss: 38.91727, Validation Loss: 5.60531\n",
      "Epoch 232/2000, Training Loss: 38.90984, Validation Loss: 5.60379\n",
      "Epoch 233/2000, Training Loss: 38.90242, Validation Loss: 5.60228\n",
      "Epoch 234/2000, Training Loss: 38.89501, Validation Loss: 5.60076\n",
      "Epoch 235/2000, Training Loss: 38.88761, Validation Loss: 5.59925\n",
      "Epoch 236/2000, Training Loss: 38.88022, Validation Loss: 5.59774\n",
      "Epoch 237/2000, Training Loss: 38.87283, Validation Loss: 5.59623\n",
      "Epoch 238/2000, Training Loss: 38.86546, Validation Loss: 5.59472\n",
      "Epoch 239/2000, Training Loss: 38.85810, Validation Loss: 5.59321\n",
      "Epoch 240/2000, Training Loss: 38.85075, Validation Loss: 5.59170\n",
      "Epoch 241/2000, Training Loss: 38.84341, Validation Loss: 5.59018\n",
      "Epoch 242/2000, Training Loss: 38.83608, Validation Loss: 5.58867\n",
      "Epoch 243/2000, Training Loss: 38.82876, Validation Loss: 5.58716\n",
      "Epoch 244/2000, Training Loss: 38.82145, Validation Loss: 5.58565\n",
      "Epoch 245/2000, Training Loss: 38.81414, Validation Loss: 5.58414\n",
      "Epoch 246/2000, Training Loss: 38.80684, Validation Loss: 5.58263\n",
      "Epoch 247/2000, Training Loss: 38.79954, Validation Loss: 5.58112\n",
      "Epoch 248/2000, Training Loss: 38.79225, Validation Loss: 5.57961\n",
      "Epoch 249/2000, Training Loss: 38.78497, Validation Loss: 5.57811\n",
      "Epoch 250/2000, Training Loss: 38.77769, Validation Loss: 5.57660\n",
      "Epoch 251/2000, Training Loss: 38.77042, Validation Loss: 5.57509\n",
      "Epoch 252/2000, Training Loss: 38.76317, Validation Loss: 5.57359\n",
      "Epoch 253/2000, Training Loss: 38.75593, Validation Loss: 5.57209\n",
      "Epoch 254/2000, Training Loss: 38.74870, Validation Loss: 5.57059\n",
      "Epoch 255/2000, Training Loss: 38.74150, Validation Loss: 5.56910\n",
      "Epoch 256/2000, Training Loss: 38.73431, Validation Loss: 5.56761\n",
      "Epoch 257/2000, Training Loss: 38.72712, Validation Loss: 5.56612\n",
      "Epoch 258/2000, Training Loss: 38.71995, Validation Loss: 5.56463\n",
      "Epoch 259/2000, Training Loss: 38.71278, Validation Loss: 5.56314\n",
      "Epoch 260/2000, Training Loss: 38.70562, Validation Loss: 5.56165\n",
      "Epoch 261/2000, Training Loss: 38.69846, Validation Loss: 5.56016\n",
      "Epoch 262/2000, Training Loss: 38.69132, Validation Loss: 5.55867\n",
      "Epoch 263/2000, Training Loss: 38.68417, Validation Loss: 5.55718\n",
      "Epoch 264/2000, Training Loss: 38.67703, Validation Loss: 5.55570\n",
      "Epoch 265/2000, Training Loss: 38.66989, Validation Loss: 5.55421\n",
      "Epoch 266/2000, Training Loss: 38.66276, Validation Loss: 5.55273\n",
      "Epoch 267/2000, Training Loss: 38.65563, Validation Loss: 5.55125\n",
      "Epoch 268/2000, Training Loss: 38.64851, Validation Loss: 5.54976\n",
      "Epoch 269/2000, Training Loss: 38.64140, Validation Loss: 5.54828\n",
      "Epoch 270/2000, Training Loss: 38.63429, Validation Loss: 5.54680\n",
      "Epoch 271/2000, Training Loss: 38.62718, Validation Loss: 5.54532\n",
      "Epoch 272/2000, Training Loss: 38.62009, Validation Loss: 5.54385\n",
      "Epoch 273/2000, Training Loss: 38.61301, Validation Loss: 5.54237\n",
      "Epoch 274/2000, Training Loss: 38.60594, Validation Loss: 5.54089\n",
      "Epoch 275/2000, Training Loss: 38.59888, Validation Loss: 5.53941\n",
      "Epoch 276/2000, Training Loss: 38.59183, Validation Loss: 5.53794\n",
      "Epoch 277/2000, Training Loss: 38.58478, Validation Loss: 5.53646\n",
      "Epoch 278/2000, Training Loss: 38.57774, Validation Loss: 5.53499\n",
      "Epoch 279/2000, Training Loss: 38.57071, Validation Loss: 5.53352\n",
      "Epoch 280/2000, Training Loss: 38.56368, Validation Loss: 5.53205\n",
      "Epoch 281/2000, Training Loss: 38.55666, Validation Loss: 5.53057\n",
      "Epoch 282/2000, Training Loss: 38.54964, Validation Loss: 5.52910\n",
      "Epoch 283/2000, Training Loss: 38.54263, Validation Loss: 5.52763\n",
      "Epoch 284/2000, Training Loss: 38.53562, Validation Loss: 5.52616\n",
      "Epoch 285/2000, Training Loss: 38.52861, Validation Loss: 5.52469\n",
      "Epoch 286/2000, Training Loss: 38.52160, Validation Loss: 5.52321\n",
      "Epoch 287/2000, Training Loss: 38.51459, Validation Loss: 5.52174\n",
      "Epoch 288/2000, Training Loss: 38.50759, Validation Loss: 5.52026\n",
      "Epoch 289/2000, Training Loss: 38.50058, Validation Loss: 5.51879\n",
      "Epoch 290/2000, Training Loss: 38.49358, Validation Loss: 5.51731\n",
      "Epoch 291/2000, Training Loss: 38.48658, Validation Loss: 5.51584\n",
      "Epoch 292/2000, Training Loss: 38.47959, Validation Loss: 5.51437\n",
      "Epoch 293/2000, Training Loss: 38.47261, Validation Loss: 5.51290\n",
      "Epoch 294/2000, Training Loss: 38.46563, Validation Loss: 5.51143\n",
      "Epoch 295/2000, Training Loss: 38.45866, Validation Loss: 5.50995\n",
      "Epoch 296/2000, Training Loss: 38.45169, Validation Loss: 5.50848\n",
      "Epoch 297/2000, Training Loss: 38.44473, Validation Loss: 5.50701\n",
      "Epoch 298/2000, Training Loss: 38.43777, Validation Loss: 5.50554\n",
      "Epoch 299/2000, Training Loss: 38.43082, Validation Loss: 5.50407\n",
      "Epoch 300/2000, Training Loss: 38.42387, Validation Loss: 5.50260\n",
      "Epoch 301/2000, Training Loss: 38.41693, Validation Loss: 5.50113\n",
      "Epoch 302/2000, Training Loss: 38.41000, Validation Loss: 5.49966\n",
      "Epoch 303/2000, Training Loss: 38.40307, Validation Loss: 5.49819\n",
      "Epoch 304/2000, Training Loss: 38.39614, Validation Loss: 5.49673\n",
      "Epoch 305/2000, Training Loss: 38.38923, Validation Loss: 5.49526\n",
      "Epoch 306/2000, Training Loss: 38.38232, Validation Loss: 5.49379\n",
      "Epoch 307/2000, Training Loss: 38.37542, Validation Loss: 5.49233\n",
      "Epoch 308/2000, Training Loss: 38.36851, Validation Loss: 5.49086\n",
      "Epoch 309/2000, Training Loss: 38.36161, Validation Loss: 5.48940\n",
      "Epoch 310/2000, Training Loss: 38.35471, Validation Loss: 5.48794\n",
      "Epoch 311/2000, Training Loss: 38.34782, Validation Loss: 5.48647\n",
      "Epoch 312/2000, Training Loss: 38.34093, Validation Loss: 5.48501\n",
      "Epoch 313/2000, Training Loss: 38.33404, Validation Loss: 5.48356\n",
      "Epoch 314/2000, Training Loss: 38.32716, Validation Loss: 5.48210\n",
      "Epoch 315/2000, Training Loss: 38.32028, Validation Loss: 5.48064\n",
      "Epoch 316/2000, Training Loss: 38.31340, Validation Loss: 5.47918\n",
      "Epoch 317/2000, Training Loss: 38.30652, Validation Loss: 5.47772\n",
      "Epoch 318/2000, Training Loss: 38.29963, Validation Loss: 5.47625\n",
      "Epoch 319/2000, Training Loss: 38.29274, Validation Loss: 5.47479\n",
      "Epoch 320/2000, Training Loss: 38.28585, Validation Loss: 5.47333\n",
      "Epoch 321/2000, Training Loss: 38.27897, Validation Loss: 5.47186\n",
      "Epoch 322/2000, Training Loss: 38.27210, Validation Loss: 5.47040\n",
      "Epoch 323/2000, Training Loss: 38.26522, Validation Loss: 5.46894\n",
      "Epoch 324/2000, Training Loss: 38.25835, Validation Loss: 5.46748\n",
      "Epoch 325/2000, Training Loss: 38.25148, Validation Loss: 5.46602\n",
      "Epoch 326/2000, Training Loss: 38.24461, Validation Loss: 5.46456\n",
      "Epoch 327/2000, Training Loss: 38.23775, Validation Loss: 5.46310\n",
      "Epoch 328/2000, Training Loss: 38.23090, Validation Loss: 5.46164\n",
      "Epoch 329/2000, Training Loss: 38.22405, Validation Loss: 5.46018\n",
      "Epoch 330/2000, Training Loss: 38.21720, Validation Loss: 5.45871\n",
      "Epoch 331/2000, Training Loss: 38.21035, Validation Loss: 5.45725\n",
      "Epoch 332/2000, Training Loss: 38.20350, Validation Loss: 5.45579\n",
      "Epoch 333/2000, Training Loss: 38.19666, Validation Loss: 5.45433\n",
      "Epoch 334/2000, Training Loss: 38.18982, Validation Loss: 5.45286\n",
      "Epoch 335/2000, Training Loss: 38.18298, Validation Loss: 5.45140\n",
      "Epoch 336/2000, Training Loss: 38.17614, Validation Loss: 5.44994\n",
      "Epoch 337/2000, Training Loss: 38.16929, Validation Loss: 5.44848\n",
      "Epoch 338/2000, Training Loss: 38.16245, Validation Loss: 5.44703\n",
      "Epoch 339/2000, Training Loss: 38.15562, Validation Loss: 5.44557\n",
      "Epoch 340/2000, Training Loss: 38.14878, Validation Loss: 5.44411\n",
      "Epoch 341/2000, Training Loss: 38.14194, Validation Loss: 5.44265\n",
      "Epoch 342/2000, Training Loss: 38.13511, Validation Loss: 5.44119\n",
      "Epoch 343/2000, Training Loss: 38.12829, Validation Loss: 5.43974\n",
      "Epoch 344/2000, Training Loss: 38.12146, Validation Loss: 5.43828\n",
      "Epoch 345/2000, Training Loss: 38.11463, Validation Loss: 5.43682\n",
      "Epoch 346/2000, Training Loss: 38.10780, Validation Loss: 5.43536\n",
      "Epoch 347/2000, Training Loss: 38.10097, Validation Loss: 5.43391\n",
      "Epoch 348/2000, Training Loss: 38.09415, Validation Loss: 5.43245\n",
      "Epoch 349/2000, Training Loss: 38.08733, Validation Loss: 5.43100\n",
      "Epoch 350/2000, Training Loss: 38.08051, Validation Loss: 5.42955\n",
      "Epoch 351/2000, Training Loss: 38.07368, Validation Loss: 5.42810\n",
      "Epoch 352/2000, Training Loss: 38.06686, Validation Loss: 5.42665\n",
      "Epoch 353/2000, Training Loss: 38.06003, Validation Loss: 5.42519\n",
      "Epoch 354/2000, Training Loss: 38.05321, Validation Loss: 5.42374\n",
      "Epoch 355/2000, Training Loss: 38.04640, Validation Loss: 5.42229\n",
      "Epoch 356/2000, Training Loss: 38.03958, Validation Loss: 5.42083\n",
      "Epoch 357/2000, Training Loss: 38.03276, Validation Loss: 5.41938\n",
      "Epoch 358/2000, Training Loss: 38.02595, Validation Loss: 5.41792\n",
      "Epoch 359/2000, Training Loss: 38.01914, Validation Loss: 5.41647\n",
      "Epoch 360/2000, Training Loss: 38.01234, Validation Loss: 5.41501\n",
      "Epoch 361/2000, Training Loss: 38.00554, Validation Loss: 5.41355\n",
      "Epoch 362/2000, Training Loss: 37.99874, Validation Loss: 5.41210\n",
      "Epoch 363/2000, Training Loss: 37.99195, Validation Loss: 5.41064\n",
      "Epoch 364/2000, Training Loss: 37.98517, Validation Loss: 5.40919\n",
      "Epoch 365/2000, Training Loss: 37.97840, Validation Loss: 5.40774\n",
      "Epoch 366/2000, Training Loss: 37.97162, Validation Loss: 5.40628\n",
      "Epoch 367/2000, Training Loss: 37.96484, Validation Loss: 5.40483\n",
      "Epoch 368/2000, Training Loss: 37.95806, Validation Loss: 5.40338\n",
      "Epoch 369/2000, Training Loss: 37.95128, Validation Loss: 5.40192\n",
      "Epoch 370/2000, Training Loss: 37.94449, Validation Loss: 5.40047\n",
      "Epoch 371/2000, Training Loss: 37.93770, Validation Loss: 5.39901\n",
      "Epoch 372/2000, Training Loss: 37.93091, Validation Loss: 5.39756\n",
      "Epoch 373/2000, Training Loss: 37.92412, Validation Loss: 5.39611\n",
      "Epoch 374/2000, Training Loss: 37.91734, Validation Loss: 5.39465\n",
      "Epoch 375/2000, Training Loss: 37.91055, Validation Loss: 5.39319\n",
      "Epoch 376/2000, Training Loss: 37.90377, Validation Loss: 5.39174\n",
      "Epoch 377/2000, Training Loss: 37.89699, Validation Loss: 5.39028\n",
      "Epoch 378/2000, Training Loss: 37.89021, Validation Loss: 5.38883\n",
      "Epoch 379/2000, Training Loss: 37.88342, Validation Loss: 5.38737\n",
      "Epoch 380/2000, Training Loss: 37.87664, Validation Loss: 5.38592\n",
      "Epoch 381/2000, Training Loss: 37.86985, Validation Loss: 5.38446\n",
      "Epoch 382/2000, Training Loss: 37.86306, Validation Loss: 5.38300\n",
      "Epoch 383/2000, Training Loss: 37.85627, Validation Loss: 5.38155\n",
      "Epoch 384/2000, Training Loss: 37.84948, Validation Loss: 5.38009\n",
      "Epoch 385/2000, Training Loss: 37.84269, Validation Loss: 5.37864\n",
      "Epoch 386/2000, Training Loss: 37.83590, Validation Loss: 5.37718\n",
      "Epoch 387/2000, Training Loss: 37.82911, Validation Loss: 5.37573\n",
      "Epoch 388/2000, Training Loss: 37.82232, Validation Loss: 5.37428\n",
      "Epoch 389/2000, Training Loss: 37.81553, Validation Loss: 5.37282\n",
      "Epoch 390/2000, Training Loss: 37.80875, Validation Loss: 5.37137\n",
      "Epoch 391/2000, Training Loss: 37.80197, Validation Loss: 5.36992\n",
      "Epoch 392/2000, Training Loss: 37.79518, Validation Loss: 5.36847\n",
      "Epoch 393/2000, Training Loss: 37.78839, Validation Loss: 5.36702\n",
      "Epoch 394/2000, Training Loss: 37.78160, Validation Loss: 5.36556\n",
      "Epoch 395/2000, Training Loss: 37.77480, Validation Loss: 5.36411\n",
      "Epoch 396/2000, Training Loss: 37.76800, Validation Loss: 5.36265\n",
      "Epoch 397/2000, Training Loss: 37.76121, Validation Loss: 5.36120\n",
      "Epoch 398/2000, Training Loss: 37.75441, Validation Loss: 5.35975\n",
      "Epoch 399/2000, Training Loss: 37.74762, Validation Loss: 5.35830\n",
      "Epoch 400/2000, Training Loss: 37.74082, Validation Loss: 5.35685\n",
      "Epoch 401/2000, Training Loss: 37.73404, Validation Loss: 5.35541\n",
      "Epoch 402/2000, Training Loss: 37.72726, Validation Loss: 5.35396\n",
      "Epoch 403/2000, Training Loss: 37.72047, Validation Loss: 5.35251\n",
      "Epoch 404/2000, Training Loss: 37.71369, Validation Loss: 5.35105\n",
      "Epoch 405/2000, Training Loss: 37.70690, Validation Loss: 5.34960\n",
      "Epoch 406/2000, Training Loss: 37.70013, Validation Loss: 5.34815\n",
      "Epoch 407/2000, Training Loss: 37.69335, Validation Loss: 5.34670\n",
      "Epoch 408/2000, Training Loss: 37.68658, Validation Loss: 5.34524\n",
      "Epoch 409/2000, Training Loss: 37.67980, Validation Loss: 5.34379\n",
      "Epoch 410/2000, Training Loss: 37.67303, Validation Loss: 5.34233\n",
      "Epoch 411/2000, Training Loss: 37.66625, Validation Loss: 5.34087\n",
      "Epoch 412/2000, Training Loss: 37.65948, Validation Loss: 5.33941\n",
      "Epoch 413/2000, Training Loss: 37.65271, Validation Loss: 5.33796\n",
      "Epoch 414/2000, Training Loss: 37.64595, Validation Loss: 5.33650\n",
      "Epoch 415/2000, Training Loss: 37.63919, Validation Loss: 5.33505\n",
      "Epoch 416/2000, Training Loss: 37.63242, Validation Loss: 5.33359\n",
      "Epoch 417/2000, Training Loss: 37.62566, Validation Loss: 5.33214\n",
      "Epoch 418/2000, Training Loss: 37.61889, Validation Loss: 5.33068\n",
      "Epoch 419/2000, Training Loss: 37.61212, Validation Loss: 5.32923\n",
      "Epoch 420/2000, Training Loss: 37.60536, Validation Loss: 5.32777\n",
      "Epoch 421/2000, Training Loss: 37.59860, Validation Loss: 5.32632\n",
      "Epoch 422/2000, Training Loss: 37.59183, Validation Loss: 5.32486\n",
      "Epoch 423/2000, Training Loss: 37.58507, Validation Loss: 5.32341\n",
      "Epoch 424/2000, Training Loss: 37.57830, Validation Loss: 5.32195\n",
      "Epoch 425/2000, Training Loss: 37.57153, Validation Loss: 5.32050\n",
      "Epoch 426/2000, Training Loss: 37.56477, Validation Loss: 5.31904\n",
      "Epoch 427/2000, Training Loss: 37.55800, Validation Loss: 5.31759\n",
      "Epoch 428/2000, Training Loss: 37.55124, Validation Loss: 5.31614\n",
      "Epoch 429/2000, Training Loss: 37.54448, Validation Loss: 5.31469\n",
      "Epoch 430/2000, Training Loss: 37.53772, Validation Loss: 5.31324\n",
      "Epoch 431/2000, Training Loss: 37.53096, Validation Loss: 5.31179\n",
      "Epoch 432/2000, Training Loss: 37.52419, Validation Loss: 5.31033\n",
      "Epoch 433/2000, Training Loss: 37.51743, Validation Loss: 5.30888\n",
      "Epoch 434/2000, Training Loss: 37.51066, Validation Loss: 5.30743\n",
      "Epoch 435/2000, Training Loss: 37.50390, Validation Loss: 5.30598\n",
      "Epoch 436/2000, Training Loss: 37.49714, Validation Loss: 5.30453\n",
      "Epoch 437/2000, Training Loss: 37.49039, Validation Loss: 5.30308\n",
      "Epoch 438/2000, Training Loss: 37.48363, Validation Loss: 5.30163\n",
      "Epoch 439/2000, Training Loss: 37.47687, Validation Loss: 5.30018\n",
      "Epoch 440/2000, Training Loss: 37.47012, Validation Loss: 5.29873\n",
      "Epoch 441/2000, Training Loss: 37.46336, Validation Loss: 5.29728\n",
      "Epoch 442/2000, Training Loss: 37.45660, Validation Loss: 5.29583\n",
      "Epoch 443/2000, Training Loss: 37.44984, Validation Loss: 5.29437\n",
      "Epoch 444/2000, Training Loss: 37.44307, Validation Loss: 5.29292\n",
      "Epoch 445/2000, Training Loss: 37.43631, Validation Loss: 5.29146\n",
      "Epoch 446/2000, Training Loss: 37.42954, Validation Loss: 5.29001\n",
      "Epoch 447/2000, Training Loss: 37.42278, Validation Loss: 5.28855\n",
      "Epoch 448/2000, Training Loss: 37.41602, Validation Loss: 5.28710\n",
      "Epoch 449/2000, Training Loss: 37.40926, Validation Loss: 5.28565\n",
      "Epoch 450/2000, Training Loss: 37.40250, Validation Loss: 5.28420\n",
      "Epoch 451/2000, Training Loss: 37.39574, Validation Loss: 5.28275\n",
      "Epoch 452/2000, Training Loss: 37.38899, Validation Loss: 5.28130\n",
      "Epoch 453/2000, Training Loss: 37.38223, Validation Loss: 5.27984\n",
      "Epoch 454/2000, Training Loss: 37.37547, Validation Loss: 5.27839\n",
      "Epoch 455/2000, Training Loss: 37.36871, Validation Loss: 5.27694\n",
      "Epoch 456/2000, Training Loss: 37.36196, Validation Loss: 5.27549\n",
      "Epoch 457/2000, Training Loss: 37.35521, Validation Loss: 5.27404\n",
      "Epoch 458/2000, Training Loss: 37.34847, Validation Loss: 5.27258\n",
      "Epoch 459/2000, Training Loss: 37.34172, Validation Loss: 5.27113\n",
      "Epoch 460/2000, Training Loss: 37.33496, Validation Loss: 5.26968\n",
      "Epoch 461/2000, Training Loss: 37.32820, Validation Loss: 5.26822\n",
      "Epoch 462/2000, Training Loss: 37.32144, Validation Loss: 5.26677\n",
      "Epoch 463/2000, Training Loss: 37.31468, Validation Loss: 5.26531\n",
      "Epoch 464/2000, Training Loss: 37.30792, Validation Loss: 5.26386\n",
      "Epoch 465/2000, Training Loss: 37.30116, Validation Loss: 5.26240\n",
      "Epoch 466/2000, Training Loss: 37.29439, Validation Loss: 5.26095\n",
      "Epoch 467/2000, Training Loss: 37.28762, Validation Loss: 5.25949\n",
      "Epoch 468/2000, Training Loss: 37.28085, Validation Loss: 5.25803\n",
      "Epoch 469/2000, Training Loss: 37.27408, Validation Loss: 5.25657\n",
      "Epoch 470/2000, Training Loss: 37.26731, Validation Loss: 5.25511\n",
      "Epoch 471/2000, Training Loss: 37.26054, Validation Loss: 5.25365\n",
      "Epoch 472/2000, Training Loss: 37.25376, Validation Loss: 5.25219\n",
      "Epoch 473/2000, Training Loss: 37.24698, Validation Loss: 5.25073\n",
      "Epoch 474/2000, Training Loss: 37.24020, Validation Loss: 5.24927\n",
      "Epoch 475/2000, Training Loss: 37.23342, Validation Loss: 5.24780\n",
      "Epoch 476/2000, Training Loss: 37.22663, Validation Loss: 5.24634\n",
      "Epoch 477/2000, Training Loss: 37.21983, Validation Loss: 5.24488\n",
      "Epoch 478/2000, Training Loss: 37.21303, Validation Loss: 5.24342\n",
      "Epoch 479/2000, Training Loss: 37.20624, Validation Loss: 5.24195\n",
      "Epoch 480/2000, Training Loss: 37.19944, Validation Loss: 5.24049\n",
      "Epoch 481/2000, Training Loss: 37.19263, Validation Loss: 5.23903\n",
      "Epoch 482/2000, Training Loss: 37.18582, Validation Loss: 5.23756\n",
      "Epoch 483/2000, Training Loss: 37.17901, Validation Loss: 5.23609\n",
      "Epoch 484/2000, Training Loss: 37.17220, Validation Loss: 5.23463\n",
      "Epoch 485/2000, Training Loss: 37.16539, Validation Loss: 5.23316\n",
      "Epoch 486/2000, Training Loss: 37.15858, Validation Loss: 5.23169\n",
      "Epoch 487/2000, Training Loss: 37.15178, Validation Loss: 5.23022\n",
      "Epoch 488/2000, Training Loss: 37.14499, Validation Loss: 5.22876\n",
      "Epoch 489/2000, Training Loss: 37.13818, Validation Loss: 5.22729\n",
      "Epoch 490/2000, Training Loss: 37.13138, Validation Loss: 5.22582\n",
      "Epoch 491/2000, Training Loss: 37.12457, Validation Loss: 5.22435\n",
      "Epoch 492/2000, Training Loss: 37.11776, Validation Loss: 5.22289\n",
      "Epoch 493/2000, Training Loss: 37.11095, Validation Loss: 5.22142\n",
      "Epoch 494/2000, Training Loss: 37.10414, Validation Loss: 5.21995\n",
      "Epoch 495/2000, Training Loss: 37.09732, Validation Loss: 5.21848\n",
      "Epoch 496/2000, Training Loss: 37.09049, Validation Loss: 5.21701\n",
      "Epoch 497/2000, Training Loss: 37.08366, Validation Loss: 5.21554\n",
      "Epoch 498/2000, Training Loss: 37.07683, Validation Loss: 5.21407\n",
      "Epoch 499/2000, Training Loss: 37.06999, Validation Loss: 5.21259\n",
      "Epoch 500/2000, Training Loss: 37.06316, Validation Loss: 5.21112\n",
      "Epoch 501/2000, Training Loss: 37.05632, Validation Loss: 5.20965\n",
      "Epoch 502/2000, Training Loss: 37.04949, Validation Loss: 5.20817\n",
      "Epoch 503/2000, Training Loss: 37.04265, Validation Loss: 5.20670\n",
      "Epoch 504/2000, Training Loss: 37.03582, Validation Loss: 5.20522\n",
      "Epoch 505/2000, Training Loss: 37.02898, Validation Loss: 5.20375\n",
      "Epoch 506/2000, Training Loss: 37.02214, Validation Loss: 5.20227\n",
      "Epoch 507/2000, Training Loss: 37.01530, Validation Loss: 5.20080\n",
      "Epoch 508/2000, Training Loss: 37.00845, Validation Loss: 5.19932\n",
      "Epoch 509/2000, Training Loss: 37.00159, Validation Loss: 5.19784\n",
      "Epoch 510/2000, Training Loss: 36.99474, Validation Loss: 5.19637\n",
      "Epoch 511/2000, Training Loss: 36.98788, Validation Loss: 5.19489\n",
      "Epoch 512/2000, Training Loss: 36.98102, Validation Loss: 5.19341\n",
      "Epoch 513/2000, Training Loss: 36.97415, Validation Loss: 5.19193\n",
      "Epoch 514/2000, Training Loss: 36.96729, Validation Loss: 5.19045\n",
      "Epoch 515/2000, Training Loss: 36.96042, Validation Loss: 5.18897\n",
      "Epoch 516/2000, Training Loss: 36.95354, Validation Loss: 5.18749\n",
      "Epoch 517/2000, Training Loss: 36.94666, Validation Loss: 5.18601\n",
      "Epoch 518/2000, Training Loss: 36.93978, Validation Loss: 5.18453\n",
      "Epoch 519/2000, Training Loss: 36.93289, Validation Loss: 5.18305\n",
      "Epoch 520/2000, Training Loss: 36.92600, Validation Loss: 5.18157\n",
      "Epoch 521/2000, Training Loss: 36.91911, Validation Loss: 5.18008\n",
      "Epoch 522/2000, Training Loss: 36.91222, Validation Loss: 5.17860\n",
      "Epoch 523/2000, Training Loss: 36.90532, Validation Loss: 5.17712\n",
      "Epoch 524/2000, Training Loss: 36.89842, Validation Loss: 5.17564\n",
      "Epoch 525/2000, Training Loss: 36.89152, Validation Loss: 5.17415\n",
      "Epoch 526/2000, Training Loss: 36.88462, Validation Loss: 5.17267\n",
      "Epoch 527/2000, Training Loss: 36.87772, Validation Loss: 5.17119\n",
      "Epoch 528/2000, Training Loss: 36.87081, Validation Loss: 5.16970\n",
      "Epoch 529/2000, Training Loss: 36.86390, Validation Loss: 5.16822\n",
      "Epoch 530/2000, Training Loss: 36.85698, Validation Loss: 5.16674\n",
      "Epoch 531/2000, Training Loss: 36.85006, Validation Loss: 5.16525\n",
      "Epoch 532/2000, Training Loss: 36.84314, Validation Loss: 5.16376\n",
      "Epoch 533/2000, Training Loss: 36.83622, Validation Loss: 5.16228\n",
      "Epoch 534/2000, Training Loss: 36.82929, Validation Loss: 5.16079\n",
      "Epoch 535/2000, Training Loss: 36.82237, Validation Loss: 5.15930\n",
      "Epoch 536/2000, Training Loss: 36.81544, Validation Loss: 5.15782\n",
      "Epoch 537/2000, Training Loss: 36.80851, Validation Loss: 5.15633\n",
      "Epoch 538/2000, Training Loss: 36.80158, Validation Loss: 5.15484\n",
      "Epoch 539/2000, Training Loss: 36.79464, Validation Loss: 5.15335\n",
      "Epoch 540/2000, Training Loss: 36.78769, Validation Loss: 5.15186\n",
      "Epoch 541/2000, Training Loss: 36.78075, Validation Loss: 5.15037\n",
      "Epoch 542/2000, Training Loss: 36.77380, Validation Loss: 5.14888\n",
      "Epoch 543/2000, Training Loss: 36.76684, Validation Loss: 5.14740\n",
      "Epoch 544/2000, Training Loss: 36.75989, Validation Loss: 5.14591\n",
      "Epoch 545/2000, Training Loss: 36.75293, Validation Loss: 5.14442\n",
      "Epoch 546/2000, Training Loss: 36.74597, Validation Loss: 5.14293\n",
      "Epoch 547/2000, Training Loss: 36.73900, Validation Loss: 5.14144\n",
      "Epoch 548/2000, Training Loss: 36.73202, Validation Loss: 5.13995\n",
      "Epoch 549/2000, Training Loss: 36.72505, Validation Loss: 5.13845\n",
      "Epoch 550/2000, Training Loss: 36.71807, Validation Loss: 5.13696\n",
      "Epoch 551/2000, Training Loss: 36.71108, Validation Loss: 5.13547\n",
      "Epoch 552/2000, Training Loss: 36.70409, Validation Loss: 5.13399\n",
      "Epoch 553/2000, Training Loss: 36.69710, Validation Loss: 5.13250\n",
      "Epoch 554/2000, Training Loss: 36.69010, Validation Loss: 5.13101\n",
      "Epoch 555/2000, Training Loss: 36.68310, Validation Loss: 5.12952\n",
      "Epoch 556/2000, Training Loss: 36.67610, Validation Loss: 5.12802\n",
      "Epoch 557/2000, Training Loss: 36.66909, Validation Loss: 5.12653\n",
      "Epoch 558/2000, Training Loss: 36.66209, Validation Loss: 5.12505\n",
      "Epoch 559/2000, Training Loss: 36.65509, Validation Loss: 5.12356\n",
      "Epoch 560/2000, Training Loss: 36.64809, Validation Loss: 5.12206\n",
      "Epoch 561/2000, Training Loss: 36.64109, Validation Loss: 5.12057\n",
      "Epoch 562/2000, Training Loss: 36.63408, Validation Loss: 5.11907\n",
      "Epoch 563/2000, Training Loss: 36.62707, Validation Loss: 5.11758\n",
      "Epoch 564/2000, Training Loss: 36.62006, Validation Loss: 5.11608\n",
      "Epoch 565/2000, Training Loss: 36.61304, Validation Loss: 5.11459\n",
      "Epoch 566/2000, Training Loss: 36.60603, Validation Loss: 5.11309\n",
      "Epoch 567/2000, Training Loss: 36.59900, Validation Loss: 5.11159\n",
      "Epoch 568/2000, Training Loss: 36.59196, Validation Loss: 5.11009\n",
      "Epoch 569/2000, Training Loss: 36.58493, Validation Loss: 5.10859\n",
      "Epoch 570/2000, Training Loss: 36.57789, Validation Loss: 5.10709\n",
      "Epoch 571/2000, Training Loss: 36.57084, Validation Loss: 5.10560\n",
      "Epoch 572/2000, Training Loss: 36.56379, Validation Loss: 5.10409\n",
      "Epoch 573/2000, Training Loss: 36.55673, Validation Loss: 5.10259\n",
      "Epoch 574/2000, Training Loss: 36.54967, Validation Loss: 5.10109\n",
      "Epoch 575/2000, Training Loss: 36.54261, Validation Loss: 5.09960\n",
      "Epoch 576/2000, Training Loss: 36.53554, Validation Loss: 5.09810\n",
      "Epoch 577/2000, Training Loss: 36.52848, Validation Loss: 5.09660\n",
      "Epoch 578/2000, Training Loss: 36.52141, Validation Loss: 5.09510\n",
      "Epoch 579/2000, Training Loss: 36.51435, Validation Loss: 5.09360\n",
      "Epoch 580/2000, Training Loss: 36.50728, Validation Loss: 5.09209\n",
      "Epoch 581/2000, Training Loss: 36.50021, Validation Loss: 5.09059\n",
      "Epoch 582/2000, Training Loss: 36.49315, Validation Loss: 5.08909\n",
      "Epoch 583/2000, Training Loss: 36.48609, Validation Loss: 5.08759\n",
      "Epoch 584/2000, Training Loss: 36.47902, Validation Loss: 5.08609\n",
      "Epoch 585/2000, Training Loss: 36.47194, Validation Loss: 5.08458\n",
      "Epoch 586/2000, Training Loss: 36.46487, Validation Loss: 5.08308\n",
      "Epoch 587/2000, Training Loss: 36.45779, Validation Loss: 5.08158\n",
      "Epoch 588/2000, Training Loss: 36.45071, Validation Loss: 5.08008\n",
      "Epoch 589/2000, Training Loss: 36.44363, Validation Loss: 5.07857\n",
      "Epoch 590/2000, Training Loss: 36.43655, Validation Loss: 5.07707\n",
      "Epoch 591/2000, Training Loss: 36.42945, Validation Loss: 5.07557\n",
      "Epoch 592/2000, Training Loss: 36.42236, Validation Loss: 5.07406\n",
      "Epoch 593/2000, Training Loss: 36.41526, Validation Loss: 5.07256\n",
      "Epoch 594/2000, Training Loss: 36.40816, Validation Loss: 5.07106\n",
      "Epoch 595/2000, Training Loss: 36.40106, Validation Loss: 5.06956\n",
      "Epoch 596/2000, Training Loss: 36.39396, Validation Loss: 5.06805\n",
      "Epoch 597/2000, Training Loss: 36.38686, Validation Loss: 5.06655\n",
      "Epoch 598/2000, Training Loss: 36.37975, Validation Loss: 5.06505\n",
      "Epoch 599/2000, Training Loss: 36.37265, Validation Loss: 5.06354\n",
      "Epoch 600/2000, Training Loss: 36.36555, Validation Loss: 5.06204\n",
      "Epoch 601/2000, Training Loss: 36.35846, Validation Loss: 5.06054\n",
      "Epoch 602/2000, Training Loss: 36.35135, Validation Loss: 5.05903\n",
      "Epoch 603/2000, Training Loss: 36.34425, Validation Loss: 5.05753\n",
      "Epoch 604/2000, Training Loss: 36.33713, Validation Loss: 5.05602\n",
      "Epoch 605/2000, Training Loss: 36.33001, Validation Loss: 5.05451\n",
      "Epoch 606/2000, Training Loss: 36.32289, Validation Loss: 5.05301\n",
      "Epoch 607/2000, Training Loss: 36.31577, Validation Loss: 5.05151\n",
      "Epoch 608/2000, Training Loss: 36.30866, Validation Loss: 5.05000\n",
      "Epoch 609/2000, Training Loss: 36.30155, Validation Loss: 5.04850\n",
      "Epoch 610/2000, Training Loss: 36.29444, Validation Loss: 5.04699\n",
      "Epoch 611/2000, Training Loss: 36.28732, Validation Loss: 5.04549\n",
      "Epoch 612/2000, Training Loss: 36.28020, Validation Loss: 5.04399\n",
      "Epoch 613/2000, Training Loss: 36.27308, Validation Loss: 5.04248\n",
      "Epoch 614/2000, Training Loss: 36.26595, Validation Loss: 5.04098\n",
      "Epoch 615/2000, Training Loss: 36.25883, Validation Loss: 5.03947\n",
      "Epoch 616/2000, Training Loss: 36.25171, Validation Loss: 5.03797\n",
      "Epoch 617/2000, Training Loss: 36.24459, Validation Loss: 5.03646\n",
      "Epoch 618/2000, Training Loss: 36.23746, Validation Loss: 5.03495\n",
      "Epoch 619/2000, Training Loss: 36.23033, Validation Loss: 5.03345\n",
      "Epoch 620/2000, Training Loss: 36.22320, Validation Loss: 5.03194\n",
      "Epoch 621/2000, Training Loss: 36.21606, Validation Loss: 5.03043\n",
      "Epoch 622/2000, Training Loss: 36.20891, Validation Loss: 5.02892\n",
      "Epoch 623/2000, Training Loss: 36.20176, Validation Loss: 5.02741\n",
      "Epoch 624/2000, Training Loss: 36.19461, Validation Loss: 5.02590\n",
      "Epoch 625/2000, Training Loss: 36.18747, Validation Loss: 5.02440\n",
      "Epoch 626/2000, Training Loss: 36.18031, Validation Loss: 5.02288\n",
      "Epoch 627/2000, Training Loss: 36.17315, Validation Loss: 5.02137\n",
      "Epoch 628/2000, Training Loss: 36.16599, Validation Loss: 5.01986\n",
      "Epoch 629/2000, Training Loss: 36.15882, Validation Loss: 5.01835\n",
      "Epoch 630/2000, Training Loss: 36.15164, Validation Loss: 5.01684\n",
      "Epoch 631/2000, Training Loss: 36.14445, Validation Loss: 5.01532\n",
      "Epoch 632/2000, Training Loss: 36.13726, Validation Loss: 5.01381\n",
      "Epoch 633/2000, Training Loss: 36.13006, Validation Loss: 5.01229\n",
      "Epoch 634/2000, Training Loss: 36.12287, Validation Loss: 5.01078\n",
      "Epoch 635/2000, Training Loss: 36.11567, Validation Loss: 5.00927\n",
      "Epoch 636/2000, Training Loss: 36.10847, Validation Loss: 5.00775\n",
      "Epoch 637/2000, Training Loss: 36.10126, Validation Loss: 5.00624\n",
      "Epoch 638/2000, Training Loss: 36.09406, Validation Loss: 5.00473\n",
      "Epoch 639/2000, Training Loss: 36.08685, Validation Loss: 5.00321\n",
      "Epoch 640/2000, Training Loss: 36.07963, Validation Loss: 5.00169\n",
      "Epoch 641/2000, Training Loss: 36.07241, Validation Loss: 5.00018\n",
      "Epoch 642/2000, Training Loss: 36.06518, Validation Loss: 4.99866\n",
      "Epoch 643/2000, Training Loss: 36.05794, Validation Loss: 4.99714\n",
      "Epoch 644/2000, Training Loss: 36.05069, Validation Loss: 4.99562\n",
      "Epoch 645/2000, Training Loss: 36.04344, Validation Loss: 4.99409\n",
      "Epoch 646/2000, Training Loss: 36.03619, Validation Loss: 4.99257\n",
      "Epoch 647/2000, Training Loss: 36.02893, Validation Loss: 4.99105\n",
      "Epoch 648/2000, Training Loss: 36.02168, Validation Loss: 4.98953\n",
      "Epoch 649/2000, Training Loss: 36.01442, Validation Loss: 4.98801\n",
      "Epoch 650/2000, Training Loss: 36.00716, Validation Loss: 4.98649\n",
      "Epoch 651/2000, Training Loss: 35.99989, Validation Loss: 4.98496\n",
      "Epoch 652/2000, Training Loss: 35.99263, Validation Loss: 4.98344\n",
      "Epoch 653/2000, Training Loss: 35.98537, Validation Loss: 4.98192\n",
      "Epoch 654/2000, Training Loss: 35.97810, Validation Loss: 4.98039\n",
      "Epoch 655/2000, Training Loss: 35.97082, Validation Loss: 4.97887\n",
      "Epoch 656/2000, Training Loss: 35.96354, Validation Loss: 4.97735\n",
      "Epoch 657/2000, Training Loss: 35.95626, Validation Loss: 4.97583\n",
      "Epoch 658/2000, Training Loss: 35.94898, Validation Loss: 4.97430\n",
      "Epoch 659/2000, Training Loss: 35.94169, Validation Loss: 4.97278\n",
      "Epoch 660/2000, Training Loss: 35.93439, Validation Loss: 4.97126\n",
      "Epoch 661/2000, Training Loss: 35.92710, Validation Loss: 4.96974\n",
      "Epoch 662/2000, Training Loss: 35.91980, Validation Loss: 4.96821\n",
      "Epoch 663/2000, Training Loss: 35.91251, Validation Loss: 4.96669\n",
      "Epoch 664/2000, Training Loss: 35.90520, Validation Loss: 4.96517\n",
      "Epoch 665/2000, Training Loss: 35.89790, Validation Loss: 4.96364\n",
      "Epoch 666/2000, Training Loss: 35.89059, Validation Loss: 4.96212\n",
      "Epoch 667/2000, Training Loss: 35.88329, Validation Loss: 4.96059\n",
      "Epoch 668/2000, Training Loss: 35.87597, Validation Loss: 4.95907\n",
      "Epoch 669/2000, Training Loss: 35.86865, Validation Loss: 4.95754\n",
      "Epoch 670/2000, Training Loss: 35.86133, Validation Loss: 4.95602\n",
      "Epoch 671/2000, Training Loss: 35.85401, Validation Loss: 4.95449\n",
      "Epoch 672/2000, Training Loss: 35.84668, Validation Loss: 4.95297\n",
      "Epoch 673/2000, Training Loss: 35.83935, Validation Loss: 4.95144\n",
      "Epoch 674/2000, Training Loss: 35.83202, Validation Loss: 4.94991\n",
      "Epoch 675/2000, Training Loss: 35.82469, Validation Loss: 4.94839\n",
      "Epoch 676/2000, Training Loss: 35.81735, Validation Loss: 4.94686\n",
      "Epoch 677/2000, Training Loss: 35.81003, Validation Loss: 4.94533\n",
      "Epoch 678/2000, Training Loss: 35.80270, Validation Loss: 4.94381\n",
      "Epoch 679/2000, Training Loss: 35.79537, Validation Loss: 4.94228\n",
      "Epoch 680/2000, Training Loss: 35.78803, Validation Loss: 4.94075\n",
      "Epoch 681/2000, Training Loss: 35.78070, Validation Loss: 4.93922\n",
      "Epoch 682/2000, Training Loss: 35.77336, Validation Loss: 4.93769\n",
      "Epoch 683/2000, Training Loss: 35.76602, Validation Loss: 4.93617\n",
      "Epoch 684/2000, Training Loss: 35.75868, Validation Loss: 4.93464\n",
      "Epoch 685/2000, Training Loss: 35.75135, Validation Loss: 4.93312\n",
      "Epoch 686/2000, Training Loss: 35.74401, Validation Loss: 4.93159\n",
      "Epoch 687/2000, Training Loss: 35.73667, Validation Loss: 4.93006\n",
      "Epoch 688/2000, Training Loss: 35.72933, Validation Loss: 4.92854\n",
      "Epoch 689/2000, Training Loss: 35.72200, Validation Loss: 4.92702\n",
      "Epoch 690/2000, Training Loss: 35.71467, Validation Loss: 4.92550\n",
      "Epoch 691/2000, Training Loss: 35.70733, Validation Loss: 4.92398\n",
      "Epoch 692/2000, Training Loss: 35.69999, Validation Loss: 4.92246\n",
      "Epoch 693/2000, Training Loss: 35.69266, Validation Loss: 4.92094\n",
      "Epoch 694/2000, Training Loss: 35.68532, Validation Loss: 4.91941\n",
      "Epoch 695/2000, Training Loss: 35.67797, Validation Loss: 4.91789\n",
      "Epoch 696/2000, Training Loss: 35.67063, Validation Loss: 4.91637\n",
      "Epoch 697/2000, Training Loss: 35.66329, Validation Loss: 4.91484\n",
      "Epoch 698/2000, Training Loss: 35.65594, Validation Loss: 4.91332\n",
      "Epoch 699/2000, Training Loss: 35.64859, Validation Loss: 4.91180\n",
      "Epoch 700/2000, Training Loss: 35.64124, Validation Loss: 4.91028\n",
      "Epoch 701/2000, Training Loss: 35.63389, Validation Loss: 4.90876\n",
      "Epoch 702/2000, Training Loss: 35.62654, Validation Loss: 4.90724\n",
      "Epoch 703/2000, Training Loss: 35.61917, Validation Loss: 4.90571\n",
      "Epoch 704/2000, Training Loss: 35.61176, Validation Loss: 4.90419\n",
      "Epoch 705/2000, Training Loss: 35.60435, Validation Loss: 4.90266\n",
      "Epoch 706/2000, Training Loss: 35.59695, Validation Loss: 4.90114\n",
      "Epoch 707/2000, Training Loss: 35.58953, Validation Loss: 4.89962\n",
      "Epoch 708/2000, Training Loss: 35.58212, Validation Loss: 4.89809\n",
      "Epoch 709/2000, Training Loss: 35.57470, Validation Loss: 4.89657\n",
      "Epoch 710/2000, Training Loss: 35.56729, Validation Loss: 4.89504\n",
      "Epoch 711/2000, Training Loss: 35.55987, Validation Loss: 4.89352\n",
      "Epoch 712/2000, Training Loss: 35.55245, Validation Loss: 4.89199\n",
      "Epoch 713/2000, Training Loss: 35.54503, Validation Loss: 4.89047\n",
      "Epoch 714/2000, Training Loss: 35.53760, Validation Loss: 4.88894\n",
      "Epoch 715/2000, Training Loss: 35.53017, Validation Loss: 4.88742\n",
      "Epoch 716/2000, Training Loss: 35.52274, Validation Loss: 4.88589\n",
      "Epoch 717/2000, Training Loss: 35.51531, Validation Loss: 4.88437\n",
      "Epoch 718/2000, Training Loss: 35.50787, Validation Loss: 4.88285\n",
      "Epoch 719/2000, Training Loss: 35.50043, Validation Loss: 4.88132\n",
      "Epoch 720/2000, Training Loss: 35.49300, Validation Loss: 4.87980\n",
      "Epoch 721/2000, Training Loss: 35.48556, Validation Loss: 4.87828\n",
      "Epoch 722/2000, Training Loss: 35.47813, Validation Loss: 4.87675\n",
      "Epoch 723/2000, Training Loss: 35.47069, Validation Loss: 4.87523\n",
      "Epoch 724/2000, Training Loss: 35.46326, Validation Loss: 4.87371\n",
      "Epoch 725/2000, Training Loss: 35.45581, Validation Loss: 4.87219\n",
      "Epoch 726/2000, Training Loss: 35.44837, Validation Loss: 4.87067\n",
      "Epoch 727/2000, Training Loss: 35.44092, Validation Loss: 4.86915\n",
      "Epoch 728/2000, Training Loss: 35.43347, Validation Loss: 4.86762\n",
      "Epoch 729/2000, Training Loss: 35.42602, Validation Loss: 4.86610\n",
      "Epoch 730/2000, Training Loss: 35.41856, Validation Loss: 4.86458\n",
      "Epoch 731/2000, Training Loss: 35.41110, Validation Loss: 4.86306\n",
      "Epoch 732/2000, Training Loss: 35.40363, Validation Loss: 4.86154\n",
      "Epoch 733/2000, Training Loss: 35.39617, Validation Loss: 4.86001\n",
      "Epoch 734/2000, Training Loss: 35.38870, Validation Loss: 4.85849\n",
      "Epoch 735/2000, Training Loss: 35.38124, Validation Loss: 4.85697\n",
      "Epoch 736/2000, Training Loss: 35.37377, Validation Loss: 4.85545\n",
      "Epoch 737/2000, Training Loss: 35.36630, Validation Loss: 4.85392\n",
      "Epoch 738/2000, Training Loss: 35.35883, Validation Loss: 4.85240\n",
      "Epoch 739/2000, Training Loss: 35.35136, Validation Loss: 4.85088\n",
      "Epoch 740/2000, Training Loss: 35.34388, Validation Loss: 4.84935\n",
      "Epoch 741/2000, Training Loss: 35.33640, Validation Loss: 4.84783\n",
      "Epoch 742/2000, Training Loss: 35.32892, Validation Loss: 4.84631\n",
      "Epoch 743/2000, Training Loss: 35.32144, Validation Loss: 4.84478\n",
      "Epoch 744/2000, Training Loss: 35.31396, Validation Loss: 4.84326\n",
      "Epoch 745/2000, Training Loss: 35.30648, Validation Loss: 4.84174\n",
      "Epoch 746/2000, Training Loss: 35.29902, Validation Loss: 4.84021\n",
      "Epoch 747/2000, Training Loss: 35.29156, Validation Loss: 4.83869\n",
      "Epoch 748/2000, Training Loss: 35.28410, Validation Loss: 4.83717\n",
      "Epoch 749/2000, Training Loss: 35.27664, Validation Loss: 4.83565\n",
      "Epoch 750/2000, Training Loss: 35.26917, Validation Loss: 4.83413\n",
      "Epoch 751/2000, Training Loss: 35.26171, Validation Loss: 4.83260\n",
      "Epoch 752/2000, Training Loss: 35.25424, Validation Loss: 4.83108\n",
      "Epoch 753/2000, Training Loss: 35.24677, Validation Loss: 4.82956\n",
      "Epoch 754/2000, Training Loss: 35.23931, Validation Loss: 4.82805\n",
      "Epoch 755/2000, Training Loss: 35.23184, Validation Loss: 4.82653\n",
      "Epoch 756/2000, Training Loss: 35.22438, Validation Loss: 4.82501\n",
      "Epoch 757/2000, Training Loss: 35.21692, Validation Loss: 4.82349\n",
      "Epoch 758/2000, Training Loss: 35.20946, Validation Loss: 4.82197\n",
      "Epoch 759/2000, Training Loss: 35.20200, Validation Loss: 4.82045\n",
      "Epoch 760/2000, Training Loss: 35.19454, Validation Loss: 4.81893\n",
      "Epoch 761/2000, Training Loss: 35.18708, Validation Loss: 4.81741\n",
      "Epoch 762/2000, Training Loss: 35.17961, Validation Loss: 4.81590\n",
      "Epoch 763/2000, Training Loss: 35.17215, Validation Loss: 4.81438\n",
      "Epoch 764/2000, Training Loss: 35.16468, Validation Loss: 4.81286\n",
      "Epoch 765/2000, Training Loss: 35.15722, Validation Loss: 4.81134\n",
      "Epoch 766/2000, Training Loss: 35.14976, Validation Loss: 4.80983\n",
      "Epoch 767/2000, Training Loss: 35.14230, Validation Loss: 4.80831\n",
      "Epoch 768/2000, Training Loss: 35.13483, Validation Loss: 4.80680\n",
      "Epoch 769/2000, Training Loss: 35.12737, Validation Loss: 4.80528\n",
      "Epoch 770/2000, Training Loss: 35.11990, Validation Loss: 4.80377\n",
      "Epoch 771/2000, Training Loss: 35.11243, Validation Loss: 4.80225\n",
      "Epoch 772/2000, Training Loss: 35.10495, Validation Loss: 4.80074\n",
      "Epoch 773/2000, Training Loss: 35.09748, Validation Loss: 4.79922\n",
      "Epoch 774/2000, Training Loss: 35.09001, Validation Loss: 4.79770\n",
      "Epoch 775/2000, Training Loss: 35.08253, Validation Loss: 4.79618\n",
      "Epoch 776/2000, Training Loss: 35.07504, Validation Loss: 4.79466\n",
      "Epoch 777/2000, Training Loss: 35.06756, Validation Loss: 4.79314\n",
      "Epoch 778/2000, Training Loss: 35.06007, Validation Loss: 4.79162\n",
      "Epoch 779/2000, Training Loss: 35.05258, Validation Loss: 4.79009\n",
      "Epoch 780/2000, Training Loss: 35.04509, Validation Loss: 4.78857\n",
      "Epoch 781/2000, Training Loss: 35.03760, Validation Loss: 4.78705\n",
      "Epoch 782/2000, Training Loss: 35.03011, Validation Loss: 4.78553\n",
      "Epoch 783/2000, Training Loss: 35.02262, Validation Loss: 4.78401\n",
      "Epoch 784/2000, Training Loss: 35.01514, Validation Loss: 4.78249\n",
      "Epoch 785/2000, Training Loss: 35.00765, Validation Loss: 4.78097\n",
      "Epoch 786/2000, Training Loss: 35.00017, Validation Loss: 4.77945\n",
      "Epoch 787/2000, Training Loss: 34.99268, Validation Loss: 4.77793\n",
      "Epoch 788/2000, Training Loss: 34.98520, Validation Loss: 4.77641\n",
      "Epoch 789/2000, Training Loss: 34.97771, Validation Loss: 4.77489\n",
      "Epoch 790/2000, Training Loss: 34.97023, Validation Loss: 4.77337\n",
      "Epoch 791/2000, Training Loss: 34.96274, Validation Loss: 4.77185\n",
      "Epoch 792/2000, Training Loss: 34.95526, Validation Loss: 4.77033\n",
      "Epoch 793/2000, Training Loss: 34.94776, Validation Loss: 4.76881\n",
      "Epoch 794/2000, Training Loss: 34.94027, Validation Loss: 4.76729\n",
      "Epoch 795/2000, Training Loss: 34.93278, Validation Loss: 4.76577\n",
      "Epoch 796/2000, Training Loss: 34.92528, Validation Loss: 4.76425\n",
      "Epoch 797/2000, Training Loss: 34.91779, Validation Loss: 4.76273\n",
      "Epoch 798/2000, Training Loss: 34.91030, Validation Loss: 4.76121\n",
      "Epoch 799/2000, Training Loss: 34.90280, Validation Loss: 4.75968\n",
      "Epoch 800/2000, Training Loss: 34.89530, Validation Loss: 4.75816\n",
      "Epoch 801/2000, Training Loss: 34.88780, Validation Loss: 4.75664\n",
      "Epoch 802/2000, Training Loss: 34.88031, Validation Loss: 4.75512\n",
      "Epoch 803/2000, Training Loss: 34.87281, Validation Loss: 4.75360\n",
      "Epoch 804/2000, Training Loss: 34.86531, Validation Loss: 4.75208\n",
      "Epoch 805/2000, Training Loss: 34.85782, Validation Loss: 4.75056\n",
      "Epoch 806/2000, Training Loss: 34.85032, Validation Loss: 4.74904\n",
      "Epoch 807/2000, Training Loss: 34.84283, Validation Loss: 4.74752\n",
      "Epoch 808/2000, Training Loss: 34.83533, Validation Loss: 4.74600\n",
      "Epoch 809/2000, Training Loss: 34.82783, Validation Loss: 4.74448\n",
      "Epoch 810/2000, Training Loss: 34.82033, Validation Loss: 4.74297\n",
      "Epoch 811/2000, Training Loss: 34.81284, Validation Loss: 4.74145\n",
      "Epoch 812/2000, Training Loss: 34.80534, Validation Loss: 4.73993\n",
      "Epoch 813/2000, Training Loss: 34.79784, Validation Loss: 4.73841\n",
      "Epoch 814/2000, Training Loss: 34.79035, Validation Loss: 4.73690\n",
      "Epoch 815/2000, Training Loss: 34.78286, Validation Loss: 4.73538\n",
      "Epoch 816/2000, Training Loss: 34.77536, Validation Loss: 4.73387\n",
      "Epoch 817/2000, Training Loss: 34.76788, Validation Loss: 4.73236\n",
      "Epoch 818/2000, Training Loss: 34.76040, Validation Loss: 4.73084\n",
      "Epoch 819/2000, Training Loss: 34.75292, Validation Loss: 4.72933\n",
      "Epoch 820/2000, Training Loss: 34.74544, Validation Loss: 4.72782\n",
      "Epoch 821/2000, Training Loss: 34.73796, Validation Loss: 4.72631\n",
      "Epoch 822/2000, Training Loss: 34.73047, Validation Loss: 4.72480\n",
      "Epoch 823/2000, Training Loss: 34.72298, Validation Loss: 4.72329\n",
      "Epoch 824/2000, Training Loss: 34.71550, Validation Loss: 4.72178\n",
      "Epoch 825/2000, Training Loss: 34.70802, Validation Loss: 4.72027\n",
      "Epoch 826/2000, Training Loss: 34.70053, Validation Loss: 4.71876\n",
      "Epoch 827/2000, Training Loss: 34.69305, Validation Loss: 4.71725\n",
      "Epoch 828/2000, Training Loss: 34.68555, Validation Loss: 4.71574\n",
      "Epoch 829/2000, Training Loss: 34.67806, Validation Loss: 4.71422\n",
      "Epoch 830/2000, Training Loss: 34.67057, Validation Loss: 4.71271\n",
      "Epoch 831/2000, Training Loss: 34.66310, Validation Loss: 4.71120\n",
      "Epoch 832/2000, Training Loss: 34.65562, Validation Loss: 4.70969\n",
      "Epoch 833/2000, Training Loss: 34.64814, Validation Loss: 4.70818\n",
      "Epoch 834/2000, Training Loss: 34.64066, Validation Loss: 4.70667\n",
      "Epoch 835/2000, Training Loss: 34.63318, Validation Loss: 4.70516\n",
      "Epoch 836/2000, Training Loss: 34.62570, Validation Loss: 4.70364\n",
      "Epoch 837/2000, Training Loss: 34.61822, Validation Loss: 4.70213\n",
      "Epoch 838/2000, Training Loss: 34.61074, Validation Loss: 4.70062\n",
      "Epoch 839/2000, Training Loss: 34.60326, Validation Loss: 4.69911\n",
      "Epoch 840/2000, Training Loss: 34.59578, Validation Loss: 4.69760\n",
      "Epoch 841/2000, Training Loss: 34.58831, Validation Loss: 4.69609\n",
      "Epoch 842/2000, Training Loss: 34.58085, Validation Loss: 4.69458\n",
      "Epoch 843/2000, Training Loss: 34.57339, Validation Loss: 4.69307\n",
      "Epoch 844/2000, Training Loss: 34.56593, Validation Loss: 4.69156\n",
      "Epoch 845/2000, Training Loss: 34.55847, Validation Loss: 4.69005\n",
      "Epoch 846/2000, Training Loss: 34.55101, Validation Loss: 4.68854\n",
      "Epoch 847/2000, Training Loss: 34.54355, Validation Loss: 4.68703\n",
      "Epoch 848/2000, Training Loss: 34.53608, Validation Loss: 4.68553\n",
      "Epoch 849/2000, Training Loss: 34.52861, Validation Loss: 4.68402\n",
      "Epoch 850/2000, Training Loss: 34.52115, Validation Loss: 4.68251\n",
      "Epoch 851/2000, Training Loss: 34.51369, Validation Loss: 4.68100\n",
      "Epoch 852/2000, Training Loss: 34.50624, Validation Loss: 4.67950\n",
      "Epoch 853/2000, Training Loss: 34.49879, Validation Loss: 4.67799\n",
      "Epoch 854/2000, Training Loss: 34.49134, Validation Loss: 4.67649\n",
      "Epoch 855/2000, Training Loss: 34.48389, Validation Loss: 4.67498\n",
      "Epoch 856/2000, Training Loss: 34.47645, Validation Loss: 4.67348\n",
      "Epoch 857/2000, Training Loss: 34.46900, Validation Loss: 4.67197\n",
      "Epoch 858/2000, Training Loss: 34.46156, Validation Loss: 4.67047\n",
      "Epoch 859/2000, Training Loss: 34.45411, Validation Loss: 4.66897\n",
      "Epoch 860/2000, Training Loss: 34.44666, Validation Loss: 4.66746\n",
      "Epoch 861/2000, Training Loss: 34.43920, Validation Loss: 4.66596\n",
      "Epoch 862/2000, Training Loss: 34.43174, Validation Loss: 4.66446\n",
      "Epoch 863/2000, Training Loss: 34.42428, Validation Loss: 4.66296\n",
      "Epoch 864/2000, Training Loss: 34.41682, Validation Loss: 4.66145\n",
      "Epoch 865/2000, Training Loss: 34.40937, Validation Loss: 4.65995\n",
      "Epoch 866/2000, Training Loss: 34.40192, Validation Loss: 4.65845\n",
      "Epoch 867/2000, Training Loss: 34.39448, Validation Loss: 4.65695\n",
      "Epoch 868/2000, Training Loss: 34.38705, Validation Loss: 4.65545\n",
      "Epoch 869/2000, Training Loss: 34.37962, Validation Loss: 4.65395\n",
      "Epoch 870/2000, Training Loss: 34.37220, Validation Loss: 4.65245\n",
      "Epoch 871/2000, Training Loss: 34.36477, Validation Loss: 4.65095\n",
      "Epoch 872/2000, Training Loss: 34.35734, Validation Loss: 4.64945\n",
      "Epoch 873/2000, Training Loss: 34.34991, Validation Loss: 4.64796\n",
      "Epoch 874/2000, Training Loss: 34.34248, Validation Loss: 4.64646\n",
      "Epoch 875/2000, Training Loss: 34.33505, Validation Loss: 4.64496\n",
      "Epoch 876/2000, Training Loss: 34.32762, Validation Loss: 4.64346\n",
      "Epoch 877/2000, Training Loss: 34.32019, Validation Loss: 4.64197\n",
      "Epoch 878/2000, Training Loss: 34.31276, Validation Loss: 4.64047\n",
      "Epoch 879/2000, Training Loss: 34.30533, Validation Loss: 4.63898\n",
      "Epoch 880/2000, Training Loss: 34.29790, Validation Loss: 4.63749\n",
      "Epoch 881/2000, Training Loss: 34.29048, Validation Loss: 4.63600\n",
      "Epoch 882/2000, Training Loss: 34.28306, Validation Loss: 4.63451\n",
      "Epoch 883/2000, Training Loss: 34.27564, Validation Loss: 4.63302\n",
      "Epoch 884/2000, Training Loss: 34.26823, Validation Loss: 4.63153\n",
      "Epoch 885/2000, Training Loss: 34.26083, Validation Loss: 4.63004\n",
      "Epoch 886/2000, Training Loss: 34.25342, Validation Loss: 4.62855\n",
      "Epoch 887/2000, Training Loss: 34.24602, Validation Loss: 4.62706\n",
      "Epoch 888/2000, Training Loss: 34.23862, Validation Loss: 4.62557\n",
      "Epoch 889/2000, Training Loss: 34.23123, Validation Loss: 4.62407\n",
      "Epoch 890/2000, Training Loss: 34.22384, Validation Loss: 4.62258\n",
      "Epoch 891/2000, Training Loss: 34.21645, Validation Loss: 4.62109\n",
      "Epoch 892/2000, Training Loss: 34.20906, Validation Loss: 4.61960\n",
      "Epoch 893/2000, Training Loss: 34.20167, Validation Loss: 4.61810\n",
      "Epoch 894/2000, Training Loss: 34.19428, Validation Loss: 4.61661\n",
      "Epoch 895/2000, Training Loss: 34.18690, Validation Loss: 4.61512\n",
      "Epoch 896/2000, Training Loss: 34.17951, Validation Loss: 4.61364\n",
      "Epoch 897/2000, Training Loss: 34.17213, Validation Loss: 4.61215\n",
      "Epoch 898/2000, Training Loss: 34.16476, Validation Loss: 4.61066\n",
      "Epoch 899/2000, Training Loss: 34.15739, Validation Loss: 4.60917\n",
      "Epoch 900/2000, Training Loss: 34.15003, Validation Loss: 4.60769\n",
      "Epoch 901/2000, Training Loss: 34.14267, Validation Loss: 4.60620\n",
      "Epoch 902/2000, Training Loss: 34.13531, Validation Loss: 4.60472\n",
      "Epoch 903/2000, Training Loss: 34.12796, Validation Loss: 4.60323\n",
      "Epoch 904/2000, Training Loss: 34.12060, Validation Loss: 4.60175\n",
      "Epoch 905/2000, Training Loss: 34.11325, Validation Loss: 4.60027\n",
      "Epoch 906/2000, Training Loss: 34.10591, Validation Loss: 4.59880\n",
      "Epoch 907/2000, Training Loss: 34.09856, Validation Loss: 4.59732\n",
      "Epoch 908/2000, Training Loss: 34.09121, Validation Loss: 4.59585\n",
      "Epoch 909/2000, Training Loss: 34.08387, Validation Loss: 4.59437\n",
      "Epoch 910/2000, Training Loss: 34.07653, Validation Loss: 4.59290\n",
      "Epoch 911/2000, Training Loss: 34.06920, Validation Loss: 4.59143\n",
      "Epoch 912/2000, Training Loss: 34.06186, Validation Loss: 4.58995\n",
      "Epoch 913/2000, Training Loss: 34.05453, Validation Loss: 4.58848\n",
      "Epoch 914/2000, Training Loss: 34.04720, Validation Loss: 4.58701\n",
      "Epoch 915/2000, Training Loss: 34.03987, Validation Loss: 4.58554\n",
      "Epoch 916/2000, Training Loss: 34.03255, Validation Loss: 4.58407\n",
      "Epoch 917/2000, Training Loss: 34.02522, Validation Loss: 4.58260\n",
      "Epoch 918/2000, Training Loss: 34.01790, Validation Loss: 4.58113\n",
      "Epoch 919/2000, Training Loss: 34.01058, Validation Loss: 4.57966\n",
      "Epoch 920/2000, Training Loss: 34.00327, Validation Loss: 4.57820\n",
      "Epoch 921/2000, Training Loss: 33.99596, Validation Loss: 4.57673\n",
      "Epoch 922/2000, Training Loss: 33.98867, Validation Loss: 4.57526\n",
      "Epoch 923/2000, Training Loss: 33.98137, Validation Loss: 4.57379\n",
      "Epoch 924/2000, Training Loss: 33.97408, Validation Loss: 4.57232\n",
      "Epoch 925/2000, Training Loss: 33.96679, Validation Loss: 4.57085\n",
      "Epoch 926/2000, Training Loss: 33.95951, Validation Loss: 4.56939\n",
      "Epoch 927/2000, Training Loss: 33.95222, Validation Loss: 4.56792\n",
      "Epoch 928/2000, Training Loss: 33.94494, Validation Loss: 4.56645\n",
      "Epoch 929/2000, Training Loss: 33.93766, Validation Loss: 4.56499\n",
      "Epoch 930/2000, Training Loss: 33.93037, Validation Loss: 4.56352\n",
      "Epoch 931/2000, Training Loss: 33.92309, Validation Loss: 4.56206\n",
      "Epoch 932/2000, Training Loss: 33.91581, Validation Loss: 4.56059\n",
      "Epoch 933/2000, Training Loss: 33.90853, Validation Loss: 4.55913\n",
      "Epoch 934/2000, Training Loss: 33.90126, Validation Loss: 4.55766\n",
      "Epoch 935/2000, Training Loss: 33.89399, Validation Loss: 4.55620\n",
      "Epoch 936/2000, Training Loss: 33.88672, Validation Loss: 4.55474\n",
      "Epoch 937/2000, Training Loss: 33.87946, Validation Loss: 4.55328\n",
      "Epoch 938/2000, Training Loss: 33.87220, Validation Loss: 4.55181\n",
      "Epoch 939/2000, Training Loss: 33.86494, Validation Loss: 4.55035\n",
      "Epoch 940/2000, Training Loss: 33.85768, Validation Loss: 4.54889\n",
      "Epoch 941/2000, Training Loss: 33.85042, Validation Loss: 4.54743\n",
      "Epoch 942/2000, Training Loss: 33.84316, Validation Loss: 4.54598\n",
      "Epoch 943/2000, Training Loss: 33.83591, Validation Loss: 4.54452\n",
      "Epoch 944/2000, Training Loss: 33.82866, Validation Loss: 4.54306\n",
      "Epoch 945/2000, Training Loss: 33.82142, Validation Loss: 4.54161\n",
      "Epoch 946/2000, Training Loss: 33.81418, Validation Loss: 4.54015\n",
      "Epoch 947/2000, Training Loss: 33.80694, Validation Loss: 4.53870\n",
      "Epoch 948/2000, Training Loss: 33.79971, Validation Loss: 4.53725\n",
      "Epoch 949/2000, Training Loss: 33.79248, Validation Loss: 4.53580\n",
      "Epoch 950/2000, Training Loss: 33.78524, Validation Loss: 4.53435\n",
      "Epoch 951/2000, Training Loss: 33.77799, Validation Loss: 4.53290\n",
      "Epoch 952/2000, Training Loss: 33.77073, Validation Loss: 4.53145\n",
      "Epoch 953/2000, Training Loss: 33.76348, Validation Loss: 4.53001\n",
      "Epoch 954/2000, Training Loss: 33.75622, Validation Loss: 4.52856\n",
      "Epoch 955/2000, Training Loss: 33.74898, Validation Loss: 4.52712\n",
      "Epoch 956/2000, Training Loss: 33.74174, Validation Loss: 4.52568\n",
      "Epoch 957/2000, Training Loss: 33.73450, Validation Loss: 4.52424\n",
      "Epoch 958/2000, Training Loss: 33.72727, Validation Loss: 4.52280\n",
      "Epoch 959/2000, Training Loss: 33.72005, Validation Loss: 4.52136\n",
      "Epoch 960/2000, Training Loss: 33.71283, Validation Loss: 4.51992\n",
      "Epoch 961/2000, Training Loss: 33.70563, Validation Loss: 4.51848\n",
      "Epoch 962/2000, Training Loss: 33.69842, Validation Loss: 4.51705\n",
      "Epoch 963/2000, Training Loss: 33.69123, Validation Loss: 4.51561\n",
      "Epoch 964/2000, Training Loss: 33.68403, Validation Loss: 4.51418\n",
      "Epoch 965/2000, Training Loss: 33.67684, Validation Loss: 4.51275\n",
      "Epoch 966/2000, Training Loss: 33.66966, Validation Loss: 4.51132\n",
      "Epoch 967/2000, Training Loss: 33.66247, Validation Loss: 4.50989\n",
      "Epoch 968/2000, Training Loss: 33.65529, Validation Loss: 4.50846\n",
      "Epoch 969/2000, Training Loss: 33.64812, Validation Loss: 4.50703\n",
      "Epoch 970/2000, Training Loss: 33.64094, Validation Loss: 4.50560\n",
      "Epoch 971/2000, Training Loss: 33.63377, Validation Loss: 4.50418\n",
      "Epoch 972/2000, Training Loss: 33.62660, Validation Loss: 4.50275\n",
      "Epoch 973/2000, Training Loss: 33.61944, Validation Loss: 4.50133\n",
      "Epoch 974/2000, Training Loss: 33.61228, Validation Loss: 4.49990\n",
      "Epoch 975/2000, Training Loss: 33.60513, Validation Loss: 4.49848\n",
      "Epoch 976/2000, Training Loss: 33.59797, Validation Loss: 4.49706\n",
      "Epoch 977/2000, Training Loss: 33.59082, Validation Loss: 4.49564\n",
      "Epoch 978/2000, Training Loss: 33.58367, Validation Loss: 4.49422\n",
      "Epoch 979/2000, Training Loss: 33.57653, Validation Loss: 4.49280\n",
      "Epoch 980/2000, Training Loss: 33.56938, Validation Loss: 4.49139\n",
      "Epoch 981/2000, Training Loss: 33.56224, Validation Loss: 4.48997\n",
      "Epoch 982/2000, Training Loss: 33.55510, Validation Loss: 4.48856\n",
      "Epoch 983/2000, Training Loss: 33.54798, Validation Loss: 4.48715\n",
      "Epoch 984/2000, Training Loss: 33.54086, Validation Loss: 4.48574\n",
      "Epoch 985/2000, Training Loss: 33.53373, Validation Loss: 4.48433\n",
      "Epoch 986/2000, Training Loss: 33.52660, Validation Loss: 4.48292\n",
      "Epoch 987/2000, Training Loss: 33.51948, Validation Loss: 4.48151\n",
      "Epoch 988/2000, Training Loss: 33.51236, Validation Loss: 4.48011\n",
      "Epoch 989/2000, Training Loss: 33.50524, Validation Loss: 4.47870\n",
      "Epoch 990/2000, Training Loss: 33.49812, Validation Loss: 4.47730\n",
      "Epoch 991/2000, Training Loss: 33.49101, Validation Loss: 4.47590\n",
      "Epoch 992/2000, Training Loss: 33.48391, Validation Loss: 4.47450\n",
      "Epoch 993/2000, Training Loss: 33.47680, Validation Loss: 4.47310\n",
      "Epoch 994/2000, Training Loss: 33.46970, Validation Loss: 4.47170\n",
      "Epoch 995/2000, Training Loss: 33.46260, Validation Loss: 4.47030\n",
      "Epoch 996/2000, Training Loss: 33.45551, Validation Loss: 4.46891\n",
      "Epoch 997/2000, Training Loss: 33.44842, Validation Loss: 4.46751\n",
      "Epoch 998/2000, Training Loss: 33.44134, Validation Loss: 4.46611\n",
      "Epoch 999/2000, Training Loss: 33.43426, Validation Loss: 4.46472\n",
      "Epoch 1000/2000, Training Loss: 33.42718, Validation Loss: 4.46333\n",
      "Epoch 1001/2000, Training Loss: 33.42010, Validation Loss: 4.46193\n",
      "Epoch 1002/2000, Training Loss: 33.41302, Validation Loss: 4.46054\n",
      "Epoch 1003/2000, Training Loss: 33.40594, Validation Loss: 4.45915\n",
      "Epoch 1004/2000, Training Loss: 33.39887, Validation Loss: 4.45776\n",
      "Epoch 1005/2000, Training Loss: 33.39180, Validation Loss: 4.45637\n",
      "Epoch 1006/2000, Training Loss: 33.38474, Validation Loss: 4.45499\n",
      "Epoch 1007/2000, Training Loss: 33.37769, Validation Loss: 4.45360\n",
      "Epoch 1008/2000, Training Loss: 33.37064, Validation Loss: 4.45221\n",
      "Epoch 1009/2000, Training Loss: 33.36359, Validation Loss: 4.45083\n",
      "Epoch 1010/2000, Training Loss: 33.35655, Validation Loss: 4.44945\n",
      "Epoch 1011/2000, Training Loss: 33.34951, Validation Loss: 4.44806\n",
      "Epoch 1012/2000, Training Loss: 33.34248, Validation Loss: 4.44668\n",
      "Epoch 1013/2000, Training Loss: 33.33545, Validation Loss: 4.44530\n",
      "Epoch 1014/2000, Training Loss: 33.32842, Validation Loss: 4.44392\n",
      "Epoch 1015/2000, Training Loss: 33.32139, Validation Loss: 4.44254\n",
      "Epoch 1016/2000, Training Loss: 33.31437, Validation Loss: 4.44116\n",
      "Epoch 1017/2000, Training Loss: 33.30736, Validation Loss: 4.43979\n",
      "Epoch 1018/2000, Training Loss: 33.30035, Validation Loss: 4.43841\n",
      "Epoch 1019/2000, Training Loss: 33.29334, Validation Loss: 4.43704\n",
      "Epoch 1020/2000, Training Loss: 33.28634, Validation Loss: 4.43566\n",
      "Epoch 1021/2000, Training Loss: 33.27935, Validation Loss: 4.43429\n",
      "Epoch 1022/2000, Training Loss: 33.27236, Validation Loss: 4.43292\n",
      "Epoch 1023/2000, Training Loss: 33.26537, Validation Loss: 4.43155\n",
      "Epoch 1024/2000, Training Loss: 33.25840, Validation Loss: 4.43019\n",
      "Epoch 1025/2000, Training Loss: 33.25143, Validation Loss: 4.42882\n",
      "Epoch 1026/2000, Training Loss: 33.24447, Validation Loss: 4.42746\n",
      "Epoch 1027/2000, Training Loss: 33.23752, Validation Loss: 4.42610\n",
      "Epoch 1028/2000, Training Loss: 33.23058, Validation Loss: 4.42474\n",
      "Epoch 1029/2000, Training Loss: 33.22364, Validation Loss: 4.42338\n",
      "Epoch 1030/2000, Training Loss: 33.21671, Validation Loss: 4.42202\n",
      "Epoch 1031/2000, Training Loss: 33.20978, Validation Loss: 4.42066\n",
      "Epoch 1032/2000, Training Loss: 33.20286, Validation Loss: 4.41930\n",
      "Epoch 1033/2000, Training Loss: 33.19594, Validation Loss: 4.41795\n",
      "Epoch 1034/2000, Training Loss: 33.18903, Validation Loss: 4.41659\n",
      "Epoch 1035/2000, Training Loss: 33.18214, Validation Loss: 4.41524\n",
      "Epoch 1036/2000, Training Loss: 33.17525, Validation Loss: 4.41389\n",
      "Epoch 1037/2000, Training Loss: 33.16836, Validation Loss: 4.41254\n",
      "Epoch 1038/2000, Training Loss: 33.16148, Validation Loss: 4.41119\n",
      "Epoch 1039/2000, Training Loss: 33.15461, Validation Loss: 4.40984\n",
      "Epoch 1040/2000, Training Loss: 33.14774, Validation Loss: 4.40849\n",
      "Epoch 1041/2000, Training Loss: 33.14088, Validation Loss: 4.40715\n",
      "Epoch 1042/2000, Training Loss: 33.13403, Validation Loss: 4.40581\n",
      "Epoch 1043/2000, Training Loss: 33.12718, Validation Loss: 4.40446\n",
      "Epoch 1044/2000, Training Loss: 33.12033, Validation Loss: 4.40312\n",
      "Epoch 1045/2000, Training Loss: 33.11349, Validation Loss: 4.40179\n",
      "Epoch 1046/2000, Training Loss: 33.10666, Validation Loss: 4.40045\n",
      "Epoch 1047/2000, Training Loss: 33.09983, Validation Loss: 4.39911\n",
      "Epoch 1048/2000, Training Loss: 33.09300, Validation Loss: 4.39778\n",
      "Epoch 1049/2000, Training Loss: 33.08619, Validation Loss: 4.39644\n",
      "Epoch 1050/2000, Training Loss: 33.07938, Validation Loss: 4.39511\n",
      "Epoch 1051/2000, Training Loss: 33.07257, Validation Loss: 4.39378\n",
      "Epoch 1052/2000, Training Loss: 33.06578, Validation Loss: 4.39245\n",
      "Epoch 1053/2000, Training Loss: 33.05899, Validation Loss: 4.39112\n",
      "Epoch 1054/2000, Training Loss: 33.05220, Validation Loss: 4.38979\n",
      "Epoch 1055/2000, Training Loss: 33.04543, Validation Loss: 4.38847\n",
      "Epoch 1056/2000, Training Loss: 33.03865, Validation Loss: 4.38714\n",
      "Epoch 1057/2000, Training Loss: 33.03188, Validation Loss: 4.38582\n",
      "Epoch 1058/2000, Training Loss: 33.02512, Validation Loss: 4.38450\n",
      "Epoch 1059/2000, Training Loss: 33.01836, Validation Loss: 4.38318\n",
      "Epoch 1060/2000, Training Loss: 33.01161, Validation Loss: 4.38186\n",
      "Epoch 1061/2000, Training Loss: 33.00486, Validation Loss: 4.38054\n",
      "Epoch 1062/2000, Training Loss: 32.99812, Validation Loss: 4.37922\n",
      "Epoch 1063/2000, Training Loss: 32.99137, Validation Loss: 4.37790\n",
      "Epoch 1064/2000, Training Loss: 32.98464, Validation Loss: 4.37659\n",
      "Epoch 1065/2000, Training Loss: 32.97791, Validation Loss: 4.37528\n",
      "Epoch 1066/2000, Training Loss: 32.97119, Validation Loss: 4.37397\n",
      "Epoch 1067/2000, Training Loss: 32.96447, Validation Loss: 4.37266\n",
      "Epoch 1068/2000, Training Loss: 32.95776, Validation Loss: 4.37135\n",
      "Epoch 1069/2000, Training Loss: 32.95106, Validation Loss: 4.37004\n",
      "Epoch 1070/2000, Training Loss: 32.94435, Validation Loss: 4.36873\n",
      "Epoch 1071/2000, Training Loss: 32.93765, Validation Loss: 4.36743\n",
      "Epoch 1072/2000, Training Loss: 32.93096, Validation Loss: 4.36612\n",
      "Epoch 1073/2000, Training Loss: 32.92427, Validation Loss: 4.36482\n",
      "Epoch 1074/2000, Training Loss: 32.91759, Validation Loss: 4.36351\n",
      "Epoch 1075/2000, Training Loss: 32.91091, Validation Loss: 4.36221\n",
      "Epoch 1076/2000, Training Loss: 32.90424, Validation Loss: 4.36092\n",
      "Epoch 1077/2000, Training Loss: 32.89758, Validation Loss: 4.35962\n",
      "Epoch 1078/2000, Training Loss: 32.89092, Validation Loss: 4.35832\n",
      "Epoch 1079/2000, Training Loss: 32.88427, Validation Loss: 4.35703\n",
      "Epoch 1080/2000, Training Loss: 32.87762, Validation Loss: 4.35574\n",
      "Epoch 1081/2000, Training Loss: 32.87098, Validation Loss: 4.35445\n",
      "Epoch 1082/2000, Training Loss: 32.86435, Validation Loss: 4.35316\n",
      "Epoch 1083/2000, Training Loss: 32.85772, Validation Loss: 4.35187\n",
      "Epoch 1084/2000, Training Loss: 32.85111, Validation Loss: 4.35059\n",
      "Epoch 1085/2000, Training Loss: 32.84450, Validation Loss: 4.34930\n",
      "Epoch 1086/2000, Training Loss: 32.83790, Validation Loss: 4.34802\n",
      "Epoch 1087/2000, Training Loss: 32.83130, Validation Loss: 4.34674\n",
      "Epoch 1088/2000, Training Loss: 32.82471, Validation Loss: 4.34546\n",
      "Epoch 1089/2000, Training Loss: 32.81813, Validation Loss: 4.34418\n",
      "Epoch 1090/2000, Training Loss: 32.81155, Validation Loss: 4.34290\n",
      "Epoch 1091/2000, Training Loss: 32.80498, Validation Loss: 4.34163\n",
      "Epoch 1092/2000, Training Loss: 32.79842, Validation Loss: 4.34035\n",
      "Epoch 1093/2000, Training Loss: 32.79187, Validation Loss: 4.33908\n",
      "Epoch 1094/2000, Training Loss: 32.78532, Validation Loss: 4.33781\n",
      "Epoch 1095/2000, Training Loss: 32.77878, Validation Loss: 4.33654\n",
      "Epoch 1096/2000, Training Loss: 32.77224, Validation Loss: 4.33527\n",
      "Epoch 1097/2000, Training Loss: 32.76572, Validation Loss: 4.33400\n",
      "Epoch 1098/2000, Training Loss: 32.75920, Validation Loss: 4.33273\n",
      "Epoch 1099/2000, Training Loss: 32.75268, Validation Loss: 4.33146\n",
      "Epoch 1100/2000, Training Loss: 32.74617, Validation Loss: 4.33020\n",
      "Epoch 1101/2000, Training Loss: 32.73966, Validation Loss: 4.32893\n",
      "Epoch 1102/2000, Training Loss: 32.73314, Validation Loss: 4.32767\n",
      "Epoch 1103/2000, Training Loss: 32.72664, Validation Loss: 4.32641\n",
      "Epoch 1104/2000, Training Loss: 32.72014, Validation Loss: 4.32516\n",
      "Epoch 1105/2000, Training Loss: 32.71364, Validation Loss: 4.32390\n",
      "Epoch 1106/2000, Training Loss: 32.70715, Validation Loss: 4.32264\n",
      "Epoch 1107/2000, Training Loss: 32.70066, Validation Loss: 4.32139\n",
      "Epoch 1108/2000, Training Loss: 32.69418, Validation Loss: 4.32013\n",
      "Epoch 1109/2000, Training Loss: 32.68770, Validation Loss: 4.31889\n",
      "Epoch 1110/2000, Training Loss: 32.68123, Validation Loss: 4.31764\n",
      "Epoch 1111/2000, Training Loss: 32.67476, Validation Loss: 4.31639\n",
      "Epoch 1112/2000, Training Loss: 32.66830, Validation Loss: 4.31515\n",
      "Epoch 1113/2000, Training Loss: 32.66185, Validation Loss: 4.31391\n",
      "Epoch 1114/2000, Training Loss: 32.65541, Validation Loss: 4.31266\n",
      "Epoch 1115/2000, Training Loss: 32.64898, Validation Loss: 4.31143\n",
      "Epoch 1116/2000, Training Loss: 32.64255, Validation Loss: 4.31019\n",
      "Epoch 1117/2000, Training Loss: 32.63613, Validation Loss: 4.30895\n",
      "Epoch 1118/2000, Training Loss: 32.62971, Validation Loss: 4.30772\n",
      "Epoch 1119/2000, Training Loss: 32.62330, Validation Loss: 4.30648\n",
      "Epoch 1120/2000, Training Loss: 32.61690, Validation Loss: 4.30525\n",
      "Epoch 1121/2000, Training Loss: 32.61050, Validation Loss: 4.30402\n",
      "Epoch 1122/2000, Training Loss: 32.60410, Validation Loss: 4.30279\n",
      "Epoch 1123/2000, Training Loss: 32.59771, Validation Loss: 4.30156\n",
      "Epoch 1124/2000, Training Loss: 32.59132, Validation Loss: 4.30033\n",
      "Epoch 1125/2000, Training Loss: 32.58494, Validation Loss: 4.29911\n",
      "Epoch 1126/2000, Training Loss: 32.57856, Validation Loss: 4.29788\n",
      "Epoch 1127/2000, Training Loss: 32.57219, Validation Loss: 4.29666\n",
      "Epoch 1128/2000, Training Loss: 32.56583, Validation Loss: 4.29544\n",
      "Epoch 1129/2000, Training Loss: 32.55947, Validation Loss: 4.29422\n",
      "Epoch 1130/2000, Training Loss: 32.55312, Validation Loss: 4.29300\n",
      "Epoch 1131/2000, Training Loss: 32.54678, Validation Loss: 4.29178\n",
      "Epoch 1132/2000, Training Loss: 32.54045, Validation Loss: 4.29057\n",
      "Epoch 1133/2000, Training Loss: 32.53412, Validation Loss: 4.28936\n",
      "Epoch 1134/2000, Training Loss: 32.52780, Validation Loss: 4.28814\n",
      "Epoch 1135/2000, Training Loss: 32.52149, Validation Loss: 4.28693\n",
      "Epoch 1136/2000, Training Loss: 32.51519, Validation Loss: 4.28573\n",
      "Epoch 1137/2000, Training Loss: 32.50890, Validation Loss: 4.28452\n",
      "Epoch 1138/2000, Training Loss: 32.50262, Validation Loss: 4.28331\n",
      "Epoch 1139/2000, Training Loss: 32.49633, Validation Loss: 4.28210\n",
      "Epoch 1140/2000, Training Loss: 32.49005, Validation Loss: 4.28089\n",
      "Epoch 1141/2000, Training Loss: 32.48376, Validation Loss: 4.27968\n",
      "Epoch 1142/2000, Training Loss: 32.47749, Validation Loss: 4.27848\n",
      "Epoch 1143/2000, Training Loss: 32.47123, Validation Loss: 4.27728\n",
      "Epoch 1144/2000, Training Loss: 32.46498, Validation Loss: 4.27607\n",
      "Epoch 1145/2000, Training Loss: 32.45874, Validation Loss: 4.27487\n",
      "Epoch 1146/2000, Training Loss: 32.45251, Validation Loss: 4.27368\n",
      "Epoch 1147/2000, Training Loss: 32.44628, Validation Loss: 4.27248\n",
      "Epoch 1148/2000, Training Loss: 32.44006, Validation Loss: 4.27129\n",
      "Epoch 1149/2000, Training Loss: 32.43384, Validation Loss: 4.27010\n",
      "Epoch 1150/2000, Training Loss: 32.42762, Validation Loss: 4.26891\n",
      "Epoch 1151/2000, Training Loss: 32.42141, Validation Loss: 4.26772\n",
      "Epoch 1152/2000, Training Loss: 32.41521, Validation Loss: 4.26653\n",
      "Epoch 1153/2000, Training Loss: 32.40901, Validation Loss: 4.26534\n",
      "Epoch 1154/2000, Training Loss: 32.40282, Validation Loss: 4.26415\n",
      "Epoch 1155/2000, Training Loss: 32.39665, Validation Loss: 4.26296\n",
      "Epoch 1156/2000, Training Loss: 32.39047, Validation Loss: 4.26178\n",
      "Epoch 1157/2000, Training Loss: 32.38431, Validation Loss: 4.26059\n",
      "Epoch 1158/2000, Training Loss: 32.37816, Validation Loss: 4.25941\n",
      "Epoch 1159/2000, Training Loss: 32.37201, Validation Loss: 4.25823\n",
      "Epoch 1160/2000, Training Loss: 32.36587, Validation Loss: 4.25705\n",
      "Epoch 1161/2000, Training Loss: 32.35974, Validation Loss: 4.25588\n",
      "Epoch 1162/2000, Training Loss: 32.35361, Validation Loss: 4.25470\n",
      "Epoch 1163/2000, Training Loss: 32.34748, Validation Loss: 4.25353\n",
      "Epoch 1164/2000, Training Loss: 32.34135, Validation Loss: 4.25235\n",
      "Epoch 1165/2000, Training Loss: 32.33524, Validation Loss: 4.25118\n",
      "Epoch 1166/2000, Training Loss: 32.32914, Validation Loss: 4.25001\n",
      "Epoch 1167/2000, Training Loss: 32.32304, Validation Loss: 4.24884\n",
      "Epoch 1168/2000, Training Loss: 32.31695, Validation Loss: 4.24768\n",
      "Epoch 1169/2000, Training Loss: 32.31088, Validation Loss: 4.24651\n",
      "Epoch 1170/2000, Training Loss: 32.30481, Validation Loss: 4.24535\n",
      "Epoch 1171/2000, Training Loss: 32.29875, Validation Loss: 4.24418\n",
      "Epoch 1172/2000, Training Loss: 32.29269, Validation Loss: 4.24302\n",
      "Epoch 1173/2000, Training Loss: 32.28664, Validation Loss: 4.24186\n",
      "Epoch 1174/2000, Training Loss: 32.28060, Validation Loss: 4.24070\n",
      "Epoch 1175/2000, Training Loss: 32.27456, Validation Loss: 4.23954\n",
      "Epoch 1176/2000, Training Loss: 32.26854, Validation Loss: 4.23839\n",
      "Epoch 1177/2000, Training Loss: 32.26252, Validation Loss: 4.23723\n",
      "Epoch 1178/2000, Training Loss: 32.25651, Validation Loss: 4.23608\n",
      "Epoch 1179/2000, Training Loss: 32.25050, Validation Loss: 4.23493\n",
      "Epoch 1180/2000, Training Loss: 32.24450, Validation Loss: 4.23378\n",
      "Epoch 1181/2000, Training Loss: 32.23851, Validation Loss: 4.23263\n",
      "Epoch 1182/2000, Training Loss: 32.23251, Validation Loss: 4.23148\n",
      "Epoch 1183/2000, Training Loss: 32.22652, Validation Loss: 4.23034\n",
      "Epoch 1184/2000, Training Loss: 32.22054, Validation Loss: 4.22920\n",
      "Epoch 1185/2000, Training Loss: 32.21456, Validation Loss: 4.22806\n",
      "Epoch 1186/2000, Training Loss: 32.20859, Validation Loss: 4.22692\n",
      "Epoch 1187/2000, Training Loss: 32.20263, Validation Loss: 4.22578\n",
      "Epoch 1188/2000, Training Loss: 32.19667, Validation Loss: 4.22464\n",
      "Epoch 1189/2000, Training Loss: 32.19072, Validation Loss: 4.22350\n",
      "Epoch 1190/2000, Training Loss: 32.18477, Validation Loss: 4.22237\n",
      "Epoch 1191/2000, Training Loss: 32.17882, Validation Loss: 4.22124\n",
      "Epoch 1192/2000, Training Loss: 32.17289, Validation Loss: 4.22010\n",
      "Epoch 1193/2000, Training Loss: 32.16695, Validation Loss: 4.21897\n",
      "Epoch 1194/2000, Training Loss: 32.16102, Validation Loss: 4.21785\n",
      "Epoch 1195/2000, Training Loss: 32.15510, Validation Loss: 4.21672\n",
      "Epoch 1196/2000, Training Loss: 32.14919, Validation Loss: 4.21559\n",
      "Epoch 1197/2000, Training Loss: 32.14328, Validation Loss: 4.21447\n",
      "Epoch 1198/2000, Training Loss: 32.13738, Validation Loss: 4.21335\n",
      "Epoch 1199/2000, Training Loss: 32.13150, Validation Loss: 4.21223\n",
      "Epoch 1200/2000, Training Loss: 32.12562, Validation Loss: 4.21111\n",
      "Epoch 1201/2000, Training Loss: 32.11974, Validation Loss: 4.20999\n",
      "Epoch 1202/2000, Training Loss: 32.11388, Validation Loss: 4.20888\n",
      "Epoch 1203/2000, Training Loss: 32.10802, Validation Loss: 4.20776\n",
      "Epoch 1204/2000, Training Loss: 32.10217, Validation Loss: 4.20665\n",
      "Epoch 1205/2000, Training Loss: 32.09632, Validation Loss: 4.20554\n",
      "Epoch 1206/2000, Training Loss: 32.09048, Validation Loss: 4.20443\n",
      "Epoch 1207/2000, Training Loss: 32.08465, Validation Loss: 4.20332\n",
      "Epoch 1208/2000, Training Loss: 32.07883, Validation Loss: 4.20221\n",
      "Epoch 1209/2000, Training Loss: 32.07302, Validation Loss: 4.20111\n",
      "Epoch 1210/2000, Training Loss: 32.06721, Validation Loss: 4.20000\n",
      "Epoch 1211/2000, Training Loss: 32.06141, Validation Loss: 4.19890\n",
      "Epoch 1212/2000, Training Loss: 32.05561, Validation Loss: 4.19780\n",
      "Epoch 1213/2000, Training Loss: 32.04982, Validation Loss: 4.19671\n",
      "Epoch 1214/2000, Training Loss: 32.04404, Validation Loss: 4.19561\n",
      "Epoch 1215/2000, Training Loss: 32.03825, Validation Loss: 4.19452\n",
      "Epoch 1216/2000, Training Loss: 32.03248, Validation Loss: 4.19342\n",
      "Epoch 1217/2000, Training Loss: 32.02671, Validation Loss: 4.19233\n",
      "Epoch 1218/2000, Training Loss: 32.02095, Validation Loss: 4.19124\n",
      "Epoch 1219/2000, Training Loss: 32.01519, Validation Loss: 4.19016\n",
      "Epoch 1220/2000, Training Loss: 32.00945, Validation Loss: 4.18907\n",
      "Epoch 1221/2000, Training Loss: 32.00371, Validation Loss: 4.18799\n",
      "Epoch 1222/2000, Training Loss: 31.99798, Validation Loss: 4.18691\n",
      "Epoch 1223/2000, Training Loss: 31.99225, Validation Loss: 4.18583\n",
      "Epoch 1224/2000, Training Loss: 31.98654, Validation Loss: 4.18475\n",
      "Epoch 1225/2000, Training Loss: 31.98083, Validation Loss: 4.18368\n",
      "Epoch 1226/2000, Training Loss: 31.97513, Validation Loss: 4.18260\n",
      "Epoch 1227/2000, Training Loss: 31.96944, Validation Loss: 4.18153\n",
      "Epoch 1228/2000, Training Loss: 31.96375, Validation Loss: 4.18046\n",
      "Epoch 1229/2000, Training Loss: 31.95806, Validation Loss: 4.17939\n",
      "Epoch 1230/2000, Training Loss: 31.95238, Validation Loss: 4.17832\n",
      "Epoch 1231/2000, Training Loss: 31.94671, Validation Loss: 4.17726\n",
      "Epoch 1232/2000, Training Loss: 31.94105, Validation Loss: 4.17619\n",
      "Epoch 1233/2000, Training Loss: 31.93539, Validation Loss: 4.17513\n",
      "Epoch 1234/2000, Training Loss: 31.92974, Validation Loss: 4.17407\n",
      "Epoch 1235/2000, Training Loss: 31.92409, Validation Loss: 4.17301\n",
      "Epoch 1236/2000, Training Loss: 31.91846, Validation Loss: 4.17195\n",
      "Epoch 1237/2000, Training Loss: 31.91283, Validation Loss: 4.17090\n",
      "Epoch 1238/2000, Training Loss: 31.90721, Validation Loss: 4.16984\n",
      "Epoch 1239/2000, Training Loss: 31.90160, Validation Loss: 4.16879\n",
      "Epoch 1240/2000, Training Loss: 31.89600, Validation Loss: 4.16774\n",
      "Epoch 1241/2000, Training Loss: 31.89040, Validation Loss: 4.16669\n",
      "Epoch 1242/2000, Training Loss: 31.88481, Validation Loss: 4.16565\n",
      "Epoch 1243/2000, Training Loss: 31.87923, Validation Loss: 4.16460\n",
      "Epoch 1244/2000, Training Loss: 31.87365, Validation Loss: 4.16356\n",
      "Epoch 1245/2000, Training Loss: 31.86807, Validation Loss: 4.16252\n",
      "Epoch 1246/2000, Training Loss: 31.86250, Validation Loss: 4.16148\n",
      "Epoch 1247/2000, Training Loss: 31.85693, Validation Loss: 4.16044\n",
      "Epoch 1248/2000, Training Loss: 31.85138, Validation Loss: 4.15940\n",
      "Epoch 1249/2000, Training Loss: 31.84583, Validation Loss: 4.15837\n",
      "Epoch 1250/2000, Training Loss: 31.84029, Validation Loss: 4.15734\n",
      "Epoch 1251/2000, Training Loss: 31.83477, Validation Loss: 4.15631\n",
      "Epoch 1252/2000, Training Loss: 31.82924, Validation Loss: 4.15528\n",
      "Epoch 1253/2000, Training Loss: 31.82373, Validation Loss: 4.15425\n",
      "Epoch 1254/2000, Training Loss: 31.81822, Validation Loss: 4.15322\n",
      "Epoch 1255/2000, Training Loss: 31.81271, Validation Loss: 4.15219\n",
      "Epoch 1256/2000, Training Loss: 31.80721, Validation Loss: 4.15117\n",
      "Epoch 1257/2000, Training Loss: 31.80172, Validation Loss: 4.15015\n",
      "Epoch 1258/2000, Training Loss: 31.79624, Validation Loss: 4.14913\n",
      "Epoch 1259/2000, Training Loss: 31.79077, Validation Loss: 4.14811\n",
      "Epoch 1260/2000, Training Loss: 31.78530, Validation Loss: 4.14709\n",
      "Epoch 1261/2000, Training Loss: 31.77984, Validation Loss: 4.14607\n",
      "Epoch 1262/2000, Training Loss: 31.77439, Validation Loss: 4.14506\n",
      "Epoch 1263/2000, Training Loss: 31.76895, Validation Loss: 4.14404\n",
      "Epoch 1264/2000, Training Loss: 31.76351, Validation Loss: 4.14303\n",
      "Epoch 1265/2000, Training Loss: 31.75806, Validation Loss: 4.14203\n",
      "Epoch 1266/2000, Training Loss: 31.75263, Validation Loss: 4.14102\n",
      "Epoch 1267/2000, Training Loss: 31.74720, Validation Loss: 4.14002\n",
      "Epoch 1268/2000, Training Loss: 31.74178, Validation Loss: 4.13902\n",
      "Epoch 1269/2000, Training Loss: 31.73637, Validation Loss: 4.13801\n",
      "Epoch 1270/2000, Training Loss: 31.73097, Validation Loss: 4.13702\n",
      "Epoch 1271/2000, Training Loss: 31.72558, Validation Loss: 4.13602\n",
      "Epoch 1272/2000, Training Loss: 31.72019, Validation Loss: 4.13503\n",
      "Epoch 1273/2000, Training Loss: 31.71480, Validation Loss: 4.13404\n",
      "Epoch 1274/2000, Training Loss: 31.70942, Validation Loss: 4.13305\n",
      "Epoch 1275/2000, Training Loss: 31.70405, Validation Loss: 4.13206\n",
      "Epoch 1276/2000, Training Loss: 31.69868, Validation Loss: 4.13107\n",
      "Epoch 1277/2000, Training Loss: 31.69331, Validation Loss: 4.13009\n",
      "Epoch 1278/2000, Training Loss: 31.68795, Validation Loss: 4.12910\n",
      "Epoch 1279/2000, Training Loss: 31.68260, Validation Loss: 4.12812\n",
      "Epoch 1280/2000, Training Loss: 31.67726, Validation Loss: 4.12714\n",
      "Epoch 1281/2000, Training Loss: 31.67192, Validation Loss: 4.12616\n",
      "Epoch 1282/2000, Training Loss: 31.66660, Validation Loss: 4.12518\n",
      "Epoch 1283/2000, Training Loss: 31.66128, Validation Loss: 4.12421\n",
      "Epoch 1284/2000, Training Loss: 31.65597, Validation Loss: 4.12323\n",
      "Epoch 1285/2000, Training Loss: 31.65067, Validation Loss: 4.12226\n",
      "Epoch 1286/2000, Training Loss: 31.64537, Validation Loss: 4.12129\n",
      "Epoch 1287/2000, Training Loss: 31.64008, Validation Loss: 4.12032\n",
      "Epoch 1288/2000, Training Loss: 31.63480, Validation Loss: 4.11935\n",
      "Epoch 1289/2000, Training Loss: 31.62952, Validation Loss: 4.11838\n",
      "Epoch 1290/2000, Training Loss: 31.62425, Validation Loss: 4.11742\n",
      "Epoch 1291/2000, Training Loss: 31.61899, Validation Loss: 4.11646\n",
      "Epoch 1292/2000, Training Loss: 31.61373, Validation Loss: 4.11550\n",
      "Epoch 1293/2000, Training Loss: 31.60848, Validation Loss: 4.11454\n",
      "Epoch 1294/2000, Training Loss: 31.60324, Validation Loss: 4.11358\n",
      "Epoch 1295/2000, Training Loss: 31.59800, Validation Loss: 4.11262\n",
      "Epoch 1296/2000, Training Loss: 31.59277, Validation Loss: 4.11166\n",
      "Epoch 1297/2000, Training Loss: 31.58755, Validation Loss: 4.11071\n",
      "Epoch 1298/2000, Training Loss: 31.58232, Validation Loss: 4.10975\n",
      "Epoch 1299/2000, Training Loss: 31.57711, Validation Loss: 4.10880\n",
      "Epoch 1300/2000, Training Loss: 31.57190, Validation Loss: 4.10785\n",
      "Epoch 1301/2000, Training Loss: 31.56669, Validation Loss: 4.10690\n",
      "Epoch 1302/2000, Training Loss: 31.56149, Validation Loss: 4.10595\n",
      "Epoch 1303/2000, Training Loss: 31.55629, Validation Loss: 4.10501\n",
      "Epoch 1304/2000, Training Loss: 31.55110, Validation Loss: 4.10406\n",
      "Epoch 1305/2000, Training Loss: 31.54593, Validation Loss: 4.10311\n",
      "Epoch 1306/2000, Training Loss: 31.54076, Validation Loss: 4.10217\n",
      "Epoch 1307/2000, Training Loss: 31.53560, Validation Loss: 4.10122\n",
      "Epoch 1308/2000, Training Loss: 31.53045, Validation Loss: 4.10028\n",
      "Epoch 1309/2000, Training Loss: 31.52531, Validation Loss: 4.09934\n",
      "Epoch 1310/2000, Training Loss: 31.52018, Validation Loss: 4.09840\n",
      "Epoch 1311/2000, Training Loss: 31.51506, Validation Loss: 4.09746\n",
      "Epoch 1312/2000, Training Loss: 31.50995, Validation Loss: 4.09653\n",
      "Epoch 1313/2000, Training Loss: 31.50484, Validation Loss: 4.09559\n",
      "Epoch 1314/2000, Training Loss: 31.49974, Validation Loss: 4.09466\n",
      "Epoch 1315/2000, Training Loss: 31.49466, Validation Loss: 4.09373\n",
      "Epoch 1316/2000, Training Loss: 31.48958, Validation Loss: 4.09280\n",
      "Epoch 1317/2000, Training Loss: 31.48451, Validation Loss: 4.09187\n",
      "Epoch 1318/2000, Training Loss: 31.47945, Validation Loss: 4.09095\n",
      "Epoch 1319/2000, Training Loss: 31.47440, Validation Loss: 4.09003\n",
      "Epoch 1320/2000, Training Loss: 31.46936, Validation Loss: 4.08911\n",
      "Epoch 1321/2000, Training Loss: 31.46433, Validation Loss: 4.08819\n",
      "Epoch 1322/2000, Training Loss: 31.45931, Validation Loss: 4.08727\n",
      "Epoch 1323/2000, Training Loss: 31.45430, Validation Loss: 4.08636\n",
      "Epoch 1324/2000, Training Loss: 31.44929, Validation Loss: 4.08544\n",
      "Epoch 1325/2000, Training Loss: 31.44429, Validation Loss: 4.08453\n",
      "Epoch 1326/2000, Training Loss: 31.43929, Validation Loss: 4.08362\n",
      "Epoch 1327/2000, Training Loss: 31.43429, Validation Loss: 4.08272\n",
      "Epoch 1328/2000, Training Loss: 31.42931, Validation Loss: 4.08181\n",
      "Epoch 1329/2000, Training Loss: 31.42434, Validation Loss: 4.08090\n",
      "Epoch 1330/2000, Training Loss: 31.41938, Validation Loss: 4.08000\n",
      "Epoch 1331/2000, Training Loss: 31.41443, Validation Loss: 4.07910\n",
      "Epoch 1332/2000, Training Loss: 31.40949, Validation Loss: 4.07820\n",
      "Epoch 1333/2000, Training Loss: 31.40455, Validation Loss: 4.07730\n",
      "Epoch 1334/2000, Training Loss: 31.39962, Validation Loss: 4.07640\n",
      "Epoch 1335/2000, Training Loss: 31.39470, Validation Loss: 4.07551\n",
      "Epoch 1336/2000, Training Loss: 31.38979, Validation Loss: 4.07461\n",
      "Epoch 1337/2000, Training Loss: 31.38488, Validation Loss: 4.07372\n",
      "Epoch 1338/2000, Training Loss: 31.37998, Validation Loss: 4.07283\n",
      "Epoch 1339/2000, Training Loss: 31.37509, Validation Loss: 4.07193\n",
      "Epoch 1340/2000, Training Loss: 31.37021, Validation Loss: 4.07104\n",
      "Epoch 1341/2000, Training Loss: 31.36534, Validation Loss: 4.07015\n",
      "Epoch 1342/2000, Training Loss: 31.36047, Validation Loss: 4.06926\n",
      "Epoch 1343/2000, Training Loss: 31.35561, Validation Loss: 4.06838\n",
      "Epoch 1344/2000, Training Loss: 31.35075, Validation Loss: 4.06749\n",
      "Epoch 1345/2000, Training Loss: 31.34590, Validation Loss: 4.06661\n",
      "Epoch 1346/2000, Training Loss: 31.34106, Validation Loss: 4.06573\n",
      "Epoch 1347/2000, Training Loss: 31.33623, Validation Loss: 4.06485\n",
      "Epoch 1348/2000, Training Loss: 31.33141, Validation Loss: 4.06398\n",
      "Epoch 1349/2000, Training Loss: 31.32659, Validation Loss: 4.06310\n",
      "Epoch 1350/2000, Training Loss: 31.32178, Validation Loss: 4.06223\n",
      "Epoch 1351/2000, Training Loss: 31.31698, Validation Loss: 4.06136\n",
      "Epoch 1352/2000, Training Loss: 31.31219, Validation Loss: 4.06049\n",
      "Epoch 1353/2000, Training Loss: 31.30740, Validation Loss: 4.05962\n",
      "Epoch 1354/2000, Training Loss: 31.30262, Validation Loss: 4.05875\n",
      "Epoch 1355/2000, Training Loss: 31.29784, Validation Loss: 4.05789\n",
      "Epoch 1356/2000, Training Loss: 31.29308, Validation Loss: 4.05702\n",
      "Epoch 1357/2000, Training Loss: 31.28831, Validation Loss: 4.05616\n",
      "Epoch 1358/2000, Training Loss: 31.28356, Validation Loss: 4.05530\n",
      "Epoch 1359/2000, Training Loss: 31.27880, Validation Loss: 4.05444\n",
      "Epoch 1360/2000, Training Loss: 31.27406, Validation Loss: 4.05358\n",
      "Epoch 1361/2000, Training Loss: 31.26932, Validation Loss: 4.05272\n",
      "Epoch 1362/2000, Training Loss: 31.26459, Validation Loss: 4.05186\n",
      "Epoch 1363/2000, Training Loss: 31.25986, Validation Loss: 4.05101\n",
      "Epoch 1364/2000, Training Loss: 31.25514, Validation Loss: 4.05015\n",
      "Epoch 1365/2000, Training Loss: 31.25043, Validation Loss: 4.04930\n",
      "Epoch 1366/2000, Training Loss: 31.24573, Validation Loss: 4.04845\n",
      "Epoch 1367/2000, Training Loss: 31.24103, Validation Loss: 4.04760\n",
      "Epoch 1368/2000, Training Loss: 31.23633, Validation Loss: 4.04676\n",
      "Epoch 1369/2000, Training Loss: 31.23165, Validation Loss: 4.04592\n",
      "Epoch 1370/2000, Training Loss: 31.22696, Validation Loss: 4.04507\n",
      "Epoch 1371/2000, Training Loss: 31.22229, Validation Loss: 4.04424\n",
      "Epoch 1372/2000, Training Loss: 31.21762, Validation Loss: 4.04340\n",
      "Epoch 1373/2000, Training Loss: 31.21297, Validation Loss: 4.04256\n",
      "Epoch 1374/2000, Training Loss: 31.20832, Validation Loss: 4.04173\n",
      "Epoch 1375/2000, Training Loss: 31.20367, Validation Loss: 4.04089\n",
      "Epoch 1376/2000, Training Loss: 31.19904, Validation Loss: 4.04006\n",
      "Epoch 1377/2000, Training Loss: 31.19441, Validation Loss: 4.03924\n",
      "Epoch 1378/2000, Training Loss: 31.18979, Validation Loss: 4.03841\n",
      "Epoch 1379/2000, Training Loss: 31.18517, Validation Loss: 4.03758\n",
      "Epoch 1380/2000, Training Loss: 31.18057, Validation Loss: 4.03676\n",
      "Epoch 1381/2000, Training Loss: 31.17597, Validation Loss: 4.03594\n",
      "Epoch 1382/2000, Training Loss: 31.17138, Validation Loss: 4.03512\n",
      "Epoch 1383/2000, Training Loss: 31.16680, Validation Loss: 4.03430\n",
      "Epoch 1384/2000, Training Loss: 31.16223, Validation Loss: 4.03348\n",
      "Epoch 1385/2000, Training Loss: 31.15767, Validation Loss: 4.03266\n",
      "Epoch 1386/2000, Training Loss: 31.15311, Validation Loss: 4.03185\n",
      "Epoch 1387/2000, Training Loss: 31.14857, Validation Loss: 4.03103\n",
      "Epoch 1388/2000, Training Loss: 31.14403, Validation Loss: 4.03022\n",
      "Epoch 1389/2000, Training Loss: 31.13950, Validation Loss: 4.02941\n",
      "Epoch 1390/2000, Training Loss: 31.13499, Validation Loss: 4.02860\n",
      "Epoch 1391/2000, Training Loss: 31.13047, Validation Loss: 4.02779\n",
      "Epoch 1392/2000, Training Loss: 31.12597, Validation Loss: 4.02698\n",
      "Epoch 1393/2000, Training Loss: 31.12147, Validation Loss: 4.02618\n",
      "Epoch 1394/2000, Training Loss: 31.11698, Validation Loss: 4.02537\n",
      "Epoch 1395/2000, Training Loss: 31.11249, Validation Loss: 4.02456\n",
      "Epoch 1396/2000, Training Loss: 31.10802, Validation Loss: 4.02376\n",
      "Epoch 1397/2000, Training Loss: 31.10356, Validation Loss: 4.02295\n",
      "Epoch 1398/2000, Training Loss: 31.09910, Validation Loss: 4.02215\n",
      "Epoch 1399/2000, Training Loss: 31.09465, Validation Loss: 4.02134\n",
      "Epoch 1400/2000, Training Loss: 31.09021, Validation Loss: 4.02054\n",
      "Epoch 1401/2000, Training Loss: 31.08578, Validation Loss: 4.01974\n",
      "Epoch 1402/2000, Training Loss: 31.08136, Validation Loss: 4.01895\n",
      "Epoch 1403/2000, Training Loss: 31.07695, Validation Loss: 4.01815\n",
      "Epoch 1404/2000, Training Loss: 31.07254, Validation Loss: 4.01736\n",
      "Epoch 1405/2000, Training Loss: 31.06814, Validation Loss: 4.01656\n",
      "Epoch 1406/2000, Training Loss: 31.06375, Validation Loss: 4.01577\n",
      "Epoch 1407/2000, Training Loss: 31.05936, Validation Loss: 4.01498\n",
      "Epoch 1408/2000, Training Loss: 31.05498, Validation Loss: 4.01419\n",
      "Epoch 1409/2000, Training Loss: 31.05061, Validation Loss: 4.01341\n",
      "Epoch 1410/2000, Training Loss: 31.04624, Validation Loss: 4.01262\n",
      "Epoch 1411/2000, Training Loss: 31.04188, Validation Loss: 4.01184\n",
      "Epoch 1412/2000, Training Loss: 31.03752, Validation Loss: 4.01106\n",
      "Epoch 1413/2000, Training Loss: 31.03318, Validation Loss: 4.01028\n",
      "Epoch 1414/2000, Training Loss: 31.02884, Validation Loss: 4.00950\n",
      "Epoch 1415/2000, Training Loss: 31.02451, Validation Loss: 4.00872\n",
      "Epoch 1416/2000, Training Loss: 31.02018, Validation Loss: 4.00794\n",
      "Epoch 1417/2000, Training Loss: 31.01587, Validation Loss: 4.00717\n",
      "Epoch 1418/2000, Training Loss: 31.01156, Validation Loss: 4.00640\n",
      "Epoch 1419/2000, Training Loss: 31.00725, Validation Loss: 4.00563\n",
      "Epoch 1420/2000, Training Loss: 31.00296, Validation Loss: 4.00486\n",
      "Epoch 1421/2000, Training Loss: 30.99867, Validation Loss: 4.00409\n",
      "Epoch 1422/2000, Training Loss: 30.99438, Validation Loss: 4.00332\n",
      "Epoch 1423/2000, Training Loss: 30.99010, Validation Loss: 4.00255\n",
      "Epoch 1424/2000, Training Loss: 30.98583, Validation Loss: 4.00179\n",
      "Epoch 1425/2000, Training Loss: 30.98157, Validation Loss: 4.00102\n",
      "Epoch 1426/2000, Training Loss: 30.97731, Validation Loss: 4.00026\n",
      "Epoch 1427/2000, Training Loss: 30.97306, Validation Loss: 3.99951\n",
      "Epoch 1428/2000, Training Loss: 30.96881, Validation Loss: 3.99875\n",
      "Epoch 1429/2000, Training Loss: 30.96457, Validation Loss: 3.99799\n",
      "Epoch 1430/2000, Training Loss: 30.96034, Validation Loss: 3.99724\n",
      "Epoch 1431/2000, Training Loss: 30.95612, Validation Loss: 3.99649\n",
      "Epoch 1432/2000, Training Loss: 30.95190, Validation Loss: 3.99574\n",
      "Epoch 1433/2000, Training Loss: 30.94769, Validation Loss: 3.99499\n",
      "Epoch 1434/2000, Training Loss: 30.94348, Validation Loss: 3.99425\n",
      "Epoch 1435/2000, Training Loss: 30.93928, Validation Loss: 3.99350\n",
      "Epoch 1436/2000, Training Loss: 30.93508, Validation Loss: 3.99276\n",
      "Epoch 1437/2000, Training Loss: 30.93089, Validation Loss: 3.99201\n",
      "Epoch 1438/2000, Training Loss: 30.92670, Validation Loss: 3.99127\n",
      "Epoch 1439/2000, Training Loss: 30.92252, Validation Loss: 3.99053\n",
      "Epoch 1440/2000, Training Loss: 30.91836, Validation Loss: 3.98979\n",
      "Epoch 1441/2000, Training Loss: 30.91420, Validation Loss: 3.98906\n",
      "Epoch 1442/2000, Training Loss: 30.91005, Validation Loss: 3.98832\n",
      "Epoch 1443/2000, Training Loss: 30.90590, Validation Loss: 3.98758\n",
      "Epoch 1444/2000, Training Loss: 30.90176, Validation Loss: 3.98685\n",
      "Epoch 1445/2000, Training Loss: 30.89762, Validation Loss: 3.98612\n",
      "Epoch 1446/2000, Training Loss: 30.89349, Validation Loss: 3.98539\n",
      "Epoch 1447/2000, Training Loss: 30.88937, Validation Loss: 3.98466\n",
      "Epoch 1448/2000, Training Loss: 30.88526, Validation Loss: 3.98393\n",
      "Epoch 1449/2000, Training Loss: 30.88115, Validation Loss: 3.98320\n",
      "Epoch 1450/2000, Training Loss: 30.87705, Validation Loss: 3.98247\n",
      "Epoch 1451/2000, Training Loss: 30.87296, Validation Loss: 3.98174\n",
      "Epoch 1452/2000, Training Loss: 30.86887, Validation Loss: 3.98102\n",
      "Epoch 1453/2000, Training Loss: 30.86479, Validation Loss: 3.98030\n",
      "Epoch 1454/2000, Training Loss: 30.86072, Validation Loss: 3.97958\n",
      "Epoch 1455/2000, Training Loss: 30.85666, Validation Loss: 3.97886\n",
      "Epoch 1456/2000, Training Loss: 30.85260, Validation Loss: 3.97814\n",
      "Epoch 1457/2000, Training Loss: 30.84854, Validation Loss: 3.97742\n",
      "Epoch 1458/2000, Training Loss: 30.84450, Validation Loss: 3.97671\n",
      "Epoch 1459/2000, Training Loss: 30.84046, Validation Loss: 3.97600\n",
      "Epoch 1460/2000, Training Loss: 30.83643, Validation Loss: 3.97529\n",
      "Epoch 1461/2000, Training Loss: 30.83241, Validation Loss: 3.97458\n",
      "Epoch 1462/2000, Training Loss: 30.82840, Validation Loss: 3.97388\n",
      "Epoch 1463/2000, Training Loss: 30.82439, Validation Loss: 3.97317\n",
      "Epoch 1464/2000, Training Loss: 30.82040, Validation Loss: 3.97247\n",
      "Epoch 1465/2000, Training Loss: 30.81641, Validation Loss: 3.97177\n",
      "Epoch 1466/2000, Training Loss: 30.81243, Validation Loss: 3.97107\n",
      "Epoch 1467/2000, Training Loss: 30.80846, Validation Loss: 3.97038\n",
      "Epoch 1468/2000, Training Loss: 30.80449, Validation Loss: 3.96968\n",
      "Epoch 1469/2000, Training Loss: 30.80053, Validation Loss: 3.96899\n",
      "Epoch 1470/2000, Training Loss: 30.79658, Validation Loss: 3.96829\n",
      "Epoch 1471/2000, Training Loss: 30.79264, Validation Loss: 3.96760\n",
      "Epoch 1472/2000, Training Loss: 30.78870, Validation Loss: 3.96691\n",
      "Epoch 1473/2000, Training Loss: 30.78478, Validation Loss: 3.96622\n",
      "Epoch 1474/2000, Training Loss: 30.78086, Validation Loss: 3.96553\n",
      "Epoch 1475/2000, Training Loss: 30.77695, Validation Loss: 3.96485\n",
      "Epoch 1476/2000, Training Loss: 30.77306, Validation Loss: 3.96416\n",
      "Epoch 1477/2000, Training Loss: 30.76917, Validation Loss: 3.96347\n",
      "Epoch 1478/2000, Training Loss: 30.76528, Validation Loss: 3.96279\n",
      "Epoch 1479/2000, Training Loss: 30.76140, Validation Loss: 3.96211\n",
      "Epoch 1480/2000, Training Loss: 30.75751, Validation Loss: 3.96143\n",
      "Epoch 1481/2000, Training Loss: 30.75364, Validation Loss: 3.96075\n",
      "Epoch 1482/2000, Training Loss: 30.74977, Validation Loss: 3.96007\n",
      "Epoch 1483/2000, Training Loss: 30.74591, Validation Loss: 3.95940\n",
      "Epoch 1484/2000, Training Loss: 30.74206, Validation Loss: 3.95872\n",
      "Epoch 1485/2000, Training Loss: 30.73822, Validation Loss: 3.95805\n",
      "Epoch 1486/2000, Training Loss: 30.73438, Validation Loss: 3.95737\n",
      "Epoch 1487/2000, Training Loss: 30.73055, Validation Loss: 3.95670\n",
      "Epoch 1488/2000, Training Loss: 30.72672, Validation Loss: 3.95604\n",
      "Epoch 1489/2000, Training Loss: 30.72290, Validation Loss: 3.95537\n",
      "Epoch 1490/2000, Training Loss: 30.71909, Validation Loss: 3.95471\n",
      "Epoch 1491/2000, Training Loss: 30.71528, Validation Loss: 3.95404\n",
      "Epoch 1492/2000, Training Loss: 30.71148, Validation Loss: 3.95338\n",
      "Epoch 1493/2000, Training Loss: 30.70768, Validation Loss: 3.95272\n",
      "Epoch 1494/2000, Training Loss: 30.70389, Validation Loss: 3.95206\n",
      "Epoch 1495/2000, Training Loss: 30.70010, Validation Loss: 3.95140\n",
      "Epoch 1496/2000, Training Loss: 30.69632, Validation Loss: 3.95074\n",
      "Epoch 1497/2000, Training Loss: 30.69254, Validation Loss: 3.95009\n",
      "Epoch 1498/2000, Training Loss: 30.68877, Validation Loss: 3.94943\n",
      "Epoch 1499/2000, Training Loss: 30.68501, Validation Loss: 3.94878\n",
      "Epoch 1500/2000, Training Loss: 30.68125, Validation Loss: 3.94812\n",
      "Epoch 1501/2000, Training Loss: 30.67750, Validation Loss: 3.94747\n",
      "Epoch 1502/2000, Training Loss: 30.67376, Validation Loss: 3.94682\n",
      "Epoch 1503/2000, Training Loss: 30.67002, Validation Loss: 3.94617\n",
      "Epoch 1504/2000, Training Loss: 30.66629, Validation Loss: 3.94552\n",
      "Epoch 1505/2000, Training Loss: 30.66256, Validation Loss: 3.94487\n",
      "Epoch 1506/2000, Training Loss: 30.65885, Validation Loss: 3.94422\n",
      "Epoch 1507/2000, Training Loss: 30.65514, Validation Loss: 3.94358\n",
      "Epoch 1508/2000, Training Loss: 30.65143, Validation Loss: 3.94294\n",
      "Epoch 1509/2000, Training Loss: 30.64773, Validation Loss: 3.94230\n",
      "Epoch 1510/2000, Training Loss: 30.64404, Validation Loss: 3.94166\n",
      "Epoch 1511/2000, Training Loss: 30.64035, Validation Loss: 3.94102\n",
      "Epoch 1512/2000, Training Loss: 30.63667, Validation Loss: 3.94038\n",
      "Epoch 1513/2000, Training Loss: 30.63300, Validation Loss: 3.93974\n",
      "Epoch 1514/2000, Training Loss: 30.62933, Validation Loss: 3.93911\n",
      "Epoch 1515/2000, Training Loss: 30.62566, Validation Loss: 3.93848\n",
      "Epoch 1516/2000, Training Loss: 30.62200, Validation Loss: 3.93785\n",
      "Epoch 1517/2000, Training Loss: 30.61835, Validation Loss: 3.93722\n",
      "Epoch 1518/2000, Training Loss: 30.61471, Validation Loss: 3.93659\n",
      "Epoch 1519/2000, Training Loss: 30.61107, Validation Loss: 3.93597\n",
      "Epoch 1520/2000, Training Loss: 30.60744, Validation Loss: 3.93534\n",
      "Epoch 1521/2000, Training Loss: 30.60382, Validation Loss: 3.93472\n",
      "Epoch 1522/2000, Training Loss: 30.60021, Validation Loss: 3.93409\n",
      "Epoch 1523/2000, Training Loss: 30.59661, Validation Loss: 3.93347\n",
      "Epoch 1524/2000, Training Loss: 30.59301, Validation Loss: 3.93285\n",
      "Epoch 1525/2000, Training Loss: 30.58942, Validation Loss: 3.93223\n",
      "Epoch 1526/2000, Training Loss: 30.58583, Validation Loss: 3.93162\n",
      "Epoch 1527/2000, Training Loss: 30.58225, Validation Loss: 3.93100\n",
      "Epoch 1528/2000, Training Loss: 30.57867, Validation Loss: 3.93038\n",
      "Epoch 1529/2000, Training Loss: 30.57510, Validation Loss: 3.92977\n",
      "Epoch 1530/2000, Training Loss: 30.57153, Validation Loss: 3.92916\n",
      "Epoch 1531/2000, Training Loss: 30.56798, Validation Loss: 3.92855\n",
      "Epoch 1532/2000, Training Loss: 30.56443, Validation Loss: 3.92794\n",
      "Epoch 1533/2000, Training Loss: 30.56088, Validation Loss: 3.92733\n",
      "Epoch 1534/2000, Training Loss: 30.55735, Validation Loss: 3.92672\n",
      "Epoch 1535/2000, Training Loss: 30.55382, Validation Loss: 3.92611\n",
      "Epoch 1536/2000, Training Loss: 30.55029, Validation Loss: 3.92550\n",
      "Epoch 1537/2000, Training Loss: 30.54678, Validation Loss: 3.92490\n",
      "Epoch 1538/2000, Training Loss: 30.54327, Validation Loss: 3.92429\n",
      "Epoch 1539/2000, Training Loss: 30.53976, Validation Loss: 3.92369\n",
      "Epoch 1540/2000, Training Loss: 30.53627, Validation Loss: 3.92309\n",
      "Epoch 1541/2000, Training Loss: 30.53278, Validation Loss: 3.92249\n",
      "Epoch 1542/2000, Training Loss: 30.52931, Validation Loss: 3.92190\n",
      "Epoch 1543/2000, Training Loss: 30.52584, Validation Loss: 3.92130\n",
      "Epoch 1544/2000, Training Loss: 30.52238, Validation Loss: 3.92070\n",
      "Epoch 1545/2000, Training Loss: 30.51893, Validation Loss: 3.92011\n",
      "Epoch 1546/2000, Training Loss: 30.51548, Validation Loss: 3.91952\n",
      "Epoch 1547/2000, Training Loss: 30.51204, Validation Loss: 3.91893\n",
      "Epoch 1548/2000, Training Loss: 30.50861, Validation Loss: 3.91834\n",
      "Epoch 1549/2000, Training Loss: 30.50519, Validation Loss: 3.91775\n",
      "Epoch 1550/2000, Training Loss: 30.50177, Validation Loss: 3.91717\n",
      "Epoch 1551/2000, Training Loss: 30.49835, Validation Loss: 3.91659\n",
      "Epoch 1552/2000, Training Loss: 30.49495, Validation Loss: 3.91600\n",
      "Epoch 1553/2000, Training Loss: 30.49155, Validation Loss: 3.91542\n",
      "Epoch 1554/2000, Training Loss: 30.48815, Validation Loss: 3.91484\n",
      "Epoch 1555/2000, Training Loss: 30.48476, Validation Loss: 3.91427\n",
      "Epoch 1556/2000, Training Loss: 30.48138, Validation Loss: 3.91369\n",
      "Epoch 1557/2000, Training Loss: 30.47800, Validation Loss: 3.91311\n",
      "Epoch 1558/2000, Training Loss: 30.47463, Validation Loss: 3.91254\n",
      "Epoch 1559/2000, Training Loss: 30.47127, Validation Loss: 3.91197\n",
      "Epoch 1560/2000, Training Loss: 30.46791, Validation Loss: 3.91140\n",
      "Epoch 1561/2000, Training Loss: 30.46456, Validation Loss: 3.91083\n",
      "Epoch 1562/2000, Training Loss: 30.46121, Validation Loss: 3.91026\n",
      "Epoch 1563/2000, Training Loss: 30.45787, Validation Loss: 3.90969\n",
      "Epoch 1564/2000, Training Loss: 30.45454, Validation Loss: 3.90912\n",
      "Epoch 1565/2000, Training Loss: 30.45121, Validation Loss: 3.90856\n",
      "Epoch 1566/2000, Training Loss: 30.44789, Validation Loss: 3.90800\n",
      "Epoch 1567/2000, Training Loss: 30.44457, Validation Loss: 3.90744\n",
      "Epoch 1568/2000, Training Loss: 30.44126, Validation Loss: 3.90688\n",
      "Epoch 1569/2000, Training Loss: 30.43795, Validation Loss: 3.90632\n",
      "Epoch 1570/2000, Training Loss: 30.43465, Validation Loss: 3.90576\n",
      "Epoch 1571/2000, Training Loss: 30.43135, Validation Loss: 3.90521\n",
      "Epoch 1572/2000, Training Loss: 30.42806, Validation Loss: 3.90466\n",
      "Epoch 1573/2000, Training Loss: 30.42477, Validation Loss: 3.90412\n",
      "Epoch 1574/2000, Training Loss: 30.42149, Validation Loss: 3.90357\n",
      "Epoch 1575/2000, Training Loss: 30.41821, Validation Loss: 3.90303\n",
      "Epoch 1576/2000, Training Loss: 30.41493, Validation Loss: 3.90249\n",
      "Epoch 1577/2000, Training Loss: 30.41166, Validation Loss: 3.90196\n",
      "Epoch 1578/2000, Training Loss: 30.40840, Validation Loss: 3.90142\n",
      "Epoch 1579/2000, Training Loss: 30.40514, Validation Loss: 3.90088\n",
      "Epoch 1580/2000, Training Loss: 30.40188, Validation Loss: 3.90034\n",
      "Epoch 1581/2000, Training Loss: 30.39863, Validation Loss: 3.89980\n",
      "Epoch 1582/2000, Training Loss: 30.39538, Validation Loss: 3.89926\n",
      "Epoch 1583/2000, Training Loss: 30.39214, Validation Loss: 3.89873\n",
      "Epoch 1584/2000, Training Loss: 30.38890, Validation Loss: 3.89820\n",
      "Epoch 1585/2000, Training Loss: 30.38567, Validation Loss: 3.89766\n",
      "Epoch 1586/2000, Training Loss: 30.38245, Validation Loss: 3.89713\n",
      "Epoch 1587/2000, Training Loss: 30.37923, Validation Loss: 3.89660\n",
      "Epoch 1588/2000, Training Loss: 30.37601, Validation Loss: 3.89607\n",
      "Epoch 1589/2000, Training Loss: 30.37280, Validation Loss: 3.89554\n",
      "Epoch 1590/2000, Training Loss: 30.36959, Validation Loss: 3.89501\n",
      "Epoch 1591/2000, Training Loss: 30.36638, Validation Loss: 3.89449\n",
      "Epoch 1592/2000, Training Loss: 30.36318, Validation Loss: 3.89396\n",
      "Epoch 1593/2000, Training Loss: 30.35998, Validation Loss: 3.89344\n",
      "Epoch 1594/2000, Training Loss: 30.35679, Validation Loss: 3.89291\n",
      "Epoch 1595/2000, Training Loss: 30.35360, Validation Loss: 3.89239\n",
      "Epoch 1596/2000, Training Loss: 30.35041, Validation Loss: 3.89187\n",
      "Epoch 1597/2000, Training Loss: 30.34723, Validation Loss: 3.89135\n",
      "Epoch 1598/2000, Training Loss: 30.34406, Validation Loss: 3.89083\n",
      "Epoch 1599/2000, Training Loss: 30.34089, Validation Loss: 3.89032\n",
      "Epoch 1600/2000, Training Loss: 30.33772, Validation Loss: 3.88980\n",
      "Epoch 1601/2000, Training Loss: 30.33457, Validation Loss: 3.88929\n",
      "Epoch 1602/2000, Training Loss: 30.33142, Validation Loss: 3.88877\n",
      "Epoch 1603/2000, Training Loss: 30.32828, Validation Loss: 3.88826\n",
      "Epoch 1604/2000, Training Loss: 30.32514, Validation Loss: 3.88775\n",
      "Epoch 1605/2000, Training Loss: 30.32201, Validation Loss: 3.88723\n",
      "Epoch 1606/2000, Training Loss: 30.31889, Validation Loss: 3.88672\n",
      "Epoch 1607/2000, Training Loss: 30.31577, Validation Loss: 3.88621\n",
      "Epoch 1608/2000, Training Loss: 30.31265, Validation Loss: 3.88570\n",
      "Epoch 1609/2000, Training Loss: 30.30954, Validation Loss: 3.88520\n",
      "Epoch 1610/2000, Training Loss: 30.30643, Validation Loss: 3.88469\n",
      "Epoch 1611/2000, Training Loss: 30.30333, Validation Loss: 3.88418\n",
      "Epoch 1612/2000, Training Loss: 30.30023, Validation Loss: 3.88368\n",
      "Epoch 1613/2000, Training Loss: 30.29713, Validation Loss: 3.88318\n",
      "Epoch 1614/2000, Training Loss: 30.29404, Validation Loss: 3.88267\n",
      "Epoch 1615/2000, Training Loss: 30.29096, Validation Loss: 3.88217\n",
      "Epoch 1616/2000, Training Loss: 30.28788, Validation Loss: 3.88168\n",
      "Epoch 1617/2000, Training Loss: 30.28480, Validation Loss: 3.88118\n",
      "Epoch 1618/2000, Training Loss: 30.28173, Validation Loss: 3.88068\n",
      "Epoch 1619/2000, Training Loss: 30.27866, Validation Loss: 3.88019\n",
      "Epoch 1620/2000, Training Loss: 30.27559, Validation Loss: 3.87969\n",
      "Epoch 1621/2000, Training Loss: 30.27253, Validation Loss: 3.87920\n",
      "Epoch 1622/2000, Training Loss: 30.26948, Validation Loss: 3.87871\n",
      "Epoch 1623/2000, Training Loss: 30.26643, Validation Loss: 3.87822\n",
      "Epoch 1624/2000, Training Loss: 30.26339, Validation Loss: 3.87773\n",
      "Epoch 1625/2000, Training Loss: 30.26035, Validation Loss: 3.87724\n",
      "Epoch 1626/2000, Training Loss: 30.25732, Validation Loss: 3.87676\n",
      "Epoch 1627/2000, Training Loss: 30.25429, Validation Loss: 3.87627\n",
      "Epoch 1628/2000, Training Loss: 30.25126, Validation Loss: 3.87579\n",
      "Epoch 1629/2000, Training Loss: 30.24824, Validation Loss: 3.87531\n",
      "Epoch 1630/2000, Training Loss: 30.24522, Validation Loss: 3.87483\n",
      "Epoch 1631/2000, Training Loss: 30.24221, Validation Loss: 3.87435\n",
      "Epoch 1632/2000, Training Loss: 30.23921, Validation Loss: 3.87387\n",
      "Epoch 1633/2000, Training Loss: 30.23621, Validation Loss: 3.87339\n",
      "Epoch 1634/2000, Training Loss: 30.23322, Validation Loss: 3.87291\n",
      "Epoch 1635/2000, Training Loss: 30.23022, Validation Loss: 3.87244\n",
      "Epoch 1636/2000, Training Loss: 30.22724, Validation Loss: 3.87197\n",
      "Epoch 1637/2000, Training Loss: 30.22426, Validation Loss: 3.87149\n",
      "Epoch 1638/2000, Training Loss: 30.22128, Validation Loss: 3.87102\n",
      "Epoch 1639/2000, Training Loss: 30.21831, Validation Loss: 3.87055\n",
      "Epoch 1640/2000, Training Loss: 30.21535, Validation Loss: 3.87008\n",
      "Epoch 1641/2000, Training Loss: 30.21239, Validation Loss: 3.86962\n",
      "Epoch 1642/2000, Training Loss: 30.20943, Validation Loss: 3.86915\n",
      "Epoch 1643/2000, Training Loss: 30.20648, Validation Loss: 3.86869\n",
      "Epoch 1644/2000, Training Loss: 30.20354, Validation Loss: 3.86822\n",
      "Epoch 1645/2000, Training Loss: 30.20060, Validation Loss: 3.86776\n",
      "Epoch 1646/2000, Training Loss: 30.19767, Validation Loss: 3.86730\n",
      "Epoch 1647/2000, Training Loss: 30.19474, Validation Loss: 3.86684\n",
      "Epoch 1648/2000, Training Loss: 30.19182, Validation Loss: 3.86638\n",
      "Epoch 1649/2000, Training Loss: 30.18890, Validation Loss: 3.86592\n",
      "Epoch 1650/2000, Training Loss: 30.18599, Validation Loss: 3.86546\n",
      "Epoch 1651/2000, Training Loss: 30.18308, Validation Loss: 3.86500\n",
      "Epoch 1652/2000, Training Loss: 30.18017, Validation Loss: 3.86455\n",
      "Epoch 1653/2000, Training Loss: 30.17727, Validation Loss: 3.86409\n",
      "Epoch 1654/2000, Training Loss: 30.17438, Validation Loss: 3.86364\n",
      "Epoch 1655/2000, Training Loss: 30.17149, Validation Loss: 3.86318\n",
      "Epoch 1656/2000, Training Loss: 30.16860, Validation Loss: 3.86273\n",
      "Epoch 1657/2000, Training Loss: 30.16572, Validation Loss: 3.86228\n",
      "Epoch 1658/2000, Training Loss: 30.16284, Validation Loss: 3.86183\n",
      "Epoch 1659/2000, Training Loss: 30.15997, Validation Loss: 3.86138\n",
      "Epoch 1660/2000, Training Loss: 30.15710, Validation Loss: 3.86093\n",
      "Epoch 1661/2000, Training Loss: 30.15424, Validation Loss: 3.86048\n",
      "Epoch 1662/2000, Training Loss: 30.15138, Validation Loss: 3.86003\n",
      "Epoch 1663/2000, Training Loss: 30.14853, Validation Loss: 3.85958\n",
      "Epoch 1664/2000, Training Loss: 30.14568, Validation Loss: 3.85914\n",
      "Epoch 1665/2000, Training Loss: 30.14284, Validation Loss: 3.85869\n",
      "Epoch 1666/2000, Training Loss: 30.14001, Validation Loss: 3.85825\n",
      "Epoch 1667/2000, Training Loss: 30.13718, Validation Loss: 3.85780\n",
      "Epoch 1668/2000, Training Loss: 30.13437, Validation Loss: 3.85736\n",
      "Epoch 1669/2000, Training Loss: 30.13155, Validation Loss: 3.85692\n",
      "Epoch 1670/2000, Training Loss: 30.12874, Validation Loss: 3.85648\n",
      "Epoch 1671/2000, Training Loss: 30.12593, Validation Loss: 3.85604\n",
      "Epoch 1672/2000, Training Loss: 30.12313, Validation Loss: 3.85560\n",
      "Epoch 1673/2000, Training Loss: 30.12034, Validation Loss: 3.85516\n",
      "Epoch 1674/2000, Training Loss: 30.11755, Validation Loss: 3.85472\n",
      "Epoch 1675/2000, Training Loss: 30.11477, Validation Loss: 3.85429\n",
      "Epoch 1676/2000, Training Loss: 30.11200, Validation Loss: 3.85386\n",
      "Epoch 1677/2000, Training Loss: 30.10923, Validation Loss: 3.85342\n",
      "Epoch 1678/2000, Training Loss: 30.10646, Validation Loss: 3.85299\n",
      "Epoch 1679/2000, Training Loss: 30.10371, Validation Loss: 3.85256\n",
      "Epoch 1680/2000, Training Loss: 30.10095, Validation Loss: 3.85213\n",
      "Epoch 1681/2000, Training Loss: 30.09821, Validation Loss: 3.85170\n",
      "Epoch 1682/2000, Training Loss: 30.09546, Validation Loss: 3.85128\n",
      "Epoch 1683/2000, Training Loss: 30.09272, Validation Loss: 3.85085\n",
      "Epoch 1684/2000, Training Loss: 30.08999, Validation Loss: 3.85043\n",
      "Epoch 1685/2000, Training Loss: 30.08725, Validation Loss: 3.85000\n",
      "Epoch 1686/2000, Training Loss: 30.08452, Validation Loss: 3.84958\n",
      "Epoch 1687/2000, Training Loss: 30.08180, Validation Loss: 3.84916\n",
      "Epoch 1688/2000, Training Loss: 30.07908, Validation Loss: 3.84874\n",
      "Epoch 1689/2000, Training Loss: 30.07636, Validation Loss: 3.84832\n",
      "Epoch 1690/2000, Training Loss: 30.07365, Validation Loss: 3.84791\n",
      "Epoch 1691/2000, Training Loss: 30.07094, Validation Loss: 3.84749\n",
      "Epoch 1692/2000, Training Loss: 30.06824, Validation Loss: 3.84707\n",
      "Epoch 1693/2000, Training Loss: 30.06554, Validation Loss: 3.84666\n",
      "Epoch 1694/2000, Training Loss: 30.06284, Validation Loss: 3.84625\n",
      "Epoch 1695/2000, Training Loss: 30.06015, Validation Loss: 3.84584\n",
      "Epoch 1696/2000, Training Loss: 30.05747, Validation Loss: 3.84543\n",
      "Epoch 1697/2000, Training Loss: 30.05479, Validation Loss: 3.84502\n",
      "Epoch 1698/2000, Training Loss: 30.05211, Validation Loss: 3.84461\n",
      "Epoch 1699/2000, Training Loss: 30.04944, Validation Loss: 3.84421\n",
      "Epoch 1700/2000, Training Loss: 30.04678, Validation Loss: 3.84380\n",
      "Epoch 1701/2000, Training Loss: 30.04412, Validation Loss: 3.84340\n",
      "Epoch 1702/2000, Training Loss: 30.04146, Validation Loss: 3.84299\n",
      "Epoch 1703/2000, Training Loss: 30.03881, Validation Loss: 3.84258\n",
      "Epoch 1704/2000, Training Loss: 30.03616, Validation Loss: 3.84218\n",
      "Epoch 1705/2000, Training Loss: 30.03352, Validation Loss: 3.84178\n",
      "Epoch 1706/2000, Training Loss: 30.03089, Validation Loss: 3.84137\n",
      "Epoch 1707/2000, Training Loss: 30.02826, Validation Loss: 3.84097\n",
      "Epoch 1708/2000, Training Loss: 30.02563, Validation Loss: 3.84057\n",
      "Epoch 1709/2000, Training Loss: 30.02301, Validation Loss: 3.84017\n",
      "Epoch 1710/2000, Training Loss: 30.02039, Validation Loss: 3.83978\n",
      "Epoch 1711/2000, Training Loss: 30.01778, Validation Loss: 3.83938\n",
      "Epoch 1712/2000, Training Loss: 30.01517, Validation Loss: 3.83898\n",
      "Epoch 1713/2000, Training Loss: 30.01256, Validation Loss: 3.83859\n",
      "Epoch 1714/2000, Training Loss: 30.00996, Validation Loss: 3.83820\n",
      "Epoch 1715/2000, Training Loss: 30.00736, Validation Loss: 3.83780\n",
      "Epoch 1716/2000, Training Loss: 30.00477, Validation Loss: 3.83741\n",
      "Epoch 1717/2000, Training Loss: 30.00219, Validation Loss: 3.83702\n",
      "Epoch 1718/2000, Training Loss: 29.99960, Validation Loss: 3.83664\n",
      "Epoch 1719/2000, Training Loss: 29.99703, Validation Loss: 3.83625\n",
      "Epoch 1720/2000, Training Loss: 29.99445, Validation Loss: 3.83586\n",
      "Epoch 1721/2000, Training Loss: 29.99187, Validation Loss: 3.83548\n",
      "Epoch 1722/2000, Training Loss: 29.98931, Validation Loss: 3.83509\n",
      "Epoch 1723/2000, Training Loss: 29.98675, Validation Loss: 3.83471\n",
      "Epoch 1724/2000, Training Loss: 29.98419, Validation Loss: 3.83433\n",
      "Epoch 1725/2000, Training Loss: 29.98164, Validation Loss: 3.83395\n",
      "Epoch 1726/2000, Training Loss: 29.97910, Validation Loss: 3.83357\n",
      "Epoch 1727/2000, Training Loss: 29.97656, Validation Loss: 3.83319\n",
      "Epoch 1728/2000, Training Loss: 29.97403, Validation Loss: 3.83282\n",
      "Epoch 1729/2000, Training Loss: 29.97150, Validation Loss: 3.83244\n",
      "Epoch 1730/2000, Training Loss: 29.96897, Validation Loss: 3.83207\n",
      "Epoch 1731/2000, Training Loss: 29.96646, Validation Loss: 3.83170\n",
      "Epoch 1732/2000, Training Loss: 29.96394, Validation Loss: 3.83132\n",
      "Epoch 1733/2000, Training Loss: 29.96143, Validation Loss: 3.83095\n",
      "Epoch 1734/2000, Training Loss: 29.95892, Validation Loss: 3.83058\n",
      "Epoch 1735/2000, Training Loss: 29.95642, Validation Loss: 3.83021\n",
      "Epoch 1736/2000, Training Loss: 29.95392, Validation Loss: 3.82985\n",
      "Epoch 1737/2000, Training Loss: 29.95143, Validation Loss: 3.82948\n",
      "Epoch 1738/2000, Training Loss: 29.94893, Validation Loss: 3.82911\n",
      "Epoch 1739/2000, Training Loss: 29.94644, Validation Loss: 3.82875\n",
      "Epoch 1740/2000, Training Loss: 29.94395, Validation Loss: 3.82839\n",
      "Epoch 1741/2000, Training Loss: 29.94147, Validation Loss: 3.82803\n",
      "Epoch 1742/2000, Training Loss: 29.93899, Validation Loss: 3.82767\n",
      "Epoch 1743/2000, Training Loss: 29.93651, Validation Loss: 3.82731\n",
      "Epoch 1744/2000, Training Loss: 29.93404, Validation Loss: 3.82695\n",
      "Epoch 1745/2000, Training Loss: 29.93157, Validation Loss: 3.82659\n",
      "Epoch 1746/2000, Training Loss: 29.92911, Validation Loss: 3.82623\n",
      "Epoch 1747/2000, Training Loss: 29.92665, Validation Loss: 3.82588\n",
      "Epoch 1748/2000, Training Loss: 29.92420, Validation Loss: 3.82553\n",
      "Epoch 1749/2000, Training Loss: 29.92174, Validation Loss: 3.82517\n",
      "Epoch 1750/2000, Training Loss: 29.91929, Validation Loss: 3.82482\n",
      "Epoch 1751/2000, Training Loss: 29.91685, Validation Loss: 3.82447\n",
      "Epoch 1752/2000, Training Loss: 29.91441, Validation Loss: 3.82412\n",
      "Epoch 1753/2000, Training Loss: 29.91197, Validation Loss: 3.82377\n",
      "Epoch 1754/2000, Training Loss: 29.90954, Validation Loss: 3.82342\n",
      "Epoch 1755/2000, Training Loss: 29.90711, Validation Loss: 3.82307\n",
      "Epoch 1756/2000, Training Loss: 29.90469, Validation Loss: 3.82272\n",
      "Epoch 1757/2000, Training Loss: 29.90228, Validation Loss: 3.82238\n",
      "Epoch 1758/2000, Training Loss: 29.89986, Validation Loss: 3.82203\n",
      "Epoch 1759/2000, Training Loss: 29.89746, Validation Loss: 3.82168\n",
      "Epoch 1760/2000, Training Loss: 29.89505, Validation Loss: 3.82133\n",
      "Epoch 1761/2000, Training Loss: 29.89265, Validation Loss: 3.82099\n",
      "Epoch 1762/2000, Training Loss: 29.89026, Validation Loss: 3.82064\n",
      "Epoch 1763/2000, Training Loss: 29.88787, Validation Loss: 3.82029\n",
      "Epoch 1764/2000, Training Loss: 29.88549, Validation Loss: 3.81994\n",
      "Epoch 1765/2000, Training Loss: 29.88311, Validation Loss: 3.81960\n",
      "Epoch 1766/2000, Training Loss: 29.88074, Validation Loss: 3.81925\n",
      "Epoch 1767/2000, Training Loss: 29.87838, Validation Loss: 3.81891\n",
      "Epoch 1768/2000, Training Loss: 29.87601, Validation Loss: 3.81856\n",
      "Epoch 1769/2000, Training Loss: 29.87366, Validation Loss: 3.81822\n",
      "Epoch 1770/2000, Training Loss: 29.87130, Validation Loss: 3.81788\n",
      "Epoch 1771/2000, Training Loss: 29.86895, Validation Loss: 3.81754\n",
      "Epoch 1772/2000, Training Loss: 29.86661, Validation Loss: 3.81720\n",
      "Epoch 1773/2000, Training Loss: 29.86427, Validation Loss: 3.81686\n",
      "Epoch 1774/2000, Training Loss: 29.86193, Validation Loss: 3.81652\n",
      "Epoch 1775/2000, Training Loss: 29.85959, Validation Loss: 3.81619\n",
      "Epoch 1776/2000, Training Loss: 29.85726, Validation Loss: 3.81585\n",
      "Epoch 1777/2000, Training Loss: 29.85493, Validation Loss: 3.81551\n",
      "Epoch 1778/2000, Training Loss: 29.85261, Validation Loss: 3.81518\n",
      "Epoch 1779/2000, Training Loss: 29.85029, Validation Loss: 3.81485\n",
      "Epoch 1780/2000, Training Loss: 29.84797, Validation Loss: 3.81451\n",
      "Epoch 1781/2000, Training Loss: 29.84566, Validation Loss: 3.81418\n",
      "Epoch 1782/2000, Training Loss: 29.84335, Validation Loss: 3.81385\n",
      "Epoch 1783/2000, Training Loss: 29.84105, Validation Loss: 3.81352\n",
      "Epoch 1784/2000, Training Loss: 29.83875, Validation Loss: 3.81320\n",
      "Epoch 1785/2000, Training Loss: 29.83645, Validation Loss: 3.81287\n",
      "Epoch 1786/2000, Training Loss: 29.83415, Validation Loss: 3.81254\n",
      "Epoch 1787/2000, Training Loss: 29.83186, Validation Loss: 3.81222\n",
      "Epoch 1788/2000, Training Loss: 29.82957, Validation Loss: 3.81190\n",
      "Epoch 1789/2000, Training Loss: 29.82729, Validation Loss: 3.81157\n",
      "Epoch 1790/2000, Training Loss: 29.82501, Validation Loss: 3.81125\n",
      "Epoch 1791/2000, Training Loss: 29.82273, Validation Loss: 3.81093\n",
      "Epoch 1792/2000, Training Loss: 29.82047, Validation Loss: 3.81061\n",
      "Epoch 1793/2000, Training Loss: 29.81820, Validation Loss: 3.81029\n",
      "Epoch 1794/2000, Training Loss: 29.81594, Validation Loss: 3.80997\n",
      "Epoch 1795/2000, Training Loss: 29.81368, Validation Loss: 3.80965\n",
      "Epoch 1796/2000, Training Loss: 29.81143, Validation Loss: 3.80934\n",
      "Epoch 1797/2000, Training Loss: 29.80919, Validation Loss: 3.80902\n",
      "Epoch 1798/2000, Training Loss: 29.80694, Validation Loss: 3.80871\n",
      "Epoch 1799/2000, Training Loss: 29.80470, Validation Loss: 3.80839\n",
      "Epoch 1800/2000, Training Loss: 29.80247, Validation Loss: 3.80808\n",
      "Epoch 1801/2000, Training Loss: 29.80024, Validation Loss: 3.80776\n",
      "Epoch 1802/2000, Training Loss: 29.79801, Validation Loss: 3.80745\n",
      "Epoch 1803/2000, Training Loss: 29.79578, Validation Loss: 3.80714\n",
      "Epoch 1804/2000, Training Loss: 29.79356, Validation Loss: 3.80683\n",
      "Epoch 1805/2000, Training Loss: 29.79134, Validation Loss: 3.80652\n",
      "Epoch 1806/2000, Training Loss: 29.78913, Validation Loss: 3.80621\n",
      "Epoch 1807/2000, Training Loss: 29.78692, Validation Loss: 3.80590\n",
      "Epoch 1808/2000, Training Loss: 29.78471, Validation Loss: 3.80560\n",
      "Epoch 1809/2000, Training Loss: 29.78251, Validation Loss: 3.80529\n",
      "Epoch 1810/2000, Training Loss: 29.78031, Validation Loss: 3.80499\n",
      "Epoch 1811/2000, Training Loss: 29.77812, Validation Loss: 3.80468\n",
      "Epoch 1812/2000, Training Loss: 29.77593, Validation Loss: 3.80438\n",
      "Epoch 1813/2000, Training Loss: 29.77374, Validation Loss: 3.80408\n",
      "Epoch 1814/2000, Training Loss: 29.77155, Validation Loss: 3.80377\n",
      "Epoch 1815/2000, Training Loss: 29.76937, Validation Loss: 3.80347\n",
      "Epoch 1816/2000, Training Loss: 29.76719, Validation Loss: 3.80317\n",
      "Epoch 1817/2000, Training Loss: 29.76501, Validation Loss: 3.80287\n",
      "Epoch 1818/2000, Training Loss: 29.76283, Validation Loss: 3.80257\n",
      "Epoch 1819/2000, Training Loss: 29.76066, Validation Loss: 3.80228\n",
      "Epoch 1820/2000, Training Loss: 29.75849, Validation Loss: 3.80198\n",
      "Epoch 1821/2000, Training Loss: 29.75633, Validation Loss: 3.80168\n",
      "Epoch 1822/2000, Training Loss: 29.75417, Validation Loss: 3.80139\n",
      "Epoch 1823/2000, Training Loss: 29.75201, Validation Loss: 3.80109\n",
      "Epoch 1824/2000, Training Loss: 29.74986, Validation Loss: 3.80080\n",
      "Epoch 1825/2000, Training Loss: 29.74771, Validation Loss: 3.80050\n",
      "Epoch 1826/2000, Training Loss: 29.74556, Validation Loss: 3.80021\n",
      "Epoch 1827/2000, Training Loss: 29.74342, Validation Loss: 3.79991\n",
      "Epoch 1828/2000, Training Loss: 29.74129, Validation Loss: 3.79962\n",
      "Epoch 1829/2000, Training Loss: 29.73915, Validation Loss: 3.79932\n",
      "Epoch 1830/2000, Training Loss: 29.73702, Validation Loss: 3.79903\n",
      "Epoch 1831/2000, Training Loss: 29.73490, Validation Loss: 3.79874\n",
      "Epoch 1832/2000, Training Loss: 29.73277, Validation Loss: 3.79845\n",
      "Epoch 1833/2000, Training Loss: 29.73065, Validation Loss: 3.79816\n",
      "Epoch 1834/2000, Training Loss: 29.72853, Validation Loss: 3.79787\n",
      "Epoch 1835/2000, Training Loss: 29.72642, Validation Loss: 3.79758\n",
      "Epoch 1836/2000, Training Loss: 29.72431, Validation Loss: 3.79729\n",
      "Epoch 1837/2000, Training Loss: 29.72220, Validation Loss: 3.79700\n",
      "Epoch 1838/2000, Training Loss: 29.72010, Validation Loss: 3.79671\n",
      "Epoch 1839/2000, Training Loss: 29.71800, Validation Loss: 3.79642\n",
      "Epoch 1840/2000, Training Loss: 29.71590, Validation Loss: 3.79614\n",
      "Epoch 1841/2000, Training Loss: 29.71381, Validation Loss: 3.79585\n",
      "Epoch 1842/2000, Training Loss: 29.71172, Validation Loss: 3.79556\n",
      "Epoch 1843/2000, Training Loss: 29.70962, Validation Loss: 3.79528\n",
      "Epoch 1844/2000, Training Loss: 29.70754, Validation Loss: 3.79499\n",
      "Epoch 1845/2000, Training Loss: 29.70546, Validation Loss: 3.79471\n",
      "Epoch 1846/2000, Training Loss: 29.70339, Validation Loss: 3.79443\n",
      "Epoch 1847/2000, Training Loss: 29.70132, Validation Loss: 3.79415\n",
      "Epoch 1848/2000, Training Loss: 29.69925, Validation Loss: 3.79386\n",
      "Epoch 1849/2000, Training Loss: 29.69719, Validation Loss: 3.79358\n",
      "Epoch 1850/2000, Training Loss: 29.69513, Validation Loss: 3.79330\n",
      "Epoch 1851/2000, Training Loss: 29.69308, Validation Loss: 3.79302\n",
      "Epoch 1852/2000, Training Loss: 29.69103, Validation Loss: 3.79274\n",
      "Epoch 1853/2000, Training Loss: 29.68898, Validation Loss: 3.79246\n",
      "Epoch 1854/2000, Training Loss: 29.68694, Validation Loss: 3.79219\n",
      "Epoch 1855/2000, Training Loss: 29.68490, Validation Loss: 3.79191\n",
      "Epoch 1856/2000, Training Loss: 29.68286, Validation Loss: 3.79164\n",
      "Epoch 1857/2000, Training Loss: 29.68083, Validation Loss: 3.79136\n",
      "Epoch 1858/2000, Training Loss: 29.67879, Validation Loss: 3.79109\n",
      "Epoch 1859/2000, Training Loss: 29.67677, Validation Loss: 3.79082\n",
      "Epoch 1860/2000, Training Loss: 29.67475, Validation Loss: 3.79054\n",
      "Epoch 1861/2000, Training Loss: 29.67273, Validation Loss: 3.79027\n",
      "Epoch 1862/2000, Training Loss: 29.67071, Validation Loss: 3.79000\n",
      "Epoch 1863/2000, Training Loss: 29.66870, Validation Loss: 3.78974\n",
      "Epoch 1864/2000, Training Loss: 29.66670, Validation Loss: 3.78947\n",
      "Epoch 1865/2000, Training Loss: 29.66469, Validation Loss: 3.78920\n",
      "Epoch 1866/2000, Training Loss: 29.66269, Validation Loss: 3.78894\n",
      "Epoch 1867/2000, Training Loss: 29.66070, Validation Loss: 3.78867\n",
      "Epoch 1868/2000, Training Loss: 29.65870, Validation Loss: 3.78841\n",
      "Epoch 1869/2000, Training Loss: 29.65671, Validation Loss: 3.78815\n",
      "Epoch 1870/2000, Training Loss: 29.65472, Validation Loss: 3.78789\n",
      "Epoch 1871/2000, Training Loss: 29.65274, Validation Loss: 3.78763\n",
      "Epoch 1872/2000, Training Loss: 29.65077, Validation Loss: 3.78737\n",
      "Epoch 1873/2000, Training Loss: 29.64879, Validation Loss: 3.78711\n",
      "Epoch 1874/2000, Training Loss: 29.64682, Validation Loss: 3.78686\n",
      "Epoch 1875/2000, Training Loss: 29.64485, Validation Loss: 3.78660\n",
      "Epoch 1876/2000, Training Loss: 29.64289, Validation Loss: 3.78635\n",
      "Epoch 1877/2000, Training Loss: 29.64093, Validation Loss: 3.78609\n",
      "Epoch 1878/2000, Training Loss: 29.63898, Validation Loss: 3.78584\n",
      "Epoch 1879/2000, Training Loss: 29.63702, Validation Loss: 3.78559\n",
      "Epoch 1880/2000, Training Loss: 29.63507, Validation Loss: 3.78533\n",
      "Epoch 1881/2000, Training Loss: 29.63312, Validation Loss: 3.78508\n",
      "Epoch 1882/2000, Training Loss: 29.63118, Validation Loss: 3.78483\n",
      "Epoch 1883/2000, Training Loss: 29.62923, Validation Loss: 3.78458\n",
      "Epoch 1884/2000, Training Loss: 29.62729, Validation Loss: 3.78433\n",
      "Epoch 1885/2000, Training Loss: 29.62535, Validation Loss: 3.78408\n",
      "Epoch 1886/2000, Training Loss: 29.62342, Validation Loss: 3.78383\n",
      "Epoch 1887/2000, Training Loss: 29.62148, Validation Loss: 3.78358\n",
      "Epoch 1888/2000, Training Loss: 29.61955, Validation Loss: 3.78333\n",
      "Epoch 1889/2000, Training Loss: 29.61762, Validation Loss: 3.78308\n",
      "Epoch 1890/2000, Training Loss: 29.61571, Validation Loss: 3.78283\n",
      "Epoch 1891/2000, Training Loss: 29.61379, Validation Loss: 3.78259\n",
      "Epoch 1892/2000, Training Loss: 29.61187, Validation Loss: 3.78234\n",
      "Epoch 1893/2000, Training Loss: 29.60997, Validation Loss: 3.78210\n",
      "Epoch 1894/2000, Training Loss: 29.60806, Validation Loss: 3.78186\n",
      "Epoch 1895/2000, Training Loss: 29.60616, Validation Loss: 3.78161\n",
      "Epoch 1896/2000, Training Loss: 29.60426, Validation Loss: 3.78137\n",
      "Epoch 1897/2000, Training Loss: 29.60236, Validation Loss: 3.78113\n",
      "Epoch 1898/2000, Training Loss: 29.60047, Validation Loss: 3.78089\n",
      "Epoch 1899/2000, Training Loss: 29.59858, Validation Loss: 3.78066\n",
      "Epoch 1900/2000, Training Loss: 29.59670, Validation Loss: 3.78042\n",
      "Epoch 1901/2000, Training Loss: 29.59482, Validation Loss: 3.78018\n",
      "Epoch 1902/2000, Training Loss: 29.59294, Validation Loss: 3.77995\n",
      "Epoch 1903/2000, Training Loss: 29.59106, Validation Loss: 3.77971\n",
      "Epoch 1904/2000, Training Loss: 29.58919, Validation Loss: 3.77948\n",
      "Epoch 1905/2000, Training Loss: 29.58732, Validation Loss: 3.77925\n",
      "Epoch 1906/2000, Training Loss: 29.58546, Validation Loss: 3.77901\n",
      "Epoch 1907/2000, Training Loss: 29.58360, Validation Loss: 3.77878\n",
      "Epoch 1908/2000, Training Loss: 29.58174, Validation Loss: 3.77855\n",
      "Epoch 1909/2000, Training Loss: 29.57988, Validation Loss: 3.77832\n",
      "Epoch 1910/2000, Training Loss: 29.57802, Validation Loss: 3.77809\n",
      "Epoch 1911/2000, Training Loss: 29.57617, Validation Loss: 3.77786\n",
      "Epoch 1912/2000, Training Loss: 29.57432, Validation Loss: 3.77763\n",
      "Epoch 1913/2000, Training Loss: 29.57248, Validation Loss: 3.77740\n",
      "Epoch 1914/2000, Training Loss: 29.57063, Validation Loss: 3.77717\n",
      "Epoch 1915/2000, Training Loss: 29.56879, Validation Loss: 3.77695\n",
      "Epoch 1916/2000, Training Loss: 29.56695, Validation Loss: 3.77672\n",
      "Epoch 1917/2000, Training Loss: 29.56512, Validation Loss: 3.77650\n",
      "Epoch 1918/2000, Training Loss: 29.56328, Validation Loss: 3.77627\n",
      "Epoch 1919/2000, Training Loss: 29.56145, Validation Loss: 3.77605\n",
      "Epoch 1920/2000, Training Loss: 29.55962, Validation Loss: 3.77582\n",
      "Epoch 1921/2000, Training Loss: 29.55780, Validation Loss: 3.77560\n",
      "Epoch 1922/2000, Training Loss: 29.55598, Validation Loss: 3.77538\n",
      "Epoch 1923/2000, Training Loss: 29.55416, Validation Loss: 3.77516\n",
      "Epoch 1924/2000, Training Loss: 29.55234, Validation Loss: 3.77494\n",
      "Epoch 1925/2000, Training Loss: 29.55053, Validation Loss: 3.77472\n",
      "Epoch 1926/2000, Training Loss: 29.54872, Validation Loss: 3.77450\n",
      "Epoch 1927/2000, Training Loss: 29.54692, Validation Loss: 3.77428\n",
      "Epoch 1928/2000, Training Loss: 29.54512, Validation Loss: 3.77406\n",
      "Epoch 1929/2000, Training Loss: 29.54333, Validation Loss: 3.77384\n",
      "Epoch 1930/2000, Training Loss: 29.54154, Validation Loss: 3.77362\n",
      "Epoch 1931/2000, Training Loss: 29.53975, Validation Loss: 3.77341\n",
      "Epoch 1932/2000, Training Loss: 29.53797, Validation Loss: 3.77319\n",
      "Epoch 1933/2000, Training Loss: 29.53619, Validation Loss: 3.77298\n",
      "Epoch 1934/2000, Training Loss: 29.53441, Validation Loss: 3.77276\n",
      "Epoch 1935/2000, Training Loss: 29.53264, Validation Loss: 3.77255\n",
      "Epoch 1936/2000, Training Loss: 29.53087, Validation Loss: 3.77233\n",
      "Epoch 1937/2000, Training Loss: 29.52910, Validation Loss: 3.77212\n",
      "Epoch 1938/2000, Training Loss: 29.52733, Validation Loss: 3.77191\n",
      "Epoch 1939/2000, Training Loss: 29.52557, Validation Loss: 3.77170\n",
      "Epoch 1940/2000, Training Loss: 29.52381, Validation Loss: 3.77149\n",
      "Epoch 1941/2000, Training Loss: 29.52206, Validation Loss: 3.77128\n",
      "Epoch 1942/2000, Training Loss: 29.52030, Validation Loss: 3.77107\n",
      "Epoch 1943/2000, Training Loss: 29.51855, Validation Loss: 3.77086\n",
      "Epoch 1944/2000, Training Loss: 29.51681, Validation Loss: 3.77065\n",
      "Epoch 1945/2000, Training Loss: 29.51507, Validation Loss: 3.77044\n",
      "Epoch 1946/2000, Training Loss: 29.51333, Validation Loss: 3.77023\n",
      "Epoch 1947/2000, Training Loss: 29.51159, Validation Loss: 3.77002\n",
      "Epoch 1948/2000, Training Loss: 29.50986, Validation Loss: 3.76982\n",
      "Epoch 1949/2000, Training Loss: 29.50813, Validation Loss: 3.76961\n",
      "Epoch 1950/2000, Training Loss: 29.50640, Validation Loss: 3.76941\n",
      "Epoch 1951/2000, Training Loss: 29.50467, Validation Loss: 3.76920\n",
      "Epoch 1952/2000, Training Loss: 29.50295, Validation Loss: 3.76900\n",
      "Epoch 1953/2000, Training Loss: 29.50123, Validation Loss: 3.76880\n",
      "Epoch 1954/2000, Training Loss: 29.49952, Validation Loss: 3.76860\n",
      "Epoch 1955/2000, Training Loss: 29.49780, Validation Loss: 3.76840\n",
      "Epoch 1956/2000, Training Loss: 29.49609, Validation Loss: 3.76819\n",
      "Epoch 1957/2000, Training Loss: 29.49438, Validation Loss: 3.76799\n",
      "Epoch 1958/2000, Training Loss: 29.49267, Validation Loss: 3.76779\n",
      "Epoch 1959/2000, Training Loss: 29.49097, Validation Loss: 3.76760\n",
      "Epoch 1960/2000, Training Loss: 29.48927, Validation Loss: 3.76740\n",
      "Epoch 1961/2000, Training Loss: 29.48757, Validation Loss: 3.76720\n",
      "Epoch 1962/2000, Training Loss: 29.48587, Validation Loss: 3.76701\n",
      "Epoch 1963/2000, Training Loss: 29.48418, Validation Loss: 3.76681\n",
      "Epoch 1964/2000, Training Loss: 29.48249, Validation Loss: 3.76662\n",
      "Epoch 1965/2000, Training Loss: 29.48080, Validation Loss: 3.76642\n",
      "Epoch 1966/2000, Training Loss: 29.47912, Validation Loss: 3.76623\n",
      "Epoch 1967/2000, Training Loss: 29.47744, Validation Loss: 3.76604\n",
      "Epoch 1968/2000, Training Loss: 29.47576, Validation Loss: 3.76584\n",
      "Epoch 1969/2000, Training Loss: 29.47408, Validation Loss: 3.76565\n",
      "Epoch 1970/2000, Training Loss: 29.47241, Validation Loss: 3.76546\n",
      "Epoch 1971/2000, Training Loss: 29.47074, Validation Loss: 3.76527\n",
      "Epoch 1972/2000, Training Loss: 29.46907, Validation Loss: 3.76507\n",
      "Epoch 1973/2000, Training Loss: 29.46741, Validation Loss: 3.76488\n",
      "Epoch 1974/2000, Training Loss: 29.46574, Validation Loss: 3.76469\n",
      "Epoch 1975/2000, Training Loss: 29.46408, Validation Loss: 3.76450\n",
      "Epoch 1976/2000, Training Loss: 29.46243, Validation Loss: 3.76431\n",
      "Epoch 1977/2000, Training Loss: 29.46077, Validation Loss: 3.76412\n",
      "Epoch 1978/2000, Training Loss: 29.45912, Validation Loss: 3.76393\n",
      "Epoch 1979/2000, Training Loss: 29.45748, Validation Loss: 3.76375\n",
      "Epoch 1980/2000, Training Loss: 29.45583, Validation Loss: 3.76357\n",
      "Epoch 1981/2000, Training Loss: 29.45418, Validation Loss: 3.76339\n",
      "Epoch 1982/2000, Training Loss: 29.45253, Validation Loss: 3.76320\n",
      "Epoch 1983/2000, Training Loss: 29.45088, Validation Loss: 3.76302\n",
      "Epoch 1984/2000, Training Loss: 29.44924, Validation Loss: 3.76284\n",
      "Epoch 1985/2000, Training Loss: 29.44760, Validation Loss: 3.76266\n",
      "Epoch 1986/2000, Training Loss: 29.44596, Validation Loss: 3.76248\n",
      "Epoch 1987/2000, Training Loss: 29.44433, Validation Loss: 3.76230\n",
      "Epoch 1988/2000, Training Loss: 29.44270, Validation Loss: 3.76213\n",
      "Epoch 1989/2000, Training Loss: 29.44107, Validation Loss: 3.76195\n",
      "Epoch 1990/2000, Training Loss: 29.43945, Validation Loss: 3.76177\n",
      "Epoch 1991/2000, Training Loss: 29.43782, Validation Loss: 3.76160\n",
      "Epoch 1992/2000, Training Loss: 29.43620, Validation Loss: 3.76143\n",
      "Epoch 1993/2000, Training Loss: 29.43459, Validation Loss: 3.76125\n",
      "Epoch 1994/2000, Training Loss: 29.43297, Validation Loss: 3.76108\n",
      "Epoch 1995/2000, Training Loss: 29.43136, Validation Loss: 3.76091\n",
      "Epoch 1996/2000, Training Loss: 29.42975, Validation Loss: 3.76073\n",
      "Epoch 1997/2000, Training Loss: 29.42814, Validation Loss: 3.76056\n",
      "Epoch 1998/2000, Training Loss: 29.42653, Validation Loss: 3.76039\n",
      "Epoch 1999/2000, Training Loss: 29.42493, Validation Loss: 3.76022\n",
      "Epoch 2000/2000, Training Loss: 29.42333, Validation Loss: 3.76005\n",
      "Training took: 163.78 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_epoch_2000_2 = NeuralNetwork().to(device)\n",
    "summary(model_epoch_2000_2, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 2000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_epoch_2000_2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_epoch_2000_2=[]\n",
    "val_loss_list_epoch_2000_2=[]\n",
    "train_accuracy_list_epoch_2000_2=[]\n",
    "val_accuracy_list_epoch_2000_2=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_epoch_2000_2.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_epoch_2000_2(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_epoch_2000_2.append(train_accuracy)\n",
    "    train_loss_list_epoch_2000_2.append(train_loss)\n",
    "\n",
    "    model_epoch_2000_2.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_epoch_2000_2(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_epoch_2000_2.append(val_accuracy)\n",
    "    val_accuracy_list_epoch_2000_2.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoCicIGMu7-h",
    "outputId": "b06b4a29-f233-4165-dab5-ed64d492f5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable epoch size with epoch size as 2000: 0.7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_epoch_2000_2.eval()\n",
    "test_predictions_epoch_2000_2 = model_epoch_2000_2(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_epoch_2000_2 = torch.round(test_predictions_epoch_2000_2)\n",
    "\n",
    "test_predictions_rounded_numpy_epoch_2000_2 = test_predictions_rounded_epoch_2000_2.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_epoch_2000_2 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_epoch_2000_2)\n",
    "\n",
    "print(f\"Accuracy for variable epoch size with epoch size as 2000: {accuracy_epoch_2000_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuHuXVhju9lZ",
    "outputId": "bc9d2997-03ee-4ed1-ff39-e8f677441e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable epoch size with epoch size as 2000: 0.49276\n"
     ]
    }
   ],
   "source": [
    "model_epoch_2000_2.eval()\n",
    "test_loss_epoch_2000_2=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_epoch_2000_2 = model_epoch_2000_2(X_test_tensor)\n",
    "    test_loss_epoch_2000_2 = loss_function(test_outputs_epoch_2000_2, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable epoch size with epoch size as 2000: {test_loss_epoch_2000_2.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EmM0FNEx7aO"
   },
   "source": [
    "Epochs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjiNNR2Fx9lf",
    "outputId": "29911a64-292f-4f59-c3be-1b384241e45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Training Loss: 41.39382, Validation Loss: 6.15777\n",
      "Epoch 2/3000, Training Loss: 41.37753, Validation Loss: 6.15524\n",
      "Epoch 3/3000, Training Loss: 41.36133, Validation Loss: 6.15273\n",
      "Epoch 4/3000, Training Loss: 41.34522, Validation Loss: 6.15023\n",
      "Epoch 5/3000, Training Loss: 41.32920, Validation Loss: 6.14774\n",
      "Epoch 6/3000, Training Loss: 41.31326, Validation Loss: 6.14526\n",
      "Epoch 7/3000, Training Loss: 41.29740, Validation Loss: 6.14280\n",
      "Epoch 8/3000, Training Loss: 41.28163, Validation Loss: 6.14035\n",
      "Epoch 9/3000, Training Loss: 41.26593, Validation Loss: 6.13791\n",
      "Epoch 10/3000, Training Loss: 41.25032, Validation Loss: 6.13549\n",
      "Epoch 11/3000, Training Loss: 41.23479, Validation Loss: 6.13307\n",
      "Epoch 12/3000, Training Loss: 41.21933, Validation Loss: 6.13067\n",
      "Epoch 13/3000, Training Loss: 41.20397, Validation Loss: 6.12828\n",
      "Epoch 14/3000, Training Loss: 41.18868, Validation Loss: 6.12591\n",
      "Epoch 15/3000, Training Loss: 41.17347, Validation Loss: 6.12354\n",
      "Epoch 16/3000, Training Loss: 41.15832, Validation Loss: 6.12119\n",
      "Epoch 17/3000, Training Loss: 41.14325, Validation Loss: 6.11884\n",
      "Epoch 18/3000, Training Loss: 41.12826, Validation Loss: 6.11651\n",
      "Epoch 19/3000, Training Loss: 41.11334, Validation Loss: 6.11419\n",
      "Epoch 20/3000, Training Loss: 41.09848, Validation Loss: 6.11188\n",
      "Epoch 21/3000, Training Loss: 41.08369, Validation Loss: 6.10958\n",
      "Epoch 22/3000, Training Loss: 41.06898, Validation Loss: 6.10729\n",
      "Epoch 23/3000, Training Loss: 41.05434, Validation Loss: 6.10501\n",
      "Epoch 24/3000, Training Loss: 41.03977, Validation Loss: 6.10274\n",
      "Epoch 25/3000, Training Loss: 41.02528, Validation Loss: 6.10048\n",
      "Epoch 26/3000, Training Loss: 41.01085, Validation Loss: 6.09823\n",
      "Epoch 27/3000, Training Loss: 40.99649, Validation Loss: 6.09600\n",
      "Epoch 28/3000, Training Loss: 40.98220, Validation Loss: 6.09377\n",
      "Epoch 29/3000, Training Loss: 40.96798, Validation Loss: 6.09155\n",
      "Epoch 30/3000, Training Loss: 40.95381, Validation Loss: 6.08934\n",
      "Epoch 31/3000, Training Loss: 40.93970, Validation Loss: 6.08715\n",
      "Epoch 32/3000, Training Loss: 40.92565, Validation Loss: 6.08496\n",
      "Epoch 33/3000, Training Loss: 40.91167, Validation Loss: 6.08278\n",
      "Epoch 34/3000, Training Loss: 40.89775, Validation Loss: 6.08061\n",
      "Epoch 35/3000, Training Loss: 40.88389, Validation Loss: 6.07845\n",
      "Epoch 36/3000, Training Loss: 40.87009, Validation Loss: 6.07630\n",
      "Epoch 37/3000, Training Loss: 40.85636, Validation Loss: 6.07416\n",
      "Epoch 38/3000, Training Loss: 40.84267, Validation Loss: 6.07203\n",
      "Epoch 39/3000, Training Loss: 40.82905, Validation Loss: 6.06991\n",
      "Epoch 40/3000, Training Loss: 40.81549, Validation Loss: 6.06780\n",
      "Epoch 41/3000, Training Loss: 40.80200, Validation Loss: 6.06569\n",
      "Epoch 42/3000, Training Loss: 40.78856, Validation Loss: 6.06359\n",
      "Epoch 43/3000, Training Loss: 40.77518, Validation Loss: 6.06151\n",
      "Epoch 44/3000, Training Loss: 40.76185, Validation Loss: 6.05942\n",
      "Epoch 45/3000, Training Loss: 40.74858, Validation Loss: 6.05735\n",
      "Epoch 46/3000, Training Loss: 40.73535, Validation Loss: 6.05529\n",
      "Epoch 47/3000, Training Loss: 40.72218, Validation Loss: 6.05323\n",
      "Epoch 48/3000, Training Loss: 40.70905, Validation Loss: 6.05118\n",
      "Epoch 49/3000, Training Loss: 40.69599, Validation Loss: 6.04914\n",
      "Epoch 50/3000, Training Loss: 40.68297, Validation Loss: 6.04711\n",
      "Epoch 51/3000, Training Loss: 40.67000, Validation Loss: 6.04509\n",
      "Epoch 52/3000, Training Loss: 40.65709, Validation Loss: 6.04307\n",
      "Epoch 53/3000, Training Loss: 40.64422, Validation Loss: 6.04106\n",
      "Epoch 54/3000, Training Loss: 40.63141, Validation Loss: 6.03906\n",
      "Epoch 55/3000, Training Loss: 40.61863, Validation Loss: 6.03707\n",
      "Epoch 56/3000, Training Loss: 40.60591, Validation Loss: 6.03509\n",
      "Epoch 57/3000, Training Loss: 40.59324, Validation Loss: 6.03311\n",
      "Epoch 58/3000, Training Loss: 40.58062, Validation Loss: 6.03114\n",
      "Epoch 59/3000, Training Loss: 40.56805, Validation Loss: 6.02917\n",
      "Epoch 60/3000, Training Loss: 40.55552, Validation Loss: 6.02722\n",
      "Epoch 61/3000, Training Loss: 40.54303, Validation Loss: 6.02527\n",
      "Epoch 62/3000, Training Loss: 40.53058, Validation Loss: 6.02332\n",
      "Epoch 63/3000, Training Loss: 40.51817, Validation Loss: 6.02139\n",
      "Epoch 64/3000, Training Loss: 40.50581, Validation Loss: 6.01946\n",
      "Epoch 65/3000, Training Loss: 40.49349, Validation Loss: 6.01753\n",
      "Epoch 66/3000, Training Loss: 40.48121, Validation Loss: 6.01562\n",
      "Epoch 67/3000, Training Loss: 40.46898, Validation Loss: 6.01371\n",
      "Epoch 68/3000, Training Loss: 40.45679, Validation Loss: 6.01180\n",
      "Epoch 69/3000, Training Loss: 40.44465, Validation Loss: 6.00990\n",
      "Epoch 70/3000, Training Loss: 40.43256, Validation Loss: 6.00801\n",
      "Epoch 71/3000, Training Loss: 40.42051, Validation Loss: 6.00613\n",
      "Epoch 72/3000, Training Loss: 40.40849, Validation Loss: 6.00425\n",
      "Epoch 73/3000, Training Loss: 40.39651, Validation Loss: 6.00238\n",
      "Epoch 74/3000, Training Loss: 40.38458, Validation Loss: 6.00051\n",
      "Epoch 75/3000, Training Loss: 40.37268, Validation Loss: 5.99866\n",
      "Epoch 76/3000, Training Loss: 40.36082, Validation Loss: 5.99680\n",
      "Epoch 77/3000, Training Loss: 40.34900, Validation Loss: 5.99496\n",
      "Epoch 78/3000, Training Loss: 40.33721, Validation Loss: 5.99312\n",
      "Epoch 79/3000, Training Loss: 40.32546, Validation Loss: 5.99128\n",
      "Epoch 80/3000, Training Loss: 40.31374, Validation Loss: 5.98945\n",
      "Epoch 81/3000, Training Loss: 40.30207, Validation Loss: 5.98763\n",
      "Epoch 82/3000, Training Loss: 40.29043, Validation Loss: 5.98581\n",
      "Epoch 83/3000, Training Loss: 40.27883, Validation Loss: 5.98400\n",
      "Epoch 84/3000, Training Loss: 40.26727, Validation Loss: 5.98219\n",
      "Epoch 85/3000, Training Loss: 40.25575, Validation Loss: 5.98039\n",
      "Epoch 86/3000, Training Loss: 40.24427, Validation Loss: 5.97859\n",
      "Epoch 87/3000, Training Loss: 40.23282, Validation Loss: 5.97680\n",
      "Epoch 88/3000, Training Loss: 40.22140, Validation Loss: 5.97501\n",
      "Epoch 89/3000, Training Loss: 40.21003, Validation Loss: 5.97323\n",
      "Epoch 90/3000, Training Loss: 40.19868, Validation Loss: 5.97146\n",
      "Epoch 91/3000, Training Loss: 40.18738, Validation Loss: 5.96968\n",
      "Epoch 92/3000, Training Loss: 40.17610, Validation Loss: 5.96792\n",
      "Epoch 93/3000, Training Loss: 40.16486, Validation Loss: 5.96616\n",
      "Epoch 94/3000, Training Loss: 40.15365, Validation Loss: 5.96440\n",
      "Epoch 95/3000, Training Loss: 40.14248, Validation Loss: 5.96265\n",
      "Epoch 96/3000, Training Loss: 40.13135, Validation Loss: 5.96091\n",
      "Epoch 97/3000, Training Loss: 40.12024, Validation Loss: 5.95916\n",
      "Epoch 98/3000, Training Loss: 40.10917, Validation Loss: 5.95743\n",
      "Epoch 99/3000, Training Loss: 40.09813, Validation Loss: 5.95570\n",
      "Epoch 100/3000, Training Loss: 40.08711, Validation Loss: 5.95397\n",
      "Epoch 101/3000, Training Loss: 40.07613, Validation Loss: 5.95225\n",
      "Epoch 102/3000, Training Loss: 40.06517, Validation Loss: 5.95053\n",
      "Epoch 103/3000, Training Loss: 40.05424, Validation Loss: 5.94882\n",
      "Epoch 104/3000, Training Loss: 40.04335, Validation Loss: 5.94711\n",
      "Epoch 105/3000, Training Loss: 40.03250, Validation Loss: 5.94540\n",
      "Epoch 106/3000, Training Loss: 40.02167, Validation Loss: 5.94370\n",
      "Epoch 107/3000, Training Loss: 40.01088, Validation Loss: 5.94201\n",
      "Epoch 108/3000, Training Loss: 40.00011, Validation Loss: 5.94031\n",
      "Epoch 109/3000, Training Loss: 39.98936, Validation Loss: 5.93862\n",
      "Epoch 110/3000, Training Loss: 39.97865, Validation Loss: 5.93694\n",
      "Epoch 111/3000, Training Loss: 39.96796, Validation Loss: 5.93525\n",
      "Epoch 112/3000, Training Loss: 39.95731, Validation Loss: 5.93358\n",
      "Epoch 113/3000, Training Loss: 39.94667, Validation Loss: 5.93190\n",
      "Epoch 114/3000, Training Loss: 39.93607, Validation Loss: 5.93023\n",
      "Epoch 115/3000, Training Loss: 39.92549, Validation Loss: 5.92856\n",
      "Epoch 116/3000, Training Loss: 39.91495, Validation Loss: 5.92690\n",
      "Epoch 117/3000, Training Loss: 39.90443, Validation Loss: 5.92524\n",
      "Epoch 118/3000, Training Loss: 39.89395, Validation Loss: 5.92359\n",
      "Epoch 119/3000, Training Loss: 39.88349, Validation Loss: 5.92194\n",
      "Epoch 120/3000, Training Loss: 39.87306, Validation Loss: 5.92030\n",
      "Epoch 121/3000, Training Loss: 39.86264, Validation Loss: 5.91865\n",
      "Epoch 122/3000, Training Loss: 39.85225, Validation Loss: 5.91702\n",
      "Epoch 123/3000, Training Loss: 39.84188, Validation Loss: 5.91538\n",
      "Epoch 124/3000, Training Loss: 39.83154, Validation Loss: 5.91375\n",
      "Epoch 125/3000, Training Loss: 39.82122, Validation Loss: 5.91213\n",
      "Epoch 126/3000, Training Loss: 39.81093, Validation Loss: 5.91050\n",
      "Epoch 127/3000, Training Loss: 39.80065, Validation Loss: 5.90888\n",
      "Epoch 128/3000, Training Loss: 39.79039, Validation Loss: 5.90727\n",
      "Epoch 129/3000, Training Loss: 39.78016, Validation Loss: 5.90566\n",
      "Epoch 130/3000, Training Loss: 39.76995, Validation Loss: 5.90405\n",
      "Epoch 131/3000, Training Loss: 39.75976, Validation Loss: 5.90244\n",
      "Epoch 132/3000, Training Loss: 39.74959, Validation Loss: 5.90084\n",
      "Epoch 133/3000, Training Loss: 39.73944, Validation Loss: 5.89924\n",
      "Epoch 134/3000, Training Loss: 39.72931, Validation Loss: 5.89764\n",
      "Epoch 135/3000, Training Loss: 39.71921, Validation Loss: 5.89605\n",
      "Epoch 136/3000, Training Loss: 39.70914, Validation Loss: 5.89446\n",
      "Epoch 137/3000, Training Loss: 39.69908, Validation Loss: 5.89288\n",
      "Epoch 138/3000, Training Loss: 39.68904, Validation Loss: 5.89129\n",
      "Epoch 139/3000, Training Loss: 39.67901, Validation Loss: 5.88971\n",
      "Epoch 140/3000, Training Loss: 39.66901, Validation Loss: 5.88814\n",
      "Epoch 141/3000, Training Loss: 39.65904, Validation Loss: 5.88656\n",
      "Epoch 142/3000, Training Loss: 39.64908, Validation Loss: 5.88500\n",
      "Epoch 143/3000, Training Loss: 39.63914, Validation Loss: 5.88343\n",
      "Epoch 144/3000, Training Loss: 39.62921, Validation Loss: 5.88187\n",
      "Epoch 145/3000, Training Loss: 39.61931, Validation Loss: 5.88031\n",
      "Epoch 146/3000, Training Loss: 39.60942, Validation Loss: 5.87875\n",
      "Epoch 147/3000, Training Loss: 39.59954, Validation Loss: 5.87720\n",
      "Epoch 148/3000, Training Loss: 39.58968, Validation Loss: 5.87564\n",
      "Epoch 149/3000, Training Loss: 39.57985, Validation Loss: 5.87409\n",
      "Epoch 150/3000, Training Loss: 39.57003, Validation Loss: 5.87255\n",
      "Epoch 151/3000, Training Loss: 39.56024, Validation Loss: 5.87100\n",
      "Epoch 152/3000, Training Loss: 39.55047, Validation Loss: 5.86946\n",
      "Epoch 153/3000, Training Loss: 39.54071, Validation Loss: 5.86792\n",
      "Epoch 154/3000, Training Loss: 39.53096, Validation Loss: 5.86638\n",
      "Epoch 155/3000, Training Loss: 39.52124, Validation Loss: 5.86485\n",
      "Epoch 156/3000, Training Loss: 39.51153, Validation Loss: 5.86332\n",
      "Epoch 157/3000, Training Loss: 39.50183, Validation Loss: 5.86179\n",
      "Epoch 158/3000, Training Loss: 39.49215, Validation Loss: 5.86026\n",
      "Epoch 159/3000, Training Loss: 39.48248, Validation Loss: 5.85874\n",
      "Epoch 160/3000, Training Loss: 39.47281, Validation Loss: 5.85722\n",
      "Epoch 161/3000, Training Loss: 39.46317, Validation Loss: 5.85570\n",
      "Epoch 162/3000, Training Loss: 39.45354, Validation Loss: 5.85418\n",
      "Epoch 163/3000, Training Loss: 39.44393, Validation Loss: 5.85267\n",
      "Epoch 164/3000, Training Loss: 39.43434, Validation Loss: 5.85116\n",
      "Epoch 165/3000, Training Loss: 39.42478, Validation Loss: 5.84965\n",
      "Epoch 166/3000, Training Loss: 39.41523, Validation Loss: 5.84814\n",
      "Epoch 167/3000, Training Loss: 39.40569, Validation Loss: 5.84663\n",
      "Epoch 168/3000, Training Loss: 39.39617, Validation Loss: 5.84513\n",
      "Epoch 169/3000, Training Loss: 39.38666, Validation Loss: 5.84362\n",
      "Epoch 170/3000, Training Loss: 39.37717, Validation Loss: 5.84212\n",
      "Epoch 171/3000, Training Loss: 39.36769, Validation Loss: 5.84062\n",
      "Epoch 172/3000, Training Loss: 39.35823, Validation Loss: 5.83912\n",
      "Epoch 173/3000, Training Loss: 39.34877, Validation Loss: 5.83763\n",
      "Epoch 174/3000, Training Loss: 39.33933, Validation Loss: 5.83614\n",
      "Epoch 175/3000, Training Loss: 39.32990, Validation Loss: 5.83465\n",
      "Epoch 176/3000, Training Loss: 39.32048, Validation Loss: 5.83316\n",
      "Epoch 177/3000, Training Loss: 39.31107, Validation Loss: 5.83167\n",
      "Epoch 178/3000, Training Loss: 39.30168, Validation Loss: 5.83018\n",
      "Epoch 179/3000, Training Loss: 39.29230, Validation Loss: 5.82869\n",
      "Epoch 180/3000, Training Loss: 39.28295, Validation Loss: 5.82721\n",
      "Epoch 181/3000, Training Loss: 39.27361, Validation Loss: 5.82573\n",
      "Epoch 182/3000, Training Loss: 39.26428, Validation Loss: 5.82425\n",
      "Epoch 183/3000, Training Loss: 39.25498, Validation Loss: 5.82278\n",
      "Epoch 184/3000, Training Loss: 39.24568, Validation Loss: 5.82131\n",
      "Epoch 185/3000, Training Loss: 39.23640, Validation Loss: 5.81984\n",
      "Epoch 186/3000, Training Loss: 39.22714, Validation Loss: 5.81837\n",
      "Epoch 187/3000, Training Loss: 39.21789, Validation Loss: 5.81690\n",
      "Epoch 188/3000, Training Loss: 39.20865, Validation Loss: 5.81543\n",
      "Epoch 189/3000, Training Loss: 39.19942, Validation Loss: 5.81396\n",
      "Epoch 190/3000, Training Loss: 39.19020, Validation Loss: 5.81250\n",
      "Epoch 191/3000, Training Loss: 39.18100, Validation Loss: 5.81104\n",
      "Epoch 192/3000, Training Loss: 39.17181, Validation Loss: 5.80957\n",
      "Epoch 193/3000, Training Loss: 39.16264, Validation Loss: 5.80811\n",
      "Epoch 194/3000, Training Loss: 39.15347, Validation Loss: 5.80665\n",
      "Epoch 195/3000, Training Loss: 39.14431, Validation Loss: 5.80519\n",
      "Epoch 196/3000, Training Loss: 39.13517, Validation Loss: 5.80374\n",
      "Epoch 197/3000, Training Loss: 39.12604, Validation Loss: 5.80228\n",
      "Epoch 198/3000, Training Loss: 39.11692, Validation Loss: 5.80083\n",
      "Epoch 199/3000, Training Loss: 39.10781, Validation Loss: 5.79937\n",
      "Epoch 200/3000, Training Loss: 39.09871, Validation Loss: 5.79792\n",
      "Epoch 201/3000, Training Loss: 39.08962, Validation Loss: 5.79647\n",
      "Epoch 202/3000, Training Loss: 39.08054, Validation Loss: 5.79503\n",
      "Epoch 203/3000, Training Loss: 39.07148, Validation Loss: 5.79358\n",
      "Epoch 204/3000, Training Loss: 39.06242, Validation Loss: 5.79213\n",
      "Epoch 205/3000, Training Loss: 39.05337, Validation Loss: 5.79069\n",
      "Epoch 206/3000, Training Loss: 39.04433, Validation Loss: 5.78924\n",
      "Epoch 207/3000, Training Loss: 39.03529, Validation Loss: 5.78780\n",
      "Epoch 208/3000, Training Loss: 39.02627, Validation Loss: 5.78636\n",
      "Epoch 209/3000, Training Loss: 39.01725, Validation Loss: 5.78492\n",
      "Epoch 210/3000, Training Loss: 39.00824, Validation Loss: 5.78348\n",
      "Epoch 211/3000, Training Loss: 38.99924, Validation Loss: 5.78205\n",
      "Epoch 212/3000, Training Loss: 38.99025, Validation Loss: 5.78061\n",
      "Epoch 213/3000, Training Loss: 38.98127, Validation Loss: 5.77918\n",
      "Epoch 214/3000, Training Loss: 38.97230, Validation Loss: 5.77774\n",
      "Epoch 215/3000, Training Loss: 38.96333, Validation Loss: 5.77631\n",
      "Epoch 216/3000, Training Loss: 38.95438, Validation Loss: 5.77487\n",
      "Epoch 217/3000, Training Loss: 38.94544, Validation Loss: 5.77344\n",
      "Epoch 218/3000, Training Loss: 38.93651, Validation Loss: 5.77201\n",
      "Epoch 219/3000, Training Loss: 38.92759, Validation Loss: 5.77058\n",
      "Epoch 220/3000, Training Loss: 38.91867, Validation Loss: 5.76915\n",
      "Epoch 221/3000, Training Loss: 38.90977, Validation Loss: 5.76772\n",
      "Epoch 222/3000, Training Loss: 38.90087, Validation Loss: 5.76630\n",
      "Epoch 223/3000, Training Loss: 38.89197, Validation Loss: 5.76487\n",
      "Epoch 224/3000, Training Loss: 38.88309, Validation Loss: 5.76345\n",
      "Epoch 225/3000, Training Loss: 38.87420, Validation Loss: 5.76203\n",
      "Epoch 226/3000, Training Loss: 38.86533, Validation Loss: 5.76061\n",
      "Epoch 227/3000, Training Loss: 38.85646, Validation Loss: 5.75919\n",
      "Epoch 228/3000, Training Loss: 38.84760, Validation Loss: 5.75777\n",
      "Epoch 229/3000, Training Loss: 38.83876, Validation Loss: 5.75635\n",
      "Epoch 230/3000, Training Loss: 38.82991, Validation Loss: 5.75493\n",
      "Epoch 231/3000, Training Loss: 38.82108, Validation Loss: 5.75352\n",
      "Epoch 232/3000, Training Loss: 38.81226, Validation Loss: 5.75210\n",
      "Epoch 233/3000, Training Loss: 38.80344, Validation Loss: 5.75069\n",
      "Epoch 234/3000, Training Loss: 38.79462, Validation Loss: 5.74927\n",
      "Epoch 235/3000, Training Loss: 38.78580, Validation Loss: 5.74786\n",
      "Epoch 236/3000, Training Loss: 38.77699, Validation Loss: 5.74644\n",
      "Epoch 237/3000, Training Loss: 38.76819, Validation Loss: 5.74503\n",
      "Epoch 238/3000, Training Loss: 38.75939, Validation Loss: 5.74362\n",
      "Epoch 239/3000, Training Loss: 38.75059, Validation Loss: 5.74220\n",
      "Epoch 240/3000, Training Loss: 38.74180, Validation Loss: 5.74079\n",
      "Epoch 241/3000, Training Loss: 38.73302, Validation Loss: 5.73938\n",
      "Epoch 242/3000, Training Loss: 38.72424, Validation Loss: 5.73797\n",
      "Epoch 243/3000, Training Loss: 38.71546, Validation Loss: 5.73656\n",
      "Epoch 244/3000, Training Loss: 38.70669, Validation Loss: 5.73516\n",
      "Epoch 245/3000, Training Loss: 38.69793, Validation Loss: 5.73375\n",
      "Epoch 246/3000, Training Loss: 38.68917, Validation Loss: 5.73235\n",
      "Epoch 247/3000, Training Loss: 38.68042, Validation Loss: 5.73094\n",
      "Epoch 248/3000, Training Loss: 38.67167, Validation Loss: 5.72954\n",
      "Epoch 249/3000, Training Loss: 38.66293, Validation Loss: 5.72814\n",
      "Epoch 250/3000, Training Loss: 38.65419, Validation Loss: 5.72673\n",
      "Epoch 251/3000, Training Loss: 38.64546, Validation Loss: 5.72533\n",
      "Epoch 252/3000, Training Loss: 38.63674, Validation Loss: 5.72393\n",
      "Epoch 253/3000, Training Loss: 38.62802, Validation Loss: 5.72253\n",
      "Epoch 254/3000, Training Loss: 38.61932, Validation Loss: 5.72113\n",
      "Epoch 255/3000, Training Loss: 38.61061, Validation Loss: 5.71973\n",
      "Epoch 256/3000, Training Loss: 38.60191, Validation Loss: 5.71833\n",
      "Epoch 257/3000, Training Loss: 38.59321, Validation Loss: 5.71692\n",
      "Epoch 258/3000, Training Loss: 38.58451, Validation Loss: 5.71552\n",
      "Epoch 259/3000, Training Loss: 38.57581, Validation Loss: 5.71412\n",
      "Epoch 260/3000, Training Loss: 38.56711, Validation Loss: 5.71272\n",
      "Epoch 261/3000, Training Loss: 38.55842, Validation Loss: 5.71132\n",
      "Epoch 262/3000, Training Loss: 38.54973, Validation Loss: 5.70992\n",
      "Epoch 263/3000, Training Loss: 38.54105, Validation Loss: 5.70852\n",
      "Epoch 264/3000, Training Loss: 38.53237, Validation Loss: 5.70712\n",
      "Epoch 265/3000, Training Loss: 38.52368, Validation Loss: 5.70572\n",
      "Epoch 266/3000, Training Loss: 38.51501, Validation Loss: 5.70432\n",
      "Epoch 267/3000, Training Loss: 38.50634, Validation Loss: 5.70292\n",
      "Epoch 268/3000, Training Loss: 38.49767, Validation Loss: 5.70152\n",
      "Epoch 269/3000, Training Loss: 38.48902, Validation Loss: 5.70012\n",
      "Epoch 270/3000, Training Loss: 38.48037, Validation Loss: 5.69871\n",
      "Epoch 271/3000, Training Loss: 38.47172, Validation Loss: 5.69731\n",
      "Epoch 272/3000, Training Loss: 38.46308, Validation Loss: 5.69591\n",
      "Epoch 273/3000, Training Loss: 38.45443, Validation Loss: 5.69451\n",
      "Epoch 274/3000, Training Loss: 38.44579, Validation Loss: 5.69310\n",
      "Epoch 275/3000, Training Loss: 38.43715, Validation Loss: 5.69170\n",
      "Epoch 276/3000, Training Loss: 38.42852, Validation Loss: 5.69030\n",
      "Epoch 277/3000, Training Loss: 38.41988, Validation Loss: 5.68890\n",
      "Epoch 278/3000, Training Loss: 38.41124, Validation Loss: 5.68750\n",
      "Epoch 279/3000, Training Loss: 38.40261, Validation Loss: 5.68609\n",
      "Epoch 280/3000, Training Loss: 38.39398, Validation Loss: 5.68469\n",
      "Epoch 281/3000, Training Loss: 38.38536, Validation Loss: 5.68329\n",
      "Epoch 282/3000, Training Loss: 38.37675, Validation Loss: 5.68190\n",
      "Epoch 283/3000, Training Loss: 38.36814, Validation Loss: 5.68050\n",
      "Epoch 284/3000, Training Loss: 38.35954, Validation Loss: 5.67910\n",
      "Epoch 285/3000, Training Loss: 38.35093, Validation Loss: 5.67770\n",
      "Epoch 286/3000, Training Loss: 38.34233, Validation Loss: 5.67631\n",
      "Epoch 287/3000, Training Loss: 38.33373, Validation Loss: 5.67491\n",
      "Epoch 288/3000, Training Loss: 38.32512, Validation Loss: 5.67351\n",
      "Epoch 289/3000, Training Loss: 38.31652, Validation Loss: 5.67211\n",
      "Epoch 290/3000, Training Loss: 38.30792, Validation Loss: 5.67071\n",
      "Epoch 291/3000, Training Loss: 38.29932, Validation Loss: 5.66932\n",
      "Epoch 292/3000, Training Loss: 38.29072, Validation Loss: 5.66792\n",
      "Epoch 293/3000, Training Loss: 38.28212, Validation Loss: 5.66652\n",
      "Epoch 294/3000, Training Loss: 38.27352, Validation Loss: 5.66513\n",
      "Epoch 295/3000, Training Loss: 38.26493, Validation Loss: 5.66373\n",
      "Epoch 296/3000, Training Loss: 38.25634, Validation Loss: 5.66234\n",
      "Epoch 297/3000, Training Loss: 38.24775, Validation Loss: 5.66094\n",
      "Epoch 298/3000, Training Loss: 38.23917, Validation Loss: 5.65955\n",
      "Epoch 299/3000, Training Loss: 38.23058, Validation Loss: 5.65815\n",
      "Epoch 300/3000, Training Loss: 38.22200, Validation Loss: 5.65675\n",
      "Epoch 301/3000, Training Loss: 38.21342, Validation Loss: 5.65536\n",
      "Epoch 302/3000, Training Loss: 38.20484, Validation Loss: 5.65396\n",
      "Epoch 303/3000, Training Loss: 38.19627, Validation Loss: 5.65257\n",
      "Epoch 304/3000, Training Loss: 38.18770, Validation Loss: 5.65117\n",
      "Epoch 305/3000, Training Loss: 38.17912, Validation Loss: 5.64978\n",
      "Epoch 306/3000, Training Loss: 38.17054, Validation Loss: 5.64838\n",
      "Epoch 307/3000, Training Loss: 38.16196, Validation Loss: 5.64698\n",
      "Epoch 308/3000, Training Loss: 38.15337, Validation Loss: 5.64559\n",
      "Epoch 309/3000, Training Loss: 38.14479, Validation Loss: 5.64419\n",
      "Epoch 310/3000, Training Loss: 38.13621, Validation Loss: 5.64280\n",
      "Epoch 311/3000, Training Loss: 38.12764, Validation Loss: 5.64140\n",
      "Epoch 312/3000, Training Loss: 38.11907, Validation Loss: 5.64000\n",
      "Epoch 313/3000, Training Loss: 38.11050, Validation Loss: 5.63861\n",
      "Epoch 314/3000, Training Loss: 38.10192, Validation Loss: 5.63721\n",
      "Epoch 315/3000, Training Loss: 38.09335, Validation Loss: 5.63581\n",
      "Epoch 316/3000, Training Loss: 38.08478, Validation Loss: 5.63441\n",
      "Epoch 317/3000, Training Loss: 38.07621, Validation Loss: 5.63301\n",
      "Epoch 318/3000, Training Loss: 38.06762, Validation Loss: 5.63161\n",
      "Epoch 319/3000, Training Loss: 38.05904, Validation Loss: 5.63022\n",
      "Epoch 320/3000, Training Loss: 38.05046, Validation Loss: 5.62881\n",
      "Epoch 321/3000, Training Loss: 38.04189, Validation Loss: 5.62741\n",
      "Epoch 322/3000, Training Loss: 38.03331, Validation Loss: 5.62601\n",
      "Epoch 323/3000, Training Loss: 38.02473, Validation Loss: 5.62461\n",
      "Epoch 324/3000, Training Loss: 38.01615, Validation Loss: 5.62321\n",
      "Epoch 325/3000, Training Loss: 38.00757, Validation Loss: 5.62180\n",
      "Epoch 326/3000, Training Loss: 37.99899, Validation Loss: 5.62040\n",
      "Epoch 327/3000, Training Loss: 37.99042, Validation Loss: 5.61900\n",
      "Epoch 328/3000, Training Loss: 37.98185, Validation Loss: 5.61759\n",
      "Epoch 329/3000, Training Loss: 37.97327, Validation Loss: 5.61619\n",
      "Epoch 330/3000, Training Loss: 37.96469, Validation Loss: 5.61478\n",
      "Epoch 331/3000, Training Loss: 37.95611, Validation Loss: 5.61338\n",
      "Epoch 332/3000, Training Loss: 37.94753, Validation Loss: 5.61197\n",
      "Epoch 333/3000, Training Loss: 37.93895, Validation Loss: 5.61056\n",
      "Epoch 334/3000, Training Loss: 37.93038, Validation Loss: 5.60916\n",
      "Epoch 335/3000, Training Loss: 37.92180, Validation Loss: 5.60776\n",
      "Epoch 336/3000, Training Loss: 37.91323, Validation Loss: 5.60635\n",
      "Epoch 337/3000, Training Loss: 37.90466, Validation Loss: 5.60495\n",
      "Epoch 338/3000, Training Loss: 37.89608, Validation Loss: 5.60354\n",
      "Epoch 339/3000, Training Loss: 37.88750, Validation Loss: 5.60214\n",
      "Epoch 340/3000, Training Loss: 37.87893, Validation Loss: 5.60074\n",
      "Epoch 341/3000, Training Loss: 37.87034, Validation Loss: 5.59933\n",
      "Epoch 342/3000, Training Loss: 37.86176, Validation Loss: 5.59793\n",
      "Epoch 343/3000, Training Loss: 37.85318, Validation Loss: 5.59653\n",
      "Epoch 344/3000, Training Loss: 37.84460, Validation Loss: 5.59513\n",
      "Epoch 345/3000, Training Loss: 37.83601, Validation Loss: 5.59372\n",
      "Epoch 346/3000, Training Loss: 37.82742, Validation Loss: 5.59232\n",
      "Epoch 347/3000, Training Loss: 37.81883, Validation Loss: 5.59092\n",
      "Epoch 348/3000, Training Loss: 37.81023, Validation Loss: 5.58952\n",
      "Epoch 349/3000, Training Loss: 37.80163, Validation Loss: 5.58811\n",
      "Epoch 350/3000, Training Loss: 37.79302, Validation Loss: 5.58671\n",
      "Epoch 351/3000, Training Loss: 37.78441, Validation Loss: 5.58530\n",
      "Epoch 352/3000, Training Loss: 37.77580, Validation Loss: 5.58389\n",
      "Epoch 353/3000, Training Loss: 37.76718, Validation Loss: 5.58248\n",
      "Epoch 354/3000, Training Loss: 37.75856, Validation Loss: 5.58107\n",
      "Epoch 355/3000, Training Loss: 37.74994, Validation Loss: 5.57966\n",
      "Epoch 356/3000, Training Loss: 37.74132, Validation Loss: 5.57825\n",
      "Epoch 357/3000, Training Loss: 37.73270, Validation Loss: 5.57684\n",
      "Epoch 358/3000, Training Loss: 37.72408, Validation Loss: 5.57542\n",
      "Epoch 359/3000, Training Loss: 37.71547, Validation Loss: 5.57401\n",
      "Epoch 360/3000, Training Loss: 37.70685, Validation Loss: 5.57260\n",
      "Epoch 361/3000, Training Loss: 37.69823, Validation Loss: 5.57119\n",
      "Epoch 362/3000, Training Loss: 37.68961, Validation Loss: 5.56978\n",
      "Epoch 363/3000, Training Loss: 37.68099, Validation Loss: 5.56836\n",
      "Epoch 364/3000, Training Loss: 37.67236, Validation Loss: 5.56695\n",
      "Epoch 365/3000, Training Loss: 37.66373, Validation Loss: 5.56554\n",
      "Epoch 366/3000, Training Loss: 37.65510, Validation Loss: 5.56412\n",
      "Epoch 367/3000, Training Loss: 37.64647, Validation Loss: 5.56270\n",
      "Epoch 368/3000, Training Loss: 37.63783, Validation Loss: 5.56128\n",
      "Epoch 369/3000, Training Loss: 37.62919, Validation Loss: 5.55986\n",
      "Epoch 370/3000, Training Loss: 37.62054, Validation Loss: 5.55844\n",
      "Epoch 371/3000, Training Loss: 37.61189, Validation Loss: 5.55701\n",
      "Epoch 372/3000, Training Loss: 37.60324, Validation Loss: 5.55559\n",
      "Epoch 373/3000, Training Loss: 37.59458, Validation Loss: 5.55417\n",
      "Epoch 374/3000, Training Loss: 37.58592, Validation Loss: 5.55274\n",
      "Epoch 375/3000, Training Loss: 37.57726, Validation Loss: 5.55132\n",
      "Epoch 376/3000, Training Loss: 37.56859, Validation Loss: 5.54989\n",
      "Epoch 377/3000, Training Loss: 37.55994, Validation Loss: 5.54847\n",
      "Epoch 378/3000, Training Loss: 37.55127, Validation Loss: 5.54704\n",
      "Epoch 379/3000, Training Loss: 37.54261, Validation Loss: 5.54561\n",
      "Epoch 380/3000, Training Loss: 37.53394, Validation Loss: 5.54418\n",
      "Epoch 381/3000, Training Loss: 37.52527, Validation Loss: 5.54275\n",
      "Epoch 382/3000, Training Loss: 37.51659, Validation Loss: 5.54132\n",
      "Epoch 383/3000, Training Loss: 37.50791, Validation Loss: 5.53988\n",
      "Epoch 384/3000, Training Loss: 37.49922, Validation Loss: 5.53845\n",
      "Epoch 385/3000, Training Loss: 37.49053, Validation Loss: 5.53702\n",
      "Epoch 386/3000, Training Loss: 37.48184, Validation Loss: 5.53558\n",
      "Epoch 387/3000, Training Loss: 37.47315, Validation Loss: 5.53415\n",
      "Epoch 388/3000, Training Loss: 37.46445, Validation Loss: 5.53271\n",
      "Epoch 389/3000, Training Loss: 37.45575, Validation Loss: 5.53127\n",
      "Epoch 390/3000, Training Loss: 37.44705, Validation Loss: 5.52983\n",
      "Epoch 391/3000, Training Loss: 37.43835, Validation Loss: 5.52839\n",
      "Epoch 392/3000, Training Loss: 37.42964, Validation Loss: 5.52695\n",
      "Epoch 393/3000, Training Loss: 37.42093, Validation Loss: 5.52551\n",
      "Epoch 394/3000, Training Loss: 37.41223, Validation Loss: 5.52407\n",
      "Epoch 395/3000, Training Loss: 37.40353, Validation Loss: 5.52263\n",
      "Epoch 396/3000, Training Loss: 37.39482, Validation Loss: 5.52119\n",
      "Epoch 397/3000, Training Loss: 37.38610, Validation Loss: 5.51975\n",
      "Epoch 398/3000, Training Loss: 37.37739, Validation Loss: 5.51830\n",
      "Epoch 399/3000, Training Loss: 37.36866, Validation Loss: 5.51686\n",
      "Epoch 400/3000, Training Loss: 37.35994, Validation Loss: 5.51541\n",
      "Epoch 401/3000, Training Loss: 37.35121, Validation Loss: 5.51397\n",
      "Epoch 402/3000, Training Loss: 37.34248, Validation Loss: 5.51253\n",
      "Epoch 403/3000, Training Loss: 37.33374, Validation Loss: 5.51108\n",
      "Epoch 404/3000, Training Loss: 37.32500, Validation Loss: 5.50963\n",
      "Epoch 405/3000, Training Loss: 37.31626, Validation Loss: 5.50819\n",
      "Epoch 406/3000, Training Loss: 37.30753, Validation Loss: 5.50674\n",
      "Epoch 407/3000, Training Loss: 37.29879, Validation Loss: 5.50529\n",
      "Epoch 408/3000, Training Loss: 37.29005, Validation Loss: 5.50384\n",
      "Epoch 409/3000, Training Loss: 37.28131, Validation Loss: 5.50240\n",
      "Epoch 410/3000, Training Loss: 37.27256, Validation Loss: 5.50095\n",
      "Epoch 411/3000, Training Loss: 37.26380, Validation Loss: 5.49950\n",
      "Epoch 412/3000, Training Loss: 37.25504, Validation Loss: 5.49805\n",
      "Epoch 413/3000, Training Loss: 37.24627, Validation Loss: 5.49660\n",
      "Epoch 414/3000, Training Loss: 37.23750, Validation Loss: 5.49515\n",
      "Epoch 415/3000, Training Loss: 37.22873, Validation Loss: 5.49369\n",
      "Epoch 416/3000, Training Loss: 37.21995, Validation Loss: 5.49224\n",
      "Epoch 417/3000, Training Loss: 37.21118, Validation Loss: 5.49078\n",
      "Epoch 418/3000, Training Loss: 37.20239, Validation Loss: 5.48933\n",
      "Epoch 419/3000, Training Loss: 37.19361, Validation Loss: 5.48787\n",
      "Epoch 420/3000, Training Loss: 37.18481, Validation Loss: 5.48641\n",
      "Epoch 421/3000, Training Loss: 37.17601, Validation Loss: 5.48496\n",
      "Epoch 422/3000, Training Loss: 37.16721, Validation Loss: 5.48350\n",
      "Epoch 423/3000, Training Loss: 37.15841, Validation Loss: 5.48204\n",
      "Epoch 424/3000, Training Loss: 37.14961, Validation Loss: 5.48057\n",
      "Epoch 425/3000, Training Loss: 37.14079, Validation Loss: 5.47911\n",
      "Epoch 426/3000, Training Loss: 37.13198, Validation Loss: 5.47765\n",
      "Epoch 427/3000, Training Loss: 37.12315, Validation Loss: 5.47618\n",
      "Epoch 428/3000, Training Loss: 37.11432, Validation Loss: 5.47472\n",
      "Epoch 429/3000, Training Loss: 37.10549, Validation Loss: 5.47325\n",
      "Epoch 430/3000, Training Loss: 37.09665, Validation Loss: 5.47178\n",
      "Epoch 431/3000, Training Loss: 37.08781, Validation Loss: 5.47031\n",
      "Epoch 432/3000, Training Loss: 37.07896, Validation Loss: 5.46884\n",
      "Epoch 433/3000, Training Loss: 37.07011, Validation Loss: 5.46737\n",
      "Epoch 434/3000, Training Loss: 37.06127, Validation Loss: 5.46590\n",
      "Epoch 435/3000, Training Loss: 37.05242, Validation Loss: 5.46443\n",
      "Epoch 436/3000, Training Loss: 37.04357, Validation Loss: 5.46296\n",
      "Epoch 437/3000, Training Loss: 37.03473, Validation Loss: 5.46148\n",
      "Epoch 438/3000, Training Loss: 37.02588, Validation Loss: 5.46001\n",
      "Epoch 439/3000, Training Loss: 37.01701, Validation Loss: 5.45853\n",
      "Epoch 440/3000, Training Loss: 37.00813, Validation Loss: 5.45705\n",
      "Epoch 441/3000, Training Loss: 36.99924, Validation Loss: 5.45557\n",
      "Epoch 442/3000, Training Loss: 36.99035, Validation Loss: 5.45409\n",
      "Epoch 443/3000, Training Loss: 36.98145, Validation Loss: 5.45260\n",
      "Epoch 444/3000, Training Loss: 36.97255, Validation Loss: 5.45112\n",
      "Epoch 445/3000, Training Loss: 36.96363, Validation Loss: 5.44963\n",
      "Epoch 446/3000, Training Loss: 36.95471, Validation Loss: 5.44814\n",
      "Epoch 447/3000, Training Loss: 36.94579, Validation Loss: 5.44665\n",
      "Epoch 448/3000, Training Loss: 36.93686, Validation Loss: 5.44516\n",
      "Epoch 449/3000, Training Loss: 36.92793, Validation Loss: 5.44367\n",
      "Epoch 450/3000, Training Loss: 36.91900, Validation Loss: 5.44218\n",
      "Epoch 451/3000, Training Loss: 36.91007, Validation Loss: 5.44069\n",
      "Epoch 452/3000, Training Loss: 36.90115, Validation Loss: 5.43920\n",
      "Epoch 453/3000, Training Loss: 36.89222, Validation Loss: 5.43770\n",
      "Epoch 454/3000, Training Loss: 36.88328, Validation Loss: 5.43621\n",
      "Epoch 455/3000, Training Loss: 36.87434, Validation Loss: 5.43472\n",
      "Epoch 456/3000, Training Loss: 36.86539, Validation Loss: 5.43323\n",
      "Epoch 457/3000, Training Loss: 36.85645, Validation Loss: 5.43173\n",
      "Epoch 458/3000, Training Loss: 36.84752, Validation Loss: 5.43024\n",
      "Epoch 459/3000, Training Loss: 36.83857, Validation Loss: 5.42875\n",
      "Epoch 460/3000, Training Loss: 36.82962, Validation Loss: 5.42725\n",
      "Epoch 461/3000, Training Loss: 36.82067, Validation Loss: 5.42576\n",
      "Epoch 462/3000, Training Loss: 36.81172, Validation Loss: 5.42426\n",
      "Epoch 463/3000, Training Loss: 36.80276, Validation Loss: 5.42276\n",
      "Epoch 464/3000, Training Loss: 36.79380, Validation Loss: 5.42125\n",
      "Epoch 465/3000, Training Loss: 36.78484, Validation Loss: 5.41975\n",
      "Epoch 466/3000, Training Loss: 36.77588, Validation Loss: 5.41825\n",
      "Epoch 467/3000, Training Loss: 36.76690, Validation Loss: 5.41674\n",
      "Epoch 468/3000, Training Loss: 36.75791, Validation Loss: 5.41523\n",
      "Epoch 469/3000, Training Loss: 36.74893, Validation Loss: 5.41373\n",
      "Epoch 470/3000, Training Loss: 36.73993, Validation Loss: 5.41222\n",
      "Epoch 471/3000, Training Loss: 36.73095, Validation Loss: 5.41071\n",
      "Epoch 472/3000, Training Loss: 36.72197, Validation Loss: 5.40920\n",
      "Epoch 473/3000, Training Loss: 36.71298, Validation Loss: 5.40769\n",
      "Epoch 474/3000, Training Loss: 36.70399, Validation Loss: 5.40618\n",
      "Epoch 475/3000, Training Loss: 36.69500, Validation Loss: 5.40467\n",
      "Epoch 476/3000, Training Loss: 36.68599, Validation Loss: 5.40315\n",
      "Epoch 477/3000, Training Loss: 36.67698, Validation Loss: 5.40164\n",
      "Epoch 478/3000, Training Loss: 36.66797, Validation Loss: 5.40012\n",
      "Epoch 479/3000, Training Loss: 36.65895, Validation Loss: 5.39861\n",
      "Epoch 480/3000, Training Loss: 36.64993, Validation Loss: 5.39709\n",
      "Epoch 481/3000, Training Loss: 36.64091, Validation Loss: 5.39557\n",
      "Epoch 482/3000, Training Loss: 36.63188, Validation Loss: 5.39405\n",
      "Epoch 483/3000, Training Loss: 36.62286, Validation Loss: 5.39253\n",
      "Epoch 484/3000, Training Loss: 36.61382, Validation Loss: 5.39101\n",
      "Epoch 485/3000, Training Loss: 36.60478, Validation Loss: 5.38949\n",
      "Epoch 486/3000, Training Loss: 36.59573, Validation Loss: 5.38797\n",
      "Epoch 487/3000, Training Loss: 36.58668, Validation Loss: 5.38644\n",
      "Epoch 488/3000, Training Loss: 36.57762, Validation Loss: 5.38492\n",
      "Epoch 489/3000, Training Loss: 36.56855, Validation Loss: 5.38339\n",
      "Epoch 490/3000, Training Loss: 36.55949, Validation Loss: 5.38186\n",
      "Epoch 491/3000, Training Loss: 36.55041, Validation Loss: 5.38033\n",
      "Epoch 492/3000, Training Loss: 36.54133, Validation Loss: 5.37880\n",
      "Epoch 493/3000, Training Loss: 36.53225, Validation Loss: 5.37727\n",
      "Epoch 494/3000, Training Loss: 36.52317, Validation Loss: 5.37574\n",
      "Epoch 495/3000, Training Loss: 36.51408, Validation Loss: 5.37420\n",
      "Epoch 496/3000, Training Loss: 36.50499, Validation Loss: 5.37267\n",
      "Epoch 497/3000, Training Loss: 36.49589, Validation Loss: 5.37113\n",
      "Epoch 498/3000, Training Loss: 36.48679, Validation Loss: 5.36959\n",
      "Epoch 499/3000, Training Loss: 36.47767, Validation Loss: 5.36805\n",
      "Epoch 500/3000, Training Loss: 36.46856, Validation Loss: 5.36651\n",
      "Epoch 501/3000, Training Loss: 36.45944, Validation Loss: 5.36497\n",
      "Epoch 502/3000, Training Loss: 36.45033, Validation Loss: 5.36343\n",
      "Epoch 503/3000, Training Loss: 36.44121, Validation Loss: 5.36189\n",
      "Epoch 504/3000, Training Loss: 36.43209, Validation Loss: 5.36035\n",
      "Epoch 505/3000, Training Loss: 36.42297, Validation Loss: 5.35881\n",
      "Epoch 506/3000, Training Loss: 36.41384, Validation Loss: 5.35726\n",
      "Epoch 507/3000, Training Loss: 36.40470, Validation Loss: 5.35572\n",
      "Epoch 508/3000, Training Loss: 36.39556, Validation Loss: 5.35417\n",
      "Epoch 509/3000, Training Loss: 36.38641, Validation Loss: 5.35263\n",
      "Epoch 510/3000, Training Loss: 36.37726, Validation Loss: 5.35108\n",
      "Epoch 511/3000, Training Loss: 36.36810, Validation Loss: 5.34954\n",
      "Epoch 512/3000, Training Loss: 36.35894, Validation Loss: 5.34799\n",
      "Epoch 513/3000, Training Loss: 36.34977, Validation Loss: 5.34644\n",
      "Epoch 514/3000, Training Loss: 36.34060, Validation Loss: 5.34489\n",
      "Epoch 515/3000, Training Loss: 36.33143, Validation Loss: 5.34334\n",
      "Epoch 516/3000, Training Loss: 36.32226, Validation Loss: 5.34179\n",
      "Epoch 517/3000, Training Loss: 36.31309, Validation Loss: 5.34024\n",
      "Epoch 518/3000, Training Loss: 36.30392, Validation Loss: 5.33868\n",
      "Epoch 519/3000, Training Loss: 36.29473, Validation Loss: 5.33712\n",
      "Epoch 520/3000, Training Loss: 36.28553, Validation Loss: 5.33557\n",
      "Epoch 521/3000, Training Loss: 36.27634, Validation Loss: 5.33401\n",
      "Epoch 522/3000, Training Loss: 36.26714, Validation Loss: 5.33245\n",
      "Epoch 523/3000, Training Loss: 36.25793, Validation Loss: 5.33089\n",
      "Epoch 524/3000, Training Loss: 36.24873, Validation Loss: 5.32933\n",
      "Epoch 525/3000, Training Loss: 36.23952, Validation Loss: 5.32777\n",
      "Epoch 526/3000, Training Loss: 36.23031, Validation Loss: 5.32621\n",
      "Epoch 527/3000, Training Loss: 36.22111, Validation Loss: 5.32465\n",
      "Epoch 528/3000, Training Loss: 36.21191, Validation Loss: 5.32309\n",
      "Epoch 529/3000, Training Loss: 36.20271, Validation Loss: 5.32153\n",
      "Epoch 530/3000, Training Loss: 36.19350, Validation Loss: 5.31997\n",
      "Epoch 531/3000, Training Loss: 36.18429, Validation Loss: 5.31840\n",
      "Epoch 532/3000, Training Loss: 36.17507, Validation Loss: 5.31684\n",
      "Epoch 533/3000, Training Loss: 36.16585, Validation Loss: 5.31527\n",
      "Epoch 534/3000, Training Loss: 36.15663, Validation Loss: 5.31371\n",
      "Epoch 535/3000, Training Loss: 36.14740, Validation Loss: 5.31214\n",
      "Epoch 536/3000, Training Loss: 36.13817, Validation Loss: 5.31058\n",
      "Epoch 537/3000, Training Loss: 36.12894, Validation Loss: 5.30901\n",
      "Epoch 538/3000, Training Loss: 36.11971, Validation Loss: 5.30744\n",
      "Epoch 539/3000, Training Loss: 36.11047, Validation Loss: 5.30586\n",
      "Epoch 540/3000, Training Loss: 36.10123, Validation Loss: 5.30429\n",
      "Epoch 541/3000, Training Loss: 36.09198, Validation Loss: 5.30272\n",
      "Epoch 542/3000, Training Loss: 36.08273, Validation Loss: 5.30114\n",
      "Epoch 543/3000, Training Loss: 36.07347, Validation Loss: 5.29956\n",
      "Epoch 544/3000, Training Loss: 36.06421, Validation Loss: 5.29798\n",
      "Epoch 545/3000, Training Loss: 36.05495, Validation Loss: 5.29640\n",
      "Epoch 546/3000, Training Loss: 36.04569, Validation Loss: 5.29482\n",
      "Epoch 547/3000, Training Loss: 36.03643, Validation Loss: 5.29323\n",
      "Epoch 548/3000, Training Loss: 36.02716, Validation Loss: 5.29165\n",
      "Epoch 549/3000, Training Loss: 36.01789, Validation Loss: 5.29006\n",
      "Epoch 550/3000, Training Loss: 36.00863, Validation Loss: 5.28848\n",
      "Epoch 551/3000, Training Loss: 35.99936, Validation Loss: 5.28689\n",
      "Epoch 552/3000, Training Loss: 35.99009, Validation Loss: 5.28530\n",
      "Epoch 553/3000, Training Loss: 35.98082, Validation Loss: 5.28371\n",
      "Epoch 554/3000, Training Loss: 35.97154, Validation Loss: 5.28212\n",
      "Epoch 555/3000, Training Loss: 35.96225, Validation Loss: 5.28053\n",
      "Epoch 556/3000, Training Loss: 35.95296, Validation Loss: 5.27894\n",
      "Epoch 557/3000, Training Loss: 35.94366, Validation Loss: 5.27735\n",
      "Epoch 558/3000, Training Loss: 35.93436, Validation Loss: 5.27575\n",
      "Epoch 559/3000, Training Loss: 35.92505, Validation Loss: 5.27416\n",
      "Epoch 560/3000, Training Loss: 35.91574, Validation Loss: 5.27257\n",
      "Epoch 561/3000, Training Loss: 35.90642, Validation Loss: 5.27097\n",
      "Epoch 562/3000, Training Loss: 35.89710, Validation Loss: 5.26938\n",
      "Epoch 563/3000, Training Loss: 35.88778, Validation Loss: 5.26778\n",
      "Epoch 564/3000, Training Loss: 35.87846, Validation Loss: 5.26618\n",
      "Epoch 565/3000, Training Loss: 35.86913, Validation Loss: 5.26458\n",
      "Epoch 566/3000, Training Loss: 35.85980, Validation Loss: 5.26299\n",
      "Epoch 567/3000, Training Loss: 35.85046, Validation Loss: 5.26139\n",
      "Epoch 568/3000, Training Loss: 35.84113, Validation Loss: 5.25979\n",
      "Epoch 569/3000, Training Loss: 35.83179, Validation Loss: 5.25819\n",
      "Epoch 570/3000, Training Loss: 35.82245, Validation Loss: 5.25659\n",
      "Epoch 571/3000, Training Loss: 35.81310, Validation Loss: 5.25499\n",
      "Epoch 572/3000, Training Loss: 35.80374, Validation Loss: 5.25339\n",
      "Epoch 573/3000, Training Loss: 35.79438, Validation Loss: 5.25178\n",
      "Epoch 574/3000, Training Loss: 35.78500, Validation Loss: 5.25018\n",
      "Epoch 575/3000, Training Loss: 35.77563, Validation Loss: 5.24857\n",
      "Epoch 576/3000, Training Loss: 35.76626, Validation Loss: 5.24697\n",
      "Epoch 577/3000, Training Loss: 35.75688, Validation Loss: 5.24536\n",
      "Epoch 578/3000, Training Loss: 35.74752, Validation Loss: 5.24375\n",
      "Epoch 579/3000, Training Loss: 35.73815, Validation Loss: 5.24214\n",
      "Epoch 580/3000, Training Loss: 35.72877, Validation Loss: 5.24053\n",
      "Epoch 581/3000, Training Loss: 35.71939, Validation Loss: 5.23892\n",
      "Epoch 582/3000, Training Loss: 35.71001, Validation Loss: 5.23731\n",
      "Epoch 583/3000, Training Loss: 35.70063, Validation Loss: 5.23569\n",
      "Epoch 584/3000, Training Loss: 35.69124, Validation Loss: 5.23408\n",
      "Epoch 585/3000, Training Loss: 35.68186, Validation Loss: 5.23246\n",
      "Epoch 586/3000, Training Loss: 35.67247, Validation Loss: 5.23084\n",
      "Epoch 587/3000, Training Loss: 35.66308, Validation Loss: 5.22921\n",
      "Epoch 588/3000, Training Loss: 35.65367, Validation Loss: 5.22759\n",
      "Epoch 589/3000, Training Loss: 35.64425, Validation Loss: 5.22596\n",
      "Epoch 590/3000, Training Loss: 35.63483, Validation Loss: 5.22433\n",
      "Epoch 591/3000, Training Loss: 35.62541, Validation Loss: 5.22270\n",
      "Epoch 592/3000, Training Loss: 35.61600, Validation Loss: 5.22107\n",
      "Epoch 593/3000, Training Loss: 35.60659, Validation Loss: 5.21945\n",
      "Epoch 594/3000, Training Loss: 35.59717, Validation Loss: 5.21782\n",
      "Epoch 595/3000, Training Loss: 35.58776, Validation Loss: 5.21618\n",
      "Epoch 596/3000, Training Loss: 35.57834, Validation Loss: 5.21455\n",
      "Epoch 597/3000, Training Loss: 35.56893, Validation Loss: 5.21291\n",
      "Epoch 598/3000, Training Loss: 35.55952, Validation Loss: 5.21127\n",
      "Epoch 599/3000, Training Loss: 35.55011, Validation Loss: 5.20964\n",
      "Epoch 600/3000, Training Loss: 35.54070, Validation Loss: 5.20800\n",
      "Epoch 601/3000, Training Loss: 35.53128, Validation Loss: 5.20636\n",
      "Epoch 602/3000, Training Loss: 35.52186, Validation Loss: 5.20472\n",
      "Epoch 603/3000, Training Loss: 35.51243, Validation Loss: 5.20308\n",
      "Epoch 604/3000, Training Loss: 35.50300, Validation Loss: 5.20144\n",
      "Epoch 605/3000, Training Loss: 35.49356, Validation Loss: 5.19979\n",
      "Epoch 606/3000, Training Loss: 35.48413, Validation Loss: 5.19815\n",
      "Epoch 607/3000, Training Loss: 35.47469, Validation Loss: 5.19651\n",
      "Epoch 608/3000, Training Loss: 35.46525, Validation Loss: 5.19486\n",
      "Epoch 609/3000, Training Loss: 35.45581, Validation Loss: 5.19321\n",
      "Epoch 610/3000, Training Loss: 35.44637, Validation Loss: 5.19156\n",
      "Epoch 611/3000, Training Loss: 35.43692, Validation Loss: 5.18991\n",
      "Epoch 612/3000, Training Loss: 35.42747, Validation Loss: 5.18826\n",
      "Epoch 613/3000, Training Loss: 35.41803, Validation Loss: 5.18661\n",
      "Epoch 614/3000, Training Loss: 35.40858, Validation Loss: 5.18496\n",
      "Epoch 615/3000, Training Loss: 35.39913, Validation Loss: 5.18330\n",
      "Epoch 616/3000, Training Loss: 35.38967, Validation Loss: 5.18165\n",
      "Epoch 617/3000, Training Loss: 35.38020, Validation Loss: 5.17999\n",
      "Epoch 618/3000, Training Loss: 35.37074, Validation Loss: 5.17834\n",
      "Epoch 619/3000, Training Loss: 35.36128, Validation Loss: 5.17668\n",
      "Epoch 620/3000, Training Loss: 35.35182, Validation Loss: 5.17502\n",
      "Epoch 621/3000, Training Loss: 35.34237, Validation Loss: 5.17337\n",
      "Epoch 622/3000, Training Loss: 35.33291, Validation Loss: 5.17171\n",
      "Epoch 623/3000, Training Loss: 35.32346, Validation Loss: 5.17005\n",
      "Epoch 624/3000, Training Loss: 35.31400, Validation Loss: 5.16840\n",
      "Epoch 625/3000, Training Loss: 35.30456, Validation Loss: 5.16674\n",
      "Epoch 626/3000, Training Loss: 35.29511, Validation Loss: 5.16508\n",
      "Epoch 627/3000, Training Loss: 35.28566, Validation Loss: 5.16341\n",
      "Epoch 628/3000, Training Loss: 35.27620, Validation Loss: 5.16175\n",
      "Epoch 629/3000, Training Loss: 35.26675, Validation Loss: 5.16009\n",
      "Epoch 630/3000, Training Loss: 35.25731, Validation Loss: 5.15843\n",
      "Epoch 631/3000, Training Loss: 35.24786, Validation Loss: 5.15677\n",
      "Epoch 632/3000, Training Loss: 35.23842, Validation Loss: 5.15511\n",
      "Epoch 633/3000, Training Loss: 35.22896, Validation Loss: 5.15345\n",
      "Epoch 634/3000, Training Loss: 35.21951, Validation Loss: 5.15179\n",
      "Epoch 635/3000, Training Loss: 35.21006, Validation Loss: 5.15013\n",
      "Epoch 636/3000, Training Loss: 35.20060, Validation Loss: 5.14846\n",
      "Epoch 637/3000, Training Loss: 35.19113, Validation Loss: 5.14680\n",
      "Epoch 638/3000, Training Loss: 35.18165, Validation Loss: 5.14514\n",
      "Epoch 639/3000, Training Loss: 35.17217, Validation Loss: 5.14347\n",
      "Epoch 640/3000, Training Loss: 35.16269, Validation Loss: 5.14180\n",
      "Epoch 641/3000, Training Loss: 35.15321, Validation Loss: 5.14014\n",
      "Epoch 642/3000, Training Loss: 35.14372, Validation Loss: 5.13847\n",
      "Epoch 643/3000, Training Loss: 35.13423, Validation Loss: 5.13680\n",
      "Epoch 644/3000, Training Loss: 35.12474, Validation Loss: 5.13514\n",
      "Epoch 645/3000, Training Loss: 35.11525, Validation Loss: 5.13347\n",
      "Epoch 646/3000, Training Loss: 35.10577, Validation Loss: 5.13180\n",
      "Epoch 647/3000, Training Loss: 35.09627, Validation Loss: 5.13013\n",
      "Epoch 648/3000, Training Loss: 35.08679, Validation Loss: 5.12847\n",
      "Epoch 649/3000, Training Loss: 35.07730, Validation Loss: 5.12680\n",
      "Epoch 650/3000, Training Loss: 35.06782, Validation Loss: 5.12513\n",
      "Epoch 651/3000, Training Loss: 35.05833, Validation Loss: 5.12346\n",
      "Epoch 652/3000, Training Loss: 35.04884, Validation Loss: 5.12179\n",
      "Epoch 653/3000, Training Loss: 35.03934, Validation Loss: 5.12012\n",
      "Epoch 654/3000, Training Loss: 35.02985, Validation Loss: 5.11845\n",
      "Epoch 655/3000, Training Loss: 35.02035, Validation Loss: 5.11678\n",
      "Epoch 656/3000, Training Loss: 35.01087, Validation Loss: 5.11511\n",
      "Epoch 657/3000, Training Loss: 35.00138, Validation Loss: 5.11343\n",
      "Epoch 658/3000, Training Loss: 34.99189, Validation Loss: 5.11176\n",
      "Epoch 659/3000, Training Loss: 34.98240, Validation Loss: 5.11008\n",
      "Epoch 660/3000, Training Loss: 34.97291, Validation Loss: 5.10840\n",
      "Epoch 661/3000, Training Loss: 34.96342, Validation Loss: 5.10672\n",
      "Epoch 662/3000, Training Loss: 34.95394, Validation Loss: 5.10504\n",
      "Epoch 663/3000, Training Loss: 34.94445, Validation Loss: 5.10335\n",
      "Epoch 664/3000, Training Loss: 34.93497, Validation Loss: 5.10167\n",
      "Epoch 665/3000, Training Loss: 34.92548, Validation Loss: 5.09998\n",
      "Epoch 666/3000, Training Loss: 34.91600, Validation Loss: 5.09830\n",
      "Epoch 667/3000, Training Loss: 34.90651, Validation Loss: 5.09661\n",
      "Epoch 668/3000, Training Loss: 34.89702, Validation Loss: 5.09493\n",
      "Epoch 669/3000, Training Loss: 34.88752, Validation Loss: 5.09324\n",
      "Epoch 670/3000, Training Loss: 34.87802, Validation Loss: 5.09155\n",
      "Epoch 671/3000, Training Loss: 34.86851, Validation Loss: 5.08986\n",
      "Epoch 672/3000, Training Loss: 34.85900, Validation Loss: 5.08817\n",
      "Epoch 673/3000, Training Loss: 34.84949, Validation Loss: 5.08648\n",
      "Epoch 674/3000, Training Loss: 34.83999, Validation Loss: 5.08479\n",
      "Epoch 675/3000, Training Loss: 34.83049, Validation Loss: 5.08310\n",
      "Epoch 676/3000, Training Loss: 34.82099, Validation Loss: 5.08141\n",
      "Epoch 677/3000, Training Loss: 34.81150, Validation Loss: 5.07972\n",
      "Epoch 678/3000, Training Loss: 34.80200, Validation Loss: 5.07803\n",
      "Epoch 679/3000, Training Loss: 34.79251, Validation Loss: 5.07634\n",
      "Epoch 680/3000, Training Loss: 34.78302, Validation Loss: 5.07465\n",
      "Epoch 681/3000, Training Loss: 34.77354, Validation Loss: 5.07296\n",
      "Epoch 682/3000, Training Loss: 34.76406, Validation Loss: 5.07127\n",
      "Epoch 683/3000, Training Loss: 34.75458, Validation Loss: 5.06958\n",
      "Epoch 684/3000, Training Loss: 34.74510, Validation Loss: 5.06788\n",
      "Epoch 685/3000, Training Loss: 34.73562, Validation Loss: 5.06619\n",
      "Epoch 686/3000, Training Loss: 34.72614, Validation Loss: 5.06449\n",
      "Epoch 687/3000, Training Loss: 34.71665, Validation Loss: 5.06280\n",
      "Epoch 688/3000, Training Loss: 34.70716, Validation Loss: 5.06110\n",
      "Epoch 689/3000, Training Loss: 34.69767, Validation Loss: 5.05941\n",
      "Epoch 690/3000, Training Loss: 34.68818, Validation Loss: 5.05771\n",
      "Epoch 691/3000, Training Loss: 34.67868, Validation Loss: 5.05601\n",
      "Epoch 692/3000, Training Loss: 34.66918, Validation Loss: 5.05431\n",
      "Epoch 693/3000, Training Loss: 34.65969, Validation Loss: 5.05262\n",
      "Epoch 694/3000, Training Loss: 34.65019, Validation Loss: 5.05092\n",
      "Epoch 695/3000, Training Loss: 34.64069, Validation Loss: 5.04922\n",
      "Epoch 696/3000, Training Loss: 34.63119, Validation Loss: 5.04752\n",
      "Epoch 697/3000, Training Loss: 34.62169, Validation Loss: 5.04581\n",
      "Epoch 698/3000, Training Loss: 34.61219, Validation Loss: 5.04411\n",
      "Epoch 699/3000, Training Loss: 34.60268, Validation Loss: 5.04241\n",
      "Epoch 700/3000, Training Loss: 34.59317, Validation Loss: 5.04071\n",
      "Epoch 701/3000, Training Loss: 34.58366, Validation Loss: 5.03900\n",
      "Epoch 702/3000, Training Loss: 34.57415, Validation Loss: 5.03730\n",
      "Epoch 703/3000, Training Loss: 34.56464, Validation Loss: 5.03559\n",
      "Epoch 704/3000, Training Loss: 34.55513, Validation Loss: 5.03389\n",
      "Epoch 705/3000, Training Loss: 34.54562, Validation Loss: 5.03218\n",
      "Epoch 706/3000, Training Loss: 34.53611, Validation Loss: 5.03048\n",
      "Epoch 707/3000, Training Loss: 34.52661, Validation Loss: 5.02877\n",
      "Epoch 708/3000, Training Loss: 34.51711, Validation Loss: 5.02705\n",
      "Epoch 709/3000, Training Loss: 34.50761, Validation Loss: 5.02534\n",
      "Epoch 710/3000, Training Loss: 34.49812, Validation Loss: 5.02362\n",
      "Epoch 711/3000, Training Loss: 34.48862, Validation Loss: 5.02191\n",
      "Epoch 712/3000, Training Loss: 34.47912, Validation Loss: 5.02019\n",
      "Epoch 713/3000, Training Loss: 34.46962, Validation Loss: 5.01847\n",
      "Epoch 714/3000, Training Loss: 34.46012, Validation Loss: 5.01675\n",
      "Epoch 715/3000, Training Loss: 34.45062, Validation Loss: 5.01503\n",
      "Epoch 716/3000, Training Loss: 34.44112, Validation Loss: 5.01331\n",
      "Epoch 717/3000, Training Loss: 34.43163, Validation Loss: 5.01159\n",
      "Epoch 718/3000, Training Loss: 34.42215, Validation Loss: 5.00988\n",
      "Epoch 719/3000, Training Loss: 34.41269, Validation Loss: 5.00816\n",
      "Epoch 720/3000, Training Loss: 34.40323, Validation Loss: 5.00645\n",
      "Epoch 721/3000, Training Loss: 34.39377, Validation Loss: 5.00474\n",
      "Epoch 722/3000, Training Loss: 34.38431, Validation Loss: 5.00302\n",
      "Epoch 723/3000, Training Loss: 34.37485, Validation Loss: 5.00130\n",
      "Epoch 724/3000, Training Loss: 34.36540, Validation Loss: 4.99958\n",
      "Epoch 725/3000, Training Loss: 34.35596, Validation Loss: 4.99787\n",
      "Epoch 726/3000, Training Loss: 34.34652, Validation Loss: 4.99615\n",
      "Epoch 727/3000, Training Loss: 34.33708, Validation Loss: 4.99443\n",
      "Epoch 728/3000, Training Loss: 34.32764, Validation Loss: 4.99271\n",
      "Epoch 729/3000, Training Loss: 34.31821, Validation Loss: 4.99100\n",
      "Epoch 730/3000, Training Loss: 34.30878, Validation Loss: 4.98928\n",
      "Epoch 731/3000, Training Loss: 34.29936, Validation Loss: 4.98756\n",
      "Epoch 732/3000, Training Loss: 34.28995, Validation Loss: 4.98584\n",
      "Epoch 733/3000, Training Loss: 34.28054, Validation Loss: 4.98413\n",
      "Epoch 734/3000, Training Loss: 34.27113, Validation Loss: 4.98241\n",
      "Epoch 735/3000, Training Loss: 34.26172, Validation Loss: 4.98070\n",
      "Epoch 736/3000, Training Loss: 34.25232, Validation Loss: 4.97898\n",
      "Epoch 737/3000, Training Loss: 34.24292, Validation Loss: 4.97727\n",
      "Epoch 738/3000, Training Loss: 34.23352, Validation Loss: 4.97555\n",
      "Epoch 739/3000, Training Loss: 34.22413, Validation Loss: 4.97383\n",
      "Epoch 740/3000, Training Loss: 34.21473, Validation Loss: 4.97211\n",
      "Epoch 741/3000, Training Loss: 34.20533, Validation Loss: 4.97040\n",
      "Epoch 742/3000, Training Loss: 34.19593, Validation Loss: 4.96868\n",
      "Epoch 743/3000, Training Loss: 34.18654, Validation Loss: 4.96696\n",
      "Epoch 744/3000, Training Loss: 34.17715, Validation Loss: 4.96525\n",
      "Epoch 745/3000, Training Loss: 34.16776, Validation Loss: 4.96353\n",
      "Epoch 746/3000, Training Loss: 34.15838, Validation Loss: 4.96182\n",
      "Epoch 747/3000, Training Loss: 34.14901, Validation Loss: 4.96010\n",
      "Epoch 748/3000, Training Loss: 34.13965, Validation Loss: 4.95837\n",
      "Epoch 749/3000, Training Loss: 34.13029, Validation Loss: 4.95665\n",
      "Epoch 750/3000, Training Loss: 34.12093, Validation Loss: 4.95493\n",
      "Epoch 751/3000, Training Loss: 34.11157, Validation Loss: 4.95321\n",
      "Epoch 752/3000, Training Loss: 34.10222, Validation Loss: 4.95149\n",
      "Epoch 753/3000, Training Loss: 34.09288, Validation Loss: 4.94977\n",
      "Epoch 754/3000, Training Loss: 34.08354, Validation Loss: 4.94805\n",
      "Epoch 755/3000, Training Loss: 34.07420, Validation Loss: 4.94633\n",
      "Epoch 756/3000, Training Loss: 34.06486, Validation Loss: 4.94461\n",
      "Epoch 757/3000, Training Loss: 34.05552, Validation Loss: 4.94289\n",
      "Epoch 758/3000, Training Loss: 34.04619, Validation Loss: 4.94117\n",
      "Epoch 759/3000, Training Loss: 34.03687, Validation Loss: 4.93945\n",
      "Epoch 760/3000, Training Loss: 34.02755, Validation Loss: 4.93774\n",
      "Epoch 761/3000, Training Loss: 34.01823, Validation Loss: 4.93602\n",
      "Epoch 762/3000, Training Loss: 34.00891, Validation Loss: 4.93430\n",
      "Epoch 763/3000, Training Loss: 33.99960, Validation Loss: 4.93259\n",
      "Epoch 764/3000, Training Loss: 33.99030, Validation Loss: 4.93088\n",
      "Epoch 765/3000, Training Loss: 33.98100, Validation Loss: 4.92916\n",
      "Epoch 766/3000, Training Loss: 33.97170, Validation Loss: 4.92745\n",
      "Epoch 767/3000, Training Loss: 33.96241, Validation Loss: 4.92574\n",
      "Epoch 768/3000, Training Loss: 33.95312, Validation Loss: 4.92403\n",
      "Epoch 769/3000, Training Loss: 33.94385, Validation Loss: 4.92233\n",
      "Epoch 770/3000, Training Loss: 33.93459, Validation Loss: 4.92062\n",
      "Epoch 771/3000, Training Loss: 33.92533, Validation Loss: 4.91891\n",
      "Epoch 772/3000, Training Loss: 33.91607, Validation Loss: 4.91720\n",
      "Epoch 773/3000, Training Loss: 33.90683, Validation Loss: 4.91549\n",
      "Epoch 774/3000, Training Loss: 33.89759, Validation Loss: 4.91378\n",
      "Epoch 775/3000, Training Loss: 33.88836, Validation Loss: 4.91208\n",
      "Epoch 776/3000, Training Loss: 33.87913, Validation Loss: 4.91037\n",
      "Epoch 777/3000, Training Loss: 33.86990, Validation Loss: 4.90866\n",
      "Epoch 778/3000, Training Loss: 33.86068, Validation Loss: 4.90696\n",
      "Epoch 779/3000, Training Loss: 33.85147, Validation Loss: 4.90526\n",
      "Epoch 780/3000, Training Loss: 33.84227, Validation Loss: 4.90355\n",
      "Epoch 781/3000, Training Loss: 33.83307, Validation Loss: 4.90185\n",
      "Epoch 782/3000, Training Loss: 33.82387, Validation Loss: 4.90015\n",
      "Epoch 783/3000, Training Loss: 33.81468, Validation Loss: 4.89844\n",
      "Epoch 784/3000, Training Loss: 33.80549, Validation Loss: 4.89674\n",
      "Epoch 785/3000, Training Loss: 33.79631, Validation Loss: 4.89504\n",
      "Epoch 786/3000, Training Loss: 33.78715, Validation Loss: 4.89334\n",
      "Epoch 787/3000, Training Loss: 33.77799, Validation Loss: 4.89164\n",
      "Epoch 788/3000, Training Loss: 33.76885, Validation Loss: 4.88995\n",
      "Epoch 789/3000, Training Loss: 33.75972, Validation Loss: 4.88825\n",
      "Epoch 790/3000, Training Loss: 33.75059, Validation Loss: 4.88656\n",
      "Epoch 791/3000, Training Loss: 33.74146, Validation Loss: 4.88486\n",
      "Epoch 792/3000, Training Loss: 33.73234, Validation Loss: 4.88317\n",
      "Epoch 793/3000, Training Loss: 33.72321, Validation Loss: 4.88148\n",
      "Epoch 794/3000, Training Loss: 33.71408, Validation Loss: 4.87978\n",
      "Epoch 795/3000, Training Loss: 33.70496, Validation Loss: 4.87809\n",
      "Epoch 796/3000, Training Loss: 33.69585, Validation Loss: 4.87640\n",
      "Epoch 797/3000, Training Loss: 33.68674, Validation Loss: 4.87470\n",
      "Epoch 798/3000, Training Loss: 33.67763, Validation Loss: 4.87301\n",
      "Epoch 799/3000, Training Loss: 33.66852, Validation Loss: 4.87132\n",
      "Epoch 800/3000, Training Loss: 33.65941, Validation Loss: 4.86962\n",
      "Epoch 801/3000, Training Loss: 33.65031, Validation Loss: 4.86793\n",
      "Epoch 802/3000, Training Loss: 33.64122, Validation Loss: 4.86624\n",
      "Epoch 803/3000, Training Loss: 33.63213, Validation Loss: 4.86455\n",
      "Epoch 804/3000, Training Loss: 33.62304, Validation Loss: 4.86286\n",
      "Epoch 805/3000, Training Loss: 33.61397, Validation Loss: 4.86117\n",
      "Epoch 806/3000, Training Loss: 33.60490, Validation Loss: 4.85948\n",
      "Epoch 807/3000, Training Loss: 33.59585, Validation Loss: 4.85780\n",
      "Epoch 808/3000, Training Loss: 33.58681, Validation Loss: 4.85611\n",
      "Epoch 809/3000, Training Loss: 33.57777, Validation Loss: 4.85442\n",
      "Epoch 810/3000, Training Loss: 33.56874, Validation Loss: 4.85273\n",
      "Epoch 811/3000, Training Loss: 33.55972, Validation Loss: 4.85104\n",
      "Epoch 812/3000, Training Loss: 33.55071, Validation Loss: 4.84935\n",
      "Epoch 813/3000, Training Loss: 33.54172, Validation Loss: 4.84767\n",
      "Epoch 814/3000, Training Loss: 33.53273, Validation Loss: 4.84598\n",
      "Epoch 815/3000, Training Loss: 33.52374, Validation Loss: 4.84429\n",
      "Epoch 816/3000, Training Loss: 33.51476, Validation Loss: 4.84261\n",
      "Epoch 817/3000, Training Loss: 33.50579, Validation Loss: 4.84092\n",
      "Epoch 818/3000, Training Loss: 33.49682, Validation Loss: 4.83923\n",
      "Epoch 819/3000, Training Loss: 33.48786, Validation Loss: 4.83755\n",
      "Epoch 820/3000, Training Loss: 33.47891, Validation Loss: 4.83586\n",
      "Epoch 821/3000, Training Loss: 33.46996, Validation Loss: 4.83418\n",
      "Epoch 822/3000, Training Loss: 33.46102, Validation Loss: 4.83250\n",
      "Epoch 823/3000, Training Loss: 33.45208, Validation Loss: 4.83082\n",
      "Epoch 824/3000, Training Loss: 33.44316, Validation Loss: 4.82913\n",
      "Epoch 825/3000, Training Loss: 33.43424, Validation Loss: 4.82746\n",
      "Epoch 826/3000, Training Loss: 33.42534, Validation Loss: 4.82578\n",
      "Epoch 827/3000, Training Loss: 33.41645, Validation Loss: 4.82411\n",
      "Epoch 828/3000, Training Loss: 33.40757, Validation Loss: 4.82244\n",
      "Epoch 829/3000, Training Loss: 33.39870, Validation Loss: 4.82077\n",
      "Epoch 830/3000, Training Loss: 33.38983, Validation Loss: 4.81910\n",
      "Epoch 831/3000, Training Loss: 33.38097, Validation Loss: 4.81743\n",
      "Epoch 832/3000, Training Loss: 33.37213, Validation Loss: 4.81576\n",
      "Epoch 833/3000, Training Loss: 33.36329, Validation Loss: 4.81410\n",
      "Epoch 834/3000, Training Loss: 33.35446, Validation Loss: 4.81243\n",
      "Epoch 835/3000, Training Loss: 33.34563, Validation Loss: 4.81077\n",
      "Epoch 836/3000, Training Loss: 33.33682, Validation Loss: 4.80910\n",
      "Epoch 837/3000, Training Loss: 33.32801, Validation Loss: 4.80744\n",
      "Epoch 838/3000, Training Loss: 33.31920, Validation Loss: 4.80577\n",
      "Epoch 839/3000, Training Loss: 33.31041, Validation Loss: 4.80411\n",
      "Epoch 840/3000, Training Loss: 33.30162, Validation Loss: 4.80245\n",
      "Epoch 841/3000, Training Loss: 33.29285, Validation Loss: 4.80079\n",
      "Epoch 842/3000, Training Loss: 33.28408, Validation Loss: 4.79913\n",
      "Epoch 843/3000, Training Loss: 33.27532, Validation Loss: 4.79748\n",
      "Epoch 844/3000, Training Loss: 33.26657, Validation Loss: 4.79582\n",
      "Epoch 845/3000, Training Loss: 33.25783, Validation Loss: 4.79416\n",
      "Epoch 846/3000, Training Loss: 33.24909, Validation Loss: 4.79251\n",
      "Epoch 847/3000, Training Loss: 33.24036, Validation Loss: 4.79085\n",
      "Epoch 848/3000, Training Loss: 33.23164, Validation Loss: 4.78920\n",
      "Epoch 849/3000, Training Loss: 33.22294, Validation Loss: 4.78755\n",
      "Epoch 850/3000, Training Loss: 33.21424, Validation Loss: 4.78589\n",
      "Epoch 851/3000, Training Loss: 33.20554, Validation Loss: 4.78424\n",
      "Epoch 852/3000, Training Loss: 33.19685, Validation Loss: 4.78259\n",
      "Epoch 853/3000, Training Loss: 33.18817, Validation Loss: 4.78094\n",
      "Epoch 854/3000, Training Loss: 33.17949, Validation Loss: 4.77929\n",
      "Epoch 855/3000, Training Loss: 33.17082, Validation Loss: 4.77764\n",
      "Epoch 856/3000, Training Loss: 33.16215, Validation Loss: 4.77599\n",
      "Epoch 857/3000, Training Loss: 33.15349, Validation Loss: 4.77435\n",
      "Epoch 858/3000, Training Loss: 33.14485, Validation Loss: 4.77270\n",
      "Epoch 859/3000, Training Loss: 33.13622, Validation Loss: 4.77106\n",
      "Epoch 860/3000, Training Loss: 33.12760, Validation Loss: 4.76942\n",
      "Epoch 861/3000, Training Loss: 33.11899, Validation Loss: 4.76778\n",
      "Epoch 862/3000, Training Loss: 33.11038, Validation Loss: 4.76614\n",
      "Epoch 863/3000, Training Loss: 33.10179, Validation Loss: 4.76451\n",
      "Epoch 864/3000, Training Loss: 33.09320, Validation Loss: 4.76288\n",
      "Epoch 865/3000, Training Loss: 33.08463, Validation Loss: 4.76125\n",
      "Epoch 866/3000, Training Loss: 33.07606, Validation Loss: 4.75962\n",
      "Epoch 867/3000, Training Loss: 33.06751, Validation Loss: 4.75799\n",
      "Epoch 868/3000, Training Loss: 33.05897, Validation Loss: 4.75636\n",
      "Epoch 869/3000, Training Loss: 33.05044, Validation Loss: 4.75473\n",
      "Epoch 870/3000, Training Loss: 33.04192, Validation Loss: 4.75311\n",
      "Epoch 871/3000, Training Loss: 33.03341, Validation Loss: 4.75148\n",
      "Epoch 872/3000, Training Loss: 33.02490, Validation Loss: 4.74986\n",
      "Epoch 873/3000, Training Loss: 33.01640, Validation Loss: 4.74823\n",
      "Epoch 874/3000, Training Loss: 33.00791, Validation Loss: 4.74660\n",
      "Epoch 875/3000, Training Loss: 32.99943, Validation Loss: 4.74497\n",
      "Epoch 876/3000, Training Loss: 32.99097, Validation Loss: 4.74334\n",
      "Epoch 877/3000, Training Loss: 32.98251, Validation Loss: 4.74172\n",
      "Epoch 878/3000, Training Loss: 32.97405, Validation Loss: 4.74009\n",
      "Epoch 879/3000, Training Loss: 32.96560, Validation Loss: 4.73847\n",
      "Epoch 880/3000, Training Loss: 32.95716, Validation Loss: 4.73685\n",
      "Epoch 881/3000, Training Loss: 32.94873, Validation Loss: 4.73523\n",
      "Epoch 882/3000, Training Loss: 32.94031, Validation Loss: 4.73362\n",
      "Epoch 883/3000, Training Loss: 32.93189, Validation Loss: 4.73201\n",
      "Epoch 884/3000, Training Loss: 32.92349, Validation Loss: 4.73040\n",
      "Epoch 885/3000, Training Loss: 32.91509, Validation Loss: 4.72879\n",
      "Epoch 886/3000, Training Loss: 32.90670, Validation Loss: 4.72718\n",
      "Epoch 887/3000, Training Loss: 32.89832, Validation Loss: 4.72557\n",
      "Epoch 888/3000, Training Loss: 32.88995, Validation Loss: 4.72397\n",
      "Epoch 889/3000, Training Loss: 32.88159, Validation Loss: 4.72236\n",
      "Epoch 890/3000, Training Loss: 32.87325, Validation Loss: 4.72076\n",
      "Epoch 891/3000, Training Loss: 32.86491, Validation Loss: 4.71916\n",
      "Epoch 892/3000, Training Loss: 32.85659, Validation Loss: 4.71756\n",
      "Epoch 893/3000, Training Loss: 32.84828, Validation Loss: 4.71596\n",
      "Epoch 894/3000, Training Loss: 32.83997, Validation Loss: 4.71436\n",
      "Epoch 895/3000, Training Loss: 32.83168, Validation Loss: 4.71276\n",
      "Epoch 896/3000, Training Loss: 32.82339, Validation Loss: 4.71117\n",
      "Epoch 897/3000, Training Loss: 32.81512, Validation Loss: 4.70957\n",
      "Epoch 898/3000, Training Loss: 32.80686, Validation Loss: 4.70798\n",
      "Epoch 899/3000, Training Loss: 32.79861, Validation Loss: 4.70639\n",
      "Epoch 900/3000, Training Loss: 32.79037, Validation Loss: 4.70480\n",
      "Epoch 901/3000, Training Loss: 32.78214, Validation Loss: 4.70321\n",
      "Epoch 902/3000, Training Loss: 32.77391, Validation Loss: 4.70162\n",
      "Epoch 903/3000, Training Loss: 32.76571, Validation Loss: 4.70003\n",
      "Epoch 904/3000, Training Loss: 32.75752, Validation Loss: 4.69845\n",
      "Epoch 905/3000, Training Loss: 32.74934, Validation Loss: 4.69686\n",
      "Epoch 906/3000, Training Loss: 32.74117, Validation Loss: 4.69528\n",
      "Epoch 907/3000, Training Loss: 32.73301, Validation Loss: 4.69369\n",
      "Epoch 908/3000, Training Loss: 32.72486, Validation Loss: 4.69211\n",
      "Epoch 909/3000, Training Loss: 32.71672, Validation Loss: 4.69053\n",
      "Epoch 910/3000, Training Loss: 32.70860, Validation Loss: 4.68895\n",
      "Epoch 911/3000, Training Loss: 32.70048, Validation Loss: 4.68737\n",
      "Epoch 912/3000, Training Loss: 32.69238, Validation Loss: 4.68580\n",
      "Epoch 913/3000, Training Loss: 32.68428, Validation Loss: 4.68422\n",
      "Epoch 914/3000, Training Loss: 32.67619, Validation Loss: 4.68265\n",
      "Epoch 915/3000, Training Loss: 32.66811, Validation Loss: 4.68107\n",
      "Epoch 916/3000, Training Loss: 32.66004, Validation Loss: 4.67950\n",
      "Epoch 917/3000, Training Loss: 32.65197, Validation Loss: 4.67792\n",
      "Epoch 918/3000, Training Loss: 32.64391, Validation Loss: 4.67635\n",
      "Epoch 919/3000, Training Loss: 32.63586, Validation Loss: 4.67479\n",
      "Epoch 920/3000, Training Loss: 32.62782, Validation Loss: 4.67322\n",
      "Epoch 921/3000, Training Loss: 32.61978, Validation Loss: 4.67165\n",
      "Epoch 922/3000, Training Loss: 32.61175, Validation Loss: 4.67008\n",
      "Epoch 923/3000, Training Loss: 32.60372, Validation Loss: 4.66852\n",
      "Epoch 924/3000, Training Loss: 32.59570, Validation Loss: 4.66695\n",
      "Epoch 925/3000, Training Loss: 32.58770, Validation Loss: 4.66539\n",
      "Epoch 926/3000, Training Loss: 32.57971, Validation Loss: 4.66383\n",
      "Epoch 927/3000, Training Loss: 32.57173, Validation Loss: 4.66227\n",
      "Epoch 928/3000, Training Loss: 32.56376, Validation Loss: 4.66071\n",
      "Epoch 929/3000, Training Loss: 32.55580, Validation Loss: 4.65915\n",
      "Epoch 930/3000, Training Loss: 32.54785, Validation Loss: 4.65759\n",
      "Epoch 931/3000, Training Loss: 32.53992, Validation Loss: 4.65604\n",
      "Epoch 932/3000, Training Loss: 32.53200, Validation Loss: 4.65449\n",
      "Epoch 933/3000, Training Loss: 32.52410, Validation Loss: 4.65294\n",
      "Epoch 934/3000, Training Loss: 32.51620, Validation Loss: 4.65139\n",
      "Epoch 935/3000, Training Loss: 32.50832, Validation Loss: 4.64984\n",
      "Epoch 936/3000, Training Loss: 32.50045, Validation Loss: 4.64830\n",
      "Epoch 937/3000, Training Loss: 32.49261, Validation Loss: 4.64676\n",
      "Epoch 938/3000, Training Loss: 32.48479, Validation Loss: 4.64522\n",
      "Epoch 939/3000, Training Loss: 32.47697, Validation Loss: 4.64368\n",
      "Epoch 940/3000, Training Loss: 32.46917, Validation Loss: 4.64215\n",
      "Epoch 941/3000, Training Loss: 32.46138, Validation Loss: 4.64062\n",
      "Epoch 942/3000, Training Loss: 32.45359, Validation Loss: 4.63909\n",
      "Epoch 943/3000, Training Loss: 32.44582, Validation Loss: 4.63756\n",
      "Epoch 944/3000, Training Loss: 32.43806, Validation Loss: 4.63603\n",
      "Epoch 945/3000, Training Loss: 32.43030, Validation Loss: 4.63450\n",
      "Epoch 946/3000, Training Loss: 32.42257, Validation Loss: 4.63298\n",
      "Epoch 947/3000, Training Loss: 32.41485, Validation Loss: 4.63145\n",
      "Epoch 948/3000, Training Loss: 32.40715, Validation Loss: 4.62993\n",
      "Epoch 949/3000, Training Loss: 32.39946, Validation Loss: 4.62841\n",
      "Epoch 950/3000, Training Loss: 32.39179, Validation Loss: 4.62690\n",
      "Epoch 951/3000, Training Loss: 32.38412, Validation Loss: 4.62538\n",
      "Epoch 952/3000, Training Loss: 32.37647, Validation Loss: 4.62387\n",
      "Epoch 953/3000, Training Loss: 32.36884, Validation Loss: 4.62235\n",
      "Epoch 954/3000, Training Loss: 32.36121, Validation Loss: 4.62084\n",
      "Epoch 955/3000, Training Loss: 32.35359, Validation Loss: 4.61933\n",
      "Epoch 956/3000, Training Loss: 32.34597, Validation Loss: 4.61782\n",
      "Epoch 957/3000, Training Loss: 32.33837, Validation Loss: 4.61631\n",
      "Epoch 958/3000, Training Loss: 32.33077, Validation Loss: 4.61480\n",
      "Epoch 959/3000, Training Loss: 32.32319, Validation Loss: 4.61330\n",
      "Epoch 960/3000, Training Loss: 32.31562, Validation Loss: 4.61179\n",
      "Epoch 961/3000, Training Loss: 32.30806, Validation Loss: 4.61029\n",
      "Epoch 962/3000, Training Loss: 32.30051, Validation Loss: 4.60878\n",
      "Epoch 963/3000, Training Loss: 32.29298, Validation Loss: 4.60728\n",
      "Epoch 964/3000, Training Loss: 32.28546, Validation Loss: 4.60578\n",
      "Epoch 965/3000, Training Loss: 32.27796, Validation Loss: 4.60428\n",
      "Epoch 966/3000, Training Loss: 32.27046, Validation Loss: 4.60278\n",
      "Epoch 967/3000, Training Loss: 32.26298, Validation Loss: 4.60129\n",
      "Epoch 968/3000, Training Loss: 32.25550, Validation Loss: 4.59979\n",
      "Epoch 969/3000, Training Loss: 32.24803, Validation Loss: 4.59830\n",
      "Epoch 970/3000, Training Loss: 32.24057, Validation Loss: 4.59680\n",
      "Epoch 971/3000, Training Loss: 32.23312, Validation Loss: 4.59531\n",
      "Epoch 972/3000, Training Loss: 32.22569, Validation Loss: 4.59383\n",
      "Epoch 973/3000, Training Loss: 32.21826, Validation Loss: 4.59234\n",
      "Epoch 974/3000, Training Loss: 32.21085, Validation Loss: 4.59085\n",
      "Epoch 975/3000, Training Loss: 32.20344, Validation Loss: 4.58937\n",
      "Epoch 976/3000, Training Loss: 32.19605, Validation Loss: 4.58789\n",
      "Epoch 977/3000, Training Loss: 32.18868, Validation Loss: 4.58640\n",
      "Epoch 978/3000, Training Loss: 32.18131, Validation Loss: 4.58492\n",
      "Epoch 979/3000, Training Loss: 32.17395, Validation Loss: 4.58345\n",
      "Epoch 980/3000, Training Loss: 32.16660, Validation Loss: 4.58197\n",
      "Epoch 981/3000, Training Loss: 32.15926, Validation Loss: 4.58049\n",
      "Epoch 982/3000, Training Loss: 32.15193, Validation Loss: 4.57902\n",
      "Epoch 983/3000, Training Loss: 32.14461, Validation Loss: 4.57754\n",
      "Epoch 984/3000, Training Loss: 32.13731, Validation Loss: 4.57607\n",
      "Epoch 985/3000, Training Loss: 32.13001, Validation Loss: 4.57460\n",
      "Epoch 986/3000, Training Loss: 32.12273, Validation Loss: 4.57314\n",
      "Epoch 987/3000, Training Loss: 32.11546, Validation Loss: 4.57168\n",
      "Epoch 988/3000, Training Loss: 32.10821, Validation Loss: 4.57021\n",
      "Epoch 989/3000, Training Loss: 32.10097, Validation Loss: 4.56875\n",
      "Epoch 990/3000, Training Loss: 32.09374, Validation Loss: 4.56729\n",
      "Epoch 991/3000, Training Loss: 32.08652, Validation Loss: 4.56583\n",
      "Epoch 992/3000, Training Loss: 32.07931, Validation Loss: 4.56438\n",
      "Epoch 993/3000, Training Loss: 32.07211, Validation Loss: 4.56292\n",
      "Epoch 994/3000, Training Loss: 32.06493, Validation Loss: 4.56147\n",
      "Epoch 995/3000, Training Loss: 32.05777, Validation Loss: 4.56001\n",
      "Epoch 996/3000, Training Loss: 32.05061, Validation Loss: 4.55856\n",
      "Epoch 997/3000, Training Loss: 32.04347, Validation Loss: 4.55711\n",
      "Epoch 998/3000, Training Loss: 32.03635, Validation Loss: 4.55567\n",
      "Epoch 999/3000, Training Loss: 32.02923, Validation Loss: 4.55422\n",
      "Epoch 1000/3000, Training Loss: 32.02212, Validation Loss: 4.55278\n",
      "Epoch 1001/3000, Training Loss: 32.01502, Validation Loss: 4.55134\n",
      "Epoch 1002/3000, Training Loss: 32.00793, Validation Loss: 4.54990\n",
      "Epoch 1003/3000, Training Loss: 32.00086, Validation Loss: 4.54846\n",
      "Epoch 1004/3000, Training Loss: 31.99381, Validation Loss: 4.54703\n",
      "Epoch 1005/3000, Training Loss: 31.98676, Validation Loss: 4.54560\n",
      "Epoch 1006/3000, Training Loss: 31.97973, Validation Loss: 4.54417\n",
      "Epoch 1007/3000, Training Loss: 31.97271, Validation Loss: 4.54274\n",
      "Epoch 1008/3000, Training Loss: 31.96570, Validation Loss: 4.54131\n",
      "Epoch 1009/3000, Training Loss: 31.95871, Validation Loss: 4.53989\n",
      "Epoch 1010/3000, Training Loss: 31.95173, Validation Loss: 4.53847\n",
      "Epoch 1011/3000, Training Loss: 31.94475, Validation Loss: 4.53705\n",
      "Epoch 1012/3000, Training Loss: 31.93778, Validation Loss: 4.53563\n",
      "Epoch 1013/3000, Training Loss: 31.93083, Validation Loss: 4.53421\n",
      "Epoch 1014/3000, Training Loss: 31.92388, Validation Loss: 4.53279\n",
      "Epoch 1015/3000, Training Loss: 31.91694, Validation Loss: 4.53138\n",
      "Epoch 1016/3000, Training Loss: 31.91001, Validation Loss: 4.52997\n",
      "Epoch 1017/3000, Training Loss: 31.90310, Validation Loss: 4.52856\n",
      "Epoch 1018/3000, Training Loss: 31.89619, Validation Loss: 4.52715\n",
      "Epoch 1019/3000, Training Loss: 31.88931, Validation Loss: 4.52574\n",
      "Epoch 1020/3000, Training Loss: 31.88243, Validation Loss: 4.52434\n",
      "Epoch 1021/3000, Training Loss: 31.87558, Validation Loss: 4.52294\n",
      "Epoch 1022/3000, Training Loss: 31.86873, Validation Loss: 4.52154\n",
      "Epoch 1023/3000, Training Loss: 31.86189, Validation Loss: 4.52014\n",
      "Epoch 1024/3000, Training Loss: 31.85507, Validation Loss: 4.51874\n",
      "Epoch 1025/3000, Training Loss: 31.84827, Validation Loss: 4.51735\n",
      "Epoch 1026/3000, Training Loss: 31.84147, Validation Loss: 4.51596\n",
      "Epoch 1027/3000, Training Loss: 31.83469, Validation Loss: 4.51457\n",
      "Epoch 1028/3000, Training Loss: 31.82791, Validation Loss: 4.51318\n",
      "Epoch 1029/3000, Training Loss: 31.82115, Validation Loss: 4.51179\n",
      "Epoch 1030/3000, Training Loss: 31.81439, Validation Loss: 4.51041\n",
      "Epoch 1031/3000, Training Loss: 31.80766, Validation Loss: 4.50903\n",
      "Epoch 1032/3000, Training Loss: 31.80093, Validation Loss: 4.50765\n",
      "Epoch 1033/3000, Training Loss: 31.79422, Validation Loss: 4.50627\n",
      "Epoch 1034/3000, Training Loss: 31.78753, Validation Loss: 4.50489\n",
      "Epoch 1035/3000, Training Loss: 31.78085, Validation Loss: 4.50352\n",
      "Epoch 1036/3000, Training Loss: 31.77418, Validation Loss: 4.50215\n",
      "Epoch 1037/3000, Training Loss: 31.76752, Validation Loss: 4.50077\n",
      "Epoch 1038/3000, Training Loss: 31.76087, Validation Loss: 4.49940\n",
      "Epoch 1039/3000, Training Loss: 31.75424, Validation Loss: 4.49803\n",
      "Epoch 1040/3000, Training Loss: 31.74762, Validation Loss: 4.49666\n",
      "Epoch 1041/3000, Training Loss: 31.74102, Validation Loss: 4.49530\n",
      "Epoch 1042/3000, Training Loss: 31.73443, Validation Loss: 4.49393\n",
      "Epoch 1043/3000, Training Loss: 31.72785, Validation Loss: 4.49257\n",
      "Epoch 1044/3000, Training Loss: 31.72129, Validation Loss: 4.49121\n",
      "Epoch 1045/3000, Training Loss: 31.71473, Validation Loss: 4.48985\n",
      "Epoch 1046/3000, Training Loss: 31.70819, Validation Loss: 4.48849\n",
      "Epoch 1047/3000, Training Loss: 31.70166, Validation Loss: 4.48713\n",
      "Epoch 1048/3000, Training Loss: 31.69514, Validation Loss: 4.48577\n",
      "Epoch 1049/3000, Training Loss: 31.68864, Validation Loss: 4.48442\n",
      "Epoch 1050/3000, Training Loss: 31.68215, Validation Loss: 4.48307\n",
      "Epoch 1051/3000, Training Loss: 31.67567, Validation Loss: 4.48172\n",
      "Epoch 1052/3000, Training Loss: 31.66921, Validation Loss: 4.48038\n",
      "Epoch 1053/3000, Training Loss: 31.66275, Validation Loss: 4.47903\n",
      "Epoch 1054/3000, Training Loss: 31.65631, Validation Loss: 4.47769\n",
      "Epoch 1055/3000, Training Loss: 31.64989, Validation Loss: 4.47635\n",
      "Epoch 1056/3000, Training Loss: 31.64348, Validation Loss: 4.47501\n",
      "Epoch 1057/3000, Training Loss: 31.63708, Validation Loss: 4.47368\n",
      "Epoch 1058/3000, Training Loss: 31.63070, Validation Loss: 4.47234\n",
      "Epoch 1059/3000, Training Loss: 31.62432, Validation Loss: 4.47101\n",
      "Epoch 1060/3000, Training Loss: 31.61796, Validation Loss: 4.46968\n",
      "Epoch 1061/3000, Training Loss: 31.61160, Validation Loss: 4.46835\n",
      "Epoch 1062/3000, Training Loss: 31.60525, Validation Loss: 4.46702\n",
      "Epoch 1063/3000, Training Loss: 31.59891, Validation Loss: 4.46570\n",
      "Epoch 1064/3000, Training Loss: 31.59259, Validation Loss: 4.46437\n",
      "Epoch 1065/3000, Training Loss: 31.58628, Validation Loss: 4.46305\n",
      "Epoch 1066/3000, Training Loss: 31.57997, Validation Loss: 4.46173\n",
      "Epoch 1067/3000, Training Loss: 31.57368, Validation Loss: 4.46041\n",
      "Epoch 1068/3000, Training Loss: 31.56741, Validation Loss: 4.45910\n",
      "Epoch 1069/3000, Training Loss: 31.56115, Validation Loss: 4.45778\n",
      "Epoch 1070/3000, Training Loss: 31.55490, Validation Loss: 4.45647\n",
      "Epoch 1071/3000, Training Loss: 31.54866, Validation Loss: 4.45516\n",
      "Epoch 1072/3000, Training Loss: 31.54244, Validation Loss: 4.45386\n",
      "Epoch 1073/3000, Training Loss: 31.53623, Validation Loss: 4.45255\n",
      "Epoch 1074/3000, Training Loss: 31.53002, Validation Loss: 4.45125\n",
      "Epoch 1075/3000, Training Loss: 31.52383, Validation Loss: 4.44995\n",
      "Epoch 1076/3000, Training Loss: 31.51766, Validation Loss: 4.44865\n",
      "Epoch 1077/3000, Training Loss: 31.51150, Validation Loss: 4.44735\n",
      "Epoch 1078/3000, Training Loss: 31.50534, Validation Loss: 4.44606\n",
      "Epoch 1079/3000, Training Loss: 31.49919, Validation Loss: 4.44477\n",
      "Epoch 1080/3000, Training Loss: 31.49306, Validation Loss: 4.44348\n",
      "Epoch 1081/3000, Training Loss: 31.48694, Validation Loss: 4.44219\n",
      "Epoch 1082/3000, Training Loss: 31.48084, Validation Loss: 4.44090\n",
      "Epoch 1083/3000, Training Loss: 31.47474, Validation Loss: 4.43962\n",
      "Epoch 1084/3000, Training Loss: 31.46867, Validation Loss: 4.43834\n",
      "Epoch 1085/3000, Training Loss: 31.46260, Validation Loss: 4.43706\n",
      "Epoch 1086/3000, Training Loss: 31.45656, Validation Loss: 4.43578\n",
      "Epoch 1087/3000, Training Loss: 31.45052, Validation Loss: 4.43451\n",
      "Epoch 1088/3000, Training Loss: 31.44449, Validation Loss: 4.43323\n",
      "Epoch 1089/3000, Training Loss: 31.43848, Validation Loss: 4.43196\n",
      "Epoch 1090/3000, Training Loss: 31.43247, Validation Loss: 4.43069\n",
      "Epoch 1091/3000, Training Loss: 31.42648, Validation Loss: 4.42942\n",
      "Epoch 1092/3000, Training Loss: 31.42051, Validation Loss: 4.42816\n",
      "Epoch 1093/3000, Training Loss: 31.41455, Validation Loss: 4.42690\n",
      "Epoch 1094/3000, Training Loss: 31.40861, Validation Loss: 4.42563\n",
      "Epoch 1095/3000, Training Loss: 31.40267, Validation Loss: 4.42437\n",
      "Epoch 1096/3000, Training Loss: 31.39675, Validation Loss: 4.42312\n",
      "Epoch 1097/3000, Training Loss: 31.39085, Validation Loss: 4.42186\n",
      "Epoch 1098/3000, Training Loss: 31.38497, Validation Loss: 4.42061\n",
      "Epoch 1099/3000, Training Loss: 31.37910, Validation Loss: 4.41936\n",
      "Epoch 1100/3000, Training Loss: 31.37324, Validation Loss: 4.41811\n",
      "Epoch 1101/3000, Training Loss: 31.36739, Validation Loss: 4.41686\n",
      "Epoch 1102/3000, Training Loss: 31.36155, Validation Loss: 4.41562\n",
      "Epoch 1103/3000, Training Loss: 31.35573, Validation Loss: 4.41438\n",
      "Epoch 1104/3000, Training Loss: 31.34992, Validation Loss: 4.41314\n",
      "Epoch 1105/3000, Training Loss: 31.34412, Validation Loss: 4.41190\n",
      "Epoch 1106/3000, Training Loss: 31.33834, Validation Loss: 4.41066\n",
      "Epoch 1107/3000, Training Loss: 31.33257, Validation Loss: 4.40943\n",
      "Epoch 1108/3000, Training Loss: 31.32681, Validation Loss: 4.40820\n",
      "Epoch 1109/3000, Training Loss: 31.32105, Validation Loss: 4.40697\n",
      "Epoch 1110/3000, Training Loss: 31.31531, Validation Loss: 4.40574\n",
      "Epoch 1111/3000, Training Loss: 31.30959, Validation Loss: 4.40451\n",
      "Epoch 1112/3000, Training Loss: 31.30387, Validation Loss: 4.40328\n",
      "Epoch 1113/3000, Training Loss: 31.29816, Validation Loss: 4.40206\n",
      "Epoch 1114/3000, Training Loss: 31.29246, Validation Loss: 4.40084\n",
      "Epoch 1115/3000, Training Loss: 31.28677, Validation Loss: 4.39961\n",
      "Epoch 1116/3000, Training Loss: 31.28110, Validation Loss: 4.39839\n",
      "Epoch 1117/3000, Training Loss: 31.27543, Validation Loss: 4.39717\n",
      "Epoch 1118/3000, Training Loss: 31.26978, Validation Loss: 4.39596\n",
      "Epoch 1119/3000, Training Loss: 31.26413, Validation Loss: 4.39474\n",
      "Epoch 1120/3000, Training Loss: 31.25850, Validation Loss: 4.39353\n",
      "Epoch 1121/3000, Training Loss: 31.25288, Validation Loss: 4.39232\n",
      "Epoch 1122/3000, Training Loss: 31.24726, Validation Loss: 4.39111\n",
      "Epoch 1123/3000, Training Loss: 31.24166, Validation Loss: 4.38990\n",
      "Epoch 1124/3000, Training Loss: 31.23605, Validation Loss: 4.38870\n",
      "Epoch 1125/3000, Training Loss: 31.23045, Validation Loss: 4.38749\n",
      "Epoch 1126/3000, Training Loss: 31.22486, Validation Loss: 4.38629\n",
      "Epoch 1127/3000, Training Loss: 31.21928, Validation Loss: 4.38509\n",
      "Epoch 1128/3000, Training Loss: 31.21370, Validation Loss: 4.38389\n",
      "Epoch 1129/3000, Training Loss: 31.20814, Validation Loss: 4.38269\n",
      "Epoch 1130/3000, Training Loss: 31.20259, Validation Loss: 4.38150\n",
      "Epoch 1131/3000, Training Loss: 31.19706, Validation Loss: 4.38030\n",
      "Epoch 1132/3000, Training Loss: 31.19153, Validation Loss: 4.37911\n",
      "Epoch 1133/3000, Training Loss: 31.18601, Validation Loss: 4.37792\n",
      "Epoch 1134/3000, Training Loss: 31.18051, Validation Loss: 4.37673\n",
      "Epoch 1135/3000, Training Loss: 31.17502, Validation Loss: 4.37555\n",
      "Epoch 1136/3000, Training Loss: 31.16955, Validation Loss: 4.37437\n",
      "Epoch 1137/3000, Training Loss: 31.16409, Validation Loss: 4.37319\n",
      "Epoch 1138/3000, Training Loss: 31.15863, Validation Loss: 4.37201\n",
      "Epoch 1139/3000, Training Loss: 31.15318, Validation Loss: 4.37084\n",
      "Epoch 1140/3000, Training Loss: 31.14774, Validation Loss: 4.36966\n",
      "Epoch 1141/3000, Training Loss: 31.14232, Validation Loss: 4.36849\n",
      "Epoch 1142/3000, Training Loss: 31.13691, Validation Loss: 4.36732\n",
      "Epoch 1143/3000, Training Loss: 31.13151, Validation Loss: 4.36615\n",
      "Epoch 1144/3000, Training Loss: 31.12613, Validation Loss: 4.36498\n",
      "Epoch 1145/3000, Training Loss: 31.12076, Validation Loss: 4.36382\n",
      "Epoch 1146/3000, Training Loss: 31.11540, Validation Loss: 4.36266\n",
      "Epoch 1147/3000, Training Loss: 31.11006, Validation Loss: 4.36150\n",
      "Epoch 1148/3000, Training Loss: 31.10473, Validation Loss: 4.36034\n",
      "Epoch 1149/3000, Training Loss: 31.09941, Validation Loss: 4.35918\n",
      "Epoch 1150/3000, Training Loss: 31.09410, Validation Loss: 4.35803\n",
      "Epoch 1151/3000, Training Loss: 31.08880, Validation Loss: 4.35688\n",
      "Epoch 1152/3000, Training Loss: 31.08352, Validation Loss: 4.35573\n",
      "Epoch 1153/3000, Training Loss: 31.07824, Validation Loss: 4.35458\n",
      "Epoch 1154/3000, Training Loss: 31.07297, Validation Loss: 4.35343\n",
      "Epoch 1155/3000, Training Loss: 31.06771, Validation Loss: 4.35228\n",
      "Epoch 1156/3000, Training Loss: 31.06246, Validation Loss: 4.35113\n",
      "Epoch 1157/3000, Training Loss: 31.05723, Validation Loss: 4.34999\n",
      "Epoch 1158/3000, Training Loss: 31.05201, Validation Loss: 4.34885\n",
      "Epoch 1159/3000, Training Loss: 31.04680, Validation Loss: 4.34771\n",
      "Epoch 1160/3000, Training Loss: 31.04161, Validation Loss: 4.34657\n",
      "Epoch 1161/3000, Training Loss: 31.03643, Validation Loss: 4.34544\n",
      "Epoch 1162/3000, Training Loss: 31.03127, Validation Loss: 4.34430\n",
      "Epoch 1163/3000, Training Loss: 31.02612, Validation Loss: 4.34317\n",
      "Epoch 1164/3000, Training Loss: 31.02098, Validation Loss: 4.34204\n",
      "Epoch 1165/3000, Training Loss: 31.01586, Validation Loss: 4.34092\n",
      "Epoch 1166/3000, Training Loss: 31.01075, Validation Loss: 4.33979\n",
      "Epoch 1167/3000, Training Loss: 31.00566, Validation Loss: 4.33867\n",
      "Epoch 1168/3000, Training Loss: 31.00057, Validation Loss: 4.33755\n",
      "Epoch 1169/3000, Training Loss: 30.99549, Validation Loss: 4.33644\n",
      "Epoch 1170/3000, Training Loss: 30.99043, Validation Loss: 4.33532\n",
      "Epoch 1171/3000, Training Loss: 30.98538, Validation Loss: 4.33421\n",
      "Epoch 1172/3000, Training Loss: 30.98034, Validation Loss: 4.33309\n",
      "Epoch 1173/3000, Training Loss: 30.97531, Validation Loss: 4.33199\n",
      "Epoch 1174/3000, Training Loss: 30.97029, Validation Loss: 4.33088\n",
      "Epoch 1175/3000, Training Loss: 30.96528, Validation Loss: 4.32977\n",
      "Epoch 1176/3000, Training Loss: 30.96029, Validation Loss: 4.32867\n",
      "Epoch 1177/3000, Training Loss: 30.95530, Validation Loss: 4.32756\n",
      "Epoch 1178/3000, Training Loss: 30.95033, Validation Loss: 4.32646\n",
      "Epoch 1179/3000, Training Loss: 30.94536, Validation Loss: 4.32536\n",
      "Epoch 1180/3000, Training Loss: 30.94041, Validation Loss: 4.32426\n",
      "Epoch 1181/3000, Training Loss: 30.93546, Validation Loss: 4.32316\n",
      "Epoch 1182/3000, Training Loss: 30.93053, Validation Loss: 4.32206\n",
      "Epoch 1183/3000, Training Loss: 30.92561, Validation Loss: 4.32097\n",
      "Epoch 1184/3000, Training Loss: 30.92069, Validation Loss: 4.31988\n",
      "Epoch 1185/3000, Training Loss: 30.91579, Validation Loss: 4.31879\n",
      "Epoch 1186/3000, Training Loss: 30.91090, Validation Loss: 4.31770\n",
      "Epoch 1187/3000, Training Loss: 30.90602, Validation Loss: 4.31662\n",
      "Epoch 1188/3000, Training Loss: 30.90116, Validation Loss: 4.31554\n",
      "Epoch 1189/3000, Training Loss: 30.89630, Validation Loss: 4.31446\n",
      "Epoch 1190/3000, Training Loss: 30.89146, Validation Loss: 4.31338\n",
      "Epoch 1191/3000, Training Loss: 30.88662, Validation Loss: 4.31231\n",
      "Epoch 1192/3000, Training Loss: 30.88179, Validation Loss: 4.31124\n",
      "Epoch 1193/3000, Training Loss: 30.87698, Validation Loss: 4.31017\n",
      "Epoch 1194/3000, Training Loss: 30.87217, Validation Loss: 4.30910\n",
      "Epoch 1195/3000, Training Loss: 30.86738, Validation Loss: 4.30804\n",
      "Epoch 1196/3000, Training Loss: 30.86259, Validation Loss: 4.30698\n",
      "Epoch 1197/3000, Training Loss: 30.85782, Validation Loss: 4.30592\n",
      "Epoch 1198/3000, Training Loss: 30.85305, Validation Loss: 4.30486\n",
      "Epoch 1199/3000, Training Loss: 30.84830, Validation Loss: 4.30380\n",
      "Epoch 1200/3000, Training Loss: 30.84356, Validation Loss: 4.30275\n",
      "Epoch 1201/3000, Training Loss: 30.83884, Validation Loss: 4.30170\n",
      "Epoch 1202/3000, Training Loss: 30.83413, Validation Loss: 4.30065\n",
      "Epoch 1203/3000, Training Loss: 30.82943, Validation Loss: 4.29960\n",
      "Epoch 1204/3000, Training Loss: 30.82475, Validation Loss: 4.29856\n",
      "Epoch 1205/3000, Training Loss: 30.82008, Validation Loss: 4.29752\n",
      "Epoch 1206/3000, Training Loss: 30.81542, Validation Loss: 4.29648\n",
      "Epoch 1207/3000, Training Loss: 30.81077, Validation Loss: 4.29544\n",
      "Epoch 1208/3000, Training Loss: 30.80614, Validation Loss: 4.29440\n",
      "Epoch 1209/3000, Training Loss: 30.80152, Validation Loss: 4.29337\n",
      "Epoch 1210/3000, Training Loss: 30.79692, Validation Loss: 4.29234\n",
      "Epoch 1211/3000, Training Loss: 30.79234, Validation Loss: 4.29131\n",
      "Epoch 1212/3000, Training Loss: 30.78777, Validation Loss: 4.29028\n",
      "Epoch 1213/3000, Training Loss: 30.78321, Validation Loss: 4.28926\n",
      "Epoch 1214/3000, Training Loss: 30.77866, Validation Loss: 4.28824\n",
      "Epoch 1215/3000, Training Loss: 30.77412, Validation Loss: 4.28721\n",
      "Epoch 1216/3000, Training Loss: 30.76959, Validation Loss: 4.28620\n",
      "Epoch 1217/3000, Training Loss: 30.76507, Validation Loss: 4.28518\n",
      "Epoch 1218/3000, Training Loss: 30.76056, Validation Loss: 4.28416\n",
      "Epoch 1219/3000, Training Loss: 30.75606, Validation Loss: 4.28315\n",
      "Epoch 1220/3000, Training Loss: 30.75158, Validation Loss: 4.28214\n",
      "Epoch 1221/3000, Training Loss: 30.74710, Validation Loss: 4.28113\n",
      "Epoch 1222/3000, Training Loss: 30.74264, Validation Loss: 4.28012\n",
      "Epoch 1223/3000, Training Loss: 30.73818, Validation Loss: 4.27912\n",
      "Epoch 1224/3000, Training Loss: 30.73374, Validation Loss: 4.27812\n",
      "Epoch 1225/3000, Training Loss: 30.72931, Validation Loss: 4.27712\n",
      "Epoch 1226/3000, Training Loss: 30.72489, Validation Loss: 4.27612\n",
      "Epoch 1227/3000, Training Loss: 30.72048, Validation Loss: 4.27512\n",
      "Epoch 1228/3000, Training Loss: 30.71608, Validation Loss: 4.27412\n",
      "Epoch 1229/3000, Training Loss: 30.71168, Validation Loss: 4.27313\n",
      "Epoch 1230/3000, Training Loss: 30.70730, Validation Loss: 4.27214\n",
      "Epoch 1231/3000, Training Loss: 30.70293, Validation Loss: 4.27115\n",
      "Epoch 1232/3000, Training Loss: 30.69857, Validation Loss: 4.27016\n",
      "Epoch 1233/3000, Training Loss: 30.69423, Validation Loss: 4.26918\n",
      "Epoch 1234/3000, Training Loss: 30.68990, Validation Loss: 4.26820\n",
      "Epoch 1235/3000, Training Loss: 30.68557, Validation Loss: 4.26721\n",
      "Epoch 1236/3000, Training Loss: 30.68126, Validation Loss: 4.26623\n",
      "Epoch 1237/3000, Training Loss: 30.67695, Validation Loss: 4.26525\n",
      "Epoch 1238/3000, Training Loss: 30.67265, Validation Loss: 4.26428\n",
      "Epoch 1239/3000, Training Loss: 30.66837, Validation Loss: 4.26330\n",
      "Epoch 1240/3000, Training Loss: 30.66409, Validation Loss: 4.26233\n",
      "Epoch 1241/3000, Training Loss: 30.65982, Validation Loss: 4.26136\n",
      "Epoch 1242/3000, Training Loss: 30.65557, Validation Loss: 4.26039\n",
      "Epoch 1243/3000, Training Loss: 30.65132, Validation Loss: 4.25942\n",
      "Epoch 1244/3000, Training Loss: 30.64709, Validation Loss: 4.25846\n",
      "Epoch 1245/3000, Training Loss: 30.64287, Validation Loss: 4.25750\n",
      "Epoch 1246/3000, Training Loss: 30.63865, Validation Loss: 4.25654\n",
      "Epoch 1247/3000, Training Loss: 30.63444, Validation Loss: 4.25558\n",
      "Epoch 1248/3000, Training Loss: 30.63025, Validation Loss: 4.25463\n",
      "Epoch 1249/3000, Training Loss: 30.62606, Validation Loss: 4.25367\n",
      "Epoch 1250/3000, Training Loss: 30.62189, Validation Loss: 4.25272\n",
      "Epoch 1251/3000, Training Loss: 30.61773, Validation Loss: 4.25177\n",
      "Epoch 1252/3000, Training Loss: 30.61357, Validation Loss: 4.25082\n",
      "Epoch 1253/3000, Training Loss: 30.60943, Validation Loss: 4.24987\n",
      "Epoch 1254/3000, Training Loss: 30.60530, Validation Loss: 4.24893\n",
      "Epoch 1255/3000, Training Loss: 30.60118, Validation Loss: 4.24799\n",
      "Epoch 1256/3000, Training Loss: 30.59707, Validation Loss: 4.24705\n",
      "Epoch 1257/3000, Training Loss: 30.59296, Validation Loss: 4.24611\n",
      "Epoch 1258/3000, Training Loss: 30.58887, Validation Loss: 4.24517\n",
      "Epoch 1259/3000, Training Loss: 30.58478, Validation Loss: 4.24423\n",
      "Epoch 1260/3000, Training Loss: 30.58070, Validation Loss: 4.24330\n",
      "Epoch 1261/3000, Training Loss: 30.57664, Validation Loss: 4.24237\n",
      "Epoch 1262/3000, Training Loss: 30.57258, Validation Loss: 4.24144\n",
      "Epoch 1263/3000, Training Loss: 30.56853, Validation Loss: 4.24051\n",
      "Epoch 1264/3000, Training Loss: 30.56449, Validation Loss: 4.23959\n",
      "Epoch 1265/3000, Training Loss: 30.56046, Validation Loss: 4.23867\n",
      "Epoch 1266/3000, Training Loss: 30.55643, Validation Loss: 4.23774\n",
      "Epoch 1267/3000, Training Loss: 30.55241, Validation Loss: 4.23682\n",
      "Epoch 1268/3000, Training Loss: 30.54841, Validation Loss: 4.23591\n",
      "Epoch 1269/3000, Training Loss: 30.54441, Validation Loss: 4.23499\n",
      "Epoch 1270/3000, Training Loss: 30.54043, Validation Loss: 4.23408\n",
      "Epoch 1271/3000, Training Loss: 30.53645, Validation Loss: 4.23317\n",
      "Epoch 1272/3000, Training Loss: 30.53249, Validation Loss: 4.23226\n",
      "Epoch 1273/3000, Training Loss: 30.52854, Validation Loss: 4.23135\n",
      "Epoch 1274/3000, Training Loss: 30.52460, Validation Loss: 4.23044\n",
      "Epoch 1275/3000, Training Loss: 30.52067, Validation Loss: 4.22954\n",
      "Epoch 1276/3000, Training Loss: 30.51675, Validation Loss: 4.22864\n",
      "Epoch 1277/3000, Training Loss: 30.51284, Validation Loss: 4.22774\n",
      "Epoch 1278/3000, Training Loss: 30.50894, Validation Loss: 4.22684\n",
      "Epoch 1279/3000, Training Loss: 30.50505, Validation Loss: 4.22594\n",
      "Epoch 1280/3000, Training Loss: 30.50117, Validation Loss: 4.22505\n",
      "Epoch 1281/3000, Training Loss: 30.49730, Validation Loss: 4.22415\n",
      "Epoch 1282/3000, Training Loss: 30.49345, Validation Loss: 4.22326\n",
      "Epoch 1283/3000, Training Loss: 30.48960, Validation Loss: 4.22237\n",
      "Epoch 1284/3000, Training Loss: 30.48577, Validation Loss: 4.22148\n",
      "Epoch 1285/3000, Training Loss: 30.48194, Validation Loss: 4.22060\n",
      "Epoch 1286/3000, Training Loss: 30.47813, Validation Loss: 4.21972\n",
      "Epoch 1287/3000, Training Loss: 30.47433, Validation Loss: 4.21884\n",
      "Epoch 1288/3000, Training Loss: 30.47054, Validation Loss: 4.21796\n",
      "Epoch 1289/3000, Training Loss: 30.46675, Validation Loss: 4.21708\n",
      "Epoch 1290/3000, Training Loss: 30.46299, Validation Loss: 4.21620\n",
      "Epoch 1291/3000, Training Loss: 30.45923, Validation Loss: 4.21533\n",
      "Epoch 1292/3000, Training Loss: 30.45549, Validation Loss: 4.21446\n",
      "Epoch 1293/3000, Training Loss: 30.45175, Validation Loss: 4.21359\n",
      "Epoch 1294/3000, Training Loss: 30.44803, Validation Loss: 4.21272\n",
      "Epoch 1295/3000, Training Loss: 30.44431, Validation Loss: 4.21186\n",
      "Epoch 1296/3000, Training Loss: 30.44061, Validation Loss: 4.21099\n",
      "Epoch 1297/3000, Training Loss: 30.43691, Validation Loss: 4.21013\n",
      "Epoch 1298/3000, Training Loss: 30.43322, Validation Loss: 4.20927\n",
      "Epoch 1299/3000, Training Loss: 30.42954, Validation Loss: 4.20841\n",
      "Epoch 1300/3000, Training Loss: 30.42587, Validation Loss: 4.20755\n",
      "Epoch 1301/3000, Training Loss: 30.42220, Validation Loss: 4.20670\n",
      "Epoch 1302/3000, Training Loss: 30.41855, Validation Loss: 4.20584\n",
      "Epoch 1303/3000, Training Loss: 30.41491, Validation Loss: 4.20499\n",
      "Epoch 1304/3000, Training Loss: 30.41128, Validation Loss: 4.20414\n",
      "Epoch 1305/3000, Training Loss: 30.40766, Validation Loss: 4.20329\n",
      "Epoch 1306/3000, Training Loss: 30.40405, Validation Loss: 4.20245\n",
      "Epoch 1307/3000, Training Loss: 30.40045, Validation Loss: 4.20160\n",
      "Epoch 1308/3000, Training Loss: 30.39686, Validation Loss: 4.20076\n",
      "Epoch 1309/3000, Training Loss: 30.39328, Validation Loss: 4.19992\n",
      "Epoch 1310/3000, Training Loss: 30.38970, Validation Loss: 4.19908\n",
      "Epoch 1311/3000, Training Loss: 30.38613, Validation Loss: 4.19825\n",
      "Epoch 1312/3000, Training Loss: 30.38257, Validation Loss: 4.19741\n",
      "Epoch 1313/3000, Training Loss: 30.37902, Validation Loss: 4.19658\n",
      "Epoch 1314/3000, Training Loss: 30.37547, Validation Loss: 4.19575\n",
      "Epoch 1315/3000, Training Loss: 30.37194, Validation Loss: 4.19492\n",
      "Epoch 1316/3000, Training Loss: 30.36841, Validation Loss: 4.19409\n",
      "Epoch 1317/3000, Training Loss: 30.36490, Validation Loss: 4.19326\n",
      "Epoch 1318/3000, Training Loss: 30.36140, Validation Loss: 4.19244\n",
      "Epoch 1319/3000, Training Loss: 30.35790, Validation Loss: 4.19162\n",
      "Epoch 1320/3000, Training Loss: 30.35441, Validation Loss: 4.19080\n",
      "Epoch 1321/3000, Training Loss: 30.35093, Validation Loss: 4.18998\n",
      "Epoch 1322/3000, Training Loss: 30.34746, Validation Loss: 4.18916\n",
      "Epoch 1323/3000, Training Loss: 30.34400, Validation Loss: 4.18834\n",
      "Epoch 1324/3000, Training Loss: 30.34055, Validation Loss: 4.18753\n",
      "Epoch 1325/3000, Training Loss: 30.33711, Validation Loss: 4.18672\n",
      "Epoch 1326/3000, Training Loss: 30.33367, Validation Loss: 4.18591\n",
      "Epoch 1327/3000, Training Loss: 30.33025, Validation Loss: 4.18510\n",
      "Epoch 1328/3000, Training Loss: 30.32684, Validation Loss: 4.18430\n",
      "Epoch 1329/3000, Training Loss: 30.32343, Validation Loss: 4.18350\n",
      "Epoch 1330/3000, Training Loss: 30.32004, Validation Loss: 4.18269\n",
      "Epoch 1331/3000, Training Loss: 30.31665, Validation Loss: 4.18189\n",
      "Epoch 1332/3000, Training Loss: 30.31328, Validation Loss: 4.18110\n",
      "Epoch 1333/3000, Training Loss: 30.30991, Validation Loss: 4.18030\n",
      "Epoch 1334/3000, Training Loss: 30.30655, Validation Loss: 4.17951\n",
      "Epoch 1335/3000, Training Loss: 30.30319, Validation Loss: 4.17871\n",
      "Epoch 1336/3000, Training Loss: 30.29984, Validation Loss: 4.17792\n",
      "Epoch 1337/3000, Training Loss: 30.29650, Validation Loss: 4.17713\n",
      "Epoch 1338/3000, Training Loss: 30.29316, Validation Loss: 4.17634\n",
      "Epoch 1339/3000, Training Loss: 30.28984, Validation Loss: 4.17555\n",
      "Epoch 1340/3000, Training Loss: 30.28652, Validation Loss: 4.17476\n",
      "Epoch 1341/3000, Training Loss: 30.28321, Validation Loss: 4.17398\n",
      "Epoch 1342/3000, Training Loss: 30.27992, Validation Loss: 4.17320\n",
      "Epoch 1343/3000, Training Loss: 30.27663, Validation Loss: 4.17242\n",
      "Epoch 1344/3000, Training Loss: 30.27335, Validation Loss: 4.17164\n",
      "Epoch 1345/3000, Training Loss: 30.27007, Validation Loss: 4.17086\n",
      "Epoch 1346/3000, Training Loss: 30.26681, Validation Loss: 4.17009\n",
      "Epoch 1347/3000, Training Loss: 30.26356, Validation Loss: 4.16932\n",
      "Epoch 1348/3000, Training Loss: 30.26031, Validation Loss: 4.16854\n",
      "Epoch 1349/3000, Training Loss: 30.25707, Validation Loss: 4.16777\n",
      "Epoch 1350/3000, Training Loss: 30.25384, Validation Loss: 4.16701\n",
      "Epoch 1351/3000, Training Loss: 30.25062, Validation Loss: 4.16624\n",
      "Epoch 1352/3000, Training Loss: 30.24741, Validation Loss: 4.16547\n",
      "Epoch 1353/3000, Training Loss: 30.24420, Validation Loss: 4.16471\n",
      "Epoch 1354/3000, Training Loss: 30.24101, Validation Loss: 4.16395\n",
      "Epoch 1355/3000, Training Loss: 30.23782, Validation Loss: 4.16319\n",
      "Epoch 1356/3000, Training Loss: 30.23465, Validation Loss: 4.16243\n",
      "Epoch 1357/3000, Training Loss: 30.23148, Validation Loss: 4.16167\n",
      "Epoch 1358/3000, Training Loss: 30.22831, Validation Loss: 4.16092\n",
      "Epoch 1359/3000, Training Loss: 30.22516, Validation Loss: 4.16016\n",
      "Epoch 1360/3000, Training Loss: 30.22201, Validation Loss: 4.15941\n",
      "Epoch 1361/3000, Training Loss: 30.21886, Validation Loss: 4.15866\n",
      "Epoch 1362/3000, Training Loss: 30.21573, Validation Loss: 4.15791\n",
      "Epoch 1363/3000, Training Loss: 30.21260, Validation Loss: 4.15716\n",
      "Epoch 1364/3000, Training Loss: 30.20949, Validation Loss: 4.15642\n",
      "Epoch 1365/3000, Training Loss: 30.20638, Validation Loss: 4.15567\n",
      "Epoch 1366/3000, Training Loss: 30.20328, Validation Loss: 4.15493\n",
      "Epoch 1367/3000, Training Loss: 30.20019, Validation Loss: 4.15419\n",
      "Epoch 1368/3000, Training Loss: 30.19712, Validation Loss: 4.15345\n",
      "Epoch 1369/3000, Training Loss: 30.19405, Validation Loss: 4.15271\n",
      "Epoch 1370/3000, Training Loss: 30.19099, Validation Loss: 4.15198\n",
      "Epoch 1371/3000, Training Loss: 30.18794, Validation Loss: 4.15124\n",
      "Epoch 1372/3000, Training Loss: 30.18490, Validation Loss: 4.15051\n",
      "Epoch 1373/3000, Training Loss: 30.18187, Validation Loss: 4.14978\n",
      "Epoch 1374/3000, Training Loss: 30.17884, Validation Loss: 4.14905\n",
      "Epoch 1375/3000, Training Loss: 30.17583, Validation Loss: 4.14832\n",
      "Epoch 1376/3000, Training Loss: 30.17282, Validation Loss: 4.14760\n",
      "Epoch 1377/3000, Training Loss: 30.16982, Validation Loss: 4.14687\n",
      "Epoch 1378/3000, Training Loss: 30.16682, Validation Loss: 4.14615\n",
      "Epoch 1379/3000, Training Loss: 30.16383, Validation Loss: 4.14543\n",
      "Epoch 1380/3000, Training Loss: 30.16085, Validation Loss: 4.14471\n",
      "Epoch 1381/3000, Training Loss: 30.15788, Validation Loss: 4.14399\n",
      "Epoch 1382/3000, Training Loss: 30.15492, Validation Loss: 4.14327\n",
      "Epoch 1383/3000, Training Loss: 30.15196, Validation Loss: 4.14256\n",
      "Epoch 1384/3000, Training Loss: 30.14901, Validation Loss: 4.14184\n",
      "Epoch 1385/3000, Training Loss: 30.14607, Validation Loss: 4.14113\n",
      "Epoch 1386/3000, Training Loss: 30.14313, Validation Loss: 4.14041\n",
      "Epoch 1387/3000, Training Loss: 30.14020, Validation Loss: 4.13970\n",
      "Epoch 1388/3000, Training Loss: 30.13727, Validation Loss: 4.13900\n",
      "Epoch 1389/3000, Training Loss: 30.13436, Validation Loss: 4.13829\n",
      "Epoch 1390/3000, Training Loss: 30.13145, Validation Loss: 4.13758\n",
      "Epoch 1391/3000, Training Loss: 30.12854, Validation Loss: 4.13688\n",
      "Epoch 1392/3000, Training Loss: 30.12564, Validation Loss: 4.13617\n",
      "Epoch 1393/3000, Training Loss: 30.12275, Validation Loss: 4.13547\n",
      "Epoch 1394/3000, Training Loss: 30.11987, Validation Loss: 4.13477\n",
      "Epoch 1395/3000, Training Loss: 30.11699, Validation Loss: 4.13407\n",
      "Epoch 1396/3000, Training Loss: 30.11412, Validation Loss: 4.13338\n",
      "Epoch 1397/3000, Training Loss: 30.11125, Validation Loss: 4.13268\n",
      "Epoch 1398/3000, Training Loss: 30.10839, Validation Loss: 4.13199\n",
      "Epoch 1399/3000, Training Loss: 30.10554, Validation Loss: 4.13129\n",
      "Epoch 1400/3000, Training Loss: 30.10270, Validation Loss: 4.13060\n",
      "Epoch 1401/3000, Training Loss: 30.09986, Validation Loss: 4.12991\n",
      "Epoch 1402/3000, Training Loss: 30.09704, Validation Loss: 4.12922\n",
      "Epoch 1403/3000, Training Loss: 30.09422, Validation Loss: 4.12854\n",
      "Epoch 1404/3000, Training Loss: 30.09141, Validation Loss: 4.12785\n",
      "Epoch 1405/3000, Training Loss: 30.08861, Validation Loss: 4.12717\n",
      "Epoch 1406/3000, Training Loss: 30.08581, Validation Loss: 4.12649\n",
      "Epoch 1407/3000, Training Loss: 30.08303, Validation Loss: 4.12581\n",
      "Epoch 1408/3000, Training Loss: 30.08024, Validation Loss: 4.12513\n",
      "Epoch 1409/3000, Training Loss: 30.07747, Validation Loss: 4.12445\n",
      "Epoch 1410/3000, Training Loss: 30.07470, Validation Loss: 4.12378\n",
      "Epoch 1411/3000, Training Loss: 30.07193, Validation Loss: 4.12310\n",
      "Epoch 1412/3000, Training Loss: 30.06918, Validation Loss: 4.12243\n",
      "Epoch 1413/3000, Training Loss: 30.06643, Validation Loss: 4.12176\n",
      "Epoch 1414/3000, Training Loss: 30.06369, Validation Loss: 4.12109\n",
      "Epoch 1415/3000, Training Loss: 30.06096, Validation Loss: 4.12043\n",
      "Epoch 1416/3000, Training Loss: 30.05824, Validation Loss: 4.11976\n",
      "Epoch 1417/3000, Training Loss: 30.05553, Validation Loss: 4.11910\n",
      "Epoch 1418/3000, Training Loss: 30.05282, Validation Loss: 4.11844\n",
      "Epoch 1419/3000, Training Loss: 30.05012, Validation Loss: 4.11778\n",
      "Epoch 1420/3000, Training Loss: 30.04742, Validation Loss: 4.11712\n",
      "Epoch 1421/3000, Training Loss: 30.04473, Validation Loss: 4.11646\n",
      "Epoch 1422/3000, Training Loss: 30.04205, Validation Loss: 4.11580\n",
      "Epoch 1423/3000, Training Loss: 30.03938, Validation Loss: 4.11515\n",
      "Epoch 1424/3000, Training Loss: 30.03671, Validation Loss: 4.11449\n",
      "Epoch 1425/3000, Training Loss: 30.03405, Validation Loss: 4.11384\n",
      "Epoch 1426/3000, Training Loss: 30.03140, Validation Loss: 4.11319\n",
      "Epoch 1427/3000, Training Loss: 30.02875, Validation Loss: 4.11254\n",
      "Epoch 1428/3000, Training Loss: 30.02611, Validation Loss: 4.11190\n",
      "Epoch 1429/3000, Training Loss: 30.02348, Validation Loss: 4.11125\n",
      "Epoch 1430/3000, Training Loss: 30.02085, Validation Loss: 4.11061\n",
      "Epoch 1431/3000, Training Loss: 30.01824, Validation Loss: 4.10997\n",
      "Epoch 1432/3000, Training Loss: 30.01562, Validation Loss: 4.10933\n",
      "Epoch 1433/3000, Training Loss: 30.01301, Validation Loss: 4.10868\n",
      "Epoch 1434/3000, Training Loss: 30.01041, Validation Loss: 4.10805\n",
      "Epoch 1435/3000, Training Loss: 30.00781, Validation Loss: 4.10741\n",
      "Epoch 1436/3000, Training Loss: 30.00523, Validation Loss: 4.10677\n",
      "Epoch 1437/3000, Training Loss: 30.00264, Validation Loss: 4.10614\n",
      "Epoch 1438/3000, Training Loss: 30.00006, Validation Loss: 4.10551\n",
      "Epoch 1439/3000, Training Loss: 29.99749, Validation Loss: 4.10487\n",
      "Epoch 1440/3000, Training Loss: 29.99492, Validation Loss: 4.10424\n",
      "Epoch 1441/3000, Training Loss: 29.99236, Validation Loss: 4.10361\n",
      "Epoch 1442/3000, Training Loss: 29.98980, Validation Loss: 4.10299\n",
      "Epoch 1443/3000, Training Loss: 29.98725, Validation Loss: 4.10236\n",
      "Epoch 1444/3000, Training Loss: 29.98471, Validation Loss: 4.10174\n",
      "Epoch 1445/3000, Training Loss: 29.98216, Validation Loss: 4.10112\n",
      "Epoch 1446/3000, Training Loss: 29.97962, Validation Loss: 4.10050\n",
      "Epoch 1447/3000, Training Loss: 29.97708, Validation Loss: 4.09988\n",
      "Epoch 1448/3000, Training Loss: 29.97455, Validation Loss: 4.09926\n",
      "Epoch 1449/3000, Training Loss: 29.97202, Validation Loss: 4.09865\n",
      "Epoch 1450/3000, Training Loss: 29.96950, Validation Loss: 4.09803\n",
      "Epoch 1451/3000, Training Loss: 29.96699, Validation Loss: 4.09742\n",
      "Epoch 1452/3000, Training Loss: 29.96449, Validation Loss: 4.09681\n",
      "Epoch 1453/3000, Training Loss: 29.96199, Validation Loss: 4.09620\n",
      "Epoch 1454/3000, Training Loss: 29.95950, Validation Loss: 4.09559\n",
      "Epoch 1455/3000, Training Loss: 29.95701, Validation Loss: 4.09498\n",
      "Epoch 1456/3000, Training Loss: 29.95453, Validation Loss: 4.09438\n",
      "Epoch 1457/3000, Training Loss: 29.95206, Validation Loss: 4.09377\n",
      "Epoch 1458/3000, Training Loss: 29.94958, Validation Loss: 4.09317\n",
      "Epoch 1459/3000, Training Loss: 29.94712, Validation Loss: 4.09256\n",
      "Epoch 1460/3000, Training Loss: 29.94466, Validation Loss: 4.09196\n",
      "Epoch 1461/3000, Training Loss: 29.94222, Validation Loss: 4.09136\n",
      "Epoch 1462/3000, Training Loss: 29.93977, Validation Loss: 4.09077\n",
      "Epoch 1463/3000, Training Loss: 29.93734, Validation Loss: 4.09017\n",
      "Epoch 1464/3000, Training Loss: 29.93490, Validation Loss: 4.08957\n",
      "Epoch 1465/3000, Training Loss: 29.93248, Validation Loss: 4.08898\n",
      "Epoch 1466/3000, Training Loss: 29.93006, Validation Loss: 4.08839\n",
      "Epoch 1467/3000, Training Loss: 29.92764, Validation Loss: 4.08780\n",
      "Epoch 1468/3000, Training Loss: 29.92523, Validation Loss: 4.08721\n",
      "Epoch 1469/3000, Training Loss: 29.92283, Validation Loss: 4.08662\n",
      "Epoch 1470/3000, Training Loss: 29.92043, Validation Loss: 4.08603\n",
      "Epoch 1471/3000, Training Loss: 29.91804, Validation Loss: 4.08544\n",
      "Epoch 1472/3000, Training Loss: 29.91566, Validation Loss: 4.08485\n",
      "Epoch 1473/3000, Training Loss: 29.91328, Validation Loss: 4.08427\n",
      "Epoch 1474/3000, Training Loss: 29.91092, Validation Loss: 4.08368\n",
      "Epoch 1475/3000, Training Loss: 29.90856, Validation Loss: 4.08310\n",
      "Epoch 1476/3000, Training Loss: 29.90620, Validation Loss: 4.08252\n",
      "Epoch 1477/3000, Training Loss: 29.90386, Validation Loss: 4.08194\n",
      "Epoch 1478/3000, Training Loss: 29.90152, Validation Loss: 4.08136\n",
      "Epoch 1479/3000, Training Loss: 29.89919, Validation Loss: 4.08078\n",
      "Epoch 1480/3000, Training Loss: 29.89686, Validation Loss: 4.08020\n",
      "Epoch 1481/3000, Training Loss: 29.89454, Validation Loss: 4.07963\n",
      "Epoch 1482/3000, Training Loss: 29.89223, Validation Loss: 4.07905\n",
      "Epoch 1483/3000, Training Loss: 29.88991, Validation Loss: 4.07848\n",
      "Epoch 1484/3000, Training Loss: 29.88761, Validation Loss: 4.07791\n",
      "Epoch 1485/3000, Training Loss: 29.88531, Validation Loss: 4.07733\n",
      "Epoch 1486/3000, Training Loss: 29.88301, Validation Loss: 4.07677\n",
      "Epoch 1487/3000, Training Loss: 29.88073, Validation Loss: 4.07620\n",
      "Epoch 1488/3000, Training Loss: 29.87844, Validation Loss: 4.07563\n",
      "Epoch 1489/3000, Training Loss: 29.87615, Validation Loss: 4.07506\n",
      "Epoch 1490/3000, Training Loss: 29.87387, Validation Loss: 4.07450\n",
      "Epoch 1491/3000, Training Loss: 29.87159, Validation Loss: 4.07393\n",
      "Epoch 1492/3000, Training Loss: 29.86932, Validation Loss: 4.07337\n",
      "Epoch 1493/3000, Training Loss: 29.86706, Validation Loss: 4.07281\n",
      "Epoch 1494/3000, Training Loss: 29.86480, Validation Loss: 4.07225\n",
      "Epoch 1495/3000, Training Loss: 29.86255, Validation Loss: 4.07170\n",
      "Epoch 1496/3000, Training Loss: 29.86031, Validation Loss: 4.07114\n",
      "Epoch 1497/3000, Training Loss: 29.85807, Validation Loss: 4.07059\n",
      "Epoch 1498/3000, Training Loss: 29.85584, Validation Loss: 4.07003\n",
      "Epoch 1499/3000, Training Loss: 29.85362, Validation Loss: 4.06948\n",
      "Epoch 1500/3000, Training Loss: 29.85140, Validation Loss: 4.06894\n",
      "Epoch 1501/3000, Training Loss: 29.84919, Validation Loss: 4.06839\n",
      "Epoch 1502/3000, Training Loss: 29.84698, Validation Loss: 4.06784\n",
      "Epoch 1503/3000, Training Loss: 29.84478, Validation Loss: 4.06730\n",
      "Epoch 1504/3000, Training Loss: 29.84259, Validation Loss: 4.06676\n",
      "Epoch 1505/3000, Training Loss: 29.84040, Validation Loss: 4.06622\n",
      "Epoch 1506/3000, Training Loss: 29.83821, Validation Loss: 4.06568\n",
      "Epoch 1507/3000, Training Loss: 29.83603, Validation Loss: 4.06514\n",
      "Epoch 1508/3000, Training Loss: 29.83386, Validation Loss: 4.06460\n",
      "Epoch 1509/3000, Training Loss: 29.83169, Validation Loss: 4.06406\n",
      "Epoch 1510/3000, Training Loss: 29.82952, Validation Loss: 4.06353\n",
      "Epoch 1511/3000, Training Loss: 29.82737, Validation Loss: 4.06300\n",
      "Epoch 1512/3000, Training Loss: 29.82521, Validation Loss: 4.06246\n",
      "Epoch 1513/3000, Training Loss: 29.82306, Validation Loss: 4.06193\n",
      "Epoch 1514/3000, Training Loss: 29.82092, Validation Loss: 4.06140\n",
      "Epoch 1515/3000, Training Loss: 29.81878, Validation Loss: 4.06087\n",
      "Epoch 1516/3000, Training Loss: 29.81665, Validation Loss: 4.06035\n",
      "Epoch 1517/3000, Training Loss: 29.81452, Validation Loss: 4.05982\n",
      "Epoch 1518/3000, Training Loss: 29.81240, Validation Loss: 4.05930\n",
      "Epoch 1519/3000, Training Loss: 29.81029, Validation Loss: 4.05877\n",
      "Epoch 1520/3000, Training Loss: 29.80818, Validation Loss: 4.05825\n",
      "Epoch 1521/3000, Training Loss: 29.80607, Validation Loss: 4.05773\n",
      "Epoch 1522/3000, Training Loss: 29.80398, Validation Loss: 4.05722\n",
      "Epoch 1523/3000, Training Loss: 29.80189, Validation Loss: 4.05670\n",
      "Epoch 1524/3000, Training Loss: 29.79981, Validation Loss: 4.05619\n",
      "Epoch 1525/3000, Training Loss: 29.79773, Validation Loss: 4.05568\n",
      "Epoch 1526/3000, Training Loss: 29.79566, Validation Loss: 4.05517\n",
      "Epoch 1527/3000, Training Loss: 29.79360, Validation Loss: 4.05466\n",
      "Epoch 1528/3000, Training Loss: 29.79153, Validation Loss: 4.05415\n",
      "Epoch 1529/3000, Training Loss: 29.78948, Validation Loss: 4.05364\n",
      "Epoch 1530/3000, Training Loss: 29.78742, Validation Loss: 4.05314\n",
      "Epoch 1531/3000, Training Loss: 29.78537, Validation Loss: 4.05263\n",
      "Epoch 1532/3000, Training Loss: 29.78333, Validation Loss: 4.05213\n",
      "Epoch 1533/3000, Training Loss: 29.78129, Validation Loss: 4.05163\n",
      "Epoch 1534/3000, Training Loss: 29.77926, Validation Loss: 4.05113\n",
      "Epoch 1535/3000, Training Loss: 29.77723, Validation Loss: 4.05063\n",
      "Epoch 1536/3000, Training Loss: 29.77521, Validation Loss: 4.05013\n",
      "Epoch 1537/3000, Training Loss: 29.77319, Validation Loss: 4.04963\n",
      "Epoch 1538/3000, Training Loss: 29.77118, Validation Loss: 4.04913\n",
      "Epoch 1539/3000, Training Loss: 29.76917, Validation Loss: 4.04864\n",
      "Epoch 1540/3000, Training Loss: 29.76717, Validation Loss: 4.04814\n",
      "Epoch 1541/3000, Training Loss: 29.76517, Validation Loss: 4.04765\n",
      "Epoch 1542/3000, Training Loss: 29.76317, Validation Loss: 4.04716\n",
      "Epoch 1543/3000, Training Loss: 29.76118, Validation Loss: 4.04667\n",
      "Epoch 1544/3000, Training Loss: 29.75920, Validation Loss: 4.04618\n",
      "Epoch 1545/3000, Training Loss: 29.75722, Validation Loss: 4.04569\n",
      "Epoch 1546/3000, Training Loss: 29.75524, Validation Loss: 4.04520\n",
      "Epoch 1547/3000, Training Loss: 29.75327, Validation Loss: 4.04471\n",
      "Epoch 1548/3000, Training Loss: 29.75131, Validation Loss: 4.04423\n",
      "Epoch 1549/3000, Training Loss: 29.74935, Validation Loss: 4.04374\n",
      "Epoch 1550/3000, Training Loss: 29.74739, Validation Loss: 4.04326\n",
      "Epoch 1551/3000, Training Loss: 29.74544, Validation Loss: 4.04278\n",
      "Epoch 1552/3000, Training Loss: 29.74349, Validation Loss: 4.04230\n",
      "Epoch 1553/3000, Training Loss: 29.74155, Validation Loss: 4.04182\n",
      "Epoch 1554/3000, Training Loss: 29.73960, Validation Loss: 4.04134\n",
      "Epoch 1555/3000, Training Loss: 29.73767, Validation Loss: 4.04086\n",
      "Epoch 1556/3000, Training Loss: 29.73573, Validation Loss: 4.04038\n",
      "Epoch 1557/3000, Training Loss: 29.73381, Validation Loss: 4.03991\n",
      "Epoch 1558/3000, Training Loss: 29.73188, Validation Loss: 4.03943\n",
      "Epoch 1559/3000, Training Loss: 29.72996, Validation Loss: 4.03896\n",
      "Epoch 1560/3000, Training Loss: 29.72804, Validation Loss: 4.03848\n",
      "Epoch 1561/3000, Training Loss: 29.72613, Validation Loss: 4.03801\n",
      "Epoch 1562/3000, Training Loss: 29.72422, Validation Loss: 4.03754\n",
      "Epoch 1563/3000, Training Loss: 29.72232, Validation Loss: 4.03707\n",
      "Epoch 1564/3000, Training Loss: 29.72041, Validation Loss: 4.03660\n",
      "Epoch 1565/3000, Training Loss: 29.71852, Validation Loss: 4.03613\n",
      "Epoch 1566/3000, Training Loss: 29.71663, Validation Loss: 4.03566\n",
      "Epoch 1567/3000, Training Loss: 29.71474, Validation Loss: 4.03519\n",
      "Epoch 1568/3000, Training Loss: 29.71286, Validation Loss: 4.03473\n",
      "Epoch 1569/3000, Training Loss: 29.71098, Validation Loss: 4.03426\n",
      "Epoch 1570/3000, Training Loss: 29.70911, Validation Loss: 4.03380\n",
      "Epoch 1571/3000, Training Loss: 29.70724, Validation Loss: 4.03334\n",
      "Epoch 1572/3000, Training Loss: 29.70538, Validation Loss: 4.03288\n",
      "Epoch 1573/3000, Training Loss: 29.70352, Validation Loss: 4.03242\n",
      "Epoch 1574/3000, Training Loss: 29.70167, Validation Loss: 4.03196\n",
      "Epoch 1575/3000, Training Loss: 29.69982, Validation Loss: 4.03150\n",
      "Epoch 1576/3000, Training Loss: 29.69797, Validation Loss: 4.03105\n",
      "Epoch 1577/3000, Training Loss: 29.69613, Validation Loss: 4.03059\n",
      "Epoch 1578/3000, Training Loss: 29.69430, Validation Loss: 4.03014\n",
      "Epoch 1579/3000, Training Loss: 29.69247, Validation Loss: 4.02968\n",
      "Epoch 1580/3000, Training Loss: 29.69064, Validation Loss: 4.02923\n",
      "Epoch 1581/3000, Training Loss: 29.68882, Validation Loss: 4.02878\n",
      "Epoch 1582/3000, Training Loss: 29.68700, Validation Loss: 4.02833\n",
      "Epoch 1583/3000, Training Loss: 29.68519, Validation Loss: 4.02788\n",
      "Epoch 1584/3000, Training Loss: 29.68337, Validation Loss: 4.02744\n",
      "Epoch 1585/3000, Training Loss: 29.68156, Validation Loss: 4.02699\n",
      "Epoch 1586/3000, Training Loss: 29.67976, Validation Loss: 4.02655\n",
      "Epoch 1587/3000, Training Loss: 29.67796, Validation Loss: 4.02610\n",
      "Epoch 1588/3000, Training Loss: 29.67616, Validation Loss: 4.02566\n",
      "Epoch 1589/3000, Training Loss: 29.67437, Validation Loss: 4.02522\n",
      "Epoch 1590/3000, Training Loss: 29.67258, Validation Loss: 4.02478\n",
      "Epoch 1591/3000, Training Loss: 29.67079, Validation Loss: 4.02434\n",
      "Epoch 1592/3000, Training Loss: 29.66901, Validation Loss: 4.02390\n",
      "Epoch 1593/3000, Training Loss: 29.66723, Validation Loss: 4.02346\n",
      "Epoch 1594/3000, Training Loss: 29.66546, Validation Loss: 4.02302\n",
      "Epoch 1595/3000, Training Loss: 29.66369, Validation Loss: 4.02259\n",
      "Epoch 1596/3000, Training Loss: 29.66193, Validation Loss: 4.02215\n",
      "Epoch 1597/3000, Training Loss: 29.66017, Validation Loss: 4.02172\n",
      "Epoch 1598/3000, Training Loss: 29.65841, Validation Loss: 4.02129\n",
      "Epoch 1599/3000, Training Loss: 29.65666, Validation Loss: 4.02085\n",
      "Epoch 1600/3000, Training Loss: 29.65491, Validation Loss: 4.02042\n",
      "Epoch 1601/3000, Training Loss: 29.65316, Validation Loss: 4.01999\n",
      "Epoch 1602/3000, Training Loss: 29.65143, Validation Loss: 4.01956\n",
      "Epoch 1603/3000, Training Loss: 29.64969, Validation Loss: 4.01914\n",
      "Epoch 1604/3000, Training Loss: 29.64796, Validation Loss: 4.01871\n",
      "Epoch 1605/3000, Training Loss: 29.64623, Validation Loss: 4.01828\n",
      "Epoch 1606/3000, Training Loss: 29.64451, Validation Loss: 4.01786\n",
      "Epoch 1607/3000, Training Loss: 29.64279, Validation Loss: 4.01743\n",
      "Epoch 1608/3000, Training Loss: 29.64108, Validation Loss: 4.01701\n",
      "Epoch 1609/3000, Training Loss: 29.63937, Validation Loss: 4.01658\n",
      "Epoch 1610/3000, Training Loss: 29.63766, Validation Loss: 4.01616\n",
      "Epoch 1611/3000, Training Loss: 29.63596, Validation Loss: 4.01574\n",
      "Epoch 1612/3000, Training Loss: 29.63426, Validation Loss: 4.01532\n",
      "Epoch 1613/3000, Training Loss: 29.63256, Validation Loss: 4.01490\n",
      "Epoch 1614/3000, Training Loss: 29.63086, Validation Loss: 4.01448\n",
      "Epoch 1615/3000, Training Loss: 29.62917, Validation Loss: 4.01406\n",
      "Epoch 1616/3000, Training Loss: 29.62749, Validation Loss: 4.01364\n",
      "Epoch 1617/3000, Training Loss: 29.62580, Validation Loss: 4.01322\n",
      "Epoch 1618/3000, Training Loss: 29.62412, Validation Loss: 4.01281\n",
      "Epoch 1619/3000, Training Loss: 29.62244, Validation Loss: 4.01239\n",
      "Epoch 1620/3000, Training Loss: 29.62077, Validation Loss: 4.01198\n",
      "Epoch 1621/3000, Training Loss: 29.61910, Validation Loss: 4.01156\n",
      "Epoch 1622/3000, Training Loss: 29.61743, Validation Loss: 4.01115\n",
      "Epoch 1623/3000, Training Loss: 29.61577, Validation Loss: 4.01074\n",
      "Epoch 1624/3000, Training Loss: 29.61411, Validation Loss: 4.01033\n",
      "Epoch 1625/3000, Training Loss: 29.61245, Validation Loss: 4.00992\n",
      "Epoch 1626/3000, Training Loss: 29.61080, Validation Loss: 4.00951\n",
      "Epoch 1627/3000, Training Loss: 29.60916, Validation Loss: 4.00910\n",
      "Epoch 1628/3000, Training Loss: 29.60752, Validation Loss: 4.00869\n",
      "Epoch 1629/3000, Training Loss: 29.60588, Validation Loss: 4.00829\n",
      "Epoch 1630/3000, Training Loss: 29.60425, Validation Loss: 4.00788\n",
      "Epoch 1631/3000, Training Loss: 29.60262, Validation Loss: 4.00748\n",
      "Epoch 1632/3000, Training Loss: 29.60099, Validation Loss: 4.00707\n",
      "Epoch 1633/3000, Training Loss: 29.59937, Validation Loss: 4.00667\n",
      "Epoch 1634/3000, Training Loss: 29.59776, Validation Loss: 4.00627\n",
      "Epoch 1635/3000, Training Loss: 29.59614, Validation Loss: 4.00587\n",
      "Epoch 1636/3000, Training Loss: 29.59453, Validation Loss: 4.00547\n",
      "Epoch 1637/3000, Training Loss: 29.59293, Validation Loss: 4.00507\n",
      "Epoch 1638/3000, Training Loss: 29.59133, Validation Loss: 4.00467\n",
      "Epoch 1639/3000, Training Loss: 29.58973, Validation Loss: 4.00427\n",
      "Epoch 1640/3000, Training Loss: 29.58813, Validation Loss: 4.00388\n",
      "Epoch 1641/3000, Training Loss: 29.58654, Validation Loss: 4.00349\n",
      "Epoch 1642/3000, Training Loss: 29.58495, Validation Loss: 4.00309\n",
      "Epoch 1643/3000, Training Loss: 29.58336, Validation Loss: 4.00270\n",
      "Epoch 1644/3000, Training Loss: 29.58178, Validation Loss: 4.00231\n",
      "Epoch 1645/3000, Training Loss: 29.58020, Validation Loss: 4.00192\n",
      "Epoch 1646/3000, Training Loss: 29.57862, Validation Loss: 4.00154\n",
      "Epoch 1647/3000, Training Loss: 29.57705, Validation Loss: 4.00115\n",
      "Epoch 1648/3000, Training Loss: 29.57549, Validation Loss: 4.00077\n",
      "Epoch 1649/3000, Training Loss: 29.57393, Validation Loss: 4.00038\n",
      "Epoch 1650/3000, Training Loss: 29.57237, Validation Loss: 4.00000\n",
      "Epoch 1651/3000, Training Loss: 29.57081, Validation Loss: 3.99962\n",
      "Epoch 1652/3000, Training Loss: 29.56926, Validation Loss: 3.99923\n",
      "Epoch 1653/3000, Training Loss: 29.56771, Validation Loss: 3.99885\n",
      "Epoch 1654/3000, Training Loss: 29.56616, Validation Loss: 3.99847\n",
      "Epoch 1655/3000, Training Loss: 29.56462, Validation Loss: 3.99809\n",
      "Epoch 1656/3000, Training Loss: 29.56307, Validation Loss: 3.99772\n",
      "Epoch 1657/3000, Training Loss: 29.56154, Validation Loss: 3.99734\n",
      "Epoch 1658/3000, Training Loss: 29.56000, Validation Loss: 3.99696\n",
      "Epoch 1659/3000, Training Loss: 29.55847, Validation Loss: 3.99659\n",
      "Epoch 1660/3000, Training Loss: 29.55694, Validation Loss: 3.99621\n",
      "Epoch 1661/3000, Training Loss: 29.55542, Validation Loss: 3.99584\n",
      "Epoch 1662/3000, Training Loss: 29.55390, Validation Loss: 3.99547\n",
      "Epoch 1663/3000, Training Loss: 29.55238, Validation Loss: 3.99509\n",
      "Epoch 1664/3000, Training Loss: 29.55086, Validation Loss: 3.99472\n",
      "Epoch 1665/3000, Training Loss: 29.54935, Validation Loss: 3.99435\n",
      "Epoch 1666/3000, Training Loss: 29.54784, Validation Loss: 3.99398\n",
      "Epoch 1667/3000, Training Loss: 29.54634, Validation Loss: 3.99362\n",
      "Epoch 1668/3000, Training Loss: 29.54484, Validation Loss: 3.99325\n",
      "Epoch 1669/3000, Training Loss: 29.54334, Validation Loss: 3.99288\n",
      "Epoch 1670/3000, Training Loss: 29.54184, Validation Loss: 3.99252\n",
      "Epoch 1671/3000, Training Loss: 29.54035, Validation Loss: 3.99216\n",
      "Epoch 1672/3000, Training Loss: 29.53887, Validation Loss: 3.99180\n",
      "Epoch 1673/3000, Training Loss: 29.53739, Validation Loss: 3.99144\n",
      "Epoch 1674/3000, Training Loss: 29.53591, Validation Loss: 3.99108\n",
      "Epoch 1675/3000, Training Loss: 29.53444, Validation Loss: 3.99072\n",
      "Epoch 1676/3000, Training Loss: 29.53297, Validation Loss: 3.99036\n",
      "Epoch 1677/3000, Training Loss: 29.53150, Validation Loss: 3.99000\n",
      "Epoch 1678/3000, Training Loss: 29.53004, Validation Loss: 3.98965\n",
      "Epoch 1679/3000, Training Loss: 29.52857, Validation Loss: 3.98929\n",
      "Epoch 1680/3000, Training Loss: 29.52712, Validation Loss: 3.98894\n",
      "Epoch 1681/3000, Training Loss: 29.52566, Validation Loss: 3.98858\n",
      "Epoch 1682/3000, Training Loss: 29.52421, Validation Loss: 3.98823\n",
      "Epoch 1683/3000, Training Loss: 29.52276, Validation Loss: 3.98788\n",
      "Epoch 1684/3000, Training Loss: 29.52132, Validation Loss: 3.98753\n",
      "Epoch 1685/3000, Training Loss: 29.51988, Validation Loss: 3.98718\n",
      "Epoch 1686/3000, Training Loss: 29.51844, Validation Loss: 3.98683\n",
      "Epoch 1687/3000, Training Loss: 29.51700, Validation Loss: 3.98648\n",
      "Epoch 1688/3000, Training Loss: 29.51557, Validation Loss: 3.98613\n",
      "Epoch 1689/3000, Training Loss: 29.51414, Validation Loss: 3.98578\n",
      "Epoch 1690/3000, Training Loss: 29.51272, Validation Loss: 3.98544\n",
      "Epoch 1691/3000, Training Loss: 29.51130, Validation Loss: 3.98509\n",
      "Epoch 1692/3000, Training Loss: 29.50988, Validation Loss: 3.98474\n",
      "Epoch 1693/3000, Training Loss: 29.50847, Validation Loss: 3.98440\n",
      "Epoch 1694/3000, Training Loss: 29.50705, Validation Loss: 3.98405\n",
      "Epoch 1695/3000, Training Loss: 29.50565, Validation Loss: 3.98371\n",
      "Epoch 1696/3000, Training Loss: 29.50424, Validation Loss: 3.98337\n",
      "Epoch 1697/3000, Training Loss: 29.50283, Validation Loss: 3.98303\n",
      "Epoch 1698/3000, Training Loss: 29.50142, Validation Loss: 3.98268\n",
      "Epoch 1699/3000, Training Loss: 29.50002, Validation Loss: 3.98234\n",
      "Epoch 1700/3000, Training Loss: 29.49862, Validation Loss: 3.98200\n",
      "Epoch 1701/3000, Training Loss: 29.49722, Validation Loss: 3.98166\n",
      "Epoch 1702/3000, Training Loss: 29.49583, Validation Loss: 3.98132\n",
      "Epoch 1703/3000, Training Loss: 29.49444, Validation Loss: 3.98099\n",
      "Epoch 1704/3000, Training Loss: 29.49306, Validation Loss: 3.98065\n",
      "Epoch 1705/3000, Training Loss: 29.49167, Validation Loss: 3.98031\n",
      "Epoch 1706/3000, Training Loss: 29.49029, Validation Loss: 3.97998\n",
      "Epoch 1707/3000, Training Loss: 29.48891, Validation Loss: 3.97964\n",
      "Epoch 1708/3000, Training Loss: 29.48754, Validation Loss: 3.97931\n",
      "Epoch 1709/3000, Training Loss: 29.48617, Validation Loss: 3.97898\n",
      "Epoch 1710/3000, Training Loss: 29.48480, Validation Loss: 3.97865\n",
      "Epoch 1711/3000, Training Loss: 29.48344, Validation Loss: 3.97831\n",
      "Epoch 1712/3000, Training Loss: 29.48207, Validation Loss: 3.97798\n",
      "Epoch 1713/3000, Training Loss: 29.48071, Validation Loss: 3.97765\n",
      "Epoch 1714/3000, Training Loss: 29.47936, Validation Loss: 3.97732\n",
      "Epoch 1715/3000, Training Loss: 29.47800, Validation Loss: 3.97699\n",
      "Epoch 1716/3000, Training Loss: 29.47665, Validation Loss: 3.97667\n",
      "Epoch 1717/3000, Training Loss: 29.47531, Validation Loss: 3.97634\n",
      "Epoch 1718/3000, Training Loss: 29.47396, Validation Loss: 3.97601\n",
      "Epoch 1719/3000, Training Loss: 29.47262, Validation Loss: 3.97569\n",
      "Epoch 1720/3000, Training Loss: 29.47128, Validation Loss: 3.97536\n",
      "Epoch 1721/3000, Training Loss: 29.46994, Validation Loss: 3.97504\n",
      "Epoch 1722/3000, Training Loss: 29.46860, Validation Loss: 3.97472\n",
      "Epoch 1723/3000, Training Loss: 29.46727, Validation Loss: 3.97439\n",
      "Epoch 1724/3000, Training Loss: 29.46594, Validation Loss: 3.97407\n",
      "Epoch 1725/3000, Training Loss: 29.46462, Validation Loss: 3.97375\n",
      "Epoch 1726/3000, Training Loss: 29.46330, Validation Loss: 3.97343\n",
      "Epoch 1727/3000, Training Loss: 29.46198, Validation Loss: 3.97312\n",
      "Epoch 1728/3000, Training Loss: 29.46066, Validation Loss: 3.97280\n",
      "Epoch 1729/3000, Training Loss: 29.45934, Validation Loss: 3.97248\n",
      "Epoch 1730/3000, Training Loss: 29.45803, Validation Loss: 3.97217\n",
      "Epoch 1731/3000, Training Loss: 29.45673, Validation Loss: 3.97186\n",
      "Epoch 1732/3000, Training Loss: 29.45542, Validation Loss: 3.97154\n",
      "Epoch 1733/3000, Training Loss: 29.45412, Validation Loss: 3.97123\n",
      "Epoch 1734/3000, Training Loss: 29.45282, Validation Loss: 3.97092\n",
      "Epoch 1735/3000, Training Loss: 29.45152, Validation Loss: 3.97061\n",
      "Epoch 1736/3000, Training Loss: 29.45023, Validation Loss: 3.97030\n",
      "Epoch 1737/3000, Training Loss: 29.44893, Validation Loss: 3.96999\n",
      "Epoch 1738/3000, Training Loss: 29.44764, Validation Loss: 3.96968\n",
      "Epoch 1739/3000, Training Loss: 29.44636, Validation Loss: 3.96938\n",
      "Epoch 1740/3000, Training Loss: 29.44507, Validation Loss: 3.96907\n",
      "Epoch 1741/3000, Training Loss: 29.44379, Validation Loss: 3.96876\n",
      "Epoch 1742/3000, Training Loss: 29.44252, Validation Loss: 3.96846\n",
      "Epoch 1743/3000, Training Loss: 29.44124, Validation Loss: 3.96816\n",
      "Epoch 1744/3000, Training Loss: 29.43996, Validation Loss: 3.96785\n",
      "Epoch 1745/3000, Training Loss: 29.43869, Validation Loss: 3.96755\n",
      "Epoch 1746/3000, Training Loss: 29.43743, Validation Loss: 3.96725\n",
      "Epoch 1747/3000, Training Loss: 29.43616, Validation Loss: 3.96695\n",
      "Epoch 1748/3000, Training Loss: 29.43490, Validation Loss: 3.96665\n",
      "Epoch 1749/3000, Training Loss: 29.43363, Validation Loss: 3.96635\n",
      "Epoch 1750/3000, Training Loss: 29.43238, Validation Loss: 3.96605\n",
      "Epoch 1751/3000, Training Loss: 29.43112, Validation Loss: 3.96576\n",
      "Epoch 1752/3000, Training Loss: 29.42987, Validation Loss: 3.96546\n",
      "Epoch 1753/3000, Training Loss: 29.42862, Validation Loss: 3.96516\n",
      "Epoch 1754/3000, Training Loss: 29.42737, Validation Loss: 3.96487\n",
      "Epoch 1755/3000, Training Loss: 29.42612, Validation Loss: 3.96457\n",
      "Epoch 1756/3000, Training Loss: 29.42488, Validation Loss: 3.96427\n",
      "Epoch 1757/3000, Training Loss: 29.42364, Validation Loss: 3.96398\n",
      "Epoch 1758/3000, Training Loss: 29.42240, Validation Loss: 3.96368\n",
      "Epoch 1759/3000, Training Loss: 29.42116, Validation Loss: 3.96339\n",
      "Epoch 1760/3000, Training Loss: 29.41992, Validation Loss: 3.96310\n",
      "Epoch 1761/3000, Training Loss: 29.41869, Validation Loss: 3.96280\n",
      "Epoch 1762/3000, Training Loss: 29.41746, Validation Loss: 3.96251\n",
      "Epoch 1763/3000, Training Loss: 29.41623, Validation Loss: 3.96222\n",
      "Epoch 1764/3000, Training Loss: 29.41501, Validation Loss: 3.96193\n",
      "Epoch 1765/3000, Training Loss: 29.41379, Validation Loss: 3.96164\n",
      "Epoch 1766/3000, Training Loss: 29.41257, Validation Loss: 3.96135\n",
      "Epoch 1767/3000, Training Loss: 29.41135, Validation Loss: 3.96107\n",
      "Epoch 1768/3000, Training Loss: 29.41013, Validation Loss: 3.96078\n",
      "Epoch 1769/3000, Training Loss: 29.40892, Validation Loss: 3.96049\n",
      "Epoch 1770/3000, Training Loss: 29.40770, Validation Loss: 3.96021\n",
      "Epoch 1771/3000, Training Loss: 29.40649, Validation Loss: 3.95992\n",
      "Epoch 1772/3000, Training Loss: 29.40529, Validation Loss: 3.95964\n",
      "Epoch 1773/3000, Training Loss: 29.40408, Validation Loss: 3.95936\n",
      "Epoch 1774/3000, Training Loss: 29.40288, Validation Loss: 3.95907\n",
      "Epoch 1775/3000, Training Loss: 29.40168, Validation Loss: 3.95879\n",
      "Epoch 1776/3000, Training Loss: 29.40048, Validation Loss: 3.95851\n",
      "Epoch 1777/3000, Training Loss: 29.39928, Validation Loss: 3.95823\n",
      "Epoch 1778/3000, Training Loss: 29.39809, Validation Loss: 3.95795\n",
      "Epoch 1779/3000, Training Loss: 29.39689, Validation Loss: 3.95767\n",
      "Epoch 1780/3000, Training Loss: 29.39570, Validation Loss: 3.95739\n",
      "Epoch 1781/3000, Training Loss: 29.39452, Validation Loss: 3.95712\n",
      "Epoch 1782/3000, Training Loss: 29.39333, Validation Loss: 3.95684\n",
      "Epoch 1783/3000, Training Loss: 29.39215, Validation Loss: 3.95656\n",
      "Epoch 1784/3000, Training Loss: 29.39097, Validation Loss: 3.95629\n",
      "Epoch 1785/3000, Training Loss: 29.38979, Validation Loss: 3.95601\n",
      "Epoch 1786/3000, Training Loss: 29.38862, Validation Loss: 3.95574\n",
      "Epoch 1787/3000, Training Loss: 29.38744, Validation Loss: 3.95546\n",
      "Epoch 1788/3000, Training Loss: 29.38627, Validation Loss: 3.95519\n",
      "Epoch 1789/3000, Training Loss: 29.38510, Validation Loss: 3.95492\n",
      "Epoch 1790/3000, Training Loss: 29.38393, Validation Loss: 3.95465\n",
      "Epoch 1791/3000, Training Loss: 29.38276, Validation Loss: 3.95437\n",
      "Epoch 1792/3000, Training Loss: 29.38160, Validation Loss: 3.95410\n",
      "Epoch 1793/3000, Training Loss: 29.38043, Validation Loss: 3.95383\n",
      "Epoch 1794/3000, Training Loss: 29.37927, Validation Loss: 3.95356\n",
      "Epoch 1795/3000, Training Loss: 29.37811, Validation Loss: 3.95329\n",
      "Epoch 1796/3000, Training Loss: 29.37695, Validation Loss: 3.95302\n",
      "Epoch 1797/3000, Training Loss: 29.37580, Validation Loss: 3.95275\n",
      "Epoch 1798/3000, Training Loss: 29.37464, Validation Loss: 3.95249\n",
      "Epoch 1799/3000, Training Loss: 29.37349, Validation Loss: 3.95222\n",
      "Epoch 1800/3000, Training Loss: 29.37234, Validation Loss: 3.95195\n",
      "Epoch 1801/3000, Training Loss: 29.37119, Validation Loss: 3.95169\n",
      "Epoch 1802/3000, Training Loss: 29.37004, Validation Loss: 3.95143\n",
      "Epoch 1803/3000, Training Loss: 29.36890, Validation Loss: 3.95116\n",
      "Epoch 1804/3000, Training Loss: 29.36776, Validation Loss: 3.95090\n",
      "Epoch 1805/3000, Training Loss: 29.36661, Validation Loss: 3.95064\n",
      "Epoch 1806/3000, Training Loss: 29.36548, Validation Loss: 3.95038\n",
      "Epoch 1807/3000, Training Loss: 29.36434, Validation Loss: 3.95011\n",
      "Epoch 1808/3000, Training Loss: 29.36320, Validation Loss: 3.94985\n",
      "Epoch 1809/3000, Training Loss: 29.36207, Validation Loss: 3.94959\n",
      "Epoch 1810/3000, Training Loss: 29.36094, Validation Loss: 3.94933\n",
      "Epoch 1811/3000, Training Loss: 29.35982, Validation Loss: 3.94907\n",
      "Epoch 1812/3000, Training Loss: 29.35869, Validation Loss: 3.94882\n",
      "Epoch 1813/3000, Training Loss: 29.35757, Validation Loss: 3.94856\n",
      "Epoch 1814/3000, Training Loss: 29.35644, Validation Loss: 3.94830\n",
      "Epoch 1815/3000, Training Loss: 29.35532, Validation Loss: 3.94805\n",
      "Epoch 1816/3000, Training Loss: 29.35421, Validation Loss: 3.94779\n",
      "Epoch 1817/3000, Training Loss: 29.35309, Validation Loss: 3.94754\n",
      "Epoch 1818/3000, Training Loss: 29.35197, Validation Loss: 3.94729\n",
      "Epoch 1819/3000, Training Loss: 29.35086, Validation Loss: 3.94703\n",
      "Epoch 1820/3000, Training Loss: 29.34975, Validation Loss: 3.94678\n",
      "Epoch 1821/3000, Training Loss: 29.34864, Validation Loss: 3.94653\n",
      "Epoch 1822/3000, Training Loss: 29.34753, Validation Loss: 3.94628\n",
      "Epoch 1823/3000, Training Loss: 29.34643, Validation Loss: 3.94603\n",
      "Epoch 1824/3000, Training Loss: 29.34532, Validation Loss: 3.94578\n",
      "Epoch 1825/3000, Training Loss: 29.34422, Validation Loss: 3.94553\n",
      "Epoch 1826/3000, Training Loss: 29.34312, Validation Loss: 3.94528\n",
      "Epoch 1827/3000, Training Loss: 29.34202, Validation Loss: 3.94503\n",
      "Epoch 1828/3000, Training Loss: 29.34093, Validation Loss: 3.94478\n",
      "Epoch 1829/3000, Training Loss: 29.33984, Validation Loss: 3.94454\n",
      "Epoch 1830/3000, Training Loss: 29.33874, Validation Loss: 3.94429\n",
      "Epoch 1831/3000, Training Loss: 29.33765, Validation Loss: 3.94404\n",
      "Epoch 1832/3000, Training Loss: 29.33656, Validation Loss: 3.94380\n",
      "Epoch 1833/3000, Training Loss: 29.33547, Validation Loss: 3.94355\n",
      "Epoch 1834/3000, Training Loss: 29.33438, Validation Loss: 3.94331\n",
      "Epoch 1835/3000, Training Loss: 29.33330, Validation Loss: 3.94306\n",
      "Epoch 1836/3000, Training Loss: 29.33221, Validation Loss: 3.94282\n",
      "Epoch 1837/3000, Training Loss: 29.33113, Validation Loss: 3.94258\n",
      "Epoch 1838/3000, Training Loss: 29.33005, Validation Loss: 3.94233\n",
      "Epoch 1839/3000, Training Loss: 29.32897, Validation Loss: 3.94209\n",
      "Epoch 1840/3000, Training Loss: 29.32789, Validation Loss: 3.94185\n",
      "Epoch 1841/3000, Training Loss: 29.32682, Validation Loss: 3.94161\n",
      "Epoch 1842/3000, Training Loss: 29.32574, Validation Loss: 3.94137\n",
      "Epoch 1843/3000, Training Loss: 29.32467, Validation Loss: 3.94113\n",
      "Epoch 1844/3000, Training Loss: 29.32360, Validation Loss: 3.94089\n",
      "Epoch 1845/3000, Training Loss: 29.32253, Validation Loss: 3.94066\n",
      "Epoch 1846/3000, Training Loss: 29.32146, Validation Loss: 3.94042\n",
      "Epoch 1847/3000, Training Loss: 29.32039, Validation Loss: 3.94018\n",
      "Epoch 1848/3000, Training Loss: 29.31933, Validation Loss: 3.93995\n",
      "Epoch 1849/3000, Training Loss: 29.31826, Validation Loss: 3.93971\n",
      "Epoch 1850/3000, Training Loss: 29.31720, Validation Loss: 3.93948\n",
      "Epoch 1851/3000, Training Loss: 29.31614, Validation Loss: 3.93924\n",
      "Epoch 1852/3000, Training Loss: 29.31509, Validation Loss: 3.93901\n",
      "Epoch 1853/3000, Training Loss: 29.31403, Validation Loss: 3.93877\n",
      "Epoch 1854/3000, Training Loss: 29.31298, Validation Loss: 3.93854\n",
      "Epoch 1855/3000, Training Loss: 29.31193, Validation Loss: 3.93831\n",
      "Epoch 1856/3000, Training Loss: 29.31088, Validation Loss: 3.93808\n",
      "Epoch 1857/3000, Training Loss: 29.30983, Validation Loss: 3.93785\n",
      "Epoch 1858/3000, Training Loss: 29.30879, Validation Loss: 3.93762\n",
      "Epoch 1859/3000, Training Loss: 29.30774, Validation Loss: 3.93739\n",
      "Epoch 1860/3000, Training Loss: 29.30669, Validation Loss: 3.93716\n",
      "Epoch 1861/3000, Training Loss: 29.30564, Validation Loss: 3.93693\n",
      "Epoch 1862/3000, Training Loss: 29.30460, Validation Loss: 3.93670\n",
      "Epoch 1863/3000, Training Loss: 29.30355, Validation Loss: 3.93648\n",
      "Epoch 1864/3000, Training Loss: 29.30251, Validation Loss: 3.93625\n",
      "Epoch 1865/3000, Training Loss: 29.30147, Validation Loss: 3.93603\n",
      "Epoch 1866/3000, Training Loss: 29.30043, Validation Loss: 3.93580\n",
      "Epoch 1867/3000, Training Loss: 29.29939, Validation Loss: 3.93558\n",
      "Epoch 1868/3000, Training Loss: 29.29836, Validation Loss: 3.93535\n",
      "Epoch 1869/3000, Training Loss: 29.29732, Validation Loss: 3.93513\n",
      "Epoch 1870/3000, Training Loss: 29.29629, Validation Loss: 3.93491\n",
      "Epoch 1871/3000, Training Loss: 29.29526, Validation Loss: 3.93468\n",
      "Epoch 1872/3000, Training Loss: 29.29423, Validation Loss: 3.93446\n",
      "Epoch 1873/3000, Training Loss: 29.29321, Validation Loss: 3.93424\n",
      "Epoch 1874/3000, Training Loss: 29.29218, Validation Loss: 3.93401\n",
      "Epoch 1875/3000, Training Loss: 29.29116, Validation Loss: 3.93379\n",
      "Epoch 1876/3000, Training Loss: 29.29014, Validation Loss: 3.93357\n",
      "Epoch 1877/3000, Training Loss: 29.28912, Validation Loss: 3.93335\n",
      "Epoch 1878/3000, Training Loss: 29.28811, Validation Loss: 3.93313\n",
      "Epoch 1879/3000, Training Loss: 29.28709, Validation Loss: 3.93291\n",
      "Epoch 1880/3000, Training Loss: 29.28608, Validation Loss: 3.93270\n",
      "Epoch 1881/3000, Training Loss: 29.28507, Validation Loss: 3.93248\n",
      "Epoch 1882/3000, Training Loss: 29.28406, Validation Loss: 3.93226\n",
      "Epoch 1883/3000, Training Loss: 29.28305, Validation Loss: 3.93204\n",
      "Epoch 1884/3000, Training Loss: 29.28204, Validation Loss: 3.93183\n",
      "Epoch 1885/3000, Training Loss: 29.28104, Validation Loss: 3.93161\n",
      "Epoch 1886/3000, Training Loss: 29.28003, Validation Loss: 3.93140\n",
      "Epoch 1887/3000, Training Loss: 29.27903, Validation Loss: 3.93118\n",
      "Epoch 1888/3000, Training Loss: 29.27803, Validation Loss: 3.93097\n",
      "Epoch 1889/3000, Training Loss: 29.27703, Validation Loss: 3.93075\n",
      "Epoch 1890/3000, Training Loss: 29.27603, Validation Loss: 3.93054\n",
      "Epoch 1891/3000, Training Loss: 29.27503, Validation Loss: 3.93033\n",
      "Epoch 1892/3000, Training Loss: 29.27404, Validation Loss: 3.93012\n",
      "Epoch 1893/3000, Training Loss: 29.27304, Validation Loss: 3.92991\n",
      "Epoch 1894/3000, Training Loss: 29.27205, Validation Loss: 3.92969\n",
      "Epoch 1895/3000, Training Loss: 29.27106, Validation Loss: 3.92948\n",
      "Epoch 1896/3000, Training Loss: 29.27007, Validation Loss: 3.92927\n",
      "Epoch 1897/3000, Training Loss: 29.26908, Validation Loss: 3.92906\n",
      "Epoch 1898/3000, Training Loss: 29.26810, Validation Loss: 3.92886\n",
      "Epoch 1899/3000, Training Loss: 29.26711, Validation Loss: 3.92865\n",
      "Epoch 1900/3000, Training Loss: 29.26613, Validation Loss: 3.92844\n",
      "Epoch 1901/3000, Training Loss: 29.26515, Validation Loss: 3.92824\n",
      "Epoch 1902/3000, Training Loss: 29.26417, Validation Loss: 3.92803\n",
      "Epoch 1903/3000, Training Loss: 29.26319, Validation Loss: 3.92783\n",
      "Epoch 1904/3000, Training Loss: 29.26221, Validation Loss: 3.92762\n",
      "Epoch 1905/3000, Training Loss: 29.26123, Validation Loss: 3.92742\n",
      "Epoch 1906/3000, Training Loss: 29.26026, Validation Loss: 3.92722\n",
      "Epoch 1907/3000, Training Loss: 29.25929, Validation Loss: 3.92702\n",
      "Epoch 1908/3000, Training Loss: 29.25832, Validation Loss: 3.92681\n",
      "Epoch 1909/3000, Training Loss: 29.25736, Validation Loss: 3.92661\n",
      "Epoch 1910/3000, Training Loss: 29.25639, Validation Loss: 3.92641\n",
      "Epoch 1911/3000, Training Loss: 29.25543, Validation Loss: 3.92621\n",
      "Epoch 1912/3000, Training Loss: 29.25447, Validation Loss: 3.92601\n",
      "Epoch 1913/3000, Training Loss: 29.25350, Validation Loss: 3.92581\n",
      "Epoch 1914/3000, Training Loss: 29.25254, Validation Loss: 3.92562\n",
      "Epoch 1915/3000, Training Loss: 29.25158, Validation Loss: 3.92542\n",
      "Epoch 1916/3000, Training Loss: 29.25062, Validation Loss: 3.92522\n",
      "Epoch 1917/3000, Training Loss: 29.24967, Validation Loss: 3.92502\n",
      "Epoch 1918/3000, Training Loss: 29.24871, Validation Loss: 3.92483\n",
      "Epoch 1919/3000, Training Loss: 29.24776, Validation Loss: 3.92463\n",
      "Epoch 1920/3000, Training Loss: 29.24680, Validation Loss: 3.92444\n",
      "Epoch 1921/3000, Training Loss: 29.24585, Validation Loss: 3.92425\n",
      "Epoch 1922/3000, Training Loss: 29.24490, Validation Loss: 3.92405\n",
      "Epoch 1923/3000, Training Loss: 29.24395, Validation Loss: 3.92386\n",
      "Epoch 1924/3000, Training Loss: 29.24300, Validation Loss: 3.92367\n",
      "Epoch 1925/3000, Training Loss: 29.24206, Validation Loss: 3.92347\n",
      "Epoch 1926/3000, Training Loss: 29.24111, Validation Loss: 3.92328\n",
      "Epoch 1927/3000, Training Loss: 29.24017, Validation Loss: 3.92309\n",
      "Epoch 1928/3000, Training Loss: 29.23923, Validation Loss: 3.92290\n",
      "Epoch 1929/3000, Training Loss: 29.23829, Validation Loss: 3.92270\n",
      "Epoch 1930/3000, Training Loss: 29.23736, Validation Loss: 3.92251\n",
      "Epoch 1931/3000, Training Loss: 29.23642, Validation Loss: 3.92232\n",
      "Epoch 1932/3000, Training Loss: 29.23548, Validation Loss: 3.92213\n",
      "Epoch 1933/3000, Training Loss: 29.23455, Validation Loss: 3.92194\n",
      "Epoch 1934/3000, Training Loss: 29.23361, Validation Loss: 3.92174\n",
      "Epoch 1935/3000, Training Loss: 29.23268, Validation Loss: 3.92155\n",
      "Epoch 1936/3000, Training Loss: 29.23175, Validation Loss: 3.92136\n",
      "Epoch 1937/3000, Training Loss: 29.23082, Validation Loss: 3.92117\n",
      "Epoch 1938/3000, Training Loss: 29.22989, Validation Loss: 3.92098\n",
      "Epoch 1939/3000, Training Loss: 29.22897, Validation Loss: 3.92079\n",
      "Epoch 1940/3000, Training Loss: 29.22804, Validation Loss: 3.92061\n",
      "Epoch 1941/3000, Training Loss: 29.22712, Validation Loss: 3.92042\n",
      "Epoch 1942/3000, Training Loss: 29.22619, Validation Loss: 3.92023\n",
      "Epoch 1943/3000, Training Loss: 29.22527, Validation Loss: 3.92004\n",
      "Epoch 1944/3000, Training Loss: 29.22435, Validation Loss: 3.91985\n",
      "Epoch 1945/3000, Training Loss: 29.22343, Validation Loss: 3.91967\n",
      "Epoch 1946/3000, Training Loss: 29.22252, Validation Loss: 3.91948\n",
      "Epoch 1947/3000, Training Loss: 29.22160, Validation Loss: 3.91929\n",
      "Epoch 1948/3000, Training Loss: 29.22068, Validation Loss: 3.91911\n",
      "Epoch 1949/3000, Training Loss: 29.21977, Validation Loss: 3.91892\n",
      "Epoch 1950/3000, Training Loss: 29.21886, Validation Loss: 3.91873\n",
      "Epoch 1951/3000, Training Loss: 29.21794, Validation Loss: 3.91855\n",
      "Epoch 1952/3000, Training Loss: 29.21703, Validation Loss: 3.91837\n",
      "Epoch 1953/3000, Training Loss: 29.21612, Validation Loss: 3.91819\n",
      "Epoch 1954/3000, Training Loss: 29.21522, Validation Loss: 3.91801\n",
      "Epoch 1955/3000, Training Loss: 29.21431, Validation Loss: 3.91783\n",
      "Epoch 1956/3000, Training Loss: 29.21340, Validation Loss: 3.91765\n",
      "Epoch 1957/3000, Training Loss: 29.21250, Validation Loss: 3.91747\n",
      "Epoch 1958/3000, Training Loss: 29.21160, Validation Loss: 3.91730\n",
      "Epoch 1959/3000, Training Loss: 29.21070, Validation Loss: 3.91712\n",
      "Epoch 1960/3000, Training Loss: 29.20980, Validation Loss: 3.91694\n",
      "Epoch 1961/3000, Training Loss: 29.20890, Validation Loss: 3.91677\n",
      "Epoch 1962/3000, Training Loss: 29.20800, Validation Loss: 3.91659\n",
      "Epoch 1963/3000, Training Loss: 29.20710, Validation Loss: 3.91642\n",
      "Epoch 1964/3000, Training Loss: 29.20621, Validation Loss: 3.91624\n",
      "Epoch 1965/3000, Training Loss: 29.20531, Validation Loss: 3.91607\n",
      "Epoch 1966/3000, Training Loss: 29.20442, Validation Loss: 3.91590\n",
      "Epoch 1967/3000, Training Loss: 29.20353, Validation Loss: 3.91572\n",
      "Epoch 1968/3000, Training Loss: 29.20264, Validation Loss: 3.91555\n",
      "Epoch 1969/3000, Training Loss: 29.20175, Validation Loss: 3.91538\n",
      "Epoch 1970/3000, Training Loss: 29.20087, Validation Loss: 3.91521\n",
      "Epoch 1971/3000, Training Loss: 29.19998, Validation Loss: 3.91504\n",
      "Epoch 1972/3000, Training Loss: 29.19910, Validation Loss: 3.91487\n",
      "Epoch 1973/3000, Training Loss: 29.19821, Validation Loss: 3.91470\n",
      "Epoch 1974/3000, Training Loss: 29.19733, Validation Loss: 3.91453\n",
      "Epoch 1975/3000, Training Loss: 29.19645, Validation Loss: 3.91436\n",
      "Epoch 1976/3000, Training Loss: 29.19557, Validation Loss: 3.91420\n",
      "Epoch 1977/3000, Training Loss: 29.19469, Validation Loss: 3.91403\n",
      "Epoch 1978/3000, Training Loss: 29.19381, Validation Loss: 3.91386\n",
      "Epoch 1979/3000, Training Loss: 29.19293, Validation Loss: 3.91370\n",
      "Epoch 1980/3000, Training Loss: 29.19205, Validation Loss: 3.91353\n",
      "Epoch 1981/3000, Training Loss: 29.19118, Validation Loss: 3.91337\n",
      "Epoch 1982/3000, Training Loss: 29.19030, Validation Loss: 3.91320\n",
      "Epoch 1983/3000, Training Loss: 29.18943, Validation Loss: 3.91304\n",
      "Epoch 1984/3000, Training Loss: 29.18855, Validation Loss: 3.91287\n",
      "Epoch 1985/3000, Training Loss: 29.18768, Validation Loss: 3.91271\n",
      "Epoch 1986/3000, Training Loss: 29.18681, Validation Loss: 3.91254\n",
      "Epoch 1987/3000, Training Loss: 29.18594, Validation Loss: 3.91238\n",
      "Epoch 1988/3000, Training Loss: 29.18507, Validation Loss: 3.91222\n",
      "Epoch 1989/3000, Training Loss: 29.18420, Validation Loss: 3.91206\n",
      "Epoch 1990/3000, Training Loss: 29.18333, Validation Loss: 3.91190\n",
      "Epoch 1991/3000, Training Loss: 29.18247, Validation Loss: 3.91174\n",
      "Epoch 1992/3000, Training Loss: 29.18160, Validation Loss: 3.91157\n",
      "Epoch 1993/3000, Training Loss: 29.18074, Validation Loss: 3.91141\n",
      "Epoch 1994/3000, Training Loss: 29.17988, Validation Loss: 3.91125\n",
      "Epoch 1995/3000, Training Loss: 29.17901, Validation Loss: 3.91109\n",
      "Epoch 1996/3000, Training Loss: 29.17815, Validation Loss: 3.91093\n",
      "Epoch 1997/3000, Training Loss: 29.17729, Validation Loss: 3.91078\n",
      "Epoch 1998/3000, Training Loss: 29.17643, Validation Loss: 3.91062\n",
      "Epoch 1999/3000, Training Loss: 29.17558, Validation Loss: 3.91046\n",
      "Epoch 2000/3000, Training Loss: 29.17472, Validation Loss: 3.91030\n",
      "Epoch 2001/3000, Training Loss: 29.17386, Validation Loss: 3.91014\n",
      "Epoch 2002/3000, Training Loss: 29.17301, Validation Loss: 3.90999\n",
      "Epoch 2003/3000, Training Loss: 29.17216, Validation Loss: 3.90983\n",
      "Epoch 2004/3000, Training Loss: 29.17131, Validation Loss: 3.90967\n",
      "Epoch 2005/3000, Training Loss: 29.17046, Validation Loss: 3.90952\n",
      "Epoch 2006/3000, Training Loss: 29.16962, Validation Loss: 3.90937\n",
      "Epoch 2007/3000, Training Loss: 29.16877, Validation Loss: 3.90921\n",
      "Epoch 2008/3000, Training Loss: 29.16793, Validation Loss: 3.90906\n",
      "Epoch 2009/3000, Training Loss: 29.16709, Validation Loss: 3.90891\n",
      "Epoch 2010/3000, Training Loss: 29.16625, Validation Loss: 3.90876\n",
      "Epoch 2011/3000, Training Loss: 29.16541, Validation Loss: 3.90861\n",
      "Epoch 2012/3000, Training Loss: 29.16457, Validation Loss: 3.90845\n",
      "Epoch 2013/3000, Training Loss: 29.16373, Validation Loss: 3.90830\n",
      "Epoch 2014/3000, Training Loss: 29.16289, Validation Loss: 3.90815\n",
      "Epoch 2015/3000, Training Loss: 29.16206, Validation Loss: 3.90800\n",
      "Epoch 2016/3000, Training Loss: 29.16122, Validation Loss: 3.90785\n",
      "Epoch 2017/3000, Training Loss: 29.16039, Validation Loss: 3.90771\n",
      "Epoch 2018/3000, Training Loss: 29.15955, Validation Loss: 3.90756\n",
      "Epoch 2019/3000, Training Loss: 29.15872, Validation Loss: 3.90741\n",
      "Epoch 2020/3000, Training Loss: 29.15788, Validation Loss: 3.90726\n",
      "Epoch 2021/3000, Training Loss: 29.15705, Validation Loss: 3.90712\n",
      "Epoch 2022/3000, Training Loss: 29.15622, Validation Loss: 3.90697\n",
      "Epoch 2023/3000, Training Loss: 29.15539, Validation Loss: 3.90683\n",
      "Epoch 2024/3000, Training Loss: 29.15456, Validation Loss: 3.90668\n",
      "Epoch 2025/3000, Training Loss: 29.15373, Validation Loss: 3.90654\n",
      "Epoch 2026/3000, Training Loss: 29.15290, Validation Loss: 3.90640\n",
      "Epoch 2027/3000, Training Loss: 29.15207, Validation Loss: 3.90625\n",
      "Epoch 2028/3000, Training Loss: 29.15125, Validation Loss: 3.90611\n",
      "Epoch 2029/3000, Training Loss: 29.15042, Validation Loss: 3.90597\n",
      "Epoch 2030/3000, Training Loss: 29.14960, Validation Loss: 3.90583\n",
      "Epoch 2031/3000, Training Loss: 29.14877, Validation Loss: 3.90568\n",
      "Epoch 2032/3000, Training Loss: 29.14795, Validation Loss: 3.90554\n",
      "Epoch 2033/3000, Training Loss: 29.14713, Validation Loss: 3.90540\n",
      "Epoch 2034/3000, Training Loss: 29.14631, Validation Loss: 3.90526\n",
      "Epoch 2035/3000, Training Loss: 29.14549, Validation Loss: 3.90512\n",
      "Epoch 2036/3000, Training Loss: 29.14467, Validation Loss: 3.90498\n",
      "Epoch 2037/3000, Training Loss: 29.14385, Validation Loss: 3.90484\n",
      "Epoch 2038/3000, Training Loss: 29.14304, Validation Loss: 3.90470\n",
      "Epoch 2039/3000, Training Loss: 29.14222, Validation Loss: 3.90457\n",
      "Epoch 2040/3000, Training Loss: 29.14141, Validation Loss: 3.90443\n",
      "Epoch 2041/3000, Training Loss: 29.14059, Validation Loss: 3.90429\n",
      "Epoch 2042/3000, Training Loss: 29.13978, Validation Loss: 3.90415\n",
      "Epoch 2043/3000, Training Loss: 29.13897, Validation Loss: 3.90401\n",
      "Epoch 2044/3000, Training Loss: 29.13817, Validation Loss: 3.90388\n",
      "Epoch 2045/3000, Training Loss: 29.13736, Validation Loss: 3.90374\n",
      "Epoch 2046/3000, Training Loss: 29.13655, Validation Loss: 3.90360\n",
      "Epoch 2047/3000, Training Loss: 29.13575, Validation Loss: 3.90347\n",
      "Epoch 2048/3000, Training Loss: 29.13495, Validation Loss: 3.90333\n",
      "Epoch 2049/3000, Training Loss: 29.13414, Validation Loss: 3.90320\n",
      "Epoch 2050/3000, Training Loss: 29.13334, Validation Loss: 3.90306\n",
      "Epoch 2051/3000, Training Loss: 29.13254, Validation Loss: 3.90293\n",
      "Epoch 2052/3000, Training Loss: 29.13174, Validation Loss: 3.90279\n",
      "Epoch 2053/3000, Training Loss: 29.13094, Validation Loss: 3.90266\n",
      "Epoch 2054/3000, Training Loss: 29.13014, Validation Loss: 3.90253\n",
      "Epoch 2055/3000, Training Loss: 29.12935, Validation Loss: 3.90239\n",
      "Epoch 2056/3000, Training Loss: 29.12855, Validation Loss: 3.90226\n",
      "Epoch 2057/3000, Training Loss: 29.12776, Validation Loss: 3.90213\n",
      "Epoch 2058/3000, Training Loss: 29.12697, Validation Loss: 3.90199\n",
      "Epoch 2059/3000, Training Loss: 29.12618, Validation Loss: 3.90186\n",
      "Epoch 2060/3000, Training Loss: 29.12539, Validation Loss: 3.90173\n",
      "Epoch 2061/3000, Training Loss: 29.12460, Validation Loss: 3.90160\n",
      "Epoch 2062/3000, Training Loss: 29.12381, Validation Loss: 3.90146\n",
      "Epoch 2063/3000, Training Loss: 29.12302, Validation Loss: 3.90133\n",
      "Epoch 2064/3000, Training Loss: 29.12224, Validation Loss: 3.90120\n",
      "Epoch 2065/3000, Training Loss: 29.12145, Validation Loss: 3.90107\n",
      "Epoch 2066/3000, Training Loss: 29.12067, Validation Loss: 3.90094\n",
      "Epoch 2067/3000, Training Loss: 29.11989, Validation Loss: 3.90081\n",
      "Epoch 2068/3000, Training Loss: 29.11910, Validation Loss: 3.90068\n",
      "Epoch 2069/3000, Training Loss: 29.11832, Validation Loss: 3.90055\n",
      "Epoch 2070/3000, Training Loss: 29.11754, Validation Loss: 3.90042\n",
      "Epoch 2071/3000, Training Loss: 29.11677, Validation Loss: 3.90029\n",
      "Epoch 2072/3000, Training Loss: 29.11599, Validation Loss: 3.90016\n",
      "Epoch 2073/3000, Training Loss: 29.11521, Validation Loss: 3.90004\n",
      "Epoch 2074/3000, Training Loss: 29.11444, Validation Loss: 3.89991\n",
      "Epoch 2075/3000, Training Loss: 29.11367, Validation Loss: 3.89978\n",
      "Epoch 2076/3000, Training Loss: 29.11290, Validation Loss: 3.89965\n",
      "Epoch 2077/3000, Training Loss: 29.11213, Validation Loss: 3.89952\n",
      "Epoch 2078/3000, Training Loss: 29.11136, Validation Loss: 3.89940\n",
      "Epoch 2079/3000, Training Loss: 29.11059, Validation Loss: 3.89927\n",
      "Epoch 2080/3000, Training Loss: 29.10982, Validation Loss: 3.89914\n",
      "Epoch 2081/3000, Training Loss: 29.10906, Validation Loss: 3.89902\n",
      "Epoch 2082/3000, Training Loss: 29.10829, Validation Loss: 3.89889\n",
      "Epoch 2083/3000, Training Loss: 29.10753, Validation Loss: 3.89877\n",
      "Epoch 2084/3000, Training Loss: 29.10676, Validation Loss: 3.89864\n",
      "Epoch 2085/3000, Training Loss: 29.10600, Validation Loss: 3.89852\n",
      "Epoch 2086/3000, Training Loss: 29.10524, Validation Loss: 3.89839\n",
      "Epoch 2087/3000, Training Loss: 29.10448, Validation Loss: 3.89827\n",
      "Epoch 2088/3000, Training Loss: 29.10372, Validation Loss: 3.89815\n",
      "Epoch 2089/3000, Training Loss: 29.10297, Validation Loss: 3.89802\n",
      "Epoch 2090/3000, Training Loss: 29.10221, Validation Loss: 3.89790\n",
      "Epoch 2091/3000, Training Loss: 29.10145, Validation Loss: 3.89778\n",
      "Epoch 2092/3000, Training Loss: 29.10070, Validation Loss: 3.89766\n",
      "Epoch 2093/3000, Training Loss: 29.09995, Validation Loss: 3.89754\n",
      "Epoch 2094/3000, Training Loss: 29.09919, Validation Loss: 3.89741\n",
      "Epoch 2095/3000, Training Loss: 29.09844, Validation Loss: 3.89729\n",
      "Epoch 2096/3000, Training Loss: 29.09769, Validation Loss: 3.89717\n",
      "Epoch 2097/3000, Training Loss: 29.09695, Validation Loss: 3.89705\n",
      "Epoch 2098/3000, Training Loss: 29.09620, Validation Loss: 3.89693\n",
      "Epoch 2099/3000, Training Loss: 29.09545, Validation Loss: 3.89680\n",
      "Epoch 2100/3000, Training Loss: 29.09470, Validation Loss: 3.89668\n",
      "Epoch 2101/3000, Training Loss: 29.09396, Validation Loss: 3.89656\n",
      "Epoch 2102/3000, Training Loss: 29.09321, Validation Loss: 3.89644\n",
      "Epoch 2103/3000, Training Loss: 29.09246, Validation Loss: 3.89632\n",
      "Epoch 2104/3000, Training Loss: 29.09172, Validation Loss: 3.89620\n",
      "Epoch 2105/3000, Training Loss: 29.09098, Validation Loss: 3.89608\n",
      "Epoch 2106/3000, Training Loss: 29.09024, Validation Loss: 3.89596\n",
      "Epoch 2107/3000, Training Loss: 29.08949, Validation Loss: 3.89584\n",
      "Epoch 2108/3000, Training Loss: 29.08875, Validation Loss: 3.89573\n",
      "Epoch 2109/3000, Training Loss: 29.08801, Validation Loss: 3.89561\n",
      "Epoch 2110/3000, Training Loss: 29.08727, Validation Loss: 3.89549\n",
      "Epoch 2111/3000, Training Loss: 29.08654, Validation Loss: 3.89537\n",
      "Epoch 2112/3000, Training Loss: 29.08580, Validation Loss: 3.89525\n",
      "Epoch 2113/3000, Training Loss: 29.08506, Validation Loss: 3.89513\n",
      "Epoch 2114/3000, Training Loss: 29.08433, Validation Loss: 3.89502\n",
      "Epoch 2115/3000, Training Loss: 29.08359, Validation Loss: 3.89490\n",
      "Epoch 2116/3000, Training Loss: 29.08286, Validation Loss: 3.89478\n",
      "Epoch 2117/3000, Training Loss: 29.08213, Validation Loss: 3.89467\n",
      "Epoch 2118/3000, Training Loss: 29.08139, Validation Loss: 3.89455\n",
      "Epoch 2119/3000, Training Loss: 29.08066, Validation Loss: 3.89443\n",
      "Epoch 2120/3000, Training Loss: 29.07993, Validation Loss: 3.89432\n",
      "Epoch 2121/3000, Training Loss: 29.07919, Validation Loss: 3.89420\n",
      "Epoch 2122/3000, Training Loss: 29.07846, Validation Loss: 3.89409\n",
      "Epoch 2123/3000, Training Loss: 29.07773, Validation Loss: 3.89398\n",
      "Epoch 2124/3000, Training Loss: 29.07699, Validation Loss: 3.89386\n",
      "Epoch 2125/3000, Training Loss: 29.07626, Validation Loss: 3.89375\n",
      "Epoch 2126/3000, Training Loss: 29.07553, Validation Loss: 3.89364\n",
      "Epoch 2127/3000, Training Loss: 29.07480, Validation Loss: 3.89352\n",
      "Epoch 2128/3000, Training Loss: 29.07406, Validation Loss: 3.89341\n",
      "Epoch 2129/3000, Training Loss: 29.07333, Validation Loss: 3.89330\n",
      "Epoch 2130/3000, Training Loss: 29.07260, Validation Loss: 3.89319\n",
      "Epoch 2131/3000, Training Loss: 29.07187, Validation Loss: 3.89307\n",
      "Epoch 2132/3000, Training Loss: 29.07114, Validation Loss: 3.89296\n",
      "Epoch 2133/3000, Training Loss: 29.07041, Validation Loss: 3.89285\n",
      "Epoch 2134/3000, Training Loss: 29.06968, Validation Loss: 3.89274\n",
      "Epoch 2135/3000, Training Loss: 29.06896, Validation Loss: 3.89263\n",
      "Epoch 2136/3000, Training Loss: 29.06823, Validation Loss: 3.89251\n",
      "Epoch 2137/3000, Training Loss: 29.06750, Validation Loss: 3.89240\n",
      "Epoch 2138/3000, Training Loss: 29.06678, Validation Loss: 3.89229\n",
      "Epoch 2139/3000, Training Loss: 29.06606, Validation Loss: 3.89218\n",
      "Epoch 2140/3000, Training Loss: 29.06533, Validation Loss: 3.89208\n",
      "Epoch 2141/3000, Training Loss: 29.06461, Validation Loss: 3.89197\n",
      "Epoch 2142/3000, Training Loss: 29.06389, Validation Loss: 3.89186\n",
      "Epoch 2143/3000, Training Loss: 29.06317, Validation Loss: 3.89175\n",
      "Epoch 2144/3000, Training Loss: 29.06245, Validation Loss: 3.89164\n",
      "Epoch 2145/3000, Training Loss: 29.06173, Validation Loss: 3.89153\n",
      "Epoch 2146/3000, Training Loss: 29.06101, Validation Loss: 3.89143\n",
      "Epoch 2147/3000, Training Loss: 29.06030, Validation Loss: 3.89132\n",
      "Epoch 2148/3000, Training Loss: 29.05958, Validation Loss: 3.89121\n",
      "Epoch 2149/3000, Training Loss: 29.05887, Validation Loss: 3.89110\n",
      "Epoch 2150/3000, Training Loss: 29.05815, Validation Loss: 3.89100\n",
      "Epoch 2151/3000, Training Loss: 29.05744, Validation Loss: 3.89089\n",
      "Epoch 2152/3000, Training Loss: 29.05673, Validation Loss: 3.89079\n",
      "Epoch 2153/3000, Training Loss: 29.05601, Validation Loss: 3.89068\n",
      "Epoch 2154/3000, Training Loss: 29.05530, Validation Loss: 3.89058\n",
      "Epoch 2155/3000, Training Loss: 29.05459, Validation Loss: 3.89047\n",
      "Epoch 2156/3000, Training Loss: 29.05388, Validation Loss: 3.89037\n",
      "Epoch 2157/3000, Training Loss: 29.05317, Validation Loss: 3.89027\n",
      "Epoch 2158/3000, Training Loss: 29.05246, Validation Loss: 3.89016\n",
      "Epoch 2159/3000, Training Loss: 29.05175, Validation Loss: 3.89006\n",
      "Epoch 2160/3000, Training Loss: 29.05105, Validation Loss: 3.88996\n",
      "Epoch 2161/3000, Training Loss: 29.05034, Validation Loss: 3.88985\n",
      "Epoch 2162/3000, Training Loss: 29.04964, Validation Loss: 3.88975\n",
      "Epoch 2163/3000, Training Loss: 29.04893, Validation Loss: 3.88965\n",
      "Epoch 2164/3000, Training Loss: 29.04823, Validation Loss: 3.88955\n",
      "Epoch 2165/3000, Training Loss: 29.04752, Validation Loss: 3.88944\n",
      "Epoch 2166/3000, Training Loss: 29.04682, Validation Loss: 3.88934\n",
      "Epoch 2167/3000, Training Loss: 29.04612, Validation Loss: 3.88924\n",
      "Epoch 2168/3000, Training Loss: 29.04542, Validation Loss: 3.88914\n",
      "Epoch 2169/3000, Training Loss: 29.04472, Validation Loss: 3.88904\n",
      "Epoch 2170/3000, Training Loss: 29.04402, Validation Loss: 3.88894\n",
      "Epoch 2171/3000, Training Loss: 29.04332, Validation Loss: 3.88884\n",
      "Epoch 2172/3000, Training Loss: 29.04262, Validation Loss: 3.88874\n",
      "Epoch 2173/3000, Training Loss: 29.04193, Validation Loss: 3.88864\n",
      "Epoch 2174/3000, Training Loss: 29.04123, Validation Loss: 3.88854\n",
      "Epoch 2175/3000, Training Loss: 29.04053, Validation Loss: 3.88844\n",
      "Epoch 2176/3000, Training Loss: 29.03984, Validation Loss: 3.88834\n",
      "Epoch 2177/3000, Training Loss: 29.03915, Validation Loss: 3.88824\n",
      "Epoch 2178/3000, Training Loss: 29.03845, Validation Loss: 3.88814\n",
      "Epoch 2179/3000, Training Loss: 29.03776, Validation Loss: 3.88804\n",
      "Epoch 2180/3000, Training Loss: 29.03707, Validation Loss: 3.88795\n",
      "Epoch 2181/3000, Training Loss: 29.03638, Validation Loss: 3.88785\n",
      "Epoch 2182/3000, Training Loss: 29.03569, Validation Loss: 3.88775\n",
      "Epoch 2183/3000, Training Loss: 29.03500, Validation Loss: 3.88765\n",
      "Epoch 2184/3000, Training Loss: 29.03431, Validation Loss: 3.88756\n",
      "Epoch 2185/3000, Training Loss: 29.03362, Validation Loss: 3.88746\n",
      "Epoch 2186/3000, Training Loss: 29.03294, Validation Loss: 3.88736\n",
      "Epoch 2187/3000, Training Loss: 29.03225, Validation Loss: 3.88727\n",
      "Epoch 2188/3000, Training Loss: 29.03156, Validation Loss: 3.88717\n",
      "Epoch 2189/3000, Training Loss: 29.03088, Validation Loss: 3.88708\n",
      "Epoch 2190/3000, Training Loss: 29.03019, Validation Loss: 3.88698\n",
      "Epoch 2191/3000, Training Loss: 29.02951, Validation Loss: 3.88689\n",
      "Epoch 2192/3000, Training Loss: 29.02882, Validation Loss: 3.88679\n",
      "Epoch 2193/3000, Training Loss: 29.02814, Validation Loss: 3.88670\n",
      "Epoch 2194/3000, Training Loss: 29.02746, Validation Loss: 3.88660\n",
      "Epoch 2195/3000, Training Loss: 29.02678, Validation Loss: 3.88651\n",
      "Epoch 2196/3000, Training Loss: 29.02610, Validation Loss: 3.88641\n",
      "Epoch 2197/3000, Training Loss: 29.02542, Validation Loss: 3.88632\n",
      "Epoch 2198/3000, Training Loss: 29.02474, Validation Loss: 3.88623\n",
      "Epoch 2199/3000, Training Loss: 29.02406, Validation Loss: 3.88613\n",
      "Epoch 2200/3000, Training Loss: 29.02339, Validation Loss: 3.88604\n",
      "Epoch 2201/3000, Training Loss: 29.02271, Validation Loss: 3.88595\n",
      "Epoch 2202/3000, Training Loss: 29.02203, Validation Loss: 3.88585\n",
      "Epoch 2203/3000, Training Loss: 29.02136, Validation Loss: 3.88576\n",
      "Epoch 2204/3000, Training Loss: 29.02069, Validation Loss: 3.88567\n",
      "Epoch 2205/3000, Training Loss: 29.02001, Validation Loss: 3.88558\n",
      "Epoch 2206/3000, Training Loss: 29.01934, Validation Loss: 3.88549\n",
      "Epoch 2207/3000, Training Loss: 29.01867, Validation Loss: 3.88539\n",
      "Epoch 2208/3000, Training Loss: 29.01799, Validation Loss: 3.88530\n",
      "Epoch 2209/3000, Training Loss: 29.01732, Validation Loss: 3.88521\n",
      "Epoch 2210/3000, Training Loss: 29.01665, Validation Loss: 3.88512\n",
      "Epoch 2211/3000, Training Loss: 29.01598, Validation Loss: 3.88503\n",
      "Epoch 2212/3000, Training Loss: 29.01531, Validation Loss: 3.88494\n",
      "Epoch 2213/3000, Training Loss: 29.01464, Validation Loss: 3.88485\n",
      "Epoch 2214/3000, Training Loss: 29.01397, Validation Loss: 3.88476\n",
      "Epoch 2215/3000, Training Loss: 29.01331, Validation Loss: 3.88467\n",
      "Epoch 2216/3000, Training Loss: 29.01264, Validation Loss: 3.88458\n",
      "Epoch 2217/3000, Training Loss: 29.01197, Validation Loss: 3.88449\n",
      "Epoch 2218/3000, Training Loss: 29.01131, Validation Loss: 3.88440\n",
      "Epoch 2219/3000, Training Loss: 29.01064, Validation Loss: 3.88431\n",
      "Epoch 2220/3000, Training Loss: 29.00998, Validation Loss: 3.88423\n",
      "Epoch 2221/3000, Training Loss: 29.00931, Validation Loss: 3.88414\n",
      "Epoch 2222/3000, Training Loss: 29.00865, Validation Loss: 3.88405\n",
      "Epoch 2223/3000, Training Loss: 29.00799, Validation Loss: 3.88396\n",
      "Epoch 2224/3000, Training Loss: 29.00732, Validation Loss: 3.88388\n",
      "Epoch 2225/3000, Training Loss: 29.00666, Validation Loss: 3.88379\n",
      "Epoch 2226/3000, Training Loss: 29.00600, Validation Loss: 3.88370\n",
      "Epoch 2227/3000, Training Loss: 29.00534, Validation Loss: 3.88362\n",
      "Epoch 2228/3000, Training Loss: 29.00468, Validation Loss: 3.88353\n",
      "Epoch 2229/3000, Training Loss: 29.00402, Validation Loss: 3.88345\n",
      "Epoch 2230/3000, Training Loss: 29.00336, Validation Loss: 3.88336\n",
      "Epoch 2231/3000, Training Loss: 29.00271, Validation Loss: 3.88328\n",
      "Epoch 2232/3000, Training Loss: 29.00205, Validation Loss: 3.88319\n",
      "Epoch 2233/3000, Training Loss: 29.00139, Validation Loss: 3.88311\n",
      "Epoch 2234/3000, Training Loss: 29.00073, Validation Loss: 3.88303\n",
      "Epoch 2235/3000, Training Loss: 29.00007, Validation Loss: 3.88294\n",
      "Epoch 2236/3000, Training Loss: 28.99941, Validation Loss: 3.88286\n",
      "Epoch 2237/3000, Training Loss: 28.99875, Validation Loss: 3.88278\n",
      "Epoch 2238/3000, Training Loss: 28.99810, Validation Loss: 3.88269\n",
      "Epoch 2239/3000, Training Loss: 28.99744, Validation Loss: 3.88261\n",
      "Epoch 2240/3000, Training Loss: 28.99679, Validation Loss: 3.88253\n",
      "Epoch 2241/3000, Training Loss: 28.99613, Validation Loss: 3.88245\n",
      "Epoch 2242/3000, Training Loss: 28.99548, Validation Loss: 3.88237\n",
      "Epoch 2243/3000, Training Loss: 28.99483, Validation Loss: 3.88229\n",
      "Epoch 2244/3000, Training Loss: 28.99417, Validation Loss: 3.88221\n",
      "Epoch 2245/3000, Training Loss: 28.99352, Validation Loss: 3.88213\n",
      "Epoch 2246/3000, Training Loss: 28.99287, Validation Loss: 3.88205\n",
      "Epoch 2247/3000, Training Loss: 28.99223, Validation Loss: 3.88197\n",
      "Epoch 2248/3000, Training Loss: 28.99158, Validation Loss: 3.88189\n",
      "Epoch 2249/3000, Training Loss: 28.99093, Validation Loss: 3.88181\n",
      "Epoch 2250/3000, Training Loss: 28.99028, Validation Loss: 3.88173\n",
      "Epoch 2251/3000, Training Loss: 28.98964, Validation Loss: 3.88165\n",
      "Epoch 2252/3000, Training Loss: 28.98899, Validation Loss: 3.88157\n",
      "Epoch 2253/3000, Training Loss: 28.98835, Validation Loss: 3.88149\n",
      "Epoch 2254/3000, Training Loss: 28.98770, Validation Loss: 3.88141\n",
      "Epoch 2255/3000, Training Loss: 28.98706, Validation Loss: 3.88134\n",
      "Epoch 2256/3000, Training Loss: 28.98642, Validation Loss: 3.88126\n",
      "Epoch 2257/3000, Training Loss: 28.98577, Validation Loss: 3.88118\n",
      "Epoch 2258/3000, Training Loss: 28.98513, Validation Loss: 3.88110\n",
      "Epoch 2259/3000, Training Loss: 28.98449, Validation Loss: 3.88103\n",
      "Epoch 2260/3000, Training Loss: 28.98385, Validation Loss: 3.88095\n",
      "Epoch 2261/3000, Training Loss: 28.98321, Validation Loss: 3.88087\n",
      "Epoch 2262/3000, Training Loss: 28.98257, Validation Loss: 3.88080\n",
      "Epoch 2263/3000, Training Loss: 28.98193, Validation Loss: 3.88072\n",
      "Epoch 2264/3000, Training Loss: 28.98129, Validation Loss: 3.88065\n",
      "Epoch 2265/3000, Training Loss: 28.98065, Validation Loss: 3.88057\n",
      "Epoch 2266/3000, Training Loss: 28.98001, Validation Loss: 3.88050\n",
      "Epoch 2267/3000, Training Loss: 28.97937, Validation Loss: 3.88042\n",
      "Epoch 2268/3000, Training Loss: 28.97874, Validation Loss: 3.88034\n",
      "Epoch 2269/3000, Training Loss: 28.97810, Validation Loss: 3.88026\n",
      "Epoch 2270/3000, Training Loss: 28.97747, Validation Loss: 3.88018\n",
      "Epoch 2271/3000, Training Loss: 28.97684, Validation Loss: 3.88011\n",
      "Epoch 2272/3000, Training Loss: 28.97620, Validation Loss: 3.88003\n",
      "Epoch 2273/3000, Training Loss: 28.97557, Validation Loss: 3.87995\n",
      "Epoch 2274/3000, Training Loss: 28.97494, Validation Loss: 3.87987\n",
      "Epoch 2275/3000, Training Loss: 28.97431, Validation Loss: 3.87980\n",
      "Epoch 2276/3000, Training Loss: 28.97368, Validation Loss: 3.87972\n",
      "Epoch 2277/3000, Training Loss: 28.97305, Validation Loss: 3.87965\n",
      "Epoch 2278/3000, Training Loss: 28.97242, Validation Loss: 3.87957\n",
      "Epoch 2279/3000, Training Loss: 28.97179, Validation Loss: 3.87949\n",
      "Epoch 2280/3000, Training Loss: 28.97116, Validation Loss: 3.87942\n",
      "Epoch 2281/3000, Training Loss: 28.97053, Validation Loss: 3.87934\n",
      "Epoch 2282/3000, Training Loss: 28.96991, Validation Loss: 3.87926\n",
      "Epoch 2283/3000, Training Loss: 28.96928, Validation Loss: 3.87918\n",
      "Epoch 2284/3000, Training Loss: 28.96865, Validation Loss: 3.87911\n",
      "Epoch 2285/3000, Training Loss: 28.96803, Validation Loss: 3.87903\n",
      "Epoch 2286/3000, Training Loss: 28.96741, Validation Loss: 3.87895\n",
      "Epoch 2287/3000, Training Loss: 28.96679, Validation Loss: 3.87888\n",
      "Epoch 2288/3000, Training Loss: 28.96616, Validation Loss: 3.87880\n",
      "Epoch 2289/3000, Training Loss: 28.96554, Validation Loss: 3.87873\n",
      "Epoch 2290/3000, Training Loss: 28.96492, Validation Loss: 3.87865\n",
      "Epoch 2291/3000, Training Loss: 28.96430, Validation Loss: 3.87858\n",
      "Epoch 2292/3000, Training Loss: 28.96368, Validation Loss: 3.87850\n",
      "Epoch 2293/3000, Training Loss: 28.96306, Validation Loss: 3.87843\n",
      "Epoch 2294/3000, Training Loss: 28.96244, Validation Loss: 3.87836\n",
      "Epoch 2295/3000, Training Loss: 28.96182, Validation Loss: 3.87828\n",
      "Epoch 2296/3000, Training Loss: 28.96120, Validation Loss: 3.87821\n",
      "Epoch 2297/3000, Training Loss: 28.96058, Validation Loss: 3.87814\n",
      "Epoch 2298/3000, Training Loss: 28.95996, Validation Loss: 3.87806\n",
      "Epoch 2299/3000, Training Loss: 28.95934, Validation Loss: 3.87799\n",
      "Epoch 2300/3000, Training Loss: 28.95872, Validation Loss: 3.87792\n",
      "Epoch 2301/3000, Training Loss: 28.95811, Validation Loss: 3.87785\n",
      "Epoch 2302/3000, Training Loss: 28.95749, Validation Loss: 3.87777\n",
      "Epoch 2303/3000, Training Loss: 28.95687, Validation Loss: 3.87770\n",
      "Epoch 2304/3000, Training Loss: 28.95626, Validation Loss: 3.87763\n",
      "Epoch 2305/3000, Training Loss: 28.95564, Validation Loss: 3.87756\n",
      "Epoch 2306/3000, Training Loss: 28.95502, Validation Loss: 3.87749\n",
      "Epoch 2307/3000, Training Loss: 28.95441, Validation Loss: 3.87743\n",
      "Epoch 2308/3000, Training Loss: 28.95379, Validation Loss: 3.87736\n",
      "Epoch 2309/3000, Training Loss: 28.95318, Validation Loss: 3.87729\n",
      "Epoch 2310/3000, Training Loss: 28.95257, Validation Loss: 3.87722\n",
      "Epoch 2311/3000, Training Loss: 28.95195, Validation Loss: 3.87715\n",
      "Epoch 2312/3000, Training Loss: 28.95134, Validation Loss: 3.87708\n",
      "Epoch 2313/3000, Training Loss: 28.95073, Validation Loss: 3.87701\n",
      "Epoch 2314/3000, Training Loss: 28.95012, Validation Loss: 3.87695\n",
      "Epoch 2315/3000, Training Loss: 28.94951, Validation Loss: 3.87688\n",
      "Epoch 2316/3000, Training Loss: 28.94890, Validation Loss: 3.87681\n",
      "Epoch 2317/3000, Training Loss: 28.94829, Validation Loss: 3.87674\n",
      "Epoch 2318/3000, Training Loss: 28.94768, Validation Loss: 3.87668\n",
      "Epoch 2319/3000, Training Loss: 28.94707, Validation Loss: 3.87661\n",
      "Epoch 2320/3000, Training Loss: 28.94646, Validation Loss: 3.87654\n",
      "Epoch 2321/3000, Training Loss: 28.94586, Validation Loss: 3.87648\n",
      "Epoch 2322/3000, Training Loss: 28.94525, Validation Loss: 3.87641\n",
      "Epoch 2323/3000, Training Loss: 28.94464, Validation Loss: 3.87634\n",
      "Epoch 2324/3000, Training Loss: 28.94404, Validation Loss: 3.87628\n",
      "Epoch 2325/3000, Training Loss: 28.94343, Validation Loss: 3.87621\n",
      "Epoch 2326/3000, Training Loss: 28.94283, Validation Loss: 3.87614\n",
      "Epoch 2327/3000, Training Loss: 28.94222, Validation Loss: 3.87608\n",
      "Epoch 2328/3000, Training Loss: 28.94162, Validation Loss: 3.87601\n",
      "Epoch 2329/3000, Training Loss: 28.94101, Validation Loss: 3.87595\n",
      "Epoch 2330/3000, Training Loss: 28.94041, Validation Loss: 3.87588\n",
      "Epoch 2331/3000, Training Loss: 28.93980, Validation Loss: 3.87582\n",
      "Epoch 2332/3000, Training Loss: 28.93920, Validation Loss: 3.87575\n",
      "Epoch 2333/3000, Training Loss: 28.93860, Validation Loss: 3.87569\n",
      "Epoch 2334/3000, Training Loss: 28.93800, Validation Loss: 3.87562\n",
      "Epoch 2335/3000, Training Loss: 28.93739, Validation Loss: 3.87556\n",
      "Epoch 2336/3000, Training Loss: 28.93679, Validation Loss: 3.87549\n",
      "Epoch 2337/3000, Training Loss: 28.93619, Validation Loss: 3.87543\n",
      "Epoch 2338/3000, Training Loss: 28.93559, Validation Loss: 3.87536\n",
      "Epoch 2339/3000, Training Loss: 28.93499, Validation Loss: 3.87530\n",
      "Epoch 2340/3000, Training Loss: 28.93439, Validation Loss: 3.87523\n",
      "Epoch 2341/3000, Training Loss: 28.93379, Validation Loss: 3.87517\n",
      "Epoch 2342/3000, Training Loss: 28.93320, Validation Loss: 3.87510\n",
      "Epoch 2343/3000, Training Loss: 28.93260, Validation Loss: 3.87504\n",
      "Epoch 2344/3000, Training Loss: 28.93200, Validation Loss: 3.87497\n",
      "Epoch 2345/3000, Training Loss: 28.93140, Validation Loss: 3.87491\n",
      "Epoch 2346/3000, Training Loss: 28.93081, Validation Loss: 3.87484\n",
      "Epoch 2347/3000, Training Loss: 28.93021, Validation Loss: 3.87477\n",
      "Epoch 2348/3000, Training Loss: 28.92962, Validation Loss: 3.87471\n",
      "Epoch 2349/3000, Training Loss: 28.92902, Validation Loss: 3.87464\n",
      "Epoch 2350/3000, Training Loss: 28.92843, Validation Loss: 3.87458\n",
      "Epoch 2351/3000, Training Loss: 28.92783, Validation Loss: 3.87451\n",
      "Epoch 2352/3000, Training Loss: 28.92724, Validation Loss: 3.87445\n",
      "Epoch 2353/3000, Training Loss: 28.92664, Validation Loss: 3.87439\n",
      "Epoch 2354/3000, Training Loss: 28.92605, Validation Loss: 3.87432\n",
      "Epoch 2355/3000, Training Loss: 28.92546, Validation Loss: 3.87426\n",
      "Epoch 2356/3000, Training Loss: 28.92486, Validation Loss: 3.87420\n",
      "Epoch 2357/3000, Training Loss: 28.92427, Validation Loss: 3.87413\n",
      "Epoch 2358/3000, Training Loss: 28.92368, Validation Loss: 3.87407\n",
      "Epoch 2359/3000, Training Loss: 28.92309, Validation Loss: 3.87401\n",
      "Epoch 2360/3000, Training Loss: 28.92249, Validation Loss: 3.87395\n",
      "Epoch 2361/3000, Training Loss: 28.92190, Validation Loss: 3.87388\n",
      "Epoch 2362/3000, Training Loss: 28.92131, Validation Loss: 3.87382\n",
      "Epoch 2363/3000, Training Loss: 28.92072, Validation Loss: 3.87376\n",
      "Epoch 2364/3000, Training Loss: 28.92013, Validation Loss: 3.87370\n",
      "Epoch 2365/3000, Training Loss: 28.91954, Validation Loss: 3.87364\n",
      "Epoch 2366/3000, Training Loss: 28.91895, Validation Loss: 3.87357\n",
      "Epoch 2367/3000, Training Loss: 28.91837, Validation Loss: 3.87351\n",
      "Epoch 2368/3000, Training Loss: 28.91778, Validation Loss: 3.87345\n",
      "Epoch 2369/3000, Training Loss: 28.91719, Validation Loss: 3.87339\n",
      "Epoch 2370/3000, Training Loss: 28.91661, Validation Loss: 3.87332\n",
      "Epoch 2371/3000, Training Loss: 28.91602, Validation Loss: 3.87326\n",
      "Epoch 2372/3000, Training Loss: 28.91544, Validation Loss: 3.87320\n",
      "Epoch 2373/3000, Training Loss: 28.91485, Validation Loss: 3.87314\n",
      "Epoch 2374/3000, Training Loss: 28.91426, Validation Loss: 3.87308\n",
      "Epoch 2375/3000, Training Loss: 28.91368, Validation Loss: 3.87302\n",
      "Epoch 2376/3000, Training Loss: 28.91309, Validation Loss: 3.87295\n",
      "Epoch 2377/3000, Training Loss: 28.91251, Validation Loss: 3.87289\n",
      "Epoch 2378/3000, Training Loss: 28.91193, Validation Loss: 3.87283\n",
      "Epoch 2379/3000, Training Loss: 28.91134, Validation Loss: 3.87277\n",
      "Epoch 2380/3000, Training Loss: 28.91076, Validation Loss: 3.87271\n",
      "Epoch 2381/3000, Training Loss: 28.91018, Validation Loss: 3.87265\n",
      "Epoch 2382/3000, Training Loss: 28.90959, Validation Loss: 3.87259\n",
      "Epoch 2383/3000, Training Loss: 28.90901, Validation Loss: 3.87253\n",
      "Epoch 2384/3000, Training Loss: 28.90843, Validation Loss: 3.87247\n",
      "Epoch 2385/3000, Training Loss: 28.90785, Validation Loss: 3.87241\n",
      "Epoch 2386/3000, Training Loss: 28.90727, Validation Loss: 3.87235\n",
      "Epoch 2387/3000, Training Loss: 28.90669, Validation Loss: 3.87229\n",
      "Epoch 2388/3000, Training Loss: 28.90611, Validation Loss: 3.87223\n",
      "Epoch 2389/3000, Training Loss: 28.90553, Validation Loss: 3.87217\n",
      "Epoch 2390/3000, Training Loss: 28.90495, Validation Loss: 3.87212\n",
      "Epoch 2391/3000, Training Loss: 28.90437, Validation Loss: 3.87206\n",
      "Epoch 2392/3000, Training Loss: 28.90379, Validation Loss: 3.87200\n",
      "Epoch 2393/3000, Training Loss: 28.90322, Validation Loss: 3.87194\n",
      "Epoch 2394/3000, Training Loss: 28.90264, Validation Loss: 3.87188\n",
      "Epoch 2395/3000, Training Loss: 28.90206, Validation Loss: 3.87182\n",
      "Epoch 2396/3000, Training Loss: 28.90149, Validation Loss: 3.87177\n",
      "Epoch 2397/3000, Training Loss: 28.90091, Validation Loss: 3.87171\n",
      "Epoch 2398/3000, Training Loss: 28.90033, Validation Loss: 3.87165\n",
      "Epoch 2399/3000, Training Loss: 28.89976, Validation Loss: 3.87159\n",
      "Epoch 2400/3000, Training Loss: 28.89919, Validation Loss: 3.87154\n",
      "Epoch 2401/3000, Training Loss: 28.89861, Validation Loss: 3.87148\n",
      "Epoch 2402/3000, Training Loss: 28.89804, Validation Loss: 3.87143\n",
      "Epoch 2403/3000, Training Loss: 28.89746, Validation Loss: 3.87137\n",
      "Epoch 2404/3000, Training Loss: 28.89689, Validation Loss: 3.87131\n",
      "Epoch 2405/3000, Training Loss: 28.89632, Validation Loss: 3.87126\n",
      "Epoch 2406/3000, Training Loss: 28.89575, Validation Loss: 3.87120\n",
      "Epoch 2407/3000, Training Loss: 28.89517, Validation Loss: 3.87114\n",
      "Epoch 2408/3000, Training Loss: 28.89460, Validation Loss: 3.87109\n",
      "Epoch 2409/3000, Training Loss: 28.89403, Validation Loss: 3.87103\n",
      "Epoch 2410/3000, Training Loss: 28.89346, Validation Loss: 3.87098\n",
      "Epoch 2411/3000, Training Loss: 28.89289, Validation Loss: 3.87092\n",
      "Epoch 2412/3000, Training Loss: 28.89233, Validation Loss: 3.87086\n",
      "Epoch 2413/3000, Training Loss: 28.89176, Validation Loss: 3.87080\n",
      "Epoch 2414/3000, Training Loss: 28.89119, Validation Loss: 3.87075\n",
      "Epoch 2415/3000, Training Loss: 28.89062, Validation Loss: 3.87069\n",
      "Epoch 2416/3000, Training Loss: 28.89005, Validation Loss: 3.87063\n",
      "Epoch 2417/3000, Training Loss: 28.88948, Validation Loss: 3.87058\n",
      "Epoch 2418/3000, Training Loss: 28.88891, Validation Loss: 3.87052\n",
      "Epoch 2419/3000, Training Loss: 28.88835, Validation Loss: 3.87046\n",
      "Epoch 2420/3000, Training Loss: 28.88778, Validation Loss: 3.87041\n",
      "Epoch 2421/3000, Training Loss: 28.88721, Validation Loss: 3.87035\n",
      "Epoch 2422/3000, Training Loss: 28.88665, Validation Loss: 3.87030\n",
      "Epoch 2423/3000, Training Loss: 28.88608, Validation Loss: 3.87024\n",
      "Epoch 2424/3000, Training Loss: 28.88552, Validation Loss: 3.87018\n",
      "Epoch 2425/3000, Training Loss: 28.88495, Validation Loss: 3.87013\n",
      "Epoch 2426/3000, Training Loss: 28.88439, Validation Loss: 3.87008\n",
      "Epoch 2427/3000, Training Loss: 28.88382, Validation Loss: 3.87002\n",
      "Epoch 2428/3000, Training Loss: 28.88325, Validation Loss: 3.86996\n",
      "Epoch 2429/3000, Training Loss: 28.88269, Validation Loss: 3.86991\n",
      "Epoch 2430/3000, Training Loss: 28.88212, Validation Loss: 3.86986\n",
      "Epoch 2431/3000, Training Loss: 28.88156, Validation Loss: 3.86980\n",
      "Epoch 2432/3000, Training Loss: 28.88099, Validation Loss: 3.86975\n",
      "Epoch 2433/3000, Training Loss: 28.88043, Validation Loss: 3.86970\n",
      "Epoch 2434/3000, Training Loss: 28.87986, Validation Loss: 3.86964\n",
      "Epoch 2435/3000, Training Loss: 28.87930, Validation Loss: 3.86959\n",
      "Epoch 2436/3000, Training Loss: 28.87873, Validation Loss: 3.86954\n",
      "Epoch 2437/3000, Training Loss: 28.87817, Validation Loss: 3.86948\n",
      "Epoch 2438/3000, Training Loss: 28.87761, Validation Loss: 3.86943\n",
      "Epoch 2439/3000, Training Loss: 28.87705, Validation Loss: 3.86938\n",
      "Epoch 2440/3000, Training Loss: 28.87648, Validation Loss: 3.86933\n",
      "Epoch 2441/3000, Training Loss: 28.87592, Validation Loss: 3.86928\n",
      "Epoch 2442/3000, Training Loss: 28.87536, Validation Loss: 3.86923\n",
      "Epoch 2443/3000, Training Loss: 28.87480, Validation Loss: 3.86917\n",
      "Epoch 2444/3000, Training Loss: 28.87424, Validation Loss: 3.86912\n",
      "Epoch 2445/3000, Training Loss: 28.87368, Validation Loss: 3.86907\n",
      "Epoch 2446/3000, Training Loss: 28.87312, Validation Loss: 3.86902\n",
      "Epoch 2447/3000, Training Loss: 28.87255, Validation Loss: 3.86897\n",
      "Epoch 2448/3000, Training Loss: 28.87199, Validation Loss: 3.86892\n",
      "Epoch 2449/3000, Training Loss: 28.87143, Validation Loss: 3.86887\n",
      "Epoch 2450/3000, Training Loss: 28.87087, Validation Loss: 3.86882\n",
      "Epoch 2451/3000, Training Loss: 28.87031, Validation Loss: 3.86877\n",
      "Epoch 2452/3000, Training Loss: 28.86975, Validation Loss: 3.86872\n",
      "Epoch 2453/3000, Training Loss: 28.86919, Validation Loss: 3.86867\n",
      "Epoch 2454/3000, Training Loss: 28.86863, Validation Loss: 3.86862\n",
      "Epoch 2455/3000, Training Loss: 28.86807, Validation Loss: 3.86857\n",
      "Epoch 2456/3000, Training Loss: 28.86751, Validation Loss: 3.86852\n",
      "Epoch 2457/3000, Training Loss: 28.86695, Validation Loss: 3.86847\n",
      "Epoch 2458/3000, Training Loss: 28.86640, Validation Loss: 3.86842\n",
      "Epoch 2459/3000, Training Loss: 28.86584, Validation Loss: 3.86837\n",
      "Epoch 2460/3000, Training Loss: 28.86528, Validation Loss: 3.86832\n",
      "Epoch 2461/3000, Training Loss: 28.86472, Validation Loss: 3.86827\n",
      "Epoch 2462/3000, Training Loss: 28.86417, Validation Loss: 3.86822\n",
      "Epoch 2463/3000, Training Loss: 28.86361, Validation Loss: 3.86817\n",
      "Epoch 2464/3000, Training Loss: 28.86305, Validation Loss: 3.86812\n",
      "Epoch 2465/3000, Training Loss: 28.86250, Validation Loss: 3.86808\n",
      "Epoch 2466/3000, Training Loss: 28.86194, Validation Loss: 3.86803\n",
      "Epoch 2467/3000, Training Loss: 28.86139, Validation Loss: 3.86798\n",
      "Epoch 2468/3000, Training Loss: 28.86083, Validation Loss: 3.86793\n",
      "Epoch 2469/3000, Training Loss: 28.86028, Validation Loss: 3.86788\n",
      "Epoch 2470/3000, Training Loss: 28.85973, Validation Loss: 3.86783\n",
      "Epoch 2471/3000, Training Loss: 28.85917, Validation Loss: 3.86778\n",
      "Epoch 2472/3000, Training Loss: 28.85862, Validation Loss: 3.86773\n",
      "Epoch 2473/3000, Training Loss: 28.85807, Validation Loss: 3.86769\n",
      "Epoch 2474/3000, Training Loss: 28.85751, Validation Loss: 3.86764\n",
      "Epoch 2475/3000, Training Loss: 28.85696, Validation Loss: 3.86759\n",
      "Epoch 2476/3000, Training Loss: 28.85641, Validation Loss: 3.86754\n",
      "Epoch 2477/3000, Training Loss: 28.85586, Validation Loss: 3.86749\n",
      "Epoch 2478/3000, Training Loss: 28.85531, Validation Loss: 3.86745\n",
      "Epoch 2479/3000, Training Loss: 28.85476, Validation Loss: 3.86740\n",
      "Epoch 2480/3000, Training Loss: 28.85421, Validation Loss: 3.86735\n",
      "Epoch 2481/3000, Training Loss: 28.85366, Validation Loss: 3.86731\n",
      "Epoch 2482/3000, Training Loss: 28.85311, Validation Loss: 3.86726\n",
      "Epoch 2483/3000, Training Loss: 28.85256, Validation Loss: 3.86721\n",
      "Epoch 2484/3000, Training Loss: 28.85201, Validation Loss: 3.86717\n",
      "Epoch 2485/3000, Training Loss: 28.85146, Validation Loss: 3.86712\n",
      "Epoch 2486/3000, Training Loss: 28.85091, Validation Loss: 3.86707\n",
      "Epoch 2487/3000, Training Loss: 28.85036, Validation Loss: 3.86703\n",
      "Epoch 2488/3000, Training Loss: 28.84981, Validation Loss: 3.86698\n",
      "Epoch 2489/3000, Training Loss: 28.84927, Validation Loss: 3.86694\n",
      "Epoch 2490/3000, Training Loss: 28.84872, Validation Loss: 3.86689\n",
      "Epoch 2491/3000, Training Loss: 28.84817, Validation Loss: 3.86685\n",
      "Epoch 2492/3000, Training Loss: 28.84762, Validation Loss: 3.86680\n",
      "Epoch 2493/3000, Training Loss: 28.84708, Validation Loss: 3.86675\n",
      "Epoch 2494/3000, Training Loss: 28.84653, Validation Loss: 3.86671\n",
      "Epoch 2495/3000, Training Loss: 28.84599, Validation Loss: 3.86666\n",
      "Epoch 2496/3000, Training Loss: 28.84544, Validation Loss: 3.86662\n",
      "Epoch 2497/3000, Training Loss: 28.84489, Validation Loss: 3.86657\n",
      "Epoch 2498/3000, Training Loss: 28.84435, Validation Loss: 3.86653\n",
      "Epoch 2499/3000, Training Loss: 28.84380, Validation Loss: 3.86648\n",
      "Epoch 2500/3000, Training Loss: 28.84326, Validation Loss: 3.86644\n",
      "Epoch 2501/3000, Training Loss: 28.84271, Validation Loss: 3.86639\n",
      "Epoch 2502/3000, Training Loss: 28.84217, Validation Loss: 3.86635\n",
      "Epoch 2503/3000, Training Loss: 28.84162, Validation Loss: 3.86631\n",
      "Epoch 2504/3000, Training Loss: 28.84108, Validation Loss: 3.86626\n",
      "Epoch 2505/3000, Training Loss: 28.84053, Validation Loss: 3.86622\n",
      "Epoch 2506/3000, Training Loss: 28.83999, Validation Loss: 3.86618\n",
      "Epoch 2507/3000, Training Loss: 28.83945, Validation Loss: 3.86613\n",
      "Epoch 2508/3000, Training Loss: 28.83891, Validation Loss: 3.86609\n",
      "Epoch 2509/3000, Training Loss: 28.83837, Validation Loss: 3.86605\n",
      "Epoch 2510/3000, Training Loss: 28.83783, Validation Loss: 3.86601\n",
      "Epoch 2511/3000, Training Loss: 28.83729, Validation Loss: 3.86596\n",
      "Epoch 2512/3000, Training Loss: 28.83675, Validation Loss: 3.86592\n",
      "Epoch 2513/3000, Training Loss: 28.83621, Validation Loss: 3.86588\n",
      "Epoch 2514/3000, Training Loss: 28.83568, Validation Loss: 3.86583\n",
      "Epoch 2515/3000, Training Loss: 28.83514, Validation Loss: 3.86579\n",
      "Epoch 2516/3000, Training Loss: 28.83460, Validation Loss: 3.86575\n",
      "Epoch 2517/3000, Training Loss: 28.83407, Validation Loss: 3.86571\n",
      "Epoch 2518/3000, Training Loss: 28.83354, Validation Loss: 3.86566\n",
      "Epoch 2519/3000, Training Loss: 28.83300, Validation Loss: 3.86562\n",
      "Epoch 2520/3000, Training Loss: 28.83247, Validation Loss: 3.86558\n",
      "Epoch 2521/3000, Training Loss: 28.83193, Validation Loss: 3.86554\n",
      "Epoch 2522/3000, Training Loss: 28.83140, Validation Loss: 3.86550\n",
      "Epoch 2523/3000, Training Loss: 28.83087, Validation Loss: 3.86545\n",
      "Epoch 2524/3000, Training Loss: 28.83034, Validation Loss: 3.86541\n",
      "Epoch 2525/3000, Training Loss: 28.82980, Validation Loss: 3.86537\n",
      "Epoch 2526/3000, Training Loss: 28.82927, Validation Loss: 3.86533\n",
      "Epoch 2527/3000, Training Loss: 28.82874, Validation Loss: 3.86529\n",
      "Epoch 2528/3000, Training Loss: 28.82821, Validation Loss: 3.86525\n",
      "Epoch 2529/3000, Training Loss: 28.82768, Validation Loss: 3.86520\n",
      "Epoch 2530/3000, Training Loss: 28.82715, Validation Loss: 3.86516\n",
      "Epoch 2531/3000, Training Loss: 28.82662, Validation Loss: 3.86512\n",
      "Epoch 2532/3000, Training Loss: 28.82609, Validation Loss: 3.86508\n",
      "Epoch 2533/3000, Training Loss: 28.82556, Validation Loss: 3.86504\n",
      "Epoch 2534/3000, Training Loss: 28.82503, Validation Loss: 3.86500\n",
      "Epoch 2535/3000, Training Loss: 28.82450, Validation Loss: 3.86495\n",
      "Epoch 2536/3000, Training Loss: 28.82397, Validation Loss: 3.86491\n",
      "Epoch 2537/3000, Training Loss: 28.82344, Validation Loss: 3.86487\n",
      "Epoch 2538/3000, Training Loss: 28.82291, Validation Loss: 3.86483\n",
      "Epoch 2539/3000, Training Loss: 28.82238, Validation Loss: 3.86479\n",
      "Epoch 2540/3000, Training Loss: 28.82185, Validation Loss: 3.86475\n",
      "Epoch 2541/3000, Training Loss: 28.82132, Validation Loss: 3.86471\n",
      "Epoch 2542/3000, Training Loss: 28.82079, Validation Loss: 3.86466\n",
      "Epoch 2543/3000, Training Loss: 28.82026, Validation Loss: 3.86463\n",
      "Epoch 2544/3000, Training Loss: 28.81974, Validation Loss: 3.86458\n",
      "Epoch 2545/3000, Training Loss: 28.81921, Validation Loss: 3.86454\n",
      "Epoch 2546/3000, Training Loss: 28.81868, Validation Loss: 3.86450\n",
      "Epoch 2547/3000, Training Loss: 28.81816, Validation Loss: 3.86446\n",
      "Epoch 2548/3000, Training Loss: 28.81763, Validation Loss: 3.86442\n",
      "Epoch 2549/3000, Training Loss: 28.81710, Validation Loss: 3.86438\n",
      "Epoch 2550/3000, Training Loss: 28.81658, Validation Loss: 3.86434\n",
      "Epoch 2551/3000, Training Loss: 28.81605, Validation Loss: 3.86430\n",
      "Epoch 2552/3000, Training Loss: 28.81553, Validation Loss: 3.86426\n",
      "Epoch 2553/3000, Training Loss: 28.81501, Validation Loss: 3.86422\n",
      "Epoch 2554/3000, Training Loss: 28.81448, Validation Loss: 3.86418\n",
      "Epoch 2555/3000, Training Loss: 28.81396, Validation Loss: 3.86414\n",
      "Epoch 2556/3000, Training Loss: 28.81344, Validation Loss: 3.86410\n",
      "Epoch 2557/3000, Training Loss: 28.81291, Validation Loss: 3.86406\n",
      "Epoch 2558/3000, Training Loss: 28.81239, Validation Loss: 3.86402\n",
      "Epoch 2559/3000, Training Loss: 28.81187, Validation Loss: 3.86398\n",
      "Epoch 2560/3000, Training Loss: 28.81134, Validation Loss: 3.86393\n",
      "Epoch 2561/3000, Training Loss: 28.81082, Validation Loss: 3.86389\n",
      "Epoch 2562/3000, Training Loss: 28.81030, Validation Loss: 3.86385\n",
      "Epoch 2563/3000, Training Loss: 28.80978, Validation Loss: 3.86381\n",
      "Epoch 2564/3000, Training Loss: 28.80926, Validation Loss: 3.86377\n",
      "Epoch 2565/3000, Training Loss: 28.80874, Validation Loss: 3.86373\n",
      "Epoch 2566/3000, Training Loss: 28.80822, Validation Loss: 3.86369\n",
      "Epoch 2567/3000, Training Loss: 28.80770, Validation Loss: 3.86365\n",
      "Epoch 2568/3000, Training Loss: 28.80718, Validation Loss: 3.86361\n",
      "Epoch 2569/3000, Training Loss: 28.80666, Validation Loss: 3.86357\n",
      "Epoch 2570/3000, Training Loss: 28.80614, Validation Loss: 3.86353\n",
      "Epoch 2571/3000, Training Loss: 28.80562, Validation Loss: 3.86349\n",
      "Epoch 2572/3000, Training Loss: 28.80510, Validation Loss: 3.86345\n",
      "Epoch 2573/3000, Training Loss: 28.80458, Validation Loss: 3.86341\n",
      "Epoch 2574/3000, Training Loss: 28.80407, Validation Loss: 3.86337\n",
      "Epoch 2575/3000, Training Loss: 28.80355, Validation Loss: 3.86334\n",
      "Epoch 2576/3000, Training Loss: 28.80303, Validation Loss: 3.86330\n",
      "Epoch 2577/3000, Training Loss: 28.80251, Validation Loss: 3.86326\n",
      "Epoch 2578/3000, Training Loss: 28.80199, Validation Loss: 3.86322\n",
      "Epoch 2579/3000, Training Loss: 28.80148, Validation Loss: 3.86318\n",
      "Epoch 2580/3000, Training Loss: 28.80096, Validation Loss: 3.86315\n",
      "Epoch 2581/3000, Training Loss: 28.80044, Validation Loss: 3.86311\n",
      "Epoch 2582/3000, Training Loss: 28.79993, Validation Loss: 3.86307\n",
      "Epoch 2583/3000, Training Loss: 28.79941, Validation Loss: 3.86303\n",
      "Epoch 2584/3000, Training Loss: 28.79890, Validation Loss: 3.86299\n",
      "Epoch 2585/3000, Training Loss: 28.79838, Validation Loss: 3.86296\n",
      "Epoch 2586/3000, Training Loss: 28.79787, Validation Loss: 3.86292\n",
      "Epoch 2587/3000, Training Loss: 28.79736, Validation Loss: 3.86288\n",
      "Epoch 2588/3000, Training Loss: 28.79685, Validation Loss: 3.86284\n",
      "Epoch 2589/3000, Training Loss: 28.79634, Validation Loss: 3.86280\n",
      "Epoch 2590/3000, Training Loss: 28.79583, Validation Loss: 3.86277\n",
      "Epoch 2591/3000, Training Loss: 28.79532, Validation Loss: 3.86273\n",
      "Epoch 2592/3000, Training Loss: 28.79481, Validation Loss: 3.86269\n",
      "Epoch 2593/3000, Training Loss: 28.79430, Validation Loss: 3.86265\n",
      "Epoch 2594/3000, Training Loss: 28.79379, Validation Loss: 3.86262\n",
      "Epoch 2595/3000, Training Loss: 28.79328, Validation Loss: 3.86258\n",
      "Epoch 2596/3000, Training Loss: 28.79277, Validation Loss: 3.86254\n",
      "Epoch 2597/3000, Training Loss: 28.79226, Validation Loss: 3.86251\n",
      "Epoch 2598/3000, Training Loss: 28.79175, Validation Loss: 3.86247\n",
      "Epoch 2599/3000, Training Loss: 28.79124, Validation Loss: 3.86244\n",
      "Epoch 2600/3000, Training Loss: 28.79073, Validation Loss: 3.86240\n",
      "Epoch 2601/3000, Training Loss: 28.79022, Validation Loss: 3.86237\n",
      "Epoch 2602/3000, Training Loss: 28.78971, Validation Loss: 3.86233\n",
      "Epoch 2603/3000, Training Loss: 28.78920, Validation Loss: 3.86229\n",
      "Epoch 2604/3000, Training Loss: 28.78869, Validation Loss: 3.86226\n",
      "Epoch 2605/3000, Training Loss: 28.78818, Validation Loss: 3.86222\n",
      "Epoch 2606/3000, Training Loss: 28.78768, Validation Loss: 3.86219\n",
      "Epoch 2607/3000, Training Loss: 28.78717, Validation Loss: 3.86215\n",
      "Epoch 2608/3000, Training Loss: 28.78666, Validation Loss: 3.86212\n",
      "Epoch 2609/3000, Training Loss: 28.78616, Validation Loss: 3.86208\n",
      "Epoch 2610/3000, Training Loss: 28.78565, Validation Loss: 3.86205\n",
      "Epoch 2611/3000, Training Loss: 28.78514, Validation Loss: 3.86201\n",
      "Epoch 2612/3000, Training Loss: 28.78464, Validation Loss: 3.86198\n",
      "Epoch 2613/3000, Training Loss: 28.78413, Validation Loss: 3.86194\n",
      "Epoch 2614/3000, Training Loss: 28.78363, Validation Loss: 3.86191\n",
      "Epoch 2615/3000, Training Loss: 28.78313, Validation Loss: 3.86187\n",
      "Epoch 2616/3000, Training Loss: 28.78262, Validation Loss: 3.86184\n",
      "Epoch 2617/3000, Training Loss: 28.78212, Validation Loss: 3.86180\n",
      "Epoch 2618/3000, Training Loss: 28.78161, Validation Loss: 3.86177\n",
      "Epoch 2619/3000, Training Loss: 28.78111, Validation Loss: 3.86174\n",
      "Epoch 2620/3000, Training Loss: 28.78061, Validation Loss: 3.86170\n",
      "Epoch 2621/3000, Training Loss: 28.78010, Validation Loss: 3.86167\n",
      "Epoch 2622/3000, Training Loss: 28.77960, Validation Loss: 3.86163\n",
      "Epoch 2623/3000, Training Loss: 28.77910, Validation Loss: 3.86160\n",
      "Epoch 2624/3000, Training Loss: 28.77860, Validation Loss: 3.86156\n",
      "Epoch 2625/3000, Training Loss: 28.77809, Validation Loss: 3.86153\n",
      "Epoch 2626/3000, Training Loss: 28.77759, Validation Loss: 3.86150\n",
      "Epoch 2627/3000, Training Loss: 28.77709, Validation Loss: 3.86146\n",
      "Epoch 2628/3000, Training Loss: 28.77659, Validation Loss: 3.86143\n",
      "Epoch 2629/3000, Training Loss: 28.77609, Validation Loss: 3.86140\n",
      "Epoch 2630/3000, Training Loss: 28.77559, Validation Loss: 3.86136\n",
      "Epoch 2631/3000, Training Loss: 28.77509, Validation Loss: 3.86133\n",
      "Epoch 2632/3000, Training Loss: 28.77459, Validation Loss: 3.86129\n",
      "Epoch 2633/3000, Training Loss: 28.77409, Validation Loss: 3.86126\n",
      "Epoch 2634/3000, Training Loss: 28.77360, Validation Loss: 3.86123\n",
      "Epoch 2635/3000, Training Loss: 28.77310, Validation Loss: 3.86119\n",
      "Epoch 2636/3000, Training Loss: 28.77260, Validation Loss: 3.86116\n",
      "Epoch 2637/3000, Training Loss: 28.77210, Validation Loss: 3.86112\n",
      "Epoch 2638/3000, Training Loss: 28.77160, Validation Loss: 3.86109\n",
      "Epoch 2639/3000, Training Loss: 28.77111, Validation Loss: 3.86106\n",
      "Epoch 2640/3000, Training Loss: 28.77061, Validation Loss: 3.86102\n",
      "Epoch 2641/3000, Training Loss: 28.77011, Validation Loss: 3.86099\n",
      "Epoch 2642/3000, Training Loss: 28.76961, Validation Loss: 3.86096\n",
      "Epoch 2643/3000, Training Loss: 28.76912, Validation Loss: 3.86092\n",
      "Epoch 2644/3000, Training Loss: 28.76862, Validation Loss: 3.86089\n",
      "Epoch 2645/3000, Training Loss: 28.76812, Validation Loss: 3.86086\n",
      "Epoch 2646/3000, Training Loss: 28.76763, Validation Loss: 3.86083\n",
      "Epoch 2647/3000, Training Loss: 28.76713, Validation Loss: 3.86079\n",
      "Epoch 2648/3000, Training Loss: 28.76663, Validation Loss: 3.86076\n",
      "Epoch 2649/3000, Training Loss: 28.76614, Validation Loss: 3.86073\n",
      "Epoch 2650/3000, Training Loss: 28.76564, Validation Loss: 3.86070\n",
      "Epoch 2651/3000, Training Loss: 28.76515, Validation Loss: 3.86067\n",
      "Epoch 2652/3000, Training Loss: 28.76465, Validation Loss: 3.86063\n",
      "Epoch 2653/3000, Training Loss: 28.76416, Validation Loss: 3.86060\n",
      "Epoch 2654/3000, Training Loss: 28.76366, Validation Loss: 3.86057\n",
      "Epoch 2655/3000, Training Loss: 28.76317, Validation Loss: 3.86054\n",
      "Epoch 2656/3000, Training Loss: 28.76267, Validation Loss: 3.86051\n",
      "Epoch 2657/3000, Training Loss: 28.76218, Validation Loss: 3.86048\n",
      "Epoch 2658/3000, Training Loss: 28.76169, Validation Loss: 3.86045\n",
      "Epoch 2659/3000, Training Loss: 28.76119, Validation Loss: 3.86041\n",
      "Epoch 2660/3000, Training Loss: 28.76070, Validation Loss: 3.86038\n",
      "Epoch 2661/3000, Training Loss: 28.76021, Validation Loss: 3.86036\n",
      "Epoch 2662/3000, Training Loss: 28.75972, Validation Loss: 3.86033\n",
      "Epoch 2663/3000, Training Loss: 28.75922, Validation Loss: 3.86030\n",
      "Epoch 2664/3000, Training Loss: 28.75873, Validation Loss: 3.86027\n",
      "Epoch 2665/3000, Training Loss: 28.75824, Validation Loss: 3.86024\n",
      "Epoch 2666/3000, Training Loss: 28.75775, Validation Loss: 3.86021\n",
      "Epoch 2667/3000, Training Loss: 28.75725, Validation Loss: 3.86018\n",
      "Epoch 2668/3000, Training Loss: 28.75676, Validation Loss: 3.86015\n",
      "Epoch 2669/3000, Training Loss: 28.75627, Validation Loss: 3.86013\n",
      "Epoch 2670/3000, Training Loss: 28.75578, Validation Loss: 3.86010\n",
      "Epoch 2671/3000, Training Loss: 28.75529, Validation Loss: 3.86007\n",
      "Epoch 2672/3000, Training Loss: 28.75479, Validation Loss: 3.86004\n",
      "Epoch 2673/3000, Training Loss: 28.75430, Validation Loss: 3.86001\n",
      "Epoch 2674/3000, Training Loss: 28.75381, Validation Loss: 3.85999\n",
      "Epoch 2675/3000, Training Loss: 28.75332, Validation Loss: 3.85996\n",
      "Epoch 2676/3000, Training Loss: 28.75283, Validation Loss: 3.85993\n",
      "Epoch 2677/3000, Training Loss: 28.75234, Validation Loss: 3.85990\n",
      "Epoch 2678/3000, Training Loss: 28.75185, Validation Loss: 3.85988\n",
      "Epoch 2679/3000, Training Loss: 28.75136, Validation Loss: 3.85985\n",
      "Epoch 2680/3000, Training Loss: 28.75087, Validation Loss: 3.85982\n",
      "Epoch 2681/3000, Training Loss: 28.75038, Validation Loss: 3.85980\n",
      "Epoch 2682/3000, Training Loss: 28.74990, Validation Loss: 3.85977\n",
      "Epoch 2683/3000, Training Loss: 28.74941, Validation Loss: 3.85974\n",
      "Epoch 2684/3000, Training Loss: 28.74891, Validation Loss: 3.85971\n",
      "Epoch 2685/3000, Training Loss: 28.74842, Validation Loss: 3.85968\n",
      "Epoch 2686/3000, Training Loss: 28.74792, Validation Loss: 3.85965\n",
      "Epoch 2687/3000, Training Loss: 28.74743, Validation Loss: 3.85962\n",
      "Epoch 2688/3000, Training Loss: 28.74694, Validation Loss: 3.85960\n",
      "Epoch 2689/3000, Training Loss: 28.74645, Validation Loss: 3.85957\n",
      "Epoch 2690/3000, Training Loss: 28.74595, Validation Loss: 3.85954\n",
      "Epoch 2691/3000, Training Loss: 28.74546, Validation Loss: 3.85951\n",
      "Epoch 2692/3000, Training Loss: 28.74497, Validation Loss: 3.85948\n",
      "Epoch 2693/3000, Training Loss: 28.74447, Validation Loss: 3.85945\n",
      "Epoch 2694/3000, Training Loss: 28.74397, Validation Loss: 3.85942\n",
      "Epoch 2695/3000, Training Loss: 28.74347, Validation Loss: 3.85939\n",
      "Epoch 2696/3000, Training Loss: 28.74297, Validation Loss: 3.85936\n",
      "Epoch 2697/3000, Training Loss: 28.74247, Validation Loss: 3.85933\n",
      "Epoch 2698/3000, Training Loss: 28.74197, Validation Loss: 3.85930\n",
      "Epoch 2699/3000, Training Loss: 28.74147, Validation Loss: 3.85927\n",
      "Epoch 2700/3000, Training Loss: 28.74098, Validation Loss: 3.85924\n",
      "Epoch 2701/3000, Training Loss: 28.74048, Validation Loss: 3.85921\n",
      "Epoch 2702/3000, Training Loss: 28.73998, Validation Loss: 3.85918\n",
      "Epoch 2703/3000, Training Loss: 28.73948, Validation Loss: 3.85915\n",
      "Epoch 2704/3000, Training Loss: 28.73898, Validation Loss: 3.85913\n",
      "Epoch 2705/3000, Training Loss: 28.73848, Validation Loss: 3.85910\n",
      "Epoch 2706/3000, Training Loss: 28.73799, Validation Loss: 3.85907\n",
      "Epoch 2707/3000, Training Loss: 28.73749, Validation Loss: 3.85904\n",
      "Epoch 2708/3000, Training Loss: 28.73700, Validation Loss: 3.85901\n",
      "Epoch 2709/3000, Training Loss: 28.73650, Validation Loss: 3.85898\n",
      "Epoch 2710/3000, Training Loss: 28.73600, Validation Loss: 3.85896\n",
      "Epoch 2711/3000, Training Loss: 28.73551, Validation Loss: 3.85893\n",
      "Epoch 2712/3000, Training Loss: 28.73501, Validation Loss: 3.85891\n",
      "Epoch 2713/3000, Training Loss: 28.73451, Validation Loss: 3.85888\n",
      "Epoch 2714/3000, Training Loss: 28.73402, Validation Loss: 3.85885\n",
      "Epoch 2715/3000, Training Loss: 28.73352, Validation Loss: 3.85883\n",
      "Epoch 2716/3000, Training Loss: 28.73303, Validation Loss: 3.85880\n",
      "Epoch 2717/3000, Training Loss: 28.73253, Validation Loss: 3.85878\n",
      "Epoch 2718/3000, Training Loss: 28.73204, Validation Loss: 3.85875\n",
      "Epoch 2719/3000, Training Loss: 28.73154, Validation Loss: 3.85872\n",
      "Epoch 2720/3000, Training Loss: 28.73105, Validation Loss: 3.85870\n",
      "Epoch 2721/3000, Training Loss: 28.73056, Validation Loss: 3.85867\n",
      "Epoch 2722/3000, Training Loss: 28.73006, Validation Loss: 3.85864\n",
      "Epoch 2723/3000, Training Loss: 28.72957, Validation Loss: 3.85862\n",
      "Epoch 2724/3000, Training Loss: 28.72908, Validation Loss: 3.85859\n",
      "Epoch 2725/3000, Training Loss: 28.72859, Validation Loss: 3.85857\n",
      "Epoch 2726/3000, Training Loss: 28.72809, Validation Loss: 3.85854\n",
      "Epoch 2727/3000, Training Loss: 28.72760, Validation Loss: 3.85852\n",
      "Epoch 2728/3000, Training Loss: 28.72711, Validation Loss: 3.85849\n",
      "Epoch 2729/3000, Training Loss: 28.72662, Validation Loss: 3.85846\n",
      "Epoch 2730/3000, Training Loss: 28.72612, Validation Loss: 3.85844\n",
      "Epoch 2731/3000, Training Loss: 28.72563, Validation Loss: 3.85841\n",
      "Epoch 2732/3000, Training Loss: 28.72514, Validation Loss: 3.85839\n",
      "Epoch 2733/3000, Training Loss: 28.72465, Validation Loss: 3.85836\n",
      "Epoch 2734/3000, Training Loss: 28.72416, Validation Loss: 3.85834\n",
      "Epoch 2735/3000, Training Loss: 28.72366, Validation Loss: 3.85832\n",
      "Epoch 2736/3000, Training Loss: 28.72317, Validation Loss: 3.85829\n",
      "Epoch 2737/3000, Training Loss: 28.72268, Validation Loss: 3.85827\n",
      "Epoch 2738/3000, Training Loss: 28.72219, Validation Loss: 3.85825\n",
      "Epoch 2739/3000, Training Loss: 28.72169, Validation Loss: 3.85823\n",
      "Epoch 2740/3000, Training Loss: 28.72120, Validation Loss: 3.85821\n",
      "Epoch 2741/3000, Training Loss: 28.72070, Validation Loss: 3.85819\n",
      "Epoch 2742/3000, Training Loss: 28.72021, Validation Loss: 3.85817\n",
      "Epoch 2743/3000, Training Loss: 28.71972, Validation Loss: 3.85814\n",
      "Epoch 2744/3000, Training Loss: 28.71922, Validation Loss: 3.85812\n",
      "Epoch 2745/3000, Training Loss: 28.71873, Validation Loss: 3.85810\n",
      "Epoch 2746/3000, Training Loss: 28.71823, Validation Loss: 3.85808\n",
      "Epoch 2747/3000, Training Loss: 28.71774, Validation Loss: 3.85806\n",
      "Epoch 2748/3000, Training Loss: 28.71725, Validation Loss: 3.85803\n",
      "Epoch 2749/3000, Training Loss: 28.71675, Validation Loss: 3.85801\n",
      "Epoch 2750/3000, Training Loss: 28.71626, Validation Loss: 3.85799\n",
      "Epoch 2751/3000, Training Loss: 28.71577, Validation Loss: 3.85797\n",
      "Epoch 2752/3000, Training Loss: 28.71527, Validation Loss: 3.85794\n",
      "Epoch 2753/3000, Training Loss: 28.71478, Validation Loss: 3.85792\n",
      "Epoch 2754/3000, Training Loss: 28.71429, Validation Loss: 3.85790\n",
      "Epoch 2755/3000, Training Loss: 28.71379, Validation Loss: 3.85788\n",
      "Epoch 2756/3000, Training Loss: 28.71330, Validation Loss: 3.85786\n",
      "Epoch 2757/3000, Training Loss: 28.71281, Validation Loss: 3.85784\n",
      "Epoch 2758/3000, Training Loss: 28.71231, Validation Loss: 3.85782\n",
      "Epoch 2759/3000, Training Loss: 28.71182, Validation Loss: 3.85779\n",
      "Epoch 2760/3000, Training Loss: 28.71133, Validation Loss: 3.85777\n",
      "Epoch 2761/3000, Training Loss: 28.71084, Validation Loss: 3.85775\n",
      "Epoch 2762/3000, Training Loss: 28.71034, Validation Loss: 3.85773\n",
      "Epoch 2763/3000, Training Loss: 28.70985, Validation Loss: 3.85771\n",
      "Epoch 2764/3000, Training Loss: 28.70936, Validation Loss: 3.85769\n",
      "Epoch 2765/3000, Training Loss: 28.70887, Validation Loss: 3.85768\n",
      "Epoch 2766/3000, Training Loss: 28.70838, Validation Loss: 3.85766\n",
      "Epoch 2767/3000, Training Loss: 28.70789, Validation Loss: 3.85764\n",
      "Epoch 2768/3000, Training Loss: 28.70739, Validation Loss: 3.85762\n",
      "Epoch 2769/3000, Training Loss: 28.70690, Validation Loss: 3.85760\n",
      "Epoch 2770/3000, Training Loss: 28.70641, Validation Loss: 3.85758\n",
      "Epoch 2771/3000, Training Loss: 28.70592, Validation Loss: 3.85756\n",
      "Epoch 2772/3000, Training Loss: 28.70543, Validation Loss: 3.85755\n",
      "Epoch 2773/3000, Training Loss: 28.70494, Validation Loss: 3.85753\n",
      "Epoch 2774/3000, Training Loss: 28.70445, Validation Loss: 3.85751\n",
      "Epoch 2775/3000, Training Loss: 28.70395, Validation Loss: 3.85749\n",
      "Epoch 2776/3000, Training Loss: 28.70346, Validation Loss: 3.85747\n",
      "Epoch 2777/3000, Training Loss: 28.70297, Validation Loss: 3.85746\n",
      "Epoch 2778/3000, Training Loss: 28.70249, Validation Loss: 3.85744\n",
      "Epoch 2779/3000, Training Loss: 28.70200, Validation Loss: 3.85742\n",
      "Epoch 2780/3000, Training Loss: 28.70151, Validation Loss: 3.85740\n",
      "Epoch 2781/3000, Training Loss: 28.70102, Validation Loss: 3.85739\n",
      "Epoch 2782/3000, Training Loss: 28.70053, Validation Loss: 3.85737\n",
      "Epoch 2783/3000, Training Loss: 28.70004, Validation Loss: 3.85735\n",
      "Epoch 2784/3000, Training Loss: 28.69955, Validation Loss: 3.85733\n",
      "Epoch 2785/3000, Training Loss: 28.69907, Validation Loss: 3.85732\n",
      "Epoch 2786/3000, Training Loss: 28.69858, Validation Loss: 3.85730\n",
      "Epoch 2787/3000, Training Loss: 28.69809, Validation Loss: 3.85728\n",
      "Epoch 2788/3000, Training Loss: 28.69760, Validation Loss: 3.85727\n",
      "Epoch 2789/3000, Training Loss: 28.69712, Validation Loss: 3.85725\n",
      "Epoch 2790/3000, Training Loss: 28.69663, Validation Loss: 3.85723\n",
      "Epoch 2791/3000, Training Loss: 28.69614, Validation Loss: 3.85721\n",
      "Epoch 2792/3000, Training Loss: 28.69565, Validation Loss: 3.85720\n",
      "Epoch 2793/3000, Training Loss: 28.69517, Validation Loss: 3.85718\n",
      "Epoch 2794/3000, Training Loss: 28.69468, Validation Loss: 3.85716\n",
      "Epoch 2795/3000, Training Loss: 28.69419, Validation Loss: 3.85715\n",
      "Epoch 2796/3000, Training Loss: 28.69371, Validation Loss: 3.85713\n",
      "Epoch 2797/3000, Training Loss: 28.69322, Validation Loss: 3.85711\n",
      "Epoch 2798/3000, Training Loss: 28.69273, Validation Loss: 3.85710\n",
      "Epoch 2799/3000, Training Loss: 28.69224, Validation Loss: 3.85708\n",
      "Epoch 2800/3000, Training Loss: 28.69175, Validation Loss: 3.85707\n",
      "Epoch 2801/3000, Training Loss: 28.69127, Validation Loss: 3.85705\n",
      "Epoch 2802/3000, Training Loss: 28.69078, Validation Loss: 3.85704\n",
      "Epoch 2803/3000, Training Loss: 28.69029, Validation Loss: 3.85702\n",
      "Epoch 2804/3000, Training Loss: 28.68980, Validation Loss: 3.85701\n",
      "Epoch 2805/3000, Training Loss: 28.68932, Validation Loss: 3.85699\n",
      "Epoch 2806/3000, Training Loss: 28.68883, Validation Loss: 3.85698\n",
      "Epoch 2807/3000, Training Loss: 28.68834, Validation Loss: 3.85696\n",
      "Epoch 2808/3000, Training Loss: 28.68786, Validation Loss: 3.85695\n",
      "Epoch 2809/3000, Training Loss: 28.68737, Validation Loss: 3.85693\n",
      "Epoch 2810/3000, Training Loss: 28.68689, Validation Loss: 3.85692\n",
      "Epoch 2811/3000, Training Loss: 28.68641, Validation Loss: 3.85690\n",
      "Epoch 2812/3000, Training Loss: 28.68592, Validation Loss: 3.85689\n",
      "Epoch 2813/3000, Training Loss: 28.68544, Validation Loss: 3.85687\n",
      "Epoch 2814/3000, Training Loss: 28.68496, Validation Loss: 3.85686\n",
      "Epoch 2815/3000, Training Loss: 28.68448, Validation Loss: 3.85684\n",
      "Epoch 2816/3000, Training Loss: 28.68400, Validation Loss: 3.85683\n",
      "Epoch 2817/3000, Training Loss: 28.68352, Validation Loss: 3.85682\n",
      "Epoch 2818/3000, Training Loss: 28.68304, Validation Loss: 3.85680\n",
      "Epoch 2819/3000, Training Loss: 28.68256, Validation Loss: 3.85679\n",
      "Epoch 2820/3000, Training Loss: 28.68208, Validation Loss: 3.85678\n",
      "Epoch 2821/3000, Training Loss: 28.68160, Validation Loss: 3.85676\n",
      "Epoch 2822/3000, Training Loss: 28.68112, Validation Loss: 3.85675\n",
      "Epoch 2823/3000, Training Loss: 28.68064, Validation Loss: 3.85674\n",
      "Epoch 2824/3000, Training Loss: 28.68016, Validation Loss: 3.85672\n",
      "Epoch 2825/3000, Training Loss: 28.67968, Validation Loss: 3.85671\n",
      "Epoch 2826/3000, Training Loss: 28.67920, Validation Loss: 3.85670\n",
      "Epoch 2827/3000, Training Loss: 28.67872, Validation Loss: 3.85669\n",
      "Epoch 2828/3000, Training Loss: 28.67825, Validation Loss: 3.85667\n",
      "Epoch 2829/3000, Training Loss: 28.67776, Validation Loss: 3.85666\n",
      "Epoch 2830/3000, Training Loss: 28.67729, Validation Loss: 3.85665\n",
      "Epoch 2831/3000, Training Loss: 28.67681, Validation Loss: 3.85664\n",
      "Epoch 2832/3000, Training Loss: 28.67633, Validation Loss: 3.85663\n",
      "Epoch 2833/3000, Training Loss: 28.67585, Validation Loss: 3.85662\n",
      "Epoch 2834/3000, Training Loss: 28.67537, Validation Loss: 3.85660\n",
      "Epoch 2835/3000, Training Loss: 28.67490, Validation Loss: 3.85659\n",
      "Epoch 2836/3000, Training Loss: 28.67442, Validation Loss: 3.85658\n",
      "Epoch 2837/3000, Training Loss: 28.67394, Validation Loss: 3.85657\n",
      "Epoch 2838/3000, Training Loss: 28.67347, Validation Loss: 3.85656\n",
      "Epoch 2839/3000, Training Loss: 28.67299, Validation Loss: 3.85655\n",
      "Epoch 2840/3000, Training Loss: 28.67251, Validation Loss: 3.85654\n",
      "Epoch 2841/3000, Training Loss: 28.67204, Validation Loss: 3.85653\n",
      "Epoch 2842/3000, Training Loss: 28.67156, Validation Loss: 3.85651\n",
      "Epoch 2843/3000, Training Loss: 28.67109, Validation Loss: 3.85651\n",
      "Epoch 2844/3000, Training Loss: 28.67061, Validation Loss: 3.85650\n",
      "Epoch 2845/3000, Training Loss: 28.67013, Validation Loss: 3.85649\n",
      "Epoch 2846/3000, Training Loss: 28.66965, Validation Loss: 3.85648\n",
      "Epoch 2847/3000, Training Loss: 28.66917, Validation Loss: 3.85647\n",
      "Epoch 2848/3000, Training Loss: 28.66869, Validation Loss: 3.85646\n",
      "Epoch 2849/3000, Training Loss: 28.66821, Validation Loss: 3.85645\n",
      "Epoch 2850/3000, Training Loss: 28.66774, Validation Loss: 3.85644\n",
      "Epoch 2851/3000, Training Loss: 28.66726, Validation Loss: 3.85643\n",
      "Epoch 2852/3000, Training Loss: 28.66678, Validation Loss: 3.85642\n",
      "Epoch 2853/3000, Training Loss: 28.66631, Validation Loss: 3.85641\n",
      "Epoch 2854/3000, Training Loss: 28.66583, Validation Loss: 3.85640\n",
      "Epoch 2855/3000, Training Loss: 28.66536, Validation Loss: 3.85639\n",
      "Epoch 2856/3000, Training Loss: 28.66488, Validation Loss: 3.85638\n",
      "Epoch 2857/3000, Training Loss: 28.66440, Validation Loss: 3.85637\n",
      "Epoch 2858/3000, Training Loss: 28.66393, Validation Loss: 3.85636\n",
      "Epoch 2859/3000, Training Loss: 28.66345, Validation Loss: 3.85635\n",
      "Epoch 2860/3000, Training Loss: 28.66298, Validation Loss: 3.85635\n",
      "Epoch 2861/3000, Training Loss: 28.66250, Validation Loss: 3.85634\n",
      "Epoch 2862/3000, Training Loss: 28.66203, Validation Loss: 3.85633\n",
      "Epoch 2863/3000, Training Loss: 28.66156, Validation Loss: 3.85632\n",
      "Epoch 2864/3000, Training Loss: 28.66108, Validation Loss: 3.85631\n",
      "Epoch 2865/3000, Training Loss: 28.66061, Validation Loss: 3.85630\n",
      "Epoch 2866/3000, Training Loss: 28.66013, Validation Loss: 3.85629\n",
      "Epoch 2867/3000, Training Loss: 28.65966, Validation Loss: 3.85628\n",
      "Epoch 2868/3000, Training Loss: 28.65919, Validation Loss: 3.85628\n",
      "Epoch 2869/3000, Training Loss: 28.65871, Validation Loss: 3.85627\n",
      "Epoch 2870/3000, Training Loss: 28.65824, Validation Loss: 3.85626\n",
      "Epoch 2871/3000, Training Loss: 28.65777, Validation Loss: 3.85625\n",
      "Epoch 2872/3000, Training Loss: 28.65729, Validation Loss: 3.85624\n",
      "Epoch 2873/3000, Training Loss: 28.65682, Validation Loss: 3.85624\n",
      "Epoch 2874/3000, Training Loss: 28.65635, Validation Loss: 3.85623\n",
      "Epoch 2875/3000, Training Loss: 28.65587, Validation Loss: 3.85622\n",
      "Epoch 2876/3000, Training Loss: 28.65540, Validation Loss: 3.85621\n",
      "Epoch 2877/3000, Training Loss: 28.65493, Validation Loss: 3.85620\n",
      "Epoch 2878/3000, Training Loss: 28.65446, Validation Loss: 3.85619\n",
      "Epoch 2879/3000, Training Loss: 28.65398, Validation Loss: 3.85619\n",
      "Epoch 2880/3000, Training Loss: 28.65351, Validation Loss: 3.85618\n",
      "Epoch 2881/3000, Training Loss: 28.65304, Validation Loss: 3.85617\n",
      "Epoch 2882/3000, Training Loss: 28.65257, Validation Loss: 3.85616\n",
      "Epoch 2883/3000, Training Loss: 28.65210, Validation Loss: 3.85615\n",
      "Epoch 2884/3000, Training Loss: 28.65163, Validation Loss: 3.85614\n",
      "Epoch 2885/3000, Training Loss: 28.65116, Validation Loss: 3.85614\n",
      "Epoch 2886/3000, Training Loss: 28.65069, Validation Loss: 3.85613\n",
      "Epoch 2887/3000, Training Loss: 28.65022, Validation Loss: 3.85612\n",
      "Epoch 2888/3000, Training Loss: 28.64975, Validation Loss: 3.85611\n",
      "Epoch 2889/3000, Training Loss: 28.64927, Validation Loss: 3.85611\n",
      "Epoch 2890/3000, Training Loss: 28.64880, Validation Loss: 3.85610\n",
      "Epoch 2891/3000, Training Loss: 28.64833, Validation Loss: 3.85609\n",
      "Epoch 2892/3000, Training Loss: 28.64787, Validation Loss: 3.85608\n",
      "Epoch 2893/3000, Training Loss: 28.64739, Validation Loss: 3.85607\n",
      "Epoch 2894/3000, Training Loss: 28.64693, Validation Loss: 3.85606\n",
      "Epoch 2895/3000, Training Loss: 28.64646, Validation Loss: 3.85606\n",
      "Epoch 2896/3000, Training Loss: 28.64599, Validation Loss: 3.85605\n",
      "Epoch 2897/3000, Training Loss: 28.64552, Validation Loss: 3.85604\n",
      "Epoch 2898/3000, Training Loss: 28.64506, Validation Loss: 3.85603\n",
      "Epoch 2899/3000, Training Loss: 28.64459, Validation Loss: 3.85603\n",
      "Epoch 2900/3000, Training Loss: 28.64412, Validation Loss: 3.85602\n",
      "Epoch 2901/3000, Training Loss: 28.64365, Validation Loss: 3.85601\n",
      "Epoch 2902/3000, Training Loss: 28.64318, Validation Loss: 3.85600\n",
      "Epoch 2903/3000, Training Loss: 28.64272, Validation Loss: 3.85600\n",
      "Epoch 2904/3000, Training Loss: 28.64225, Validation Loss: 3.85599\n",
      "Epoch 2905/3000, Training Loss: 28.64178, Validation Loss: 3.85598\n",
      "Epoch 2906/3000, Training Loss: 28.64132, Validation Loss: 3.85598\n",
      "Epoch 2907/3000, Training Loss: 28.64085, Validation Loss: 3.85597\n",
      "Epoch 2908/3000, Training Loss: 28.64038, Validation Loss: 3.85596\n",
      "Epoch 2909/3000, Training Loss: 28.63992, Validation Loss: 3.85596\n",
      "Epoch 2910/3000, Training Loss: 28.63945, Validation Loss: 3.85595\n",
      "Epoch 2911/3000, Training Loss: 28.63899, Validation Loss: 3.85594\n",
      "Epoch 2912/3000, Training Loss: 28.63852, Validation Loss: 3.85594\n",
      "Epoch 2913/3000, Training Loss: 28.63806, Validation Loss: 3.85593\n",
      "Epoch 2914/3000, Training Loss: 28.63759, Validation Loss: 3.85592\n",
      "Epoch 2915/3000, Training Loss: 28.63712, Validation Loss: 3.85592\n",
      "Epoch 2916/3000, Training Loss: 28.63666, Validation Loss: 3.85591\n",
      "Epoch 2917/3000, Training Loss: 28.63619, Validation Loss: 3.85591\n",
      "Epoch 2918/3000, Training Loss: 28.63572, Validation Loss: 3.85590\n",
      "Epoch 2919/3000, Training Loss: 28.63526, Validation Loss: 3.85589\n",
      "Epoch 2920/3000, Training Loss: 28.63480, Validation Loss: 3.85589\n",
      "Epoch 2921/3000, Training Loss: 28.63433, Validation Loss: 3.85588\n",
      "Epoch 2922/3000, Training Loss: 28.63387, Validation Loss: 3.85588\n",
      "Epoch 2923/3000, Training Loss: 28.63340, Validation Loss: 3.85587\n",
      "Epoch 2924/3000, Training Loss: 28.63294, Validation Loss: 3.85587\n",
      "Epoch 2925/3000, Training Loss: 28.63247, Validation Loss: 3.85586\n",
      "Epoch 2926/3000, Training Loss: 28.63201, Validation Loss: 3.85586\n",
      "Epoch 2927/3000, Training Loss: 28.63154, Validation Loss: 3.85585\n",
      "Epoch 2928/3000, Training Loss: 28.63108, Validation Loss: 3.85585\n",
      "Epoch 2929/3000, Training Loss: 28.63061, Validation Loss: 3.85584\n",
      "Epoch 2930/3000, Training Loss: 28.63015, Validation Loss: 3.85584\n",
      "Epoch 2931/3000, Training Loss: 28.62968, Validation Loss: 3.85584\n",
      "Epoch 2932/3000, Training Loss: 28.62922, Validation Loss: 3.85583\n",
      "Epoch 2933/3000, Training Loss: 28.62876, Validation Loss: 3.85583\n",
      "Epoch 2934/3000, Training Loss: 28.62829, Validation Loss: 3.85582\n",
      "Epoch 2935/3000, Training Loss: 28.62783, Validation Loss: 3.85582\n",
      "Epoch 2936/3000, Training Loss: 28.62736, Validation Loss: 3.85582\n",
      "Epoch 2937/3000, Training Loss: 28.62690, Validation Loss: 3.85581\n",
      "Epoch 2938/3000, Training Loss: 28.62643, Validation Loss: 3.85581\n",
      "Epoch 2939/3000, Training Loss: 28.62597, Validation Loss: 3.85580\n",
      "Epoch 2940/3000, Training Loss: 28.62550, Validation Loss: 3.85580\n",
      "Epoch 2941/3000, Training Loss: 28.62504, Validation Loss: 3.85579\n",
      "Epoch 2942/3000, Training Loss: 28.62457, Validation Loss: 3.85579\n",
      "Epoch 2943/3000, Training Loss: 28.62411, Validation Loss: 3.85578\n",
      "Epoch 2944/3000, Training Loss: 28.62365, Validation Loss: 3.85578\n",
      "Epoch 2945/3000, Training Loss: 28.62318, Validation Loss: 3.85578\n",
      "Epoch 2946/3000, Training Loss: 28.62272, Validation Loss: 3.85577\n",
      "Epoch 2947/3000, Training Loss: 28.62226, Validation Loss: 3.85577\n",
      "Epoch 2948/3000, Training Loss: 28.62179, Validation Loss: 3.85576\n",
      "Epoch 2949/3000, Training Loss: 28.62133, Validation Loss: 3.85576\n",
      "Epoch 2950/3000, Training Loss: 28.62087, Validation Loss: 3.85575\n",
      "Epoch 2951/3000, Training Loss: 28.62041, Validation Loss: 3.85575\n",
      "Epoch 2952/3000, Training Loss: 28.61995, Validation Loss: 3.85574\n",
      "Epoch 2953/3000, Training Loss: 28.61949, Validation Loss: 3.85574\n",
      "Epoch 2954/3000, Training Loss: 28.61902, Validation Loss: 3.85574\n",
      "Epoch 2955/3000, Training Loss: 28.61856, Validation Loss: 3.85573\n",
      "Epoch 2956/3000, Training Loss: 28.61810, Validation Loss: 3.85572\n",
      "Epoch 2957/3000, Training Loss: 28.61765, Validation Loss: 3.85572\n",
      "Epoch 2958/3000, Training Loss: 28.61718, Validation Loss: 3.85571\n",
      "Epoch 2959/3000, Training Loss: 28.61672, Validation Loss: 3.85570\n",
      "Epoch 2960/3000, Training Loss: 28.61626, Validation Loss: 3.85569\n",
      "Epoch 2961/3000, Training Loss: 28.61580, Validation Loss: 3.85568\n",
      "Epoch 2962/3000, Training Loss: 28.61534, Validation Loss: 3.85568\n",
      "Epoch 2963/3000, Training Loss: 28.61487, Validation Loss: 3.85567\n",
      "Epoch 2964/3000, Training Loss: 28.61441, Validation Loss: 3.85566\n",
      "Epoch 2965/3000, Training Loss: 28.61395, Validation Loss: 3.85565\n",
      "Epoch 2966/3000, Training Loss: 28.61349, Validation Loss: 3.85564\n",
      "Epoch 2967/3000, Training Loss: 28.61303, Validation Loss: 3.85563\n",
      "Epoch 2968/3000, Training Loss: 28.61258, Validation Loss: 3.85563\n",
      "Epoch 2969/3000, Training Loss: 28.61212, Validation Loss: 3.85562\n",
      "Epoch 2970/3000, Training Loss: 28.61166, Validation Loss: 3.85561\n",
      "Epoch 2971/3000, Training Loss: 28.61120, Validation Loss: 3.85560\n",
      "Epoch 2972/3000, Training Loss: 28.61074, Validation Loss: 3.85559\n",
      "Epoch 2973/3000, Training Loss: 28.61027, Validation Loss: 3.85558\n",
      "Epoch 2974/3000, Training Loss: 28.60982, Validation Loss: 3.85557\n",
      "Epoch 2975/3000, Training Loss: 28.60936, Validation Loss: 3.85557\n",
      "Epoch 2976/3000, Training Loss: 28.60890, Validation Loss: 3.85556\n",
      "Epoch 2977/3000, Training Loss: 28.60844, Validation Loss: 3.85555\n",
      "Epoch 2978/3000, Training Loss: 28.60798, Validation Loss: 3.85554\n",
      "Epoch 2979/3000, Training Loss: 28.60752, Validation Loss: 3.85553\n",
      "Epoch 2980/3000, Training Loss: 28.60706, Validation Loss: 3.85552\n",
      "Epoch 2981/3000, Training Loss: 28.60660, Validation Loss: 3.85552\n",
      "Epoch 2982/3000, Training Loss: 28.60614, Validation Loss: 3.85551\n",
      "Epoch 2983/3000, Training Loss: 28.60568, Validation Loss: 3.85550\n",
      "Epoch 2984/3000, Training Loss: 28.60522, Validation Loss: 3.85550\n",
      "Epoch 2985/3000, Training Loss: 28.60476, Validation Loss: 3.85549\n",
      "Epoch 2986/3000, Training Loss: 28.60430, Validation Loss: 3.85549\n",
      "Epoch 2987/3000, Training Loss: 28.60384, Validation Loss: 3.85549\n",
      "Epoch 2988/3000, Training Loss: 28.60338, Validation Loss: 3.85548\n",
      "Epoch 2989/3000, Training Loss: 28.60292, Validation Loss: 3.85548\n",
      "Epoch 2990/3000, Training Loss: 28.60246, Validation Loss: 3.85547\n",
      "Epoch 2991/3000, Training Loss: 28.60200, Validation Loss: 3.85547\n",
      "Epoch 2992/3000, Training Loss: 28.60154, Validation Loss: 3.85547\n",
      "Epoch 2993/3000, Training Loss: 28.60108, Validation Loss: 3.85546\n",
      "Epoch 2994/3000, Training Loss: 28.60062, Validation Loss: 3.85546\n",
      "Epoch 2995/3000, Training Loss: 28.60016, Validation Loss: 3.85546\n",
      "Epoch 2996/3000, Training Loss: 28.59970, Validation Loss: 3.85546\n",
      "Epoch 2997/3000, Training Loss: 28.59924, Validation Loss: 3.85545\n",
      "Epoch 2998/3000, Training Loss: 28.59878, Validation Loss: 3.85545\n",
      "Epoch 2999/3000, Training Loss: 28.59832, Validation Loss: 3.85545\n",
      "Epoch 3000/3000, Training Loss: 28.59786, Validation Loss: 3.85544\n",
      "Training took: 247.33 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_epoch_3000_3 = NeuralNetwork().to(device)\n",
    "summary(model_epoch_3000_3, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 3000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_epoch_3000_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_epoch_3000_3=[]\n",
    "val_loss_list_epoch_3000_3=[]\n",
    "train_accuracy_list_epoch_3000_3=[]\n",
    "val_accuracy_list_epoch_3000_3=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_epoch_3000_3.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_epoch_3000_3(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_epoch_3000_3.append(train_accuracy)\n",
    "    train_loss_list_epoch_3000_3.append(train_loss)\n",
    "\n",
    "    model_epoch_3000_3.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_epoch_3000_3(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_epoch_3000_3.append(val_accuracy)\n",
    "    val_accuracy_list_epoch_3000_3.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eApUUE4yOdQ",
    "outputId": "177ece8a-eeb9-4f72-f139-5937372d3e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for variable epoch size with epoch size as 3000: 0.7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_epoch_3000_3.eval()\n",
    "test_predictions_epoch_3000_3 = model_epoch_3000_3(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_epoch_3000_3 = torch.round(test_predictions_epoch_3000_3)\n",
    "\n",
    "test_predictions_rounded_numpy_epoch_3000_3 = test_predictions_rounded_epoch_3000_3.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_epoch_3000_3 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_epoch_3000_3)\n",
    "\n",
    "print(f\"Accuracy for variable epoch size with epoch size as 3000: {accuracy_epoch_3000_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJ9joTugylw_",
    "outputId": "05b34452-4a8e-4a93-d8ad-246738bab3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for variable epoch size with epoch size as 3000: 0.47348\n"
     ]
    }
   ],
   "source": [
    "model_epoch_3000_3.eval()\n",
    "test_loss_epoch_3000_3=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_epoch_3000_3 = model_epoch_3000_3(X_test_tensor)\n",
    "    test_loss_epoch_3000_3 = loss_function(test_outputs_epoch_3000_3, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Test Loss for variable epoch size with epoch size as 3000: {test_loss_epoch_3000_3.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lbaBJAElwoK"
   },
   "source": [
    "References:\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLpHNBsxDd2s"
   },
   "source": [
    "Optimizing the base model trying four different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYEn9tIU_Lce"
   },
   "source": [
    "Adding dropouts in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "v9m6hBPMIXHM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "class BaseModelWithDropouts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModelWithDropouts, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8J22HKXBM7F",
    "outputId": "b9ae0186-8c0a-405a-be9e-110e785e86a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 41.31572, Validation Loss: 6.05197\n",
      "Epoch 2/1000, Training Loss: 41.27688, Validation Loss: 6.04968\n",
      "Epoch 3/1000, Training Loss: 41.00581, Validation Loss: 6.04742\n",
      "Epoch 4/1000, Training Loss: 41.17298, Validation Loss: 6.04527\n",
      "Epoch 5/1000, Training Loss: 41.15601, Validation Loss: 6.04292\n",
      "Epoch 6/1000, Training Loss: 41.30719, Validation Loss: 6.04066\n",
      "Epoch 7/1000, Training Loss: 41.13238, Validation Loss: 6.03862\n",
      "Epoch 8/1000, Training Loss: 41.06537, Validation Loss: 6.03640\n",
      "Epoch 9/1000, Training Loss: 41.22566, Validation Loss: 6.03423\n",
      "Epoch 10/1000, Training Loss: 41.27486, Validation Loss: 6.03209\n",
      "Epoch 11/1000, Training Loss: 41.11401, Validation Loss: 6.02993\n",
      "Epoch 12/1000, Training Loss: 41.18369, Validation Loss: 6.02779\n",
      "Epoch 13/1000, Training Loss: 41.13097, Validation Loss: 6.02568\n",
      "Epoch 14/1000, Training Loss: 41.01951, Validation Loss: 6.02350\n",
      "Epoch 15/1000, Training Loss: 41.04592, Validation Loss: 6.02131\n",
      "Epoch 16/1000, Training Loss: 41.21373, Validation Loss: 6.01923\n",
      "Epoch 17/1000, Training Loss: 40.94131, Validation Loss: 6.01708\n",
      "Epoch 18/1000, Training Loss: 41.16370, Validation Loss: 6.01497\n",
      "Epoch 19/1000, Training Loss: 41.12079, Validation Loss: 6.01281\n",
      "Epoch 20/1000, Training Loss: 41.08401, Validation Loss: 6.01080\n",
      "Epoch 21/1000, Training Loss: 41.06407, Validation Loss: 6.00872\n",
      "Epoch 22/1000, Training Loss: 41.09265, Validation Loss: 6.00656\n",
      "Epoch 23/1000, Training Loss: 40.95042, Validation Loss: 6.00450\n",
      "Epoch 24/1000, Training Loss: 41.03439, Validation Loss: 6.00233\n",
      "Epoch 25/1000, Training Loss: 40.92122, Validation Loss: 6.00025\n",
      "Epoch 26/1000, Training Loss: 41.10131, Validation Loss: 5.99817\n",
      "Epoch 27/1000, Training Loss: 41.26424, Validation Loss: 5.99601\n",
      "Epoch 28/1000, Training Loss: 40.91697, Validation Loss: 5.99392\n",
      "Epoch 29/1000, Training Loss: 40.79174, Validation Loss: 5.99188\n",
      "Epoch 30/1000, Training Loss: 40.86193, Validation Loss: 5.98981\n",
      "Epoch 31/1000, Training Loss: 40.87432, Validation Loss: 5.98773\n",
      "Epoch 32/1000, Training Loss: 40.89018, Validation Loss: 5.98567\n",
      "Epoch 33/1000, Training Loss: 40.99033, Validation Loss: 5.98361\n",
      "Epoch 34/1000, Training Loss: 41.01913, Validation Loss: 5.98166\n",
      "Epoch 35/1000, Training Loss: 40.74681, Validation Loss: 5.97967\n",
      "Epoch 36/1000, Training Loss: 41.01155, Validation Loss: 5.97764\n",
      "Epoch 37/1000, Training Loss: 40.87224, Validation Loss: 5.97564\n",
      "Epoch 38/1000, Training Loss: 40.62764, Validation Loss: 5.97361\n",
      "Epoch 39/1000, Training Loss: 40.91045, Validation Loss: 5.97173\n",
      "Epoch 40/1000, Training Loss: 40.78436, Validation Loss: 5.96982\n",
      "Epoch 41/1000, Training Loss: 40.73096, Validation Loss: 5.96775\n",
      "Epoch 42/1000, Training Loss: 40.93417, Validation Loss: 5.96574\n",
      "Epoch 43/1000, Training Loss: 40.74921, Validation Loss: 5.96375\n",
      "Epoch 44/1000, Training Loss: 41.03423, Validation Loss: 5.96186\n",
      "Epoch 45/1000, Training Loss: 40.73987, Validation Loss: 5.95997\n",
      "Epoch 46/1000, Training Loss: 40.53567, Validation Loss: 5.95800\n",
      "Epoch 47/1000, Training Loss: 40.74696, Validation Loss: 5.95606\n",
      "Epoch 48/1000, Training Loss: 40.87063, Validation Loss: 5.95412\n",
      "Epoch 49/1000, Training Loss: 40.73626, Validation Loss: 5.95211\n",
      "Epoch 50/1000, Training Loss: 40.78578, Validation Loss: 5.95014\n",
      "Epoch 51/1000, Training Loss: 40.55481, Validation Loss: 5.94824\n",
      "Epoch 52/1000, Training Loss: 40.41839, Validation Loss: 5.94632\n",
      "Epoch 53/1000, Training Loss: 40.62127, Validation Loss: 5.94449\n",
      "Epoch 54/1000, Training Loss: 40.78221, Validation Loss: 5.94265\n",
      "Epoch 55/1000, Training Loss: 40.73844, Validation Loss: 5.94073\n",
      "Epoch 56/1000, Training Loss: 40.56879, Validation Loss: 5.93883\n",
      "Epoch 57/1000, Training Loss: 40.66325, Validation Loss: 5.93696\n",
      "Epoch 58/1000, Training Loss: 40.60200, Validation Loss: 5.93500\n",
      "Epoch 59/1000, Training Loss: 40.62344, Validation Loss: 5.93315\n",
      "Epoch 60/1000, Training Loss: 40.62058, Validation Loss: 5.93133\n",
      "Epoch 61/1000, Training Loss: 40.66082, Validation Loss: 5.92945\n",
      "Epoch 62/1000, Training Loss: 40.52690, Validation Loss: 5.92751\n",
      "Epoch 63/1000, Training Loss: 40.58983, Validation Loss: 5.92578\n",
      "Epoch 64/1000, Training Loss: 40.45840, Validation Loss: 5.92387\n",
      "Epoch 65/1000, Training Loss: 40.58743, Validation Loss: 5.92203\n",
      "Epoch 66/1000, Training Loss: 40.45587, Validation Loss: 5.92016\n",
      "Epoch 67/1000, Training Loss: 40.47283, Validation Loss: 5.91826\n",
      "Epoch 68/1000, Training Loss: 40.51530, Validation Loss: 5.91637\n",
      "Epoch 69/1000, Training Loss: 40.55092, Validation Loss: 5.91454\n",
      "Epoch 70/1000, Training Loss: 40.59816, Validation Loss: 5.91270\n",
      "Epoch 71/1000, Training Loss: 40.43661, Validation Loss: 5.91085\n",
      "Epoch 72/1000, Training Loss: 40.63524, Validation Loss: 5.90913\n",
      "Epoch 73/1000, Training Loss: 40.39799, Validation Loss: 5.90724\n",
      "Epoch 74/1000, Training Loss: 40.62464, Validation Loss: 5.90544\n",
      "Epoch 75/1000, Training Loss: 40.25003, Validation Loss: 5.90362\n",
      "Epoch 76/1000, Training Loss: 40.29204, Validation Loss: 5.90175\n",
      "Epoch 77/1000, Training Loss: 40.39924, Validation Loss: 5.89988\n",
      "Epoch 78/1000, Training Loss: 40.61098, Validation Loss: 5.89819\n",
      "Epoch 79/1000, Training Loss: 40.43879, Validation Loss: 5.89641\n",
      "Epoch 80/1000, Training Loss: 40.39632, Validation Loss: 5.89462\n",
      "Epoch 81/1000, Training Loss: 40.50603, Validation Loss: 5.89300\n",
      "Epoch 82/1000, Training Loss: 40.55692, Validation Loss: 5.89124\n",
      "Epoch 83/1000, Training Loss: 40.36077, Validation Loss: 5.88946\n",
      "Epoch 84/1000, Training Loss: 40.61937, Validation Loss: 5.88773\n",
      "Epoch 85/1000, Training Loss: 40.38431, Validation Loss: 5.88604\n",
      "Epoch 86/1000, Training Loss: 40.45485, Validation Loss: 5.88429\n",
      "Epoch 87/1000, Training Loss: 40.40551, Validation Loss: 5.88249\n",
      "Epoch 88/1000, Training Loss: 40.37219, Validation Loss: 5.88076\n",
      "Epoch 89/1000, Training Loss: 40.34388, Validation Loss: 5.87894\n",
      "Epoch 90/1000, Training Loss: 40.35407, Validation Loss: 5.87710\n",
      "Epoch 91/1000, Training Loss: 40.33481, Validation Loss: 5.87535\n",
      "Epoch 92/1000, Training Loss: 40.38138, Validation Loss: 5.87359\n",
      "Epoch 93/1000, Training Loss: 40.17068, Validation Loss: 5.87190\n",
      "Epoch 94/1000, Training Loss: 40.25692, Validation Loss: 5.87010\n",
      "Epoch 95/1000, Training Loss: 40.28111, Validation Loss: 5.86845\n",
      "Epoch 96/1000, Training Loss: 40.39524, Validation Loss: 5.86673\n",
      "Epoch 97/1000, Training Loss: 40.27824, Validation Loss: 5.86500\n",
      "Epoch 98/1000, Training Loss: 40.47505, Validation Loss: 5.86331\n",
      "Epoch 99/1000, Training Loss: 40.23679, Validation Loss: 5.86149\n",
      "Epoch 100/1000, Training Loss: 40.25496, Validation Loss: 5.85987\n",
      "Epoch 101/1000, Training Loss: 40.48575, Validation Loss: 5.85820\n",
      "Epoch 102/1000, Training Loss: 40.09379, Validation Loss: 5.85647\n",
      "Epoch 103/1000, Training Loss: 40.16176, Validation Loss: 5.85475\n",
      "Epoch 104/1000, Training Loss: 40.36748, Validation Loss: 5.85310\n",
      "Epoch 105/1000, Training Loss: 40.31150, Validation Loss: 5.85140\n",
      "Epoch 106/1000, Training Loss: 40.09354, Validation Loss: 5.84970\n",
      "Epoch 107/1000, Training Loss: 40.38275, Validation Loss: 5.84798\n",
      "Epoch 108/1000, Training Loss: 40.39825, Validation Loss: 5.84630\n",
      "Epoch 109/1000, Training Loss: 40.39298, Validation Loss: 5.84470\n",
      "Epoch 110/1000, Training Loss: 40.07376, Validation Loss: 5.84296\n",
      "Epoch 111/1000, Training Loss: 40.04136, Validation Loss: 5.84120\n",
      "Epoch 112/1000, Training Loss: 40.26723, Validation Loss: 5.83952\n",
      "Epoch 113/1000, Training Loss: 40.21022, Validation Loss: 5.83793\n",
      "Epoch 114/1000, Training Loss: 40.20450, Validation Loss: 5.83632\n",
      "Epoch 115/1000, Training Loss: 40.23442, Validation Loss: 5.83470\n",
      "Epoch 116/1000, Training Loss: 40.19870, Validation Loss: 5.83309\n",
      "Epoch 117/1000, Training Loss: 39.93187, Validation Loss: 5.83139\n",
      "Epoch 118/1000, Training Loss: 40.20964, Validation Loss: 5.82978\n",
      "Epoch 119/1000, Training Loss: 40.10237, Validation Loss: 5.82816\n",
      "Epoch 120/1000, Training Loss: 40.07635, Validation Loss: 5.82648\n",
      "Epoch 121/1000, Training Loss: 40.01048, Validation Loss: 5.82486\n",
      "Epoch 122/1000, Training Loss: 39.90705, Validation Loss: 5.82318\n",
      "Epoch 123/1000, Training Loss: 40.03603, Validation Loss: 5.82152\n",
      "Epoch 124/1000, Training Loss: 40.11217, Validation Loss: 5.81991\n",
      "Epoch 125/1000, Training Loss: 40.15572, Validation Loss: 5.81825\n",
      "Epoch 126/1000, Training Loss: 39.88750, Validation Loss: 5.81664\n",
      "Epoch 127/1000, Training Loss: 40.10612, Validation Loss: 5.81500\n",
      "Epoch 128/1000, Training Loss: 40.16268, Validation Loss: 5.81343\n",
      "Epoch 129/1000, Training Loss: 40.03618, Validation Loss: 5.81175\n",
      "Epoch 130/1000, Training Loss: 40.00089, Validation Loss: 5.81018\n",
      "Epoch 131/1000, Training Loss: 40.01767, Validation Loss: 5.80860\n",
      "Epoch 132/1000, Training Loss: 39.86775, Validation Loss: 5.80696\n",
      "Epoch 133/1000, Training Loss: 40.20438, Validation Loss: 5.80544\n",
      "Epoch 134/1000, Training Loss: 39.97543, Validation Loss: 5.80378\n",
      "Epoch 135/1000, Training Loss: 40.02067, Validation Loss: 5.80224\n",
      "Epoch 136/1000, Training Loss: 39.92590, Validation Loss: 5.80058\n",
      "Epoch 137/1000, Training Loss: 40.01598, Validation Loss: 5.79895\n",
      "Epoch 138/1000, Training Loss: 40.19660, Validation Loss: 5.79732\n",
      "Epoch 139/1000, Training Loss: 39.80094, Validation Loss: 5.79569\n",
      "Epoch 140/1000, Training Loss: 39.72056, Validation Loss: 5.79405\n",
      "Epoch 141/1000, Training Loss: 40.00099, Validation Loss: 5.79244\n",
      "Epoch 142/1000, Training Loss: 39.88740, Validation Loss: 5.79086\n",
      "Epoch 143/1000, Training Loss: 40.04348, Validation Loss: 5.78932\n",
      "Epoch 144/1000, Training Loss: 40.06281, Validation Loss: 5.78772\n",
      "Epoch 145/1000, Training Loss: 39.97453, Validation Loss: 5.78611\n",
      "Epoch 146/1000, Training Loss: 39.70423, Validation Loss: 5.78450\n",
      "Epoch 147/1000, Training Loss: 39.84093, Validation Loss: 5.78287\n",
      "Epoch 148/1000, Training Loss: 40.00104, Validation Loss: 5.78133\n",
      "Epoch 149/1000, Training Loss: 39.74505, Validation Loss: 5.77975\n",
      "Epoch 150/1000, Training Loss: 39.79637, Validation Loss: 5.77820\n",
      "Epoch 151/1000, Training Loss: 39.80553, Validation Loss: 5.77663\n",
      "Epoch 152/1000, Training Loss: 39.57307, Validation Loss: 5.77495\n",
      "Epoch 153/1000, Training Loss: 39.87793, Validation Loss: 5.77344\n",
      "Epoch 154/1000, Training Loss: 39.65911, Validation Loss: 5.77189\n",
      "Epoch 155/1000, Training Loss: 39.82058, Validation Loss: 5.77035\n",
      "Epoch 156/1000, Training Loss: 39.72761, Validation Loss: 5.76878\n",
      "Epoch 157/1000, Training Loss: 39.68498, Validation Loss: 5.76717\n",
      "Epoch 158/1000, Training Loss: 39.96700, Validation Loss: 5.76556\n",
      "Epoch 159/1000, Training Loss: 39.66138, Validation Loss: 5.76403\n",
      "Epoch 160/1000, Training Loss: 39.76137, Validation Loss: 5.76246\n",
      "Epoch 161/1000, Training Loss: 39.95103, Validation Loss: 5.76094\n",
      "Epoch 162/1000, Training Loss: 39.73499, Validation Loss: 5.75941\n",
      "Epoch 163/1000, Training Loss: 39.85913, Validation Loss: 5.75790\n",
      "Epoch 164/1000, Training Loss: 39.72659, Validation Loss: 5.75626\n",
      "Epoch 165/1000, Training Loss: 39.73945, Validation Loss: 5.75477\n",
      "Epoch 166/1000, Training Loss: 39.68571, Validation Loss: 5.75327\n",
      "Epoch 167/1000, Training Loss: 39.65014, Validation Loss: 5.75171\n",
      "Epoch 168/1000, Training Loss: 39.65672, Validation Loss: 5.75012\n",
      "Epoch 169/1000, Training Loss: 39.71928, Validation Loss: 5.74866\n",
      "Epoch 170/1000, Training Loss: 39.67833, Validation Loss: 5.74711\n",
      "Epoch 171/1000, Training Loss: 39.71285, Validation Loss: 5.74560\n",
      "Epoch 172/1000, Training Loss: 39.54218, Validation Loss: 5.74397\n",
      "Epoch 173/1000, Training Loss: 39.56325, Validation Loss: 5.74235\n",
      "Epoch 174/1000, Training Loss: 39.64844, Validation Loss: 5.74081\n",
      "Epoch 175/1000, Training Loss: 39.75030, Validation Loss: 5.73933\n",
      "Epoch 176/1000, Training Loss: 39.84071, Validation Loss: 5.73776\n",
      "Epoch 177/1000, Training Loss: 39.45180, Validation Loss: 5.73624\n",
      "Epoch 178/1000, Training Loss: 39.49864, Validation Loss: 5.73466\n",
      "Epoch 179/1000, Training Loss: 39.49720, Validation Loss: 5.73313\n",
      "Epoch 180/1000, Training Loss: 39.64988, Validation Loss: 5.73166\n",
      "Epoch 181/1000, Training Loss: 39.49217, Validation Loss: 5.73007\n",
      "Epoch 182/1000, Training Loss: 39.47837, Validation Loss: 5.72848\n",
      "Epoch 183/1000, Training Loss: 39.41491, Validation Loss: 5.72694\n",
      "Epoch 184/1000, Training Loss: 39.43159, Validation Loss: 5.72535\n",
      "Epoch 185/1000, Training Loss: 39.63747, Validation Loss: 5.72389\n",
      "Epoch 186/1000, Training Loss: 39.50004, Validation Loss: 5.72225\n",
      "Epoch 187/1000, Training Loss: 39.40926, Validation Loss: 5.72070\n",
      "Epoch 188/1000, Training Loss: 39.44115, Validation Loss: 5.71916\n",
      "Epoch 189/1000, Training Loss: 39.47407, Validation Loss: 5.71763\n",
      "Epoch 190/1000, Training Loss: 39.42565, Validation Loss: 5.71603\n",
      "Epoch 191/1000, Training Loss: 39.54023, Validation Loss: 5.71449\n",
      "Epoch 192/1000, Training Loss: 39.49980, Validation Loss: 5.71296\n",
      "Epoch 193/1000, Training Loss: 39.28881, Validation Loss: 5.71145\n",
      "Epoch 194/1000, Training Loss: 39.61960, Validation Loss: 5.70995\n",
      "Epoch 195/1000, Training Loss: 39.43264, Validation Loss: 5.70845\n",
      "Epoch 196/1000, Training Loss: 39.52265, Validation Loss: 5.70696\n",
      "Epoch 197/1000, Training Loss: 39.58256, Validation Loss: 5.70553\n",
      "Epoch 198/1000, Training Loss: 39.47450, Validation Loss: 5.70401\n",
      "Epoch 199/1000, Training Loss: 39.46707, Validation Loss: 5.70247\n",
      "Epoch 200/1000, Training Loss: 39.36395, Validation Loss: 5.70093\n",
      "Epoch 201/1000, Training Loss: 39.37686, Validation Loss: 5.69941\n",
      "Epoch 202/1000, Training Loss: 39.26665, Validation Loss: 5.69783\n",
      "Epoch 203/1000, Training Loss: 39.50322, Validation Loss: 5.69630\n",
      "Epoch 204/1000, Training Loss: 39.27173, Validation Loss: 5.69479\n",
      "Epoch 205/1000, Training Loss: 39.37486, Validation Loss: 5.69334\n",
      "Epoch 206/1000, Training Loss: 39.45734, Validation Loss: 5.69194\n",
      "Epoch 207/1000, Training Loss: 39.29374, Validation Loss: 5.69052\n",
      "Epoch 208/1000, Training Loss: 39.49798, Validation Loss: 5.68899\n",
      "Epoch 209/1000, Training Loss: 39.28951, Validation Loss: 5.68741\n",
      "Epoch 210/1000, Training Loss: 39.43199, Validation Loss: 5.68586\n",
      "Epoch 211/1000, Training Loss: 39.45079, Validation Loss: 5.68435\n",
      "Epoch 212/1000, Training Loss: 39.63499, Validation Loss: 5.68293\n",
      "Epoch 213/1000, Training Loss: 39.47776, Validation Loss: 5.68150\n",
      "Epoch 214/1000, Training Loss: 39.18481, Validation Loss: 5.67983\n",
      "Epoch 215/1000, Training Loss: 39.34503, Validation Loss: 5.67828\n",
      "Epoch 216/1000, Training Loss: 39.05354, Validation Loss: 5.67666\n",
      "Epoch 217/1000, Training Loss: 39.40005, Validation Loss: 5.67518\n",
      "Epoch 218/1000, Training Loss: 39.21703, Validation Loss: 5.67366\n",
      "Epoch 219/1000, Training Loss: 39.32647, Validation Loss: 5.67210\n",
      "Epoch 220/1000, Training Loss: 39.27654, Validation Loss: 5.67057\n",
      "Epoch 221/1000, Training Loss: 39.40195, Validation Loss: 5.66916\n",
      "Epoch 222/1000, Training Loss: 39.21552, Validation Loss: 5.66767\n",
      "Epoch 223/1000, Training Loss: 39.24820, Validation Loss: 5.66616\n",
      "Epoch 224/1000, Training Loss: 39.21789, Validation Loss: 5.66465\n",
      "Epoch 225/1000, Training Loss: 38.93978, Validation Loss: 5.66308\n",
      "Epoch 226/1000, Training Loss: 39.18531, Validation Loss: 5.66169\n",
      "Epoch 227/1000, Training Loss: 39.16781, Validation Loss: 5.66020\n",
      "Epoch 228/1000, Training Loss: 39.19922, Validation Loss: 5.65872\n",
      "Epoch 229/1000, Training Loss: 39.26465, Validation Loss: 5.65727\n",
      "Epoch 230/1000, Training Loss: 38.95657, Validation Loss: 5.65565\n",
      "Epoch 231/1000, Training Loss: 39.03044, Validation Loss: 5.65414\n",
      "Epoch 232/1000, Training Loss: 39.09682, Validation Loss: 5.65266\n",
      "Epoch 233/1000, Training Loss: 39.16328, Validation Loss: 5.65119\n",
      "Epoch 234/1000, Training Loss: 39.21629, Validation Loss: 5.64980\n",
      "Epoch 235/1000, Training Loss: 38.99254, Validation Loss: 5.64826\n",
      "Epoch 236/1000, Training Loss: 39.31786, Validation Loss: 5.64686\n",
      "Epoch 237/1000, Training Loss: 39.05095, Validation Loss: 5.64532\n",
      "Epoch 238/1000, Training Loss: 38.91909, Validation Loss: 5.64372\n",
      "Epoch 239/1000, Training Loss: 39.11912, Validation Loss: 5.64219\n",
      "Epoch 240/1000, Training Loss: 39.25645, Validation Loss: 5.64074\n",
      "Epoch 241/1000, Training Loss: 39.10138, Validation Loss: 5.63921\n",
      "Epoch 242/1000, Training Loss: 38.93761, Validation Loss: 5.63770\n",
      "Epoch 243/1000, Training Loss: 38.80802, Validation Loss: 5.63609\n",
      "Epoch 244/1000, Training Loss: 39.27070, Validation Loss: 5.63466\n",
      "Epoch 245/1000, Training Loss: 39.11018, Validation Loss: 5.63316\n",
      "Epoch 246/1000, Training Loss: 39.05302, Validation Loss: 5.63168\n",
      "Epoch 247/1000, Training Loss: 39.29019, Validation Loss: 5.63024\n",
      "Epoch 248/1000, Training Loss: 38.87519, Validation Loss: 5.62868\n",
      "Epoch 249/1000, Training Loss: 39.03614, Validation Loss: 5.62714\n",
      "Epoch 250/1000, Training Loss: 39.07097, Validation Loss: 5.62565\n",
      "Epoch 251/1000, Training Loss: 39.00566, Validation Loss: 5.62424\n",
      "Epoch 252/1000, Training Loss: 38.89719, Validation Loss: 5.62271\n",
      "Epoch 253/1000, Training Loss: 39.05639, Validation Loss: 5.62120\n",
      "Epoch 254/1000, Training Loss: 39.23208, Validation Loss: 5.61980\n",
      "Epoch 255/1000, Training Loss: 38.82622, Validation Loss: 5.61818\n",
      "Epoch 256/1000, Training Loss: 39.00431, Validation Loss: 5.61666\n",
      "Epoch 257/1000, Training Loss: 38.95934, Validation Loss: 5.61520\n",
      "Epoch 258/1000, Training Loss: 39.04293, Validation Loss: 5.61375\n",
      "Epoch 259/1000, Training Loss: 38.96304, Validation Loss: 5.61231\n",
      "Epoch 260/1000, Training Loss: 38.77508, Validation Loss: 5.61078\n",
      "Epoch 261/1000, Training Loss: 38.95463, Validation Loss: 5.60932\n",
      "Epoch 262/1000, Training Loss: 38.96996, Validation Loss: 5.60785\n",
      "Epoch 263/1000, Training Loss: 39.11878, Validation Loss: 5.60646\n",
      "Epoch 264/1000, Training Loss: 39.09576, Validation Loss: 5.60504\n",
      "Epoch 265/1000, Training Loss: 38.79048, Validation Loss: 5.60348\n",
      "Epoch 266/1000, Training Loss: 38.90161, Validation Loss: 5.60200\n",
      "Epoch 267/1000, Training Loss: 38.93505, Validation Loss: 5.60052\n",
      "Epoch 268/1000, Training Loss: 39.03645, Validation Loss: 5.59911\n",
      "Epoch 269/1000, Training Loss: 39.22337, Validation Loss: 5.59782\n",
      "Epoch 270/1000, Training Loss: 38.98499, Validation Loss: 5.59640\n",
      "Epoch 271/1000, Training Loss: 38.79867, Validation Loss: 5.59496\n",
      "Epoch 272/1000, Training Loss: 38.78708, Validation Loss: 5.59348\n",
      "Epoch 273/1000, Training Loss: 39.10796, Validation Loss: 5.59204\n",
      "Epoch 274/1000, Training Loss: 38.76512, Validation Loss: 5.59042\n",
      "Epoch 275/1000, Training Loss: 39.05826, Validation Loss: 5.58901\n",
      "Epoch 276/1000, Training Loss: 38.86338, Validation Loss: 5.58751\n",
      "Epoch 277/1000, Training Loss: 38.94323, Validation Loss: 5.58605\n",
      "Epoch 278/1000, Training Loss: 38.82747, Validation Loss: 5.58452\n",
      "Epoch 279/1000, Training Loss: 38.94130, Validation Loss: 5.58318\n",
      "Epoch 280/1000, Training Loss: 38.78382, Validation Loss: 5.58176\n",
      "Epoch 281/1000, Training Loss: 38.71207, Validation Loss: 5.58021\n",
      "Epoch 282/1000, Training Loss: 38.75201, Validation Loss: 5.57874\n",
      "Epoch 283/1000, Training Loss: 38.97512, Validation Loss: 5.57741\n",
      "Epoch 284/1000, Training Loss: 38.83250, Validation Loss: 5.57592\n",
      "Epoch 285/1000, Training Loss: 38.78159, Validation Loss: 5.57443\n",
      "Epoch 286/1000, Training Loss: 38.89396, Validation Loss: 5.57295\n",
      "Epoch 287/1000, Training Loss: 38.83024, Validation Loss: 5.57156\n",
      "Epoch 288/1000, Training Loss: 38.74605, Validation Loss: 5.57002\n",
      "Epoch 289/1000, Training Loss: 38.94602, Validation Loss: 5.56863\n",
      "Epoch 290/1000, Training Loss: 38.73796, Validation Loss: 5.56712\n",
      "Epoch 291/1000, Training Loss: 38.81762, Validation Loss: 5.56568\n",
      "Epoch 292/1000, Training Loss: 38.77838, Validation Loss: 5.56425\n",
      "Epoch 293/1000, Training Loss: 38.71913, Validation Loss: 5.56274\n",
      "Epoch 294/1000, Training Loss: 38.72877, Validation Loss: 5.56126\n",
      "Epoch 295/1000, Training Loss: 38.93051, Validation Loss: 5.55976\n",
      "Epoch 296/1000, Training Loss: 38.72833, Validation Loss: 5.55829\n",
      "Epoch 297/1000, Training Loss: 38.52928, Validation Loss: 5.55676\n",
      "Epoch 298/1000, Training Loss: 38.48578, Validation Loss: 5.55523\n",
      "Epoch 299/1000, Training Loss: 38.70039, Validation Loss: 5.55378\n",
      "Epoch 300/1000, Training Loss: 38.73448, Validation Loss: 5.55233\n",
      "Epoch 301/1000, Training Loss: 38.57225, Validation Loss: 5.55085\n",
      "Epoch 302/1000, Training Loss: 38.63246, Validation Loss: 5.54941\n",
      "Epoch 303/1000, Training Loss: 38.63166, Validation Loss: 5.54795\n",
      "Epoch 304/1000, Training Loss: 38.83093, Validation Loss: 5.54651\n",
      "Epoch 305/1000, Training Loss: 38.76510, Validation Loss: 5.54507\n",
      "Epoch 306/1000, Training Loss: 38.77794, Validation Loss: 5.54361\n",
      "Epoch 307/1000, Training Loss: 38.59622, Validation Loss: 5.54210\n",
      "Epoch 308/1000, Training Loss: 38.64642, Validation Loss: 5.54060\n",
      "Epoch 309/1000, Training Loss: 38.38827, Validation Loss: 5.53899\n",
      "Epoch 310/1000, Training Loss: 38.86239, Validation Loss: 5.53762\n",
      "Epoch 311/1000, Training Loss: 38.86968, Validation Loss: 5.53622\n",
      "Epoch 312/1000, Training Loss: 38.71543, Validation Loss: 5.53475\n",
      "Epoch 313/1000, Training Loss: 38.76934, Validation Loss: 5.53339\n",
      "Epoch 314/1000, Training Loss: 38.29835, Validation Loss: 5.53195\n",
      "Epoch 315/1000, Training Loss: 38.43417, Validation Loss: 5.53050\n",
      "Epoch 316/1000, Training Loss: 38.60142, Validation Loss: 5.52900\n",
      "Epoch 317/1000, Training Loss: 38.44924, Validation Loss: 5.52748\n",
      "Epoch 318/1000, Training Loss: 38.66002, Validation Loss: 5.52605\n",
      "Epoch 319/1000, Training Loss: 38.62997, Validation Loss: 5.52464\n",
      "Epoch 320/1000, Training Loss: 38.58121, Validation Loss: 5.52322\n",
      "Epoch 321/1000, Training Loss: 38.58955, Validation Loss: 5.52168\n",
      "Epoch 322/1000, Training Loss: 38.58878, Validation Loss: 5.52023\n",
      "Epoch 323/1000, Training Loss: 38.52661, Validation Loss: 5.51873\n",
      "Epoch 324/1000, Training Loss: 38.58605, Validation Loss: 5.51731\n",
      "Epoch 325/1000, Training Loss: 38.65107, Validation Loss: 5.51595\n",
      "Epoch 326/1000, Training Loss: 38.55045, Validation Loss: 5.51455\n",
      "Epoch 327/1000, Training Loss: 38.45673, Validation Loss: 5.51306\n",
      "Epoch 328/1000, Training Loss: 38.62122, Validation Loss: 5.51167\n",
      "Epoch 329/1000, Training Loss: 38.39125, Validation Loss: 5.51020\n",
      "Epoch 330/1000, Training Loss: 38.47182, Validation Loss: 5.50879\n",
      "Epoch 331/1000, Training Loss: 38.48862, Validation Loss: 5.50731\n",
      "Epoch 332/1000, Training Loss: 38.38152, Validation Loss: 5.50585\n",
      "Epoch 333/1000, Training Loss: 38.26797, Validation Loss: 5.50428\n",
      "Epoch 334/1000, Training Loss: 38.38310, Validation Loss: 5.50285\n",
      "Epoch 335/1000, Training Loss: 38.41207, Validation Loss: 5.50141\n",
      "Epoch 336/1000, Training Loss: 38.52676, Validation Loss: 5.49999\n",
      "Epoch 337/1000, Training Loss: 38.29958, Validation Loss: 5.49851\n",
      "Epoch 338/1000, Training Loss: 38.42095, Validation Loss: 5.49700\n",
      "Epoch 339/1000, Training Loss: 38.43613, Validation Loss: 5.49549\n",
      "Epoch 340/1000, Training Loss: 38.45387, Validation Loss: 5.49408\n",
      "Epoch 341/1000, Training Loss: 38.52575, Validation Loss: 5.49262\n",
      "Epoch 342/1000, Training Loss: 38.51443, Validation Loss: 5.49123\n",
      "Epoch 343/1000, Training Loss: 38.34031, Validation Loss: 5.48974\n",
      "Epoch 344/1000, Training Loss: 38.26120, Validation Loss: 5.48821\n",
      "Epoch 345/1000, Training Loss: 38.51236, Validation Loss: 5.48682\n",
      "Epoch 346/1000, Training Loss: 38.17262, Validation Loss: 5.48527\n",
      "Epoch 347/1000, Training Loss: 38.37878, Validation Loss: 5.48385\n",
      "Epoch 348/1000, Training Loss: 38.24964, Validation Loss: 5.48227\n",
      "Epoch 349/1000, Training Loss: 38.33625, Validation Loss: 5.48089\n",
      "Epoch 350/1000, Training Loss: 38.24566, Validation Loss: 5.47938\n",
      "Epoch 351/1000, Training Loss: 38.41751, Validation Loss: 5.47792\n",
      "Epoch 352/1000, Training Loss: 38.03228, Validation Loss: 5.47636\n",
      "Epoch 353/1000, Training Loss: 38.42452, Validation Loss: 5.47493\n",
      "Epoch 354/1000, Training Loss: 38.41926, Validation Loss: 5.47337\n",
      "Epoch 355/1000, Training Loss: 38.41000, Validation Loss: 5.47192\n",
      "Epoch 356/1000, Training Loss: 38.40423, Validation Loss: 5.47055\n",
      "Epoch 357/1000, Training Loss: 38.17699, Validation Loss: 5.46916\n",
      "Epoch 358/1000, Training Loss: 38.23854, Validation Loss: 5.46767\n",
      "Epoch 359/1000, Training Loss: 38.28657, Validation Loss: 5.46614\n",
      "Epoch 360/1000, Training Loss: 38.06649, Validation Loss: 5.46457\n",
      "Epoch 361/1000, Training Loss: 38.43226, Validation Loss: 5.46309\n",
      "Epoch 362/1000, Training Loss: 38.36683, Validation Loss: 5.46165\n",
      "Epoch 363/1000, Training Loss: 38.45003, Validation Loss: 5.46027\n",
      "Epoch 364/1000, Training Loss: 38.47452, Validation Loss: 5.45889\n",
      "Epoch 365/1000, Training Loss: 38.43407, Validation Loss: 5.45749\n",
      "Epoch 366/1000, Training Loss: 38.36540, Validation Loss: 5.45612\n",
      "Epoch 367/1000, Training Loss: 38.30120, Validation Loss: 5.45471\n",
      "Epoch 368/1000, Training Loss: 38.40704, Validation Loss: 5.45330\n",
      "Epoch 369/1000, Training Loss: 38.29497, Validation Loss: 5.45185\n",
      "Epoch 370/1000, Training Loss: 38.07281, Validation Loss: 5.45036\n",
      "Epoch 371/1000, Training Loss: 38.18623, Validation Loss: 5.44886\n",
      "Epoch 372/1000, Training Loss: 38.33165, Validation Loss: 5.44744\n",
      "Epoch 373/1000, Training Loss: 38.35511, Validation Loss: 5.44605\n",
      "Epoch 374/1000, Training Loss: 38.18796, Validation Loss: 5.44457\n",
      "Epoch 375/1000, Training Loss: 38.17886, Validation Loss: 5.44308\n",
      "Epoch 376/1000, Training Loss: 38.20238, Validation Loss: 5.44166\n",
      "Epoch 377/1000, Training Loss: 37.87817, Validation Loss: 5.44009\n",
      "Epoch 378/1000, Training Loss: 38.20347, Validation Loss: 5.43871\n",
      "Epoch 379/1000, Training Loss: 38.33608, Validation Loss: 5.43735\n",
      "Epoch 380/1000, Training Loss: 38.15689, Validation Loss: 5.43592\n",
      "Epoch 381/1000, Training Loss: 38.21962, Validation Loss: 5.43450\n",
      "Epoch 382/1000, Training Loss: 38.13901, Validation Loss: 5.43304\n",
      "Epoch 383/1000, Training Loss: 38.36144, Validation Loss: 5.43165\n",
      "Epoch 384/1000, Training Loss: 38.18510, Validation Loss: 5.43012\n",
      "Epoch 385/1000, Training Loss: 38.25783, Validation Loss: 5.42873\n",
      "Epoch 386/1000, Training Loss: 38.08242, Validation Loss: 5.42723\n",
      "Epoch 387/1000, Training Loss: 38.17761, Validation Loss: 5.42579\n",
      "Epoch 388/1000, Training Loss: 37.84214, Validation Loss: 5.42422\n",
      "Epoch 389/1000, Training Loss: 38.11883, Validation Loss: 5.42278\n",
      "Epoch 390/1000, Training Loss: 38.19640, Validation Loss: 5.42140\n",
      "Epoch 391/1000, Training Loss: 38.15097, Validation Loss: 5.42003\n",
      "Epoch 392/1000, Training Loss: 38.12423, Validation Loss: 5.41861\n",
      "Epoch 393/1000, Training Loss: 38.27987, Validation Loss: 5.41723\n",
      "Epoch 394/1000, Training Loss: 38.06599, Validation Loss: 5.41581\n",
      "Epoch 395/1000, Training Loss: 38.06537, Validation Loss: 5.41438\n",
      "Epoch 396/1000, Training Loss: 38.18967, Validation Loss: 5.41292\n",
      "Epoch 397/1000, Training Loss: 37.96469, Validation Loss: 5.41142\n",
      "Epoch 398/1000, Training Loss: 38.25579, Validation Loss: 5.41008\n",
      "Epoch 399/1000, Training Loss: 38.12193, Validation Loss: 5.40872\n",
      "Epoch 400/1000, Training Loss: 38.13305, Validation Loss: 5.40726\n",
      "Epoch 401/1000, Training Loss: 38.23836, Validation Loss: 5.40581\n",
      "Epoch 402/1000, Training Loss: 37.96832, Validation Loss: 5.40437\n",
      "Epoch 403/1000, Training Loss: 38.00176, Validation Loss: 5.40289\n",
      "Epoch 404/1000, Training Loss: 37.76033, Validation Loss: 5.40136\n",
      "Epoch 405/1000, Training Loss: 38.01851, Validation Loss: 5.39991\n",
      "Epoch 406/1000, Training Loss: 37.82556, Validation Loss: 5.39843\n",
      "Epoch 407/1000, Training Loss: 38.00486, Validation Loss: 5.39697\n",
      "Epoch 408/1000, Training Loss: 38.16687, Validation Loss: 5.39559\n",
      "Epoch 409/1000, Training Loss: 38.00791, Validation Loss: 5.39414\n",
      "Epoch 410/1000, Training Loss: 38.15085, Validation Loss: 5.39268\n",
      "Epoch 411/1000, Training Loss: 38.00537, Validation Loss: 5.39126\n",
      "Epoch 412/1000, Training Loss: 37.83199, Validation Loss: 5.38971\n",
      "Epoch 413/1000, Training Loss: 37.97608, Validation Loss: 5.38826\n",
      "Epoch 414/1000, Training Loss: 37.62489, Validation Loss: 5.38669\n",
      "Epoch 415/1000, Training Loss: 38.00873, Validation Loss: 5.38531\n",
      "Epoch 416/1000, Training Loss: 37.95160, Validation Loss: 5.38391\n",
      "Epoch 417/1000, Training Loss: 37.90666, Validation Loss: 5.38247\n",
      "Epoch 418/1000, Training Loss: 37.88276, Validation Loss: 5.38100\n",
      "Epoch 419/1000, Training Loss: 38.01833, Validation Loss: 5.37961\n",
      "Epoch 420/1000, Training Loss: 37.78498, Validation Loss: 5.37816\n",
      "Epoch 421/1000, Training Loss: 37.98534, Validation Loss: 5.37674\n",
      "Epoch 422/1000, Training Loss: 37.97414, Validation Loss: 5.37527\n",
      "Epoch 423/1000, Training Loss: 37.83629, Validation Loss: 5.37376\n",
      "Epoch 424/1000, Training Loss: 37.63759, Validation Loss: 5.37217\n",
      "Epoch 425/1000, Training Loss: 37.96762, Validation Loss: 5.37075\n",
      "Epoch 426/1000, Training Loss: 37.68942, Validation Loss: 5.36933\n",
      "Epoch 427/1000, Training Loss: 37.73701, Validation Loss: 5.36784\n",
      "Epoch 428/1000, Training Loss: 37.73490, Validation Loss: 5.36641\n",
      "Epoch 429/1000, Training Loss: 37.97387, Validation Loss: 5.36490\n",
      "Epoch 430/1000, Training Loss: 37.75620, Validation Loss: 5.36338\n",
      "Epoch 431/1000, Training Loss: 37.77598, Validation Loss: 5.36189\n",
      "Epoch 432/1000, Training Loss: 37.42716, Validation Loss: 5.36029\n",
      "Epoch 433/1000, Training Loss: 37.72366, Validation Loss: 5.35875\n",
      "Epoch 434/1000, Training Loss: 37.88985, Validation Loss: 5.35736\n",
      "Epoch 435/1000, Training Loss: 37.65031, Validation Loss: 5.35580\n",
      "Epoch 436/1000, Training Loss: 37.78623, Validation Loss: 5.35435\n",
      "Epoch 437/1000, Training Loss: 37.84058, Validation Loss: 5.35291\n",
      "Epoch 438/1000, Training Loss: 37.93064, Validation Loss: 5.35146\n",
      "Epoch 439/1000, Training Loss: 37.75029, Validation Loss: 5.34998\n",
      "Epoch 440/1000, Training Loss: 37.65551, Validation Loss: 5.34845\n",
      "Epoch 441/1000, Training Loss: 37.59323, Validation Loss: 5.34688\n",
      "Epoch 442/1000, Training Loss: 37.69986, Validation Loss: 5.34537\n",
      "Epoch 443/1000, Training Loss: 37.57405, Validation Loss: 5.34376\n",
      "Epoch 444/1000, Training Loss: 38.15709, Validation Loss: 5.34235\n",
      "Epoch 445/1000, Training Loss: 38.11698, Validation Loss: 5.34098\n",
      "Epoch 446/1000, Training Loss: 37.82358, Validation Loss: 5.33951\n",
      "Epoch 447/1000, Training Loss: 37.43384, Validation Loss: 5.33793\n",
      "Epoch 448/1000, Training Loss: 37.70575, Validation Loss: 5.33643\n",
      "Epoch 449/1000, Training Loss: 37.85030, Validation Loss: 5.33503\n",
      "Epoch 450/1000, Training Loss: 37.94796, Validation Loss: 5.33355\n",
      "Epoch 451/1000, Training Loss: 37.60363, Validation Loss: 5.33202\n",
      "Epoch 452/1000, Training Loss: 37.68606, Validation Loss: 5.33054\n",
      "Epoch 453/1000, Training Loss: 37.74936, Validation Loss: 5.32914\n",
      "Epoch 454/1000, Training Loss: 37.61549, Validation Loss: 5.32766\n",
      "Epoch 455/1000, Training Loss: 38.01770, Validation Loss: 5.32630\n",
      "Epoch 456/1000, Training Loss: 37.67719, Validation Loss: 5.32484\n",
      "Epoch 457/1000, Training Loss: 37.78211, Validation Loss: 5.32343\n",
      "Epoch 458/1000, Training Loss: 37.58574, Validation Loss: 5.32191\n",
      "Epoch 459/1000, Training Loss: 37.57715, Validation Loss: 5.32040\n",
      "Epoch 460/1000, Training Loss: 37.61912, Validation Loss: 5.31899\n",
      "Epoch 461/1000, Training Loss: 37.34521, Validation Loss: 5.31742\n",
      "Epoch 462/1000, Training Loss: 37.66609, Validation Loss: 5.31595\n",
      "Epoch 463/1000, Training Loss: 37.90096, Validation Loss: 5.31459\n",
      "Epoch 464/1000, Training Loss: 37.55999, Validation Loss: 5.31311\n",
      "Epoch 465/1000, Training Loss: 37.66592, Validation Loss: 5.31164\n",
      "Epoch 466/1000, Training Loss: 37.56568, Validation Loss: 5.31021\n",
      "Epoch 467/1000, Training Loss: 37.78608, Validation Loss: 5.30887\n",
      "Epoch 468/1000, Training Loss: 37.38616, Validation Loss: 5.30737\n",
      "Epoch 469/1000, Training Loss: 37.62302, Validation Loss: 5.30587\n",
      "Epoch 470/1000, Training Loss: 37.83661, Validation Loss: 5.30444\n",
      "Epoch 471/1000, Training Loss: 37.61458, Validation Loss: 5.30299\n",
      "Epoch 472/1000, Training Loss: 37.50865, Validation Loss: 5.30147\n",
      "Epoch 473/1000, Training Loss: 37.61463, Validation Loss: 5.30003\n",
      "Epoch 474/1000, Training Loss: 37.79260, Validation Loss: 5.29858\n",
      "Epoch 475/1000, Training Loss: 37.63304, Validation Loss: 5.29717\n",
      "Epoch 476/1000, Training Loss: 37.50973, Validation Loss: 5.29568\n",
      "Epoch 477/1000, Training Loss: 37.37266, Validation Loss: 5.29420\n",
      "Epoch 478/1000, Training Loss: 37.19407, Validation Loss: 5.29268\n",
      "Epoch 479/1000, Training Loss: 37.25779, Validation Loss: 5.29114\n",
      "Epoch 480/1000, Training Loss: 37.24410, Validation Loss: 5.28965\n",
      "Epoch 481/1000, Training Loss: 37.21411, Validation Loss: 5.28804\n",
      "Epoch 482/1000, Training Loss: 37.29217, Validation Loss: 5.28652\n",
      "Epoch 483/1000, Training Loss: 37.58459, Validation Loss: 5.28507\n",
      "Epoch 484/1000, Training Loss: 37.43822, Validation Loss: 5.28351\n",
      "Epoch 485/1000, Training Loss: 37.16478, Validation Loss: 5.28192\n",
      "Epoch 486/1000, Training Loss: 37.81107, Validation Loss: 5.28048\n",
      "Epoch 487/1000, Training Loss: 37.39711, Validation Loss: 5.27898\n",
      "Epoch 488/1000, Training Loss: 37.39457, Validation Loss: 5.27746\n",
      "Epoch 489/1000, Training Loss: 37.63586, Validation Loss: 5.27600\n",
      "Epoch 490/1000, Training Loss: 37.26038, Validation Loss: 5.27448\n",
      "Epoch 491/1000, Training Loss: 37.39631, Validation Loss: 5.27300\n",
      "Epoch 492/1000, Training Loss: 37.50383, Validation Loss: 5.27151\n",
      "Epoch 493/1000, Training Loss: 37.42432, Validation Loss: 5.27001\n",
      "Epoch 494/1000, Training Loss: 37.15111, Validation Loss: 5.26852\n",
      "Epoch 495/1000, Training Loss: 37.47919, Validation Loss: 5.26699\n",
      "Epoch 496/1000, Training Loss: 37.39345, Validation Loss: 5.26550\n",
      "Epoch 497/1000, Training Loss: 37.40482, Validation Loss: 5.26400\n",
      "Epoch 498/1000, Training Loss: 37.62322, Validation Loss: 5.26260\n",
      "Epoch 499/1000, Training Loss: 37.26103, Validation Loss: 5.26111\n",
      "Epoch 500/1000, Training Loss: 37.22316, Validation Loss: 5.25954\n",
      "Epoch 501/1000, Training Loss: 37.39709, Validation Loss: 5.25800\n",
      "Epoch 502/1000, Training Loss: 37.36565, Validation Loss: 5.25657\n",
      "Epoch 503/1000, Training Loss: 37.35926, Validation Loss: 5.25513\n",
      "Epoch 504/1000, Training Loss: 37.47845, Validation Loss: 5.25375\n",
      "Epoch 505/1000, Training Loss: 37.46126, Validation Loss: 5.25233\n",
      "Epoch 506/1000, Training Loss: 37.18224, Validation Loss: 5.25082\n",
      "Epoch 507/1000, Training Loss: 37.59201, Validation Loss: 5.24946\n",
      "Epoch 508/1000, Training Loss: 37.10805, Validation Loss: 5.24794\n",
      "Epoch 509/1000, Training Loss: 37.09667, Validation Loss: 5.24641\n",
      "Epoch 510/1000, Training Loss: 37.22796, Validation Loss: 5.24483\n",
      "Epoch 511/1000, Training Loss: 37.07394, Validation Loss: 5.24329\n",
      "Epoch 512/1000, Training Loss: 37.28231, Validation Loss: 5.24187\n",
      "Epoch 513/1000, Training Loss: 37.05630, Validation Loss: 5.24024\n",
      "Epoch 514/1000, Training Loss: 37.50994, Validation Loss: 5.23877\n",
      "Epoch 515/1000, Training Loss: 37.19794, Validation Loss: 5.23725\n",
      "Epoch 516/1000, Training Loss: 37.40517, Validation Loss: 5.23583\n",
      "Epoch 517/1000, Training Loss: 37.30778, Validation Loss: 5.23443\n",
      "Epoch 518/1000, Training Loss: 37.20341, Validation Loss: 5.23290\n",
      "Epoch 519/1000, Training Loss: 37.13364, Validation Loss: 5.23145\n",
      "Epoch 520/1000, Training Loss: 37.36773, Validation Loss: 5.22996\n",
      "Epoch 521/1000, Training Loss: 36.91090, Validation Loss: 5.22838\n",
      "Epoch 522/1000, Training Loss: 37.07763, Validation Loss: 5.22680\n",
      "Epoch 523/1000, Training Loss: 37.21338, Validation Loss: 5.22531\n",
      "Epoch 524/1000, Training Loss: 37.60933, Validation Loss: 5.22399\n",
      "Epoch 525/1000, Training Loss: 37.44750, Validation Loss: 5.22259\n",
      "Epoch 526/1000, Training Loss: 36.86196, Validation Loss: 5.22100\n",
      "Epoch 527/1000, Training Loss: 37.04775, Validation Loss: 5.21945\n",
      "Epoch 528/1000, Training Loss: 37.19024, Validation Loss: 5.21787\n",
      "Epoch 529/1000, Training Loss: 37.19675, Validation Loss: 5.21641\n",
      "Epoch 530/1000, Training Loss: 37.05863, Validation Loss: 5.21493\n",
      "Epoch 531/1000, Training Loss: 37.20340, Validation Loss: 5.21348\n",
      "Epoch 532/1000, Training Loss: 37.17005, Validation Loss: 5.21196\n",
      "Epoch 533/1000, Training Loss: 37.23879, Validation Loss: 5.21055\n",
      "Epoch 534/1000, Training Loss: 37.22304, Validation Loss: 5.20910\n",
      "Epoch 535/1000, Training Loss: 37.06419, Validation Loss: 5.20763\n",
      "Epoch 536/1000, Training Loss: 37.36907, Validation Loss: 5.20623\n",
      "Epoch 537/1000, Training Loss: 37.41984, Validation Loss: 5.20480\n",
      "Epoch 538/1000, Training Loss: 37.09041, Validation Loss: 5.20324\n",
      "Epoch 539/1000, Training Loss: 36.86957, Validation Loss: 5.20164\n",
      "Epoch 540/1000, Training Loss: 36.95659, Validation Loss: 5.20011\n",
      "Epoch 541/1000, Training Loss: 36.94850, Validation Loss: 5.19851\n",
      "Epoch 542/1000, Training Loss: 36.74302, Validation Loss: 5.19689\n",
      "Epoch 543/1000, Training Loss: 36.96464, Validation Loss: 5.19538\n",
      "Epoch 544/1000, Training Loss: 37.02919, Validation Loss: 5.19391\n",
      "Epoch 545/1000, Training Loss: 36.80852, Validation Loss: 5.19229\n",
      "Epoch 546/1000, Training Loss: 37.10048, Validation Loss: 5.19078\n",
      "Epoch 547/1000, Training Loss: 36.85289, Validation Loss: 5.18923\n",
      "Epoch 548/1000, Training Loss: 36.95344, Validation Loss: 5.18775\n",
      "Epoch 549/1000, Training Loss: 36.95489, Validation Loss: 5.18619\n",
      "Epoch 550/1000, Training Loss: 36.90986, Validation Loss: 5.18472\n",
      "Epoch 551/1000, Training Loss: 37.17547, Validation Loss: 5.18329\n",
      "Epoch 552/1000, Training Loss: 36.91604, Validation Loss: 5.18174\n",
      "Epoch 553/1000, Training Loss: 37.31441, Validation Loss: 5.18033\n",
      "Epoch 554/1000, Training Loss: 36.98344, Validation Loss: 5.17885\n",
      "Epoch 555/1000, Training Loss: 37.32085, Validation Loss: 5.17738\n",
      "Epoch 556/1000, Training Loss: 36.71798, Validation Loss: 5.17573\n",
      "Epoch 557/1000, Training Loss: 36.92350, Validation Loss: 5.17423\n",
      "Epoch 558/1000, Training Loss: 36.94017, Validation Loss: 5.17275\n",
      "Epoch 559/1000, Training Loss: 36.65102, Validation Loss: 5.17121\n",
      "Epoch 560/1000, Training Loss: 37.06240, Validation Loss: 5.16974\n",
      "Epoch 561/1000, Training Loss: 37.23477, Validation Loss: 5.16832\n",
      "Epoch 562/1000, Training Loss: 36.90221, Validation Loss: 5.16677\n",
      "Epoch 563/1000, Training Loss: 36.84589, Validation Loss: 5.16522\n",
      "Epoch 564/1000, Training Loss: 36.89161, Validation Loss: 5.16375\n",
      "Epoch 565/1000, Training Loss: 37.31346, Validation Loss: 5.16240\n",
      "Epoch 566/1000, Training Loss: 37.00437, Validation Loss: 5.16091\n",
      "Epoch 567/1000, Training Loss: 36.70878, Validation Loss: 5.15929\n",
      "Epoch 568/1000, Training Loss: 37.06892, Validation Loss: 5.15785\n",
      "Epoch 569/1000, Training Loss: 36.81715, Validation Loss: 5.15629\n",
      "Epoch 570/1000, Training Loss: 36.73767, Validation Loss: 5.15471\n",
      "Epoch 571/1000, Training Loss: 36.79874, Validation Loss: 5.15322\n",
      "Epoch 572/1000, Training Loss: 36.75722, Validation Loss: 5.15167\n",
      "Epoch 573/1000, Training Loss: 37.06744, Validation Loss: 5.15028\n",
      "Epoch 574/1000, Training Loss: 36.73272, Validation Loss: 5.14867\n",
      "Epoch 575/1000, Training Loss: 36.92638, Validation Loss: 5.14726\n",
      "Epoch 576/1000, Training Loss: 36.49420, Validation Loss: 5.14558\n",
      "Epoch 577/1000, Training Loss: 36.44898, Validation Loss: 5.14390\n",
      "Epoch 578/1000, Training Loss: 36.66822, Validation Loss: 5.14238\n",
      "Epoch 579/1000, Training Loss: 36.90745, Validation Loss: 5.14083\n",
      "Epoch 580/1000, Training Loss: 36.60737, Validation Loss: 5.13925\n",
      "Epoch 581/1000, Training Loss: 36.81842, Validation Loss: 5.13768\n",
      "Epoch 582/1000, Training Loss: 37.09786, Validation Loss: 5.13626\n",
      "Epoch 583/1000, Training Loss: 36.51748, Validation Loss: 5.13465\n",
      "Epoch 584/1000, Training Loss: 36.80608, Validation Loss: 5.13321\n",
      "Epoch 585/1000, Training Loss: 36.91346, Validation Loss: 5.13173\n",
      "Epoch 586/1000, Training Loss: 36.64894, Validation Loss: 5.13007\n",
      "Epoch 587/1000, Training Loss: 36.71455, Validation Loss: 5.12849\n",
      "Epoch 588/1000, Training Loss: 36.96961, Validation Loss: 5.12697\n",
      "Epoch 589/1000, Training Loss: 36.55088, Validation Loss: 5.12540\n",
      "Epoch 590/1000, Training Loss: 36.57999, Validation Loss: 5.12381\n",
      "Epoch 591/1000, Training Loss: 36.56596, Validation Loss: 5.12222\n",
      "Epoch 592/1000, Training Loss: 36.74964, Validation Loss: 5.12066\n",
      "Epoch 593/1000, Training Loss: 36.67582, Validation Loss: 5.11907\n",
      "Epoch 594/1000, Training Loss: 36.76137, Validation Loss: 5.11759\n",
      "Epoch 595/1000, Training Loss: 36.41536, Validation Loss: 5.11604\n",
      "Epoch 596/1000, Training Loss: 36.49004, Validation Loss: 5.11451\n",
      "Epoch 597/1000, Training Loss: 36.26636, Validation Loss: 5.11281\n",
      "Epoch 598/1000, Training Loss: 36.74325, Validation Loss: 5.11122\n",
      "Epoch 599/1000, Training Loss: 36.55993, Validation Loss: 5.10968\n",
      "Epoch 600/1000, Training Loss: 37.10638, Validation Loss: 5.10839\n",
      "Epoch 601/1000, Training Loss: 36.63275, Validation Loss: 5.10694\n",
      "Epoch 602/1000, Training Loss: 36.69820, Validation Loss: 5.10540\n",
      "Epoch 603/1000, Training Loss: 36.36434, Validation Loss: 5.10367\n",
      "Epoch 604/1000, Training Loss: 36.63424, Validation Loss: 5.10213\n",
      "Epoch 605/1000, Training Loss: 36.63790, Validation Loss: 5.10060\n",
      "Epoch 606/1000, Training Loss: 36.54294, Validation Loss: 5.09905\n",
      "Epoch 607/1000, Training Loss: 36.75470, Validation Loss: 5.09752\n",
      "Epoch 608/1000, Training Loss: 36.69779, Validation Loss: 5.09605\n",
      "Epoch 609/1000, Training Loss: 36.67658, Validation Loss: 5.09450\n",
      "Epoch 610/1000, Training Loss: 36.49985, Validation Loss: 5.09297\n",
      "Epoch 611/1000, Training Loss: 36.73592, Validation Loss: 5.09152\n",
      "Epoch 612/1000, Training Loss: 36.05852, Validation Loss: 5.08982\n",
      "Epoch 613/1000, Training Loss: 36.41420, Validation Loss: 5.08826\n",
      "Epoch 614/1000, Training Loss: 36.84826, Validation Loss: 5.08686\n",
      "Epoch 615/1000, Training Loss: 36.57108, Validation Loss: 5.08541\n",
      "Epoch 616/1000, Training Loss: 36.89793, Validation Loss: 5.08398\n",
      "Epoch 617/1000, Training Loss: 36.36262, Validation Loss: 5.08246\n",
      "Epoch 618/1000, Training Loss: 36.61578, Validation Loss: 5.08096\n",
      "Epoch 619/1000, Training Loss: 36.47575, Validation Loss: 5.07935\n",
      "Epoch 620/1000, Training Loss: 36.27141, Validation Loss: 5.07783\n",
      "Epoch 621/1000, Training Loss: 36.44095, Validation Loss: 5.07634\n",
      "Epoch 622/1000, Training Loss: 36.17163, Validation Loss: 5.07469\n",
      "Epoch 623/1000, Training Loss: 36.66815, Validation Loss: 5.07320\n",
      "Epoch 624/1000, Training Loss: 36.60133, Validation Loss: 5.07165\n",
      "Epoch 625/1000, Training Loss: 36.42765, Validation Loss: 5.07007\n",
      "Epoch 626/1000, Training Loss: 36.26290, Validation Loss: 5.06846\n",
      "Epoch 627/1000, Training Loss: 36.27499, Validation Loss: 5.06688\n",
      "Epoch 628/1000, Training Loss: 36.63826, Validation Loss: 5.06542\n",
      "Epoch 629/1000, Training Loss: 36.77177, Validation Loss: 5.06399\n",
      "Epoch 630/1000, Training Loss: 36.35897, Validation Loss: 5.06248\n",
      "Epoch 631/1000, Training Loss: 36.33842, Validation Loss: 5.06098\n",
      "Epoch 632/1000, Training Loss: 36.52718, Validation Loss: 5.05962\n",
      "Epoch 633/1000, Training Loss: 36.21386, Validation Loss: 5.05808\n",
      "Epoch 634/1000, Training Loss: 36.39611, Validation Loss: 5.05650\n",
      "Epoch 635/1000, Training Loss: 36.46168, Validation Loss: 5.05500\n",
      "Epoch 636/1000, Training Loss: 36.42187, Validation Loss: 5.05356\n",
      "Epoch 637/1000, Training Loss: 36.27382, Validation Loss: 5.05186\n",
      "Epoch 638/1000, Training Loss: 36.26908, Validation Loss: 5.05029\n",
      "Epoch 639/1000, Training Loss: 36.68322, Validation Loss: 5.04882\n",
      "Epoch 640/1000, Training Loss: 36.39658, Validation Loss: 5.04734\n",
      "Epoch 641/1000, Training Loss: 36.21652, Validation Loss: 5.04581\n",
      "Epoch 642/1000, Training Loss: 36.10364, Validation Loss: 5.04421\n",
      "Epoch 643/1000, Training Loss: 36.20765, Validation Loss: 5.04265\n",
      "Epoch 644/1000, Training Loss: 36.57982, Validation Loss: 5.04124\n",
      "Epoch 645/1000, Training Loss: 36.15833, Validation Loss: 5.03963\n",
      "Epoch 646/1000, Training Loss: 36.52598, Validation Loss: 5.03825\n",
      "Epoch 647/1000, Training Loss: 36.27753, Validation Loss: 5.03669\n",
      "Epoch 648/1000, Training Loss: 36.74782, Validation Loss: 5.03534\n",
      "Epoch 649/1000, Training Loss: 36.46119, Validation Loss: 5.03392\n",
      "Epoch 650/1000, Training Loss: 36.02760, Validation Loss: 5.03230\n",
      "Epoch 651/1000, Training Loss: 36.11815, Validation Loss: 5.03071\n",
      "Epoch 652/1000, Training Loss: 36.01017, Validation Loss: 5.02908\n",
      "Epoch 653/1000, Training Loss: 36.39166, Validation Loss: 5.02751\n",
      "Epoch 654/1000, Training Loss: 35.97369, Validation Loss: 5.02585\n",
      "Epoch 655/1000, Training Loss: 36.37578, Validation Loss: 5.02439\n",
      "Epoch 656/1000, Training Loss: 36.08007, Validation Loss: 5.02276\n",
      "Epoch 657/1000, Training Loss: 35.87800, Validation Loss: 5.02119\n",
      "Epoch 658/1000, Training Loss: 36.57210, Validation Loss: 5.01976\n",
      "Epoch 659/1000, Training Loss: 36.08791, Validation Loss: 5.01816\n",
      "Epoch 660/1000, Training Loss: 35.82765, Validation Loss: 5.01649\n",
      "Epoch 661/1000, Training Loss: 36.17458, Validation Loss: 5.01486\n",
      "Epoch 662/1000, Training Loss: 36.64079, Validation Loss: 5.01350\n",
      "Epoch 663/1000, Training Loss: 35.59043, Validation Loss: 5.01177\n",
      "Epoch 664/1000, Training Loss: 36.10075, Validation Loss: 5.01021\n",
      "Epoch 665/1000, Training Loss: 36.01594, Validation Loss: 5.00857\n",
      "Epoch 666/1000, Training Loss: 36.27523, Validation Loss: 5.00711\n",
      "Epoch 667/1000, Training Loss: 36.15683, Validation Loss: 5.00553\n",
      "Epoch 668/1000, Training Loss: 35.97005, Validation Loss: 5.00402\n",
      "Epoch 669/1000, Training Loss: 36.04330, Validation Loss: 5.00244\n",
      "Epoch 670/1000, Training Loss: 35.85002, Validation Loss: 5.00072\n",
      "Epoch 671/1000, Training Loss: 36.09733, Validation Loss: 4.99923\n",
      "Epoch 672/1000, Training Loss: 36.18495, Validation Loss: 4.99769\n",
      "Epoch 673/1000, Training Loss: 36.39239, Validation Loss: 4.99622\n",
      "Epoch 674/1000, Training Loss: 36.01052, Validation Loss: 4.99459\n",
      "Epoch 675/1000, Training Loss: 36.33476, Validation Loss: 4.99312\n",
      "Epoch 676/1000, Training Loss: 36.08421, Validation Loss: 4.99157\n",
      "Epoch 677/1000, Training Loss: 36.13405, Validation Loss: 4.99010\n",
      "Epoch 678/1000, Training Loss: 36.29783, Validation Loss: 4.98867\n",
      "Epoch 679/1000, Training Loss: 35.76600, Validation Loss: 4.98706\n",
      "Epoch 680/1000, Training Loss: 36.29747, Validation Loss: 4.98558\n",
      "Epoch 681/1000, Training Loss: 36.03444, Validation Loss: 4.98414\n",
      "Epoch 682/1000, Training Loss: 36.02509, Validation Loss: 4.98268\n",
      "Epoch 683/1000, Training Loss: 36.67755, Validation Loss: 4.98141\n",
      "Epoch 684/1000, Training Loss: 35.94785, Validation Loss: 4.97993\n",
      "Epoch 685/1000, Training Loss: 36.27598, Validation Loss: 4.97849\n",
      "Epoch 686/1000, Training Loss: 36.22581, Validation Loss: 4.97703\n",
      "Epoch 687/1000, Training Loss: 35.82609, Validation Loss: 4.97542\n",
      "Epoch 688/1000, Training Loss: 36.02196, Validation Loss: 4.97398\n",
      "Epoch 689/1000, Training Loss: 36.18000, Validation Loss: 4.97254\n",
      "Epoch 690/1000, Training Loss: 36.23223, Validation Loss: 4.97113\n",
      "Epoch 691/1000, Training Loss: 36.01187, Validation Loss: 4.96955\n",
      "Epoch 692/1000, Training Loss: 35.90630, Validation Loss: 4.96797\n",
      "Epoch 693/1000, Training Loss: 35.69536, Validation Loss: 4.96633\n",
      "Epoch 694/1000, Training Loss: 35.36396, Validation Loss: 4.96465\n",
      "Epoch 695/1000, Training Loss: 35.94522, Validation Loss: 4.96311\n",
      "Epoch 696/1000, Training Loss: 35.55364, Validation Loss: 4.96149\n",
      "Epoch 697/1000, Training Loss: 35.80072, Validation Loss: 4.95991\n",
      "Epoch 698/1000, Training Loss: 36.03537, Validation Loss: 4.95842\n",
      "Epoch 699/1000, Training Loss: 35.88286, Validation Loss: 4.95690\n",
      "Epoch 700/1000, Training Loss: 36.09298, Validation Loss: 4.95545\n",
      "Epoch 701/1000, Training Loss: 36.04987, Validation Loss: 4.95394\n",
      "Epoch 702/1000, Training Loss: 35.77897, Validation Loss: 4.95236\n",
      "Epoch 703/1000, Training Loss: 36.02567, Validation Loss: 4.95086\n",
      "Epoch 704/1000, Training Loss: 36.15367, Validation Loss: 4.94947\n",
      "Epoch 705/1000, Training Loss: 35.36381, Validation Loss: 4.94776\n",
      "Epoch 706/1000, Training Loss: 35.81945, Validation Loss: 4.94627\n",
      "Epoch 707/1000, Training Loss: 36.25047, Validation Loss: 4.94481\n",
      "Epoch 708/1000, Training Loss: 35.76308, Validation Loss: 4.94322\n",
      "Epoch 709/1000, Training Loss: 36.16309, Validation Loss: 4.94179\n",
      "Epoch 710/1000, Training Loss: 35.64537, Validation Loss: 4.94013\n",
      "Epoch 711/1000, Training Loss: 35.79777, Validation Loss: 4.93853\n",
      "Epoch 712/1000, Training Loss: 35.73768, Validation Loss: 4.93701\n",
      "Epoch 713/1000, Training Loss: 35.68440, Validation Loss: 4.93539\n",
      "Epoch 714/1000, Training Loss: 35.69537, Validation Loss: 4.93389\n",
      "Epoch 715/1000, Training Loss: 36.02344, Validation Loss: 4.93249\n",
      "Epoch 716/1000, Training Loss: 36.05566, Validation Loss: 4.93103\n",
      "Epoch 717/1000, Training Loss: 35.89005, Validation Loss: 4.92955\n",
      "Epoch 718/1000, Training Loss: 35.47725, Validation Loss: 4.92792\n",
      "Epoch 719/1000, Training Loss: 35.50110, Validation Loss: 4.92633\n",
      "Epoch 720/1000, Training Loss: 35.75861, Validation Loss: 4.92479\n",
      "Epoch 721/1000, Training Loss: 35.61843, Validation Loss: 4.92328\n",
      "Epoch 722/1000, Training Loss: 35.56599, Validation Loss: 4.92170\n",
      "Epoch 723/1000, Training Loss: 35.75556, Validation Loss: 4.92017\n",
      "Epoch 724/1000, Training Loss: 35.40535, Validation Loss: 4.91850\n",
      "Epoch 725/1000, Training Loss: 35.82303, Validation Loss: 4.91708\n",
      "Epoch 726/1000, Training Loss: 35.65276, Validation Loss: 4.91546\n",
      "Epoch 727/1000, Training Loss: 35.43655, Validation Loss: 4.91377\n",
      "Epoch 728/1000, Training Loss: 35.53602, Validation Loss: 4.91216\n",
      "Epoch 729/1000, Training Loss: 35.64902, Validation Loss: 4.91067\n",
      "Epoch 730/1000, Training Loss: 35.87837, Validation Loss: 4.90921\n",
      "Epoch 731/1000, Training Loss: 35.68704, Validation Loss: 4.90769\n",
      "Epoch 732/1000, Training Loss: 35.57408, Validation Loss: 4.90612\n",
      "Epoch 733/1000, Training Loss: 35.56561, Validation Loss: 4.90457\n",
      "Epoch 734/1000, Training Loss: 36.24352, Validation Loss: 4.90319\n",
      "Epoch 735/1000, Training Loss: 35.63298, Validation Loss: 4.90167\n",
      "Epoch 736/1000, Training Loss: 35.96228, Validation Loss: 4.90024\n",
      "Epoch 737/1000, Training Loss: 35.94543, Validation Loss: 4.89878\n",
      "Epoch 738/1000, Training Loss: 35.90457, Validation Loss: 4.89735\n",
      "Epoch 739/1000, Training Loss: 35.81330, Validation Loss: 4.89584\n",
      "Epoch 740/1000, Training Loss: 35.42866, Validation Loss: 4.89423\n",
      "Epoch 741/1000, Training Loss: 35.42423, Validation Loss: 4.89267\n",
      "Epoch 742/1000, Training Loss: 35.49603, Validation Loss: 4.89113\n",
      "Epoch 743/1000, Training Loss: 35.77039, Validation Loss: 4.88958\n",
      "Epoch 744/1000, Training Loss: 35.60313, Validation Loss: 4.88808\n",
      "Epoch 745/1000, Training Loss: 35.62206, Validation Loss: 4.88657\n",
      "Epoch 746/1000, Training Loss: 35.29182, Validation Loss: 4.88505\n",
      "Epoch 747/1000, Training Loss: 35.53737, Validation Loss: 4.88355\n",
      "Epoch 748/1000, Training Loss: 35.75114, Validation Loss: 4.88210\n",
      "Epoch 749/1000, Training Loss: 35.28197, Validation Loss: 4.88046\n",
      "Epoch 750/1000, Training Loss: 35.63251, Validation Loss: 4.87896\n",
      "Epoch 751/1000, Training Loss: 35.50604, Validation Loss: 4.87736\n",
      "Epoch 752/1000, Training Loss: 35.77265, Validation Loss: 4.87599\n",
      "Epoch 753/1000, Training Loss: 35.75882, Validation Loss: 4.87453\n",
      "Epoch 754/1000, Training Loss: 35.39221, Validation Loss: 4.87293\n",
      "Epoch 755/1000, Training Loss: 35.41026, Validation Loss: 4.87138\n",
      "Epoch 756/1000, Training Loss: 35.52677, Validation Loss: 4.86992\n",
      "Epoch 757/1000, Training Loss: 35.21880, Validation Loss: 4.86829\n",
      "Epoch 758/1000, Training Loss: 35.12860, Validation Loss: 4.86673\n",
      "Epoch 759/1000, Training Loss: 35.63090, Validation Loss: 4.86515\n",
      "Epoch 760/1000, Training Loss: 35.52415, Validation Loss: 4.86376\n",
      "Epoch 761/1000, Training Loss: 35.14922, Validation Loss: 4.86210\n",
      "Epoch 762/1000, Training Loss: 35.38279, Validation Loss: 4.86059\n",
      "Epoch 763/1000, Training Loss: 35.62245, Validation Loss: 4.85914\n",
      "Epoch 764/1000, Training Loss: 35.41326, Validation Loss: 4.85756\n",
      "Epoch 765/1000, Training Loss: 34.90667, Validation Loss: 4.85587\n",
      "Epoch 766/1000, Training Loss: 35.81816, Validation Loss: 4.85444\n",
      "Epoch 767/1000, Training Loss: 35.67966, Validation Loss: 4.85310\n",
      "Epoch 768/1000, Training Loss: 35.42287, Validation Loss: 4.85158\n",
      "Epoch 769/1000, Training Loss: 35.56480, Validation Loss: 4.85015\n",
      "Epoch 770/1000, Training Loss: 35.39853, Validation Loss: 4.84861\n",
      "Epoch 771/1000, Training Loss: 35.31345, Validation Loss: 4.84705\n",
      "Epoch 772/1000, Training Loss: 35.23595, Validation Loss: 4.84541\n",
      "Epoch 773/1000, Training Loss: 35.22637, Validation Loss: 4.84382\n",
      "Epoch 774/1000, Training Loss: 35.45748, Validation Loss: 4.84241\n",
      "Epoch 775/1000, Training Loss: 35.30917, Validation Loss: 4.84090\n",
      "Epoch 776/1000, Training Loss: 35.53378, Validation Loss: 4.83941\n",
      "Epoch 777/1000, Training Loss: 35.34818, Validation Loss: 4.83785\n",
      "Epoch 778/1000, Training Loss: 35.64794, Validation Loss: 4.83642\n",
      "Epoch 779/1000, Training Loss: 35.45922, Validation Loss: 4.83494\n",
      "Epoch 780/1000, Training Loss: 34.90106, Validation Loss: 4.83334\n",
      "Epoch 781/1000, Training Loss: 35.23224, Validation Loss: 4.83189\n",
      "Epoch 782/1000, Training Loss: 35.46781, Validation Loss: 4.83045\n",
      "Epoch 783/1000, Training Loss: 34.96752, Validation Loss: 4.82881\n",
      "Epoch 784/1000, Training Loss: 35.37670, Validation Loss: 4.82728\n",
      "Epoch 785/1000, Training Loss: 35.39654, Validation Loss: 4.82580\n",
      "Epoch 786/1000, Training Loss: 35.52584, Validation Loss: 4.82437\n",
      "Epoch 787/1000, Training Loss: 35.52222, Validation Loss: 4.82289\n",
      "Epoch 788/1000, Training Loss: 35.19139, Validation Loss: 4.82139\n",
      "Epoch 789/1000, Training Loss: 35.53234, Validation Loss: 4.82002\n",
      "Epoch 790/1000, Training Loss: 35.32788, Validation Loss: 4.81851\n",
      "Epoch 791/1000, Training Loss: 34.96856, Validation Loss: 4.81685\n",
      "Epoch 792/1000, Training Loss: 34.75679, Validation Loss: 4.81516\n",
      "Epoch 793/1000, Training Loss: 35.14211, Validation Loss: 4.81354\n",
      "Epoch 794/1000, Training Loss: 35.21883, Validation Loss: 4.81209\n",
      "Epoch 795/1000, Training Loss: 35.16794, Validation Loss: 4.81057\n",
      "Epoch 796/1000, Training Loss: 35.29887, Validation Loss: 4.80910\n",
      "Epoch 797/1000, Training Loss: 35.15531, Validation Loss: 4.80759\n",
      "Epoch 798/1000, Training Loss: 35.24464, Validation Loss: 4.80614\n",
      "Epoch 799/1000, Training Loss: 35.31926, Validation Loss: 4.80467\n",
      "Epoch 800/1000, Training Loss: 35.05164, Validation Loss: 4.80315\n",
      "Epoch 801/1000, Training Loss: 34.94406, Validation Loss: 4.80157\n",
      "Epoch 802/1000, Training Loss: 34.91362, Validation Loss: 4.79995\n",
      "Epoch 803/1000, Training Loss: 34.96947, Validation Loss: 4.79829\n",
      "Epoch 804/1000, Training Loss: 34.92189, Validation Loss: 4.79667\n",
      "Epoch 805/1000, Training Loss: 35.43197, Validation Loss: 4.79526\n",
      "Epoch 806/1000, Training Loss: 34.91726, Validation Loss: 4.79369\n",
      "Epoch 807/1000, Training Loss: 35.40796, Validation Loss: 4.79223\n",
      "Epoch 808/1000, Training Loss: 34.69895, Validation Loss: 4.79059\n",
      "Epoch 809/1000, Training Loss: 35.28028, Validation Loss: 4.78905\n",
      "Epoch 810/1000, Training Loss: 34.91032, Validation Loss: 4.78753\n",
      "Epoch 811/1000, Training Loss: 35.46818, Validation Loss: 4.78604\n",
      "Epoch 812/1000, Training Loss: 35.37836, Validation Loss: 4.78469\n",
      "Epoch 813/1000, Training Loss: 34.86145, Validation Loss: 4.78318\n",
      "Epoch 814/1000, Training Loss: 34.74360, Validation Loss: 4.78159\n",
      "Epoch 815/1000, Training Loss: 35.50384, Validation Loss: 4.78024\n",
      "Epoch 816/1000, Training Loss: 35.13641, Validation Loss: 4.77878\n",
      "Epoch 817/1000, Training Loss: 34.64423, Validation Loss: 4.77722\n",
      "Epoch 818/1000, Training Loss: 35.61904, Validation Loss: 4.77592\n",
      "Epoch 819/1000, Training Loss: 35.27992, Validation Loss: 4.77448\n",
      "Epoch 820/1000, Training Loss: 34.88786, Validation Loss: 4.77292\n",
      "Epoch 821/1000, Training Loss: 34.87885, Validation Loss: 4.77142\n",
      "Epoch 822/1000, Training Loss: 34.66607, Validation Loss: 4.76988\n",
      "Epoch 823/1000, Training Loss: 34.94709, Validation Loss: 4.76838\n",
      "Epoch 824/1000, Training Loss: 34.97166, Validation Loss: 4.76684\n",
      "Epoch 825/1000, Training Loss: 35.11946, Validation Loss: 4.76538\n",
      "Epoch 826/1000, Training Loss: 34.96306, Validation Loss: 4.76389\n",
      "Epoch 827/1000, Training Loss: 34.62637, Validation Loss: 4.76222\n",
      "Epoch 828/1000, Training Loss: 34.61254, Validation Loss: 4.76060\n",
      "Epoch 829/1000, Training Loss: 34.99633, Validation Loss: 4.75907\n",
      "Epoch 830/1000, Training Loss: 35.10862, Validation Loss: 4.75753\n",
      "Epoch 831/1000, Training Loss: 34.83072, Validation Loss: 4.75604\n",
      "Epoch 832/1000, Training Loss: 34.31862, Validation Loss: 4.75441\n",
      "Epoch 833/1000, Training Loss: 34.86939, Validation Loss: 4.75292\n",
      "Epoch 834/1000, Training Loss: 34.83903, Validation Loss: 4.75144\n",
      "Epoch 835/1000, Training Loss: 35.23939, Validation Loss: 4.75011\n",
      "Epoch 836/1000, Training Loss: 34.63102, Validation Loss: 4.74845\n",
      "Epoch 837/1000, Training Loss: 35.24317, Validation Loss: 4.74709\n",
      "Epoch 838/1000, Training Loss: 35.23571, Validation Loss: 4.74566\n",
      "Epoch 839/1000, Training Loss: 34.84496, Validation Loss: 4.74418\n",
      "Epoch 840/1000, Training Loss: 34.88587, Validation Loss: 4.74273\n",
      "Epoch 841/1000, Training Loss: 35.18507, Validation Loss: 4.74134\n",
      "Epoch 842/1000, Training Loss: 34.98297, Validation Loss: 4.73990\n",
      "Epoch 843/1000, Training Loss: 35.08374, Validation Loss: 4.73846\n",
      "Epoch 844/1000, Training Loss: 34.95746, Validation Loss: 4.73704\n",
      "Epoch 845/1000, Training Loss: 34.63176, Validation Loss: 4.73554\n",
      "Epoch 846/1000, Training Loss: 34.61920, Validation Loss: 4.73398\n",
      "Epoch 847/1000, Training Loss: 34.79613, Validation Loss: 4.73254\n",
      "Epoch 848/1000, Training Loss: 34.85989, Validation Loss: 4.73115\n",
      "Epoch 849/1000, Training Loss: 34.36931, Validation Loss: 4.72964\n",
      "Epoch 850/1000, Training Loss: 34.67408, Validation Loss: 4.72810\n",
      "Epoch 851/1000, Training Loss: 34.39880, Validation Loss: 4.72654\n",
      "Epoch 852/1000, Training Loss: 34.56002, Validation Loss: 4.72503\n",
      "Epoch 853/1000, Training Loss: 34.78590, Validation Loss: 4.72362\n",
      "Epoch 854/1000, Training Loss: 34.71710, Validation Loss: 4.72211\n",
      "Epoch 855/1000, Training Loss: 34.67339, Validation Loss: 4.72053\n",
      "Epoch 856/1000, Training Loss: 34.70902, Validation Loss: 4.71895\n",
      "Epoch 857/1000, Training Loss: 35.10667, Validation Loss: 4.71762\n",
      "Epoch 858/1000, Training Loss: 34.57466, Validation Loss: 4.71604\n",
      "Epoch 859/1000, Training Loss: 34.82144, Validation Loss: 4.71454\n",
      "Epoch 860/1000, Training Loss: 35.08583, Validation Loss: 4.71310\n",
      "Epoch 861/1000, Training Loss: 34.88033, Validation Loss: 4.71163\n",
      "Epoch 862/1000, Training Loss: 34.94096, Validation Loss: 4.71022\n",
      "Epoch 863/1000, Training Loss: 34.60454, Validation Loss: 4.70866\n",
      "Epoch 864/1000, Training Loss: 34.58666, Validation Loss: 4.70715\n",
      "Epoch 865/1000, Training Loss: 34.93240, Validation Loss: 4.70574\n",
      "Epoch 866/1000, Training Loss: 34.35010, Validation Loss: 4.70413\n",
      "Epoch 867/1000, Training Loss: 34.85515, Validation Loss: 4.70264\n",
      "Epoch 868/1000, Training Loss: 34.63425, Validation Loss: 4.70119\n",
      "Epoch 869/1000, Training Loss: 35.10629, Validation Loss: 4.69990\n",
      "Epoch 870/1000, Training Loss: 34.39145, Validation Loss: 4.69832\n",
      "Epoch 871/1000, Training Loss: 34.80331, Validation Loss: 4.69678\n",
      "Epoch 872/1000, Training Loss: 34.84608, Validation Loss: 4.69531\n",
      "Epoch 873/1000, Training Loss: 34.50060, Validation Loss: 4.69381\n",
      "Epoch 874/1000, Training Loss: 34.39696, Validation Loss: 4.69233\n",
      "Epoch 875/1000, Training Loss: 34.43448, Validation Loss: 4.69088\n",
      "Epoch 876/1000, Training Loss: 34.76023, Validation Loss: 4.68933\n",
      "Epoch 877/1000, Training Loss: 34.95742, Validation Loss: 4.68799\n",
      "Epoch 878/1000, Training Loss: 34.52001, Validation Loss: 4.68658\n",
      "Epoch 879/1000, Training Loss: 34.64462, Validation Loss: 4.68511\n",
      "Epoch 880/1000, Training Loss: 34.77506, Validation Loss: 4.68379\n",
      "Epoch 881/1000, Training Loss: 34.73014, Validation Loss: 4.68239\n",
      "Epoch 882/1000, Training Loss: 34.53272, Validation Loss: 4.68102\n",
      "Epoch 883/1000, Training Loss: 34.63758, Validation Loss: 4.67965\n",
      "Epoch 884/1000, Training Loss: 34.60564, Validation Loss: 4.67818\n",
      "Epoch 885/1000, Training Loss: 34.85039, Validation Loss: 4.67685\n",
      "Epoch 886/1000, Training Loss: 34.68266, Validation Loss: 4.67541\n",
      "Epoch 887/1000, Training Loss: 34.83209, Validation Loss: 4.67409\n",
      "Epoch 888/1000, Training Loss: 34.71234, Validation Loss: 4.67273\n",
      "Epoch 889/1000, Training Loss: 34.83661, Validation Loss: 4.67136\n",
      "Epoch 890/1000, Training Loss: 34.39304, Validation Loss: 4.66982\n",
      "Epoch 891/1000, Training Loss: 34.85149, Validation Loss: 4.66846\n",
      "Epoch 892/1000, Training Loss: 34.82353, Validation Loss: 4.66704\n",
      "Epoch 893/1000, Training Loss: 34.59185, Validation Loss: 4.66562\n",
      "Epoch 894/1000, Training Loss: 34.36088, Validation Loss: 4.66420\n",
      "Epoch 895/1000, Training Loss: 34.62465, Validation Loss: 4.66288\n",
      "Epoch 896/1000, Training Loss: 34.26948, Validation Loss: 4.66140\n",
      "Epoch 897/1000, Training Loss: 34.41803, Validation Loss: 4.65996\n",
      "Epoch 898/1000, Training Loss: 34.21185, Validation Loss: 4.65851\n",
      "Epoch 899/1000, Training Loss: 34.69547, Validation Loss: 4.65711\n",
      "Epoch 900/1000, Training Loss: 34.45990, Validation Loss: 4.65571\n",
      "Epoch 901/1000, Training Loss: 33.98343, Validation Loss: 4.65412\n",
      "Epoch 902/1000, Training Loss: 34.40131, Validation Loss: 4.65274\n",
      "Epoch 903/1000, Training Loss: 34.38101, Validation Loss: 4.65125\n",
      "Epoch 904/1000, Training Loss: 34.75849, Validation Loss: 4.64981\n",
      "Epoch 905/1000, Training Loss: 34.74600, Validation Loss: 4.64850\n",
      "Epoch 906/1000, Training Loss: 34.66259, Validation Loss: 4.64714\n",
      "Epoch 907/1000, Training Loss: 35.05721, Validation Loss: 4.64592\n",
      "Epoch 908/1000, Training Loss: 34.42236, Validation Loss: 4.64455\n",
      "Epoch 909/1000, Training Loss: 34.05609, Validation Loss: 4.64295\n",
      "Epoch 910/1000, Training Loss: 33.98647, Validation Loss: 4.64146\n",
      "Epoch 911/1000, Training Loss: 34.45770, Validation Loss: 4.64003\n",
      "Epoch 912/1000, Training Loss: 34.47212, Validation Loss: 4.63857\n",
      "Epoch 913/1000, Training Loss: 34.64335, Validation Loss: 4.63718\n",
      "Epoch 914/1000, Training Loss: 35.15945, Validation Loss: 4.63604\n",
      "Epoch 915/1000, Training Loss: 34.59620, Validation Loss: 4.63468\n",
      "Epoch 916/1000, Training Loss: 34.47990, Validation Loss: 4.63331\n",
      "Epoch 917/1000, Training Loss: 34.29155, Validation Loss: 4.63189\n",
      "Epoch 918/1000, Training Loss: 34.79217, Validation Loss: 4.63054\n",
      "Epoch 919/1000, Training Loss: 34.57101, Validation Loss: 4.62919\n",
      "Epoch 920/1000, Training Loss: 34.75383, Validation Loss: 4.62781\n",
      "Epoch 921/1000, Training Loss: 34.39832, Validation Loss: 4.62644\n",
      "Epoch 922/1000, Training Loss: 34.37701, Validation Loss: 4.62504\n",
      "Epoch 923/1000, Training Loss: 33.59726, Validation Loss: 4.62351\n",
      "Epoch 924/1000, Training Loss: 34.33844, Validation Loss: 4.62206\n",
      "Epoch 925/1000, Training Loss: 33.97133, Validation Loss: 4.62051\n",
      "Epoch 926/1000, Training Loss: 34.39406, Validation Loss: 4.61911\n",
      "Epoch 927/1000, Training Loss: 34.45130, Validation Loss: 4.61777\n",
      "Epoch 928/1000, Training Loss: 34.22506, Validation Loss: 4.61627\n",
      "Epoch 929/1000, Training Loss: 34.43736, Validation Loss: 4.61496\n",
      "Epoch 930/1000, Training Loss: 34.30839, Validation Loss: 4.61364\n",
      "Epoch 931/1000, Training Loss: 34.25752, Validation Loss: 4.61234\n",
      "Epoch 932/1000, Training Loss: 34.29828, Validation Loss: 4.61088\n",
      "Epoch 933/1000, Training Loss: 34.60376, Validation Loss: 4.60957\n",
      "Epoch 934/1000, Training Loss: 34.12543, Validation Loss: 4.60816\n",
      "Epoch 935/1000, Training Loss: 33.85418, Validation Loss: 4.60667\n",
      "Epoch 936/1000, Training Loss: 34.32665, Validation Loss: 4.60537\n",
      "Epoch 937/1000, Training Loss: 34.11613, Validation Loss: 4.60399\n",
      "Epoch 938/1000, Training Loss: 34.64999, Validation Loss: 4.60280\n",
      "Epoch 939/1000, Training Loss: 34.58872, Validation Loss: 4.60146\n",
      "Epoch 940/1000, Training Loss: 34.58388, Validation Loss: 4.60008\n",
      "Epoch 941/1000, Training Loss: 34.21939, Validation Loss: 4.59873\n",
      "Epoch 942/1000, Training Loss: 34.28476, Validation Loss: 4.59741\n",
      "Epoch 943/1000, Training Loss: 34.15752, Validation Loss: 4.59604\n",
      "Epoch 944/1000, Training Loss: 34.26674, Validation Loss: 4.59466\n",
      "Epoch 945/1000, Training Loss: 34.00014, Validation Loss: 4.59323\n",
      "Epoch 946/1000, Training Loss: 33.84859, Validation Loss: 4.59176\n",
      "Epoch 947/1000, Training Loss: 34.23749, Validation Loss: 4.59045\n",
      "Epoch 948/1000, Training Loss: 34.59258, Validation Loss: 4.58923\n",
      "Epoch 949/1000, Training Loss: 34.38986, Validation Loss: 4.58785\n",
      "Epoch 950/1000, Training Loss: 34.70743, Validation Loss: 4.58668\n",
      "Epoch 951/1000, Training Loss: 34.26164, Validation Loss: 4.58533\n",
      "Epoch 952/1000, Training Loss: 34.19708, Validation Loss: 4.58404\n",
      "Epoch 953/1000, Training Loss: 33.70527, Validation Loss: 4.58247\n",
      "Epoch 954/1000, Training Loss: 34.14294, Validation Loss: 4.58119\n",
      "Epoch 955/1000, Training Loss: 34.23710, Validation Loss: 4.57992\n",
      "Epoch 956/1000, Training Loss: 33.80080, Validation Loss: 4.57848\n",
      "Epoch 957/1000, Training Loss: 34.12863, Validation Loss: 4.57713\n",
      "Epoch 958/1000, Training Loss: 34.05246, Validation Loss: 4.57568\n",
      "Epoch 959/1000, Training Loss: 34.29466, Validation Loss: 4.57432\n",
      "Epoch 960/1000, Training Loss: 34.24309, Validation Loss: 4.57296\n",
      "Epoch 961/1000, Training Loss: 34.29100, Validation Loss: 4.57161\n",
      "Epoch 962/1000, Training Loss: 34.02414, Validation Loss: 4.57018\n",
      "Epoch 963/1000, Training Loss: 34.09378, Validation Loss: 4.56877\n",
      "Epoch 964/1000, Training Loss: 33.95065, Validation Loss: 4.56731\n",
      "Epoch 965/1000, Training Loss: 33.98239, Validation Loss: 4.56589\n",
      "Epoch 966/1000, Training Loss: 33.88322, Validation Loss: 4.56449\n",
      "Epoch 967/1000, Training Loss: 34.10304, Validation Loss: 4.56307\n",
      "Epoch 968/1000, Training Loss: 34.62200, Validation Loss: 4.56192\n",
      "Epoch 969/1000, Training Loss: 34.02021, Validation Loss: 4.56055\n",
      "Epoch 970/1000, Training Loss: 33.88667, Validation Loss: 4.55915\n",
      "Epoch 971/1000, Training Loss: 34.14037, Validation Loss: 4.55796\n",
      "Epoch 972/1000, Training Loss: 33.87655, Validation Loss: 4.55666\n",
      "Epoch 973/1000, Training Loss: 34.20319, Validation Loss: 4.55541\n",
      "Epoch 974/1000, Training Loss: 34.59287, Validation Loss: 4.55421\n",
      "Epoch 975/1000, Training Loss: 34.36553, Validation Loss: 4.55303\n",
      "Epoch 976/1000, Training Loss: 34.56098, Validation Loss: 4.55188\n",
      "Epoch 977/1000, Training Loss: 34.16082, Validation Loss: 4.55060\n",
      "Epoch 978/1000, Training Loss: 34.17718, Validation Loss: 4.54932\n",
      "Epoch 979/1000, Training Loss: 33.88002, Validation Loss: 4.54799\n",
      "Epoch 980/1000, Training Loss: 34.33191, Validation Loss: 4.54673\n",
      "Epoch 981/1000, Training Loss: 33.78610, Validation Loss: 4.54540\n",
      "Epoch 982/1000, Training Loss: 34.12028, Validation Loss: 4.54406\n",
      "Epoch 983/1000, Training Loss: 33.38363, Validation Loss: 4.54258\n",
      "Epoch 984/1000, Training Loss: 34.10707, Validation Loss: 4.54143\n",
      "Epoch 985/1000, Training Loss: 34.41881, Validation Loss: 4.54031\n",
      "Epoch 986/1000, Training Loss: 33.62478, Validation Loss: 4.53886\n",
      "Epoch 987/1000, Training Loss: 34.10021, Validation Loss: 4.53752\n",
      "Epoch 988/1000, Training Loss: 33.97373, Validation Loss: 4.53604\n",
      "Epoch 989/1000, Training Loss: 34.04974, Validation Loss: 4.53467\n",
      "Epoch 990/1000, Training Loss: 34.07544, Validation Loss: 4.53346\n",
      "Epoch 991/1000, Training Loss: 33.63220, Validation Loss: 4.53205\n",
      "Epoch 992/1000, Training Loss: 33.72521, Validation Loss: 4.53077\n",
      "Epoch 993/1000, Training Loss: 33.66173, Validation Loss: 4.52943\n",
      "Epoch 994/1000, Training Loss: 34.08469, Validation Loss: 4.52829\n",
      "Epoch 995/1000, Training Loss: 33.63280, Validation Loss: 4.52692\n",
      "Epoch 996/1000, Training Loss: 33.69308, Validation Loss: 4.52551\n",
      "Epoch 997/1000, Training Loss: 34.25892, Validation Loss: 4.52445\n",
      "Epoch 998/1000, Training Loss: 33.87804, Validation Loss: 4.52311\n",
      "Epoch 999/1000, Training Loss: 33.32746, Validation Loss: 4.52176\n",
      "Epoch 1000/1000, Training Loss: 33.77132, Validation Loss: 4.52042\n",
      "Training took: 123.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_with_dropouts_base_1 = BaseModelWithDropouts().to(device)\n",
    "summary(model_with_dropouts_base_1, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.ASGD(model_with_dropouts_base_1.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_with_dropouts_base_1=[]\n",
    "val_loss_list_with_dropouts_base_1=[]\n",
    "train_accuracy_list_with_dropouts_base_1=[]\n",
    "val_accuracy_list_with_dropouts_base_1=[]\n",
    "test_accuracy_list_with_dropouts_base_1=[]\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_with_dropouts_base_1.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_with_dropouts_base_1(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_with_dropouts_base_1.append(train_accuracy)\n",
    "    train_loss_list_with_dropouts_base_1.append(train_loss)\n",
    "\n",
    "    model_with_dropouts_base_1.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_with_dropouts_base_1(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_with_dropouts_base_1.append(val_accuracy)\n",
    "    val_loss_list_with_dropouts_base_1.append(val_loss)\n",
    "\n",
    "    test_predictions_with_dropouts_base_1 = model_with_dropouts_base_1(X_test_tensor).view(-1)\n",
    "    test_predictions_rounded_with_dropouts_base_1 = torch.round(test_predictions_with_dropouts_base_1)\n",
    "    test_predictions_rounded_numpy_with_dropouts_base_1 = test_predictions_rounded_with_dropouts_base_1.cpu().detach().numpy()\n",
    "    y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "    accuracy_dropouts_base_1 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_with_dropouts_base_1)\n",
    "    test_accuracy_list_with_dropouts_base_1.append(accuracy_dropouts_base_1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsOJTc50EucR",
    "outputId": "0706f595-6406-4368-d5a2-a1833e9cb032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.7792, Precision: 0.7500, Recall: 0.5556, F1 Score: 0.6383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_with_dropouts_base_1.eval()\n",
    "test_predictions_with_dropouts_base_1 = model_with_dropouts_base_1(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_with_dropouts_base_1 = torch.round(test_predictions_with_dropouts_base_1)\n",
    "\n",
    "test_predictions_rounded_numpy_with_dropouts_base_1 = test_predictions_rounded_with_dropouts_base_1.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_dropouts_base_1 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_with_dropouts_base_1)\n",
    "precision_dropouts_base_1, recall_dropouts_base_1, f1_dropouts_base_1, _ = precision_recall_fscore_support(y_test_numpy, test_predictions_rounded_numpy_with_dropouts_base_1, average='binary')\n",
    "\n",
    "print(f\"Best Model Accuracy: {accuracy_dropouts_base_1:.4f}, Precision: {precision_dropouts_base_1:.4f}, Recall: {recall_dropouts_base_1:.4f}, F1 Score: {f1_dropouts_base_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgSF3lTjP0wO",
    "outputId": "cbe5c5a4-d142-461b-d8d6-e0b0e42f8460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Loss: 0.53143\n"
     ]
    }
   ],
   "source": [
    "model_with_dropouts_base_1.eval()\n",
    "test_loss=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_dropouts_base_1 = model_with_dropouts_base_1(X_test_tensor)\n",
    "    test_loss_dropouts_base_1 = loss_function(test_outputs_dropouts_base_1, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Best Model Test Loss: {test_loss_dropouts_base_1.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "0488oU5rQQUK",
    "outputId": "3d26f6dd-9c6c-424d-8832-2e201709b3b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.plot(epochs_range, train_loss_list_with_dropouts_base_1, label='Best Model Training Loss')\n",
    "plt.plot(epochs_range, val_loss_list_with_dropouts_base_1, label='Best Model Validation Loss')\n",
    "plt.plot(epochs_range, [test_loss_dropouts_base_1.cpu().item()]*epochs, label='Best Model Final Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "25cZ5dq_Q99R",
    "outputId": "aa756c5a-6e7f-4b4c-ce40-30dbf32a0701"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.plot(epochs_range, train_accuracy_list_with_dropouts_base_1, label='Best Model Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy_list_with_dropouts_base_1, label='Best Model Validation Accuracy')\n",
    "plt.plot(epochs_range, [accuracy_dropouts_base_1]*epochs, label='Best Model Final Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "zFul9RO_-nx6",
    "outputId": "c37ab17e-302c-4ba5-fc48-4384b4ab51ee"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.plot(epochs_range, test_accuracy_list_optimizer_asgd_1, label='Base Model Testing Accuracy')\n",
    "plt.plot(epochs_range, test_accuracy_list_with_dropouts_base_1, label='Base Model with Dropouts Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "RpEX6BxKRe4k",
    "outputId": "252e7f5a-e2b3-4efd-87f8-27c76c9a7b5f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_numpy, test_predictions_rounded_numpy_with_dropouts_base_1)\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-574CqBVbI3",
    "outputId": "78540a3a-7741-4fbb-e2ec-618bcffe2d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "GkxzpDdNUZrm",
    "outputId": "85a9cd78-126d-4c4a-8fab-488ec26669b6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "from torchmetrics.classification import BinaryROC\n",
    "\n",
    "broc = BinaryROC(thresholds=5).to(device)\n",
    "broc(test_predictions_with_dropouts_base_1, y_test_tensor.to(torch.int))\n",
    "broc.update(test_predictions_with_dropouts_base_1, y_test_tensor.to(torch.int))\n",
    "fig_, ax_ = broc.plot(score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "RgMx9C0EVEOj"
   },
   "outputs": [],
   "source": [
    "torch.save(model_with_dropouts_base_1.state_dict(), 'best_model_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lih25HI4C5k"
   },
   "source": [
    "Adding Normalization in the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "WFtCvXuRfhlu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "class BaseModelWithNoamalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModelWithNoamalization, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDj9sgxMrojq",
    "outputId": "402b01d8-2f06-457c-e318-e7af9f24528a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 40.39201, Validation Loss: 5.86738\n",
      "Epoch 2/1000, Training Loss: 40.23627, Validation Loss: 5.83498\n",
      "Epoch 3/1000, Training Loss: 40.08461, Validation Loss: 5.80755\n",
      "Epoch 4/1000, Training Loss: 39.93665, Validation Loss: 5.78067\n",
      "Epoch 5/1000, Training Loss: 39.79234, Validation Loss: 5.75438\n",
      "Epoch 6/1000, Training Loss: 39.65113, Validation Loss: 5.72864\n",
      "Epoch 7/1000, Training Loss: 39.51314, Validation Loss: 5.70346\n",
      "Epoch 8/1000, Training Loss: 39.37823, Validation Loss: 5.67874\n",
      "Epoch 9/1000, Training Loss: 39.24557, Validation Loss: 5.65447\n",
      "Epoch 10/1000, Training Loss: 39.11546, Validation Loss: 5.63065\n",
      "Epoch 11/1000, Training Loss: 38.98810, Validation Loss: 5.60725\n",
      "Epoch 12/1000, Training Loss: 38.86311, Validation Loss: 5.58425\n",
      "Epoch 13/1000, Training Loss: 38.73931, Validation Loss: 5.56161\n",
      "Epoch 14/1000, Training Loss: 38.60603, Validation Loss: 5.53882\n",
      "Epoch 15/1000, Training Loss: 38.46314, Validation Loss: 5.51709\n",
      "Epoch 16/1000, Training Loss: 38.34908, Validation Loss: 5.49594\n",
      "Epoch 17/1000, Training Loss: 38.23742, Validation Loss: 5.47514\n",
      "Epoch 18/1000, Training Loss: 38.12815, Validation Loss: 5.45474\n",
      "Epoch 19/1000, Training Loss: 38.02137, Validation Loss: 5.43480\n",
      "Epoch 20/1000, Training Loss: 37.91672, Validation Loss: 5.41520\n",
      "Epoch 21/1000, Training Loss: 37.81423, Validation Loss: 5.39597\n",
      "Epoch 22/1000, Training Loss: 37.71368, Validation Loss: 5.37709\n",
      "Epoch 23/1000, Training Loss: 37.61492, Validation Loss: 5.35858\n",
      "Epoch 24/1000, Training Loss: 37.51800, Validation Loss: 5.34041\n",
      "Epoch 25/1000, Training Loss: 37.42332, Validation Loss: 5.32255\n",
      "Epoch 26/1000, Training Loss: 37.33061, Validation Loss: 5.30499\n",
      "Epoch 27/1000, Training Loss: 37.23937, Validation Loss: 5.28768\n",
      "Epoch 28/1000, Training Loss: 37.14829, Validation Loss: 5.27051\n",
      "Epoch 29/1000, Training Loss: 37.04833, Validation Loss: 5.25283\n",
      "Epoch 30/1000, Training Loss: 36.93946, Validation Loss: 5.23636\n",
      "Epoch 31/1000, Training Loss: 36.85613, Validation Loss: 5.22027\n",
      "Epoch 32/1000, Training Loss: 36.77438, Validation Loss: 5.20446\n",
      "Epoch 33/1000, Training Loss: 36.69437, Validation Loss: 5.18894\n",
      "Epoch 34/1000, Training Loss: 36.61610, Validation Loss: 5.17367\n",
      "Epoch 35/1000, Training Loss: 36.53960, Validation Loss: 5.15868\n",
      "Epoch 36/1000, Training Loss: 36.46451, Validation Loss: 5.14394\n",
      "Epoch 37/1000, Training Loss: 36.39063, Validation Loss: 5.12945\n",
      "Epoch 38/1000, Training Loss: 36.31783, Validation Loss: 5.11515\n",
      "Epoch 39/1000, Training Loss: 36.24650, Validation Loss: 5.10112\n",
      "Epoch 40/1000, Training Loss: 36.17667, Validation Loss: 5.08732\n",
      "Epoch 41/1000, Training Loss: 36.10804, Validation Loss: 5.07374\n",
      "Epoch 42/1000, Training Loss: 36.04058, Validation Loss: 5.06043\n",
      "Epoch 43/1000, Training Loss: 35.97418, Validation Loss: 5.04729\n",
      "Epoch 44/1000, Training Loss: 35.90880, Validation Loss: 5.03433\n",
      "Epoch 45/1000, Training Loss: 35.84447, Validation Loss: 5.02159\n",
      "Epoch 46/1000, Training Loss: 35.78128, Validation Loss: 5.00905\n",
      "Epoch 47/1000, Training Loss: 35.71909, Validation Loss: 4.99670\n",
      "Epoch 48/1000, Training Loss: 35.65784, Validation Loss: 4.98452\n",
      "Epoch 49/1000, Training Loss: 35.59782, Validation Loss: 4.97255\n",
      "Epoch 50/1000, Training Loss: 35.53893, Validation Loss: 4.96077\n",
      "Epoch 51/1000, Training Loss: 35.48106, Validation Loss: 4.94916\n",
      "Epoch 52/1000, Training Loss: 35.42412, Validation Loss: 4.93774\n",
      "Epoch 53/1000, Training Loss: 35.36814, Validation Loss: 4.92646\n",
      "Epoch 54/1000, Training Loss: 35.31306, Validation Loss: 4.91536\n",
      "Epoch 55/1000, Training Loss: 35.25890, Validation Loss: 4.90445\n",
      "Epoch 56/1000, Training Loss: 35.20561, Validation Loss: 4.89372\n",
      "Epoch 57/1000, Training Loss: 35.15330, Validation Loss: 4.88320\n",
      "Epoch 58/1000, Training Loss: 35.10191, Validation Loss: 4.87284\n",
      "Epoch 59/1000, Training Loss: 35.05148, Validation Loss: 4.86267\n",
      "Epoch 60/1000, Training Loss: 35.00192, Validation Loss: 4.85263\n",
      "Epoch 61/1000, Training Loss: 34.95315, Validation Loss: 4.84273\n",
      "Epoch 62/1000, Training Loss: 34.90530, Validation Loss: 4.83302\n",
      "Epoch 63/1000, Training Loss: 34.85838, Validation Loss: 4.82345\n",
      "Epoch 64/1000, Training Loss: 34.81214, Validation Loss: 4.81402\n",
      "Epoch 65/1000, Training Loss: 34.76653, Validation Loss: 4.80471\n",
      "Epoch 66/1000, Training Loss: 34.72166, Validation Loss: 4.79555\n",
      "Epoch 67/1000, Training Loss: 34.67749, Validation Loss: 4.78651\n",
      "Epoch 68/1000, Training Loss: 34.63403, Validation Loss: 4.77759\n",
      "Epoch 69/1000, Training Loss: 34.59119, Validation Loss: 4.76879\n",
      "Epoch 70/1000, Training Loss: 34.54899, Validation Loss: 4.76014\n",
      "Epoch 71/1000, Training Loss: 34.50740, Validation Loss: 4.75161\n",
      "Epoch 72/1000, Training Loss: 34.46653, Validation Loss: 4.74326\n",
      "Epoch 73/1000, Training Loss: 34.42630, Validation Loss: 4.73502\n",
      "Epoch 74/1000, Training Loss: 34.38664, Validation Loss: 4.72696\n",
      "Epoch 75/1000, Training Loss: 34.34765, Validation Loss: 4.71902\n",
      "Epoch 76/1000, Training Loss: 34.30924, Validation Loss: 4.71120\n",
      "Epoch 77/1000, Training Loss: 34.27142, Validation Loss: 4.70346\n",
      "Epoch 78/1000, Training Loss: 34.23409, Validation Loss: 4.69583\n",
      "Epoch 79/1000, Training Loss: 34.19729, Validation Loss: 4.68831\n",
      "Epoch 80/1000, Training Loss: 34.16122, Validation Loss: 4.68089\n",
      "Epoch 81/1000, Training Loss: 34.12573, Validation Loss: 4.67360\n",
      "Epoch 82/1000, Training Loss: 34.09084, Validation Loss: 4.66639\n",
      "Epoch 83/1000, Training Loss: 34.05638, Validation Loss: 4.65928\n",
      "Epoch 84/1000, Training Loss: 34.02236, Validation Loss: 4.65225\n",
      "Epoch 85/1000, Training Loss: 33.98882, Validation Loss: 4.64531\n",
      "Epoch 86/1000, Training Loss: 33.95569, Validation Loss: 4.63846\n",
      "Epoch 87/1000, Training Loss: 33.92294, Validation Loss: 4.63168\n",
      "Epoch 88/1000, Training Loss: 33.89063, Validation Loss: 4.62498\n",
      "Epoch 89/1000, Training Loss: 33.85872, Validation Loss: 4.61837\n",
      "Epoch 90/1000, Training Loss: 33.82718, Validation Loss: 4.61186\n",
      "Epoch 91/1000, Training Loss: 33.79594, Validation Loss: 4.60542\n",
      "Epoch 92/1000, Training Loss: 33.76495, Validation Loss: 4.59907\n",
      "Epoch 93/1000, Training Loss: 33.73395, Validation Loss: 4.59278\n",
      "Epoch 94/1000, Training Loss: 33.70211, Validation Loss: 4.58654\n",
      "Epoch 95/1000, Training Loss: 33.66958, Validation Loss: 4.58042\n",
      "Epoch 96/1000, Training Loss: 33.64008, Validation Loss: 4.57437\n",
      "Epoch 97/1000, Training Loss: 33.61121, Validation Loss: 4.56839\n",
      "Epoch 98/1000, Training Loss: 33.58281, Validation Loss: 4.56247\n",
      "Epoch 99/1000, Training Loss: 33.55483, Validation Loss: 4.55663\n",
      "Epoch 100/1000, Training Loss: 33.52721, Validation Loss: 4.55085\n",
      "Epoch 101/1000, Training Loss: 33.49991, Validation Loss: 4.54513\n",
      "Epoch 102/1000, Training Loss: 33.47282, Validation Loss: 4.53947\n",
      "Epoch 103/1000, Training Loss: 33.44600, Validation Loss: 4.53388\n",
      "Epoch 104/1000, Training Loss: 33.41955, Validation Loss: 4.52837\n",
      "Epoch 105/1000, Training Loss: 33.39349, Validation Loss: 4.52292\n",
      "Epoch 106/1000, Training Loss: 33.36772, Validation Loss: 4.51753\n",
      "Epoch 107/1000, Training Loss: 33.34232, Validation Loss: 4.51220\n",
      "Epoch 108/1000, Training Loss: 33.31719, Validation Loss: 4.50694\n",
      "Epoch 109/1000, Training Loss: 33.29236, Validation Loss: 4.50176\n",
      "Epoch 110/1000, Training Loss: 33.26783, Validation Loss: 4.49664\n",
      "Epoch 111/1000, Training Loss: 33.24356, Validation Loss: 4.49156\n",
      "Epoch 112/1000, Training Loss: 33.21953, Validation Loss: 4.48654\n",
      "Epoch 113/1000, Training Loss: 33.19578, Validation Loss: 4.48157\n",
      "Epoch 114/1000, Training Loss: 33.17230, Validation Loss: 4.47666\n",
      "Epoch 115/1000, Training Loss: 33.14907, Validation Loss: 4.47181\n",
      "Epoch 116/1000, Training Loss: 33.12608, Validation Loss: 4.46702\n",
      "Epoch 117/1000, Training Loss: 33.10336, Validation Loss: 4.46228\n",
      "Epoch 118/1000, Training Loss: 33.08087, Validation Loss: 4.45757\n",
      "Epoch 119/1000, Training Loss: 33.05858, Validation Loss: 4.45291\n",
      "Epoch 120/1000, Training Loss: 33.03649, Validation Loss: 4.44830\n",
      "Epoch 121/1000, Training Loss: 33.01458, Validation Loss: 4.44372\n",
      "Epoch 122/1000, Training Loss: 32.99288, Validation Loss: 4.43919\n",
      "Epoch 123/1000, Training Loss: 32.97144, Validation Loss: 4.43470\n",
      "Epoch 124/1000, Training Loss: 32.95021, Validation Loss: 4.43024\n",
      "Epoch 125/1000, Training Loss: 32.92918, Validation Loss: 4.42584\n",
      "Epoch 126/1000, Training Loss: 32.90837, Validation Loss: 4.42149\n",
      "Epoch 127/1000, Training Loss: 32.88776, Validation Loss: 4.41718\n",
      "Epoch 128/1000, Training Loss: 32.86733, Validation Loss: 4.41291\n",
      "Epoch 129/1000, Training Loss: 32.84707, Validation Loss: 4.40868\n",
      "Epoch 130/1000, Training Loss: 32.82704, Validation Loss: 4.40447\n",
      "Epoch 131/1000, Training Loss: 32.80715, Validation Loss: 4.40031\n",
      "Epoch 132/1000, Training Loss: 32.78745, Validation Loss: 4.39619\n",
      "Epoch 133/1000, Training Loss: 32.76795, Validation Loss: 4.39210\n",
      "Epoch 134/1000, Training Loss: 32.74866, Validation Loss: 4.38807\n",
      "Epoch 135/1000, Training Loss: 32.72953, Validation Loss: 4.38408\n",
      "Epoch 136/1000, Training Loss: 32.71060, Validation Loss: 4.38011\n",
      "Epoch 137/1000, Training Loss: 32.69183, Validation Loss: 4.37619\n",
      "Epoch 138/1000, Training Loss: 32.67326, Validation Loss: 4.37233\n",
      "Epoch 139/1000, Training Loss: 32.65485, Validation Loss: 4.36850\n",
      "Epoch 140/1000, Training Loss: 32.63662, Validation Loss: 4.36470\n",
      "Epoch 141/1000, Training Loss: 32.61856, Validation Loss: 4.36095\n",
      "Epoch 142/1000, Training Loss: 32.60074, Validation Loss: 4.35725\n",
      "Epoch 143/1000, Training Loss: 32.58307, Validation Loss: 4.35358\n",
      "Epoch 144/1000, Training Loss: 32.56549, Validation Loss: 4.34993\n",
      "Epoch 145/1000, Training Loss: 32.54799, Validation Loss: 4.34632\n",
      "Epoch 146/1000, Training Loss: 32.53065, Validation Loss: 4.34274\n",
      "Epoch 147/1000, Training Loss: 32.51350, Validation Loss: 4.33920\n",
      "Epoch 148/1000, Training Loss: 32.49649, Validation Loss: 4.33567\n",
      "Epoch 149/1000, Training Loss: 32.47960, Validation Loss: 4.33219\n",
      "Epoch 150/1000, Training Loss: 32.46287, Validation Loss: 4.32876\n",
      "Epoch 151/1000, Training Loss: 32.44633, Validation Loss: 4.32535\n",
      "Epoch 152/1000, Training Loss: 32.42988, Validation Loss: 4.32199\n",
      "Epoch 153/1000, Training Loss: 32.41356, Validation Loss: 4.31864\n",
      "Epoch 154/1000, Training Loss: 32.39728, Validation Loss: 4.31534\n",
      "Epoch 155/1000, Training Loss: 32.38122, Validation Loss: 4.31205\n",
      "Epoch 156/1000, Training Loss: 32.36527, Validation Loss: 4.30880\n",
      "Epoch 157/1000, Training Loss: 32.34947, Validation Loss: 4.30556\n",
      "Epoch 158/1000, Training Loss: 32.33376, Validation Loss: 4.30237\n",
      "Epoch 159/1000, Training Loss: 32.31824, Validation Loss: 4.29919\n",
      "Epoch 160/1000, Training Loss: 32.30280, Validation Loss: 4.29605\n",
      "Epoch 161/1000, Training Loss: 32.28762, Validation Loss: 4.29294\n",
      "Epoch 162/1000, Training Loss: 32.27255, Validation Loss: 4.28987\n",
      "Epoch 163/1000, Training Loss: 32.25761, Validation Loss: 4.28685\n",
      "Epoch 164/1000, Training Loss: 32.24282, Validation Loss: 4.28384\n",
      "Epoch 165/1000, Training Loss: 32.22812, Validation Loss: 4.28086\n",
      "Epoch 166/1000, Training Loss: 32.21363, Validation Loss: 4.27791\n",
      "Epoch 167/1000, Training Loss: 32.19934, Validation Loss: 4.27500\n",
      "Epoch 168/1000, Training Loss: 32.18518, Validation Loss: 4.27213\n",
      "Epoch 169/1000, Training Loss: 32.17117, Validation Loss: 4.26927\n",
      "Epoch 170/1000, Training Loss: 32.15724, Validation Loss: 4.26645\n",
      "Epoch 171/1000, Training Loss: 32.14342, Validation Loss: 4.26366\n",
      "Epoch 172/1000, Training Loss: 32.12968, Validation Loss: 4.26089\n",
      "Epoch 173/1000, Training Loss: 32.11601, Validation Loss: 4.25815\n",
      "Epoch 174/1000, Training Loss: 32.10240, Validation Loss: 4.25542\n",
      "Epoch 175/1000, Training Loss: 32.08887, Validation Loss: 4.25272\n",
      "Epoch 176/1000, Training Loss: 32.07541, Validation Loss: 4.25004\n",
      "Epoch 177/1000, Training Loss: 32.06199, Validation Loss: 4.24737\n",
      "Epoch 178/1000, Training Loss: 32.04859, Validation Loss: 4.24472\n",
      "Epoch 179/1000, Training Loss: 32.03514, Validation Loss: 4.24209\n",
      "Epoch 180/1000, Training Loss: 32.02143, Validation Loss: 4.23947\n",
      "Epoch 181/1000, Training Loss: 32.00692, Validation Loss: 4.23687\n",
      "Epoch 182/1000, Training Loss: 31.98814, Validation Loss: 4.23428\n",
      "Epoch 183/1000, Training Loss: 31.97064, Validation Loss: 4.23171\n",
      "Epoch 184/1000, Training Loss: 31.95780, Validation Loss: 4.22921\n",
      "Epoch 185/1000, Training Loss: 31.94514, Validation Loss: 4.22675\n",
      "Epoch 186/1000, Training Loss: 31.93267, Validation Loss: 4.22429\n",
      "Epoch 187/1000, Training Loss: 31.92030, Validation Loss: 4.22186\n",
      "Epoch 188/1000, Training Loss: 31.90806, Validation Loss: 4.21945\n",
      "Epoch 189/1000, Training Loss: 31.89594, Validation Loss: 4.21707\n",
      "Epoch 190/1000, Training Loss: 31.88392, Validation Loss: 4.21471\n",
      "Epoch 191/1000, Training Loss: 31.87199, Validation Loss: 4.21237\n",
      "Epoch 192/1000, Training Loss: 31.86016, Validation Loss: 4.21004\n",
      "Epoch 193/1000, Training Loss: 31.84838, Validation Loss: 4.20774\n",
      "Epoch 194/1000, Training Loss: 31.83666, Validation Loss: 4.20546\n",
      "Epoch 195/1000, Training Loss: 31.82503, Validation Loss: 4.20320\n",
      "Epoch 196/1000, Training Loss: 31.81345, Validation Loss: 4.20101\n",
      "Epoch 197/1000, Training Loss: 31.80199, Validation Loss: 4.19882\n",
      "Epoch 198/1000, Training Loss: 31.79059, Validation Loss: 4.19665\n",
      "Epoch 199/1000, Training Loss: 31.77927, Validation Loss: 4.19450\n",
      "Epoch 200/1000, Training Loss: 31.76802, Validation Loss: 4.19236\n",
      "Epoch 201/1000, Training Loss: 31.75685, Validation Loss: 4.19024\n",
      "Epoch 202/1000, Training Loss: 31.74576, Validation Loss: 4.18814\n",
      "Epoch 203/1000, Training Loss: 31.73476, Validation Loss: 4.18605\n",
      "Epoch 204/1000, Training Loss: 31.72384, Validation Loss: 4.18399\n",
      "Epoch 205/1000, Training Loss: 31.71303, Validation Loss: 4.18195\n",
      "Epoch 206/1000, Training Loss: 31.70228, Validation Loss: 4.17991\n",
      "Epoch 207/1000, Training Loss: 31.69159, Validation Loss: 4.17789\n",
      "Epoch 208/1000, Training Loss: 31.68095, Validation Loss: 4.17589\n",
      "Epoch 209/1000, Training Loss: 31.67037, Validation Loss: 4.17390\n",
      "Epoch 210/1000, Training Loss: 31.65986, Validation Loss: 4.17192\n",
      "Epoch 211/1000, Training Loss: 31.64942, Validation Loss: 4.16997\n",
      "Epoch 212/1000, Training Loss: 31.63904, Validation Loss: 4.16804\n",
      "Epoch 213/1000, Training Loss: 31.62874, Validation Loss: 4.16612\n",
      "Epoch 214/1000, Training Loss: 31.61851, Validation Loss: 4.16423\n",
      "Epoch 215/1000, Training Loss: 31.60834, Validation Loss: 4.16234\n",
      "Epoch 216/1000, Training Loss: 31.59824, Validation Loss: 4.16047\n",
      "Epoch 217/1000, Training Loss: 31.58822, Validation Loss: 4.15862\n",
      "Epoch 218/1000, Training Loss: 31.57825, Validation Loss: 4.15678\n",
      "Epoch 219/1000, Training Loss: 31.56835, Validation Loss: 4.15495\n",
      "Epoch 220/1000, Training Loss: 31.55849, Validation Loss: 4.15313\n",
      "Epoch 221/1000, Training Loss: 31.54868, Validation Loss: 4.15131\n",
      "Epoch 222/1000, Training Loss: 31.53892, Validation Loss: 4.14951\n",
      "Epoch 223/1000, Training Loss: 31.52923, Validation Loss: 4.14771\n",
      "Epoch 224/1000, Training Loss: 31.51962, Validation Loss: 4.14594\n",
      "Epoch 225/1000, Training Loss: 31.51007, Validation Loss: 4.14419\n",
      "Epoch 226/1000, Training Loss: 31.50059, Validation Loss: 4.14246\n",
      "Epoch 227/1000, Training Loss: 31.49116, Validation Loss: 4.14076\n",
      "Epoch 228/1000, Training Loss: 31.48179, Validation Loss: 4.13905\n",
      "Epoch 229/1000, Training Loss: 31.47247, Validation Loss: 4.13736\n",
      "Epoch 230/1000, Training Loss: 31.46321, Validation Loss: 4.13569\n",
      "Epoch 231/1000, Training Loss: 31.45400, Validation Loss: 4.13402\n",
      "Epoch 232/1000, Training Loss: 31.44481, Validation Loss: 4.13237\n",
      "Epoch 233/1000, Training Loss: 31.43569, Validation Loss: 4.13074\n",
      "Epoch 234/1000, Training Loss: 31.42663, Validation Loss: 4.12912\n",
      "Epoch 235/1000, Training Loss: 31.41764, Validation Loss: 4.12750\n",
      "Epoch 236/1000, Training Loss: 31.40869, Validation Loss: 4.12590\n",
      "Epoch 237/1000, Training Loss: 31.39978, Validation Loss: 4.12431\n",
      "Epoch 238/1000, Training Loss: 31.39090, Validation Loss: 4.12273\n",
      "Epoch 239/1000, Training Loss: 31.38208, Validation Loss: 4.12115\n",
      "Epoch 240/1000, Training Loss: 31.37330, Validation Loss: 4.11959\n",
      "Epoch 241/1000, Training Loss: 31.36457, Validation Loss: 4.11804\n",
      "Epoch 242/1000, Training Loss: 31.35589, Validation Loss: 4.11650\n",
      "Epoch 243/1000, Training Loss: 31.34726, Validation Loss: 4.11500\n",
      "Epoch 244/1000, Training Loss: 31.33869, Validation Loss: 4.11350\n",
      "Epoch 245/1000, Training Loss: 31.33019, Validation Loss: 4.11201\n",
      "Epoch 246/1000, Training Loss: 31.32171, Validation Loss: 4.11053\n",
      "Epoch 247/1000, Training Loss: 31.31327, Validation Loss: 4.10906\n",
      "Epoch 248/1000, Training Loss: 31.30489, Validation Loss: 4.10760\n",
      "Epoch 249/1000, Training Loss: 31.29654, Validation Loss: 4.10614\n",
      "Epoch 250/1000, Training Loss: 31.28828, Validation Loss: 4.10470\n",
      "Epoch 251/1000, Training Loss: 31.28008, Validation Loss: 4.10326\n",
      "Epoch 252/1000, Training Loss: 31.27189, Validation Loss: 4.10183\n",
      "Epoch 253/1000, Training Loss: 31.26375, Validation Loss: 4.10042\n",
      "Epoch 254/1000, Training Loss: 31.25565, Validation Loss: 4.09901\n",
      "Epoch 255/1000, Training Loss: 31.24760, Validation Loss: 4.09761\n",
      "Epoch 256/1000, Training Loss: 31.23957, Validation Loss: 4.09623\n",
      "Epoch 257/1000, Training Loss: 31.23156, Validation Loss: 4.09487\n",
      "Epoch 258/1000, Training Loss: 31.22360, Validation Loss: 4.09352\n",
      "Epoch 259/1000, Training Loss: 31.21567, Validation Loss: 4.09216\n",
      "Epoch 260/1000, Training Loss: 31.20780, Validation Loss: 4.09082\n",
      "Epoch 261/1000, Training Loss: 31.19997, Validation Loss: 4.08950\n",
      "Epoch 262/1000, Training Loss: 31.19214, Validation Loss: 4.08817\n",
      "Epoch 263/1000, Training Loss: 31.18436, Validation Loss: 4.08686\n",
      "Epoch 264/1000, Training Loss: 31.17661, Validation Loss: 4.08556\n",
      "Epoch 265/1000, Training Loss: 31.16891, Validation Loss: 4.08426\n",
      "Epoch 266/1000, Training Loss: 31.16126, Validation Loss: 4.08297\n",
      "Epoch 267/1000, Training Loss: 31.15364, Validation Loss: 4.08170\n",
      "Epoch 268/1000, Training Loss: 31.14608, Validation Loss: 4.08042\n",
      "Epoch 269/1000, Training Loss: 31.13853, Validation Loss: 4.07917\n",
      "Epoch 270/1000, Training Loss: 31.13105, Validation Loss: 4.07792\n",
      "Epoch 271/1000, Training Loss: 31.12360, Validation Loss: 4.07668\n",
      "Epoch 272/1000, Training Loss: 31.11617, Validation Loss: 4.07544\n",
      "Epoch 273/1000, Training Loss: 31.10878, Validation Loss: 4.07421\n",
      "Epoch 274/1000, Training Loss: 31.10141, Validation Loss: 4.07299\n",
      "Epoch 275/1000, Training Loss: 31.09407, Validation Loss: 4.07177\n",
      "Epoch 276/1000, Training Loss: 31.08673, Validation Loss: 4.07056\n",
      "Epoch 277/1000, Training Loss: 31.07944, Validation Loss: 4.06936\n",
      "Epoch 278/1000, Training Loss: 31.07217, Validation Loss: 4.06816\n",
      "Epoch 279/1000, Training Loss: 31.06492, Validation Loss: 4.06697\n",
      "Epoch 280/1000, Training Loss: 31.05771, Validation Loss: 4.06578\n",
      "Epoch 281/1000, Training Loss: 31.05054, Validation Loss: 4.06460\n",
      "Epoch 282/1000, Training Loss: 31.04339, Validation Loss: 4.06344\n",
      "Epoch 283/1000, Training Loss: 31.03630, Validation Loss: 4.06228\n",
      "Epoch 284/1000, Training Loss: 31.02922, Validation Loss: 4.06114\n",
      "Epoch 285/1000, Training Loss: 31.02218, Validation Loss: 4.06000\n",
      "Epoch 286/1000, Training Loss: 31.01518, Validation Loss: 4.05887\n",
      "Epoch 287/1000, Training Loss: 31.00821, Validation Loss: 4.05774\n",
      "Epoch 288/1000, Training Loss: 31.00125, Validation Loss: 4.05662\n",
      "Epoch 289/1000, Training Loss: 30.99431, Validation Loss: 4.05550\n",
      "Epoch 290/1000, Training Loss: 30.98744, Validation Loss: 4.05439\n",
      "Epoch 291/1000, Training Loss: 30.98055, Validation Loss: 4.05330\n",
      "Epoch 292/1000, Training Loss: 30.97373, Validation Loss: 4.05221\n",
      "Epoch 293/1000, Training Loss: 30.96694, Validation Loss: 4.05112\n",
      "Epoch 294/1000, Training Loss: 30.96018, Validation Loss: 4.05004\n",
      "Epoch 295/1000, Training Loss: 30.95342, Validation Loss: 4.04897\n",
      "Epoch 296/1000, Training Loss: 30.94671, Validation Loss: 4.04791\n",
      "Epoch 297/1000, Training Loss: 30.93996, Validation Loss: 4.04685\n",
      "Epoch 298/1000, Training Loss: 30.93328, Validation Loss: 4.04580\n",
      "Epoch 299/1000, Training Loss: 30.92661, Validation Loss: 4.04475\n",
      "Epoch 300/1000, Training Loss: 30.91995, Validation Loss: 4.04370\n",
      "Epoch 301/1000, Training Loss: 30.91331, Validation Loss: 4.04265\n",
      "Epoch 302/1000, Training Loss: 30.90671, Validation Loss: 4.04160\n",
      "Epoch 303/1000, Training Loss: 30.90013, Validation Loss: 4.04057\n",
      "Epoch 304/1000, Training Loss: 30.89356, Validation Loss: 4.03954\n",
      "Epoch 305/1000, Training Loss: 30.88702, Validation Loss: 4.03853\n",
      "Epoch 306/1000, Training Loss: 30.88052, Validation Loss: 4.03752\n",
      "Epoch 307/1000, Training Loss: 30.87399, Validation Loss: 4.03652\n",
      "Epoch 308/1000, Training Loss: 30.86752, Validation Loss: 4.03551\n",
      "Epoch 309/1000, Training Loss: 30.86107, Validation Loss: 4.03452\n",
      "Epoch 310/1000, Training Loss: 30.85463, Validation Loss: 4.03352\n",
      "Epoch 311/1000, Training Loss: 30.84822, Validation Loss: 4.03254\n",
      "Epoch 312/1000, Training Loss: 30.84184, Validation Loss: 4.03157\n",
      "Epoch 313/1000, Training Loss: 30.83545, Validation Loss: 4.03061\n",
      "Epoch 314/1000, Training Loss: 30.82914, Validation Loss: 4.02966\n",
      "Epoch 315/1000, Training Loss: 30.82282, Validation Loss: 4.02870\n",
      "Epoch 316/1000, Training Loss: 30.81653, Validation Loss: 4.02775\n",
      "Epoch 317/1000, Training Loss: 30.81025, Validation Loss: 4.02682\n",
      "Epoch 318/1000, Training Loss: 30.80403, Validation Loss: 4.02589\n",
      "Epoch 319/1000, Training Loss: 30.79784, Validation Loss: 4.02496\n",
      "Epoch 320/1000, Training Loss: 30.79167, Validation Loss: 4.02404\n",
      "Epoch 321/1000, Training Loss: 30.78547, Validation Loss: 4.02314\n",
      "Epoch 322/1000, Training Loss: 30.77932, Validation Loss: 4.02224\n",
      "Epoch 323/1000, Training Loss: 30.77319, Validation Loss: 4.02133\n",
      "Epoch 324/1000, Training Loss: 30.76701, Validation Loss: 4.02044\n",
      "Epoch 325/1000, Training Loss: 30.76088, Validation Loss: 4.01956\n",
      "Epoch 326/1000, Training Loss: 30.75476, Validation Loss: 4.01867\n",
      "Epoch 327/1000, Training Loss: 30.74867, Validation Loss: 4.01778\n",
      "Epoch 328/1000, Training Loss: 30.74256, Validation Loss: 4.01690\n",
      "Epoch 329/1000, Training Loss: 30.73648, Validation Loss: 4.01604\n",
      "Epoch 330/1000, Training Loss: 30.73041, Validation Loss: 4.01517\n",
      "Epoch 331/1000, Training Loss: 30.72434, Validation Loss: 4.01429\n",
      "Epoch 332/1000, Training Loss: 30.71833, Validation Loss: 4.01344\n",
      "Epoch 333/1000, Training Loss: 30.71227, Validation Loss: 4.01258\n",
      "Epoch 334/1000, Training Loss: 30.70625, Validation Loss: 4.01172\n",
      "Epoch 335/1000, Training Loss: 30.70024, Validation Loss: 4.01087\n",
      "Epoch 336/1000, Training Loss: 30.69428, Validation Loss: 4.01003\n",
      "Epoch 337/1000, Training Loss: 30.68831, Validation Loss: 4.00919\n",
      "Epoch 338/1000, Training Loss: 30.68235, Validation Loss: 4.00837\n",
      "Epoch 339/1000, Training Loss: 30.67644, Validation Loss: 4.00753\n",
      "Epoch 340/1000, Training Loss: 30.67057, Validation Loss: 4.00670\n",
      "Epoch 341/1000, Training Loss: 30.66468, Validation Loss: 4.00586\n",
      "Epoch 342/1000, Training Loss: 30.65885, Validation Loss: 4.00504\n",
      "Epoch 343/1000, Training Loss: 30.65300, Validation Loss: 4.00422\n",
      "Epoch 344/1000, Training Loss: 30.64718, Validation Loss: 4.00341\n",
      "Epoch 345/1000, Training Loss: 30.64138, Validation Loss: 4.00260\n",
      "Epoch 346/1000, Training Loss: 30.63556, Validation Loss: 4.00180\n",
      "Epoch 347/1000, Training Loss: 30.62976, Validation Loss: 4.00100\n",
      "Epoch 348/1000, Training Loss: 30.62397, Validation Loss: 4.00021\n",
      "Epoch 349/1000, Training Loss: 30.61820, Validation Loss: 3.99943\n",
      "Epoch 350/1000, Training Loss: 30.61242, Validation Loss: 3.99866\n",
      "Epoch 351/1000, Training Loss: 30.60667, Validation Loss: 3.99789\n",
      "Epoch 352/1000, Training Loss: 30.60096, Validation Loss: 3.99712\n",
      "Epoch 353/1000, Training Loss: 30.59522, Validation Loss: 3.99637\n",
      "Epoch 354/1000, Training Loss: 30.58951, Validation Loss: 3.99562\n",
      "Epoch 355/1000, Training Loss: 30.58381, Validation Loss: 3.99486\n",
      "Epoch 356/1000, Training Loss: 30.57813, Validation Loss: 3.99411\n",
      "Epoch 357/1000, Training Loss: 30.57245, Validation Loss: 3.99337\n",
      "Epoch 358/1000, Training Loss: 30.56678, Validation Loss: 3.99263\n",
      "Epoch 359/1000, Training Loss: 30.56113, Validation Loss: 3.99188\n",
      "Epoch 360/1000, Training Loss: 30.55547, Validation Loss: 3.99114\n",
      "Epoch 361/1000, Training Loss: 30.54981, Validation Loss: 3.99040\n",
      "Epoch 362/1000, Training Loss: 30.54417, Validation Loss: 3.98967\n",
      "Epoch 363/1000, Training Loss: 30.53856, Validation Loss: 3.98895\n",
      "Epoch 364/1000, Training Loss: 30.53297, Validation Loss: 3.98822\n",
      "Epoch 365/1000, Training Loss: 30.52739, Validation Loss: 3.98751\n",
      "Epoch 366/1000, Training Loss: 30.52183, Validation Loss: 3.98679\n",
      "Epoch 367/1000, Training Loss: 30.51627, Validation Loss: 3.98609\n",
      "Epoch 368/1000, Training Loss: 30.51073, Validation Loss: 3.98538\n",
      "Epoch 369/1000, Training Loss: 30.50523, Validation Loss: 3.98468\n",
      "Epoch 370/1000, Training Loss: 30.49972, Validation Loss: 3.98399\n",
      "Epoch 371/1000, Training Loss: 30.49424, Validation Loss: 3.98331\n",
      "Epoch 372/1000, Training Loss: 30.48877, Validation Loss: 3.98262\n",
      "Epoch 373/1000, Training Loss: 30.48330, Validation Loss: 3.98193\n",
      "Epoch 374/1000, Training Loss: 30.47787, Validation Loss: 3.98126\n",
      "Epoch 375/1000, Training Loss: 30.47243, Validation Loss: 3.98059\n",
      "Epoch 376/1000, Training Loss: 30.46703, Validation Loss: 3.97994\n",
      "Epoch 377/1000, Training Loss: 30.46166, Validation Loss: 3.97929\n",
      "Epoch 378/1000, Training Loss: 30.45631, Validation Loss: 3.97864\n",
      "Epoch 379/1000, Training Loss: 30.45095, Validation Loss: 3.97799\n",
      "Epoch 380/1000, Training Loss: 30.44562, Validation Loss: 3.97735\n",
      "Epoch 381/1000, Training Loss: 30.44030, Validation Loss: 3.97672\n",
      "Epoch 382/1000, Training Loss: 30.43504, Validation Loss: 3.97609\n",
      "Epoch 383/1000, Training Loss: 30.42975, Validation Loss: 3.97545\n",
      "Epoch 384/1000, Training Loss: 30.42451, Validation Loss: 3.97482\n",
      "Epoch 385/1000, Training Loss: 30.41925, Validation Loss: 3.97419\n",
      "Epoch 386/1000, Training Loss: 30.41401, Validation Loss: 3.97357\n",
      "Epoch 387/1000, Training Loss: 30.40877, Validation Loss: 3.97294\n",
      "Epoch 388/1000, Training Loss: 30.40354, Validation Loss: 3.97235\n",
      "Epoch 389/1000, Training Loss: 30.39827, Validation Loss: 3.97173\n",
      "Epoch 390/1000, Training Loss: 30.39305, Validation Loss: 3.97113\n",
      "Epoch 391/1000, Training Loss: 30.38782, Validation Loss: 3.97053\n",
      "Epoch 392/1000, Training Loss: 30.38259, Validation Loss: 3.96993\n",
      "Epoch 393/1000, Training Loss: 30.37741, Validation Loss: 3.96934\n",
      "Epoch 394/1000, Training Loss: 30.37220, Validation Loss: 3.96874\n",
      "Epoch 395/1000, Training Loss: 30.36699, Validation Loss: 3.96815\n",
      "Epoch 396/1000, Training Loss: 30.36182, Validation Loss: 3.96759\n",
      "Epoch 397/1000, Training Loss: 30.35663, Validation Loss: 3.96700\n",
      "Epoch 398/1000, Training Loss: 30.35146, Validation Loss: 3.96642\n",
      "Epoch 399/1000, Training Loss: 30.34633, Validation Loss: 3.96584\n",
      "Epoch 400/1000, Training Loss: 30.34119, Validation Loss: 3.96526\n",
      "Epoch 401/1000, Training Loss: 30.33607, Validation Loss: 3.96470\n",
      "Epoch 402/1000, Training Loss: 30.33099, Validation Loss: 3.96413\n",
      "Epoch 403/1000, Training Loss: 30.32588, Validation Loss: 3.96357\n",
      "Epoch 404/1000, Training Loss: 30.32082, Validation Loss: 3.96301\n",
      "Epoch 405/1000, Training Loss: 30.31576, Validation Loss: 3.96245\n",
      "Epoch 406/1000, Training Loss: 30.31072, Validation Loss: 3.96189\n",
      "Epoch 407/1000, Training Loss: 30.30569, Validation Loss: 3.96134\n",
      "Epoch 408/1000, Training Loss: 30.30066, Validation Loss: 3.96080\n",
      "Epoch 409/1000, Training Loss: 30.29566, Validation Loss: 3.96026\n",
      "Epoch 410/1000, Training Loss: 30.29057, Validation Loss: 3.95973\n",
      "Epoch 411/1000, Training Loss: 30.28556, Validation Loss: 3.95919\n",
      "Epoch 412/1000, Training Loss: 30.28050, Validation Loss: 3.95868\n",
      "Epoch 413/1000, Training Loss: 30.27550, Validation Loss: 3.95816\n",
      "Epoch 414/1000, Training Loss: 30.27048, Validation Loss: 3.95764\n",
      "Epoch 415/1000, Training Loss: 30.26546, Validation Loss: 3.95712\n",
      "Epoch 416/1000, Training Loss: 30.26048, Validation Loss: 3.95662\n",
      "Epoch 417/1000, Training Loss: 30.25546, Validation Loss: 3.95612\n",
      "Epoch 418/1000, Training Loss: 30.25053, Validation Loss: 3.95559\n",
      "Epoch 419/1000, Training Loss: 30.24556, Validation Loss: 3.95508\n",
      "Epoch 420/1000, Training Loss: 30.24062, Validation Loss: 3.95456\n",
      "Epoch 421/1000, Training Loss: 30.23566, Validation Loss: 3.95406\n",
      "Epoch 422/1000, Training Loss: 30.23069, Validation Loss: 3.95355\n",
      "Epoch 423/1000, Training Loss: 30.22577, Validation Loss: 3.95304\n",
      "Epoch 424/1000, Training Loss: 30.22081, Validation Loss: 3.95253\n",
      "Epoch 425/1000, Training Loss: 30.21585, Validation Loss: 3.95204\n",
      "Epoch 426/1000, Training Loss: 30.21092, Validation Loss: 3.95153\n",
      "Epoch 427/1000, Training Loss: 30.20605, Validation Loss: 3.95101\n",
      "Epoch 428/1000, Training Loss: 30.20119, Validation Loss: 3.95051\n",
      "Epoch 429/1000, Training Loss: 30.19636, Validation Loss: 3.94999\n",
      "Epoch 430/1000, Training Loss: 30.19150, Validation Loss: 3.94949\n",
      "Epoch 431/1000, Training Loss: 30.18668, Validation Loss: 3.94897\n",
      "Epoch 432/1000, Training Loss: 30.18183, Validation Loss: 3.94848\n",
      "Epoch 433/1000, Training Loss: 30.17704, Validation Loss: 3.94800\n",
      "Epoch 434/1000, Training Loss: 30.17223, Validation Loss: 3.94748\n",
      "Epoch 435/1000, Training Loss: 30.16742, Validation Loss: 3.94698\n",
      "Epoch 436/1000, Training Loss: 30.16258, Validation Loss: 3.94651\n",
      "Epoch 437/1000, Training Loss: 30.15778, Validation Loss: 3.94599\n",
      "Epoch 438/1000, Training Loss: 30.15297, Validation Loss: 3.94550\n",
      "Epoch 439/1000, Training Loss: 30.14822, Validation Loss: 3.94499\n",
      "Epoch 440/1000, Training Loss: 30.14340, Validation Loss: 3.94451\n",
      "Epoch 441/1000, Training Loss: 30.13864, Validation Loss: 3.94404\n",
      "Epoch 442/1000, Training Loss: 30.13385, Validation Loss: 3.94354\n",
      "Epoch 443/1000, Training Loss: 30.12911, Validation Loss: 3.94304\n",
      "Epoch 444/1000, Training Loss: 30.12432, Validation Loss: 3.94256\n",
      "Epoch 445/1000, Training Loss: 30.11963, Validation Loss: 3.94207\n",
      "Epoch 446/1000, Training Loss: 30.11491, Validation Loss: 3.94157\n",
      "Epoch 447/1000, Training Loss: 30.11025, Validation Loss: 3.94113\n",
      "Epoch 448/1000, Training Loss: 30.10553, Validation Loss: 3.94064\n",
      "Epoch 449/1000, Training Loss: 30.10089, Validation Loss: 3.94017\n",
      "Epoch 450/1000, Training Loss: 30.09618, Validation Loss: 3.93969\n",
      "Epoch 451/1000, Training Loss: 30.09146, Validation Loss: 3.93925\n",
      "Epoch 452/1000, Training Loss: 30.08682, Validation Loss: 3.93875\n",
      "Epoch 453/1000, Training Loss: 30.08211, Validation Loss: 3.93831\n",
      "Epoch 454/1000, Training Loss: 30.07748, Validation Loss: 3.93784\n",
      "Epoch 455/1000, Training Loss: 30.07278, Validation Loss: 3.93735\n",
      "Epoch 456/1000, Training Loss: 30.06811, Validation Loss: 3.93692\n",
      "Epoch 457/1000, Training Loss: 30.06342, Validation Loss: 3.93645\n",
      "Epoch 458/1000, Training Loss: 30.05875, Validation Loss: 3.93602\n",
      "Epoch 459/1000, Training Loss: 30.05407, Validation Loss: 3.93555\n",
      "Epoch 460/1000, Training Loss: 30.04943, Validation Loss: 3.93510\n",
      "Epoch 461/1000, Training Loss: 30.04474, Validation Loss: 3.93471\n",
      "Epoch 462/1000, Training Loss: 30.04013, Validation Loss: 3.93424\n",
      "Epoch 463/1000, Training Loss: 30.03543, Validation Loss: 3.93378\n",
      "Epoch 464/1000, Training Loss: 30.03071, Validation Loss: 3.93335\n",
      "Epoch 465/1000, Training Loss: 30.02606, Validation Loss: 3.93290\n",
      "Epoch 466/1000, Training Loss: 30.02143, Validation Loss: 3.93247\n",
      "Epoch 467/1000, Training Loss: 30.01670, Validation Loss: 3.93204\n",
      "Epoch 468/1000, Training Loss: 30.01209, Validation Loss: 3.93163\n",
      "Epoch 469/1000, Training Loss: 30.00745, Validation Loss: 3.93119\n",
      "Epoch 470/1000, Training Loss: 30.00267, Validation Loss: 3.93077\n",
      "Epoch 471/1000, Training Loss: 29.99803, Validation Loss: 3.93034\n",
      "Epoch 472/1000, Training Loss: 29.99326, Validation Loss: 3.92992\n",
      "Epoch 473/1000, Training Loss: 29.98857, Validation Loss: 3.92952\n",
      "Epoch 474/1000, Training Loss: 29.98380, Validation Loss: 3.92911\n",
      "Epoch 475/1000, Training Loss: 29.97918, Validation Loss: 3.92868\n",
      "Epoch 476/1000, Training Loss: 29.97441, Validation Loss: 3.92829\n",
      "Epoch 477/1000, Training Loss: 29.96974, Validation Loss: 3.92789\n",
      "Epoch 478/1000, Training Loss: 29.96509, Validation Loss: 3.92748\n",
      "Epoch 479/1000, Training Loss: 29.96040, Validation Loss: 3.92712\n",
      "Epoch 480/1000, Training Loss: 29.95576, Validation Loss: 3.92670\n",
      "Epoch 481/1000, Training Loss: 29.95100, Validation Loss: 3.92632\n",
      "Epoch 482/1000, Training Loss: 29.94643, Validation Loss: 3.92591\n",
      "Epoch 483/1000, Training Loss: 29.94168, Validation Loss: 3.92553\n",
      "Epoch 484/1000, Training Loss: 29.93705, Validation Loss: 3.92510\n",
      "Epoch 485/1000, Training Loss: 29.93238, Validation Loss: 3.92473\n",
      "Epoch 486/1000, Training Loss: 29.92774, Validation Loss: 3.92432\n",
      "Epoch 487/1000, Training Loss: 29.92310, Validation Loss: 3.92394\n",
      "Epoch 488/1000, Training Loss: 29.91842, Validation Loss: 3.92353\n",
      "Epoch 489/1000, Training Loss: 29.91377, Validation Loss: 3.92316\n",
      "Epoch 490/1000, Training Loss: 29.90914, Validation Loss: 3.92276\n",
      "Epoch 491/1000, Training Loss: 29.90449, Validation Loss: 3.92240\n",
      "Epoch 492/1000, Training Loss: 29.89990, Validation Loss: 3.92197\n",
      "Epoch 493/1000, Training Loss: 29.89529, Validation Loss: 3.92155\n",
      "Epoch 494/1000, Training Loss: 29.89069, Validation Loss: 3.92119\n",
      "Epoch 495/1000, Training Loss: 29.88608, Validation Loss: 3.92077\n",
      "Epoch 496/1000, Training Loss: 29.88159, Validation Loss: 3.92040\n",
      "Epoch 497/1000, Training Loss: 29.87691, Validation Loss: 3.91997\n",
      "Epoch 498/1000, Training Loss: 29.87238, Validation Loss: 3.91955\n",
      "Epoch 499/1000, Training Loss: 29.86770, Validation Loss: 3.91917\n",
      "Epoch 500/1000, Training Loss: 29.86317, Validation Loss: 3.91876\n",
      "Epoch 501/1000, Training Loss: 29.85862, Validation Loss: 3.91838\n",
      "Epoch 502/1000, Training Loss: 29.85402, Validation Loss: 3.91797\n",
      "Epoch 503/1000, Training Loss: 29.84943, Validation Loss: 3.91762\n",
      "Epoch 504/1000, Training Loss: 29.84481, Validation Loss: 3.91722\n",
      "Epoch 505/1000, Training Loss: 29.84032, Validation Loss: 3.91688\n",
      "Epoch 506/1000, Training Loss: 29.83573, Validation Loss: 3.91648\n",
      "Epoch 507/1000, Training Loss: 29.83116, Validation Loss: 3.91612\n",
      "Epoch 508/1000, Training Loss: 29.82655, Validation Loss: 3.91575\n",
      "Epoch 509/1000, Training Loss: 29.82199, Validation Loss: 3.91536\n",
      "Epoch 510/1000, Training Loss: 29.81739, Validation Loss: 3.91501\n",
      "Epoch 511/1000, Training Loss: 29.81284, Validation Loss: 3.91461\n",
      "Epoch 512/1000, Training Loss: 29.80824, Validation Loss: 3.91428\n",
      "Epoch 513/1000, Training Loss: 29.80367, Validation Loss: 3.91393\n",
      "Epoch 514/1000, Training Loss: 29.79913, Validation Loss: 3.91354\n",
      "Epoch 515/1000, Training Loss: 29.79458, Validation Loss: 3.91317\n",
      "Epoch 516/1000, Training Loss: 29.78994, Validation Loss: 3.91282\n",
      "Epoch 517/1000, Training Loss: 29.78546, Validation Loss: 3.91245\n",
      "Epoch 518/1000, Training Loss: 29.78085, Validation Loss: 3.91210\n",
      "Epoch 519/1000, Training Loss: 29.77630, Validation Loss: 3.91176\n",
      "Epoch 520/1000, Training Loss: 29.77178, Validation Loss: 3.91142\n",
      "Epoch 521/1000, Training Loss: 29.76721, Validation Loss: 3.91110\n",
      "Epoch 522/1000, Training Loss: 29.76277, Validation Loss: 3.91075\n",
      "Epoch 523/1000, Training Loss: 29.75818, Validation Loss: 3.91039\n",
      "Epoch 524/1000, Training Loss: 29.75356, Validation Loss: 3.91005\n",
      "Epoch 525/1000, Training Loss: 29.74902, Validation Loss: 3.90972\n",
      "Epoch 526/1000, Training Loss: 29.74453, Validation Loss: 3.90934\n",
      "Epoch 527/1000, Training Loss: 29.73995, Validation Loss: 3.90908\n",
      "Epoch 528/1000, Training Loss: 29.73547, Validation Loss: 3.90868\n",
      "Epoch 529/1000, Training Loss: 29.73096, Validation Loss: 3.90833\n",
      "Epoch 530/1000, Training Loss: 29.72632, Validation Loss: 3.90802\n",
      "Epoch 531/1000, Training Loss: 29.72186, Validation Loss: 3.90765\n",
      "Epoch 532/1000, Training Loss: 29.71730, Validation Loss: 3.90737\n",
      "Epoch 533/1000, Training Loss: 29.71281, Validation Loss: 3.90701\n",
      "Epoch 534/1000, Training Loss: 29.70827, Validation Loss: 3.90673\n",
      "Epoch 535/1000, Training Loss: 29.70379, Validation Loss: 3.90639\n",
      "Epoch 536/1000, Training Loss: 29.69923, Validation Loss: 3.90610\n",
      "Epoch 537/1000, Training Loss: 29.69489, Validation Loss: 3.90578\n",
      "Epoch 538/1000, Training Loss: 29.69027, Validation Loss: 3.90547\n",
      "Epoch 539/1000, Training Loss: 29.68584, Validation Loss: 3.90514\n",
      "Epoch 540/1000, Training Loss: 29.68133, Validation Loss: 3.90484\n",
      "Epoch 541/1000, Training Loss: 29.67681, Validation Loss: 3.90453\n",
      "Epoch 542/1000, Training Loss: 29.67239, Validation Loss: 3.90424\n",
      "Epoch 543/1000, Training Loss: 29.66786, Validation Loss: 3.90393\n",
      "Epoch 544/1000, Training Loss: 29.66343, Validation Loss: 3.90363\n",
      "Epoch 545/1000, Training Loss: 29.65897, Validation Loss: 3.90336\n",
      "Epoch 546/1000, Training Loss: 29.65455, Validation Loss: 3.90304\n",
      "Epoch 547/1000, Training Loss: 29.65006, Validation Loss: 3.90271\n",
      "Epoch 548/1000, Training Loss: 29.64554, Validation Loss: 3.90242\n",
      "Epoch 549/1000, Training Loss: 29.64115, Validation Loss: 3.90208\n",
      "Epoch 550/1000, Training Loss: 29.63665, Validation Loss: 3.90173\n",
      "Epoch 551/1000, Training Loss: 29.63220, Validation Loss: 3.90147\n",
      "Epoch 552/1000, Training Loss: 29.62779, Validation Loss: 3.90114\n",
      "Epoch 553/1000, Training Loss: 29.62332, Validation Loss: 3.90080\n",
      "Epoch 554/1000, Training Loss: 29.61890, Validation Loss: 3.90052\n",
      "Epoch 555/1000, Training Loss: 29.61438, Validation Loss: 3.90020\n",
      "Epoch 556/1000, Training Loss: 29.61002, Validation Loss: 3.89990\n",
      "Epoch 557/1000, Training Loss: 29.60559, Validation Loss: 3.89964\n",
      "Epoch 558/1000, Training Loss: 29.60120, Validation Loss: 3.89928\n",
      "Epoch 559/1000, Training Loss: 29.59676, Validation Loss: 3.89898\n",
      "Epoch 560/1000, Training Loss: 29.59229, Validation Loss: 3.89872\n",
      "Epoch 561/1000, Training Loss: 29.58791, Validation Loss: 3.89838\n",
      "Epoch 562/1000, Training Loss: 29.58347, Validation Loss: 3.89808\n",
      "Epoch 563/1000, Training Loss: 29.57898, Validation Loss: 3.89781\n",
      "Epoch 564/1000, Training Loss: 29.57462, Validation Loss: 3.89750\n",
      "Epoch 565/1000, Training Loss: 29.57017, Validation Loss: 3.89723\n",
      "Epoch 566/1000, Training Loss: 29.56568, Validation Loss: 3.89696\n",
      "Epoch 567/1000, Training Loss: 29.56136, Validation Loss: 3.89668\n",
      "Epoch 568/1000, Training Loss: 29.55690, Validation Loss: 3.89641\n",
      "Epoch 569/1000, Training Loss: 29.55245, Validation Loss: 3.89617\n",
      "Epoch 570/1000, Training Loss: 29.54818, Validation Loss: 3.89588\n",
      "Epoch 571/1000, Training Loss: 29.54374, Validation Loss: 3.89560\n",
      "Epoch 572/1000, Training Loss: 29.53933, Validation Loss: 3.89530\n",
      "Epoch 573/1000, Training Loss: 29.53490, Validation Loss: 3.89506\n",
      "Epoch 574/1000, Training Loss: 29.53056, Validation Loss: 3.89479\n",
      "Epoch 575/1000, Training Loss: 29.52610, Validation Loss: 3.89454\n",
      "Epoch 576/1000, Training Loss: 29.52171, Validation Loss: 3.89426\n",
      "Epoch 577/1000, Training Loss: 29.51738, Validation Loss: 3.89400\n",
      "Epoch 578/1000, Training Loss: 29.51292, Validation Loss: 3.89375\n",
      "Epoch 579/1000, Training Loss: 29.50850, Validation Loss: 3.89349\n",
      "Epoch 580/1000, Training Loss: 29.50412, Validation Loss: 3.89323\n",
      "Epoch 581/1000, Training Loss: 29.49969, Validation Loss: 3.89298\n",
      "Epoch 582/1000, Training Loss: 29.49527, Validation Loss: 3.89271\n",
      "Epoch 583/1000, Training Loss: 29.49080, Validation Loss: 3.89250\n",
      "Epoch 584/1000, Training Loss: 29.48645, Validation Loss: 3.89226\n",
      "Epoch 585/1000, Training Loss: 29.48205, Validation Loss: 3.89197\n",
      "Epoch 586/1000, Training Loss: 29.47754, Validation Loss: 3.89170\n",
      "Epoch 587/1000, Training Loss: 29.47313, Validation Loss: 3.89144\n",
      "Epoch 588/1000, Training Loss: 29.46860, Validation Loss: 3.89118\n",
      "Epoch 589/1000, Training Loss: 29.46426, Validation Loss: 3.89089\n",
      "Epoch 590/1000, Training Loss: 29.45983, Validation Loss: 3.89063\n",
      "Epoch 591/1000, Training Loss: 29.45537, Validation Loss: 3.89035\n",
      "Epoch 592/1000, Training Loss: 29.45095, Validation Loss: 3.89005\n",
      "Epoch 593/1000, Training Loss: 29.44656, Validation Loss: 3.88985\n",
      "Epoch 594/1000, Training Loss: 29.44230, Validation Loss: 3.88954\n",
      "Epoch 595/1000, Training Loss: 29.43790, Validation Loss: 3.88924\n",
      "Epoch 596/1000, Training Loss: 29.43345, Validation Loss: 3.88897\n",
      "Epoch 597/1000, Training Loss: 29.42906, Validation Loss: 3.88869\n",
      "Epoch 598/1000, Training Loss: 29.42462, Validation Loss: 3.88842\n",
      "Epoch 599/1000, Training Loss: 29.42020, Validation Loss: 3.88814\n",
      "Epoch 600/1000, Training Loss: 29.41581, Validation Loss: 3.88787\n",
      "Epoch 601/1000, Training Loss: 29.41145, Validation Loss: 3.88759\n",
      "Epoch 602/1000, Training Loss: 29.40704, Validation Loss: 3.88733\n",
      "Epoch 603/1000, Training Loss: 29.40273, Validation Loss: 3.88704\n",
      "Epoch 604/1000, Training Loss: 29.39833, Validation Loss: 3.88677\n",
      "Epoch 605/1000, Training Loss: 29.39386, Validation Loss: 3.88648\n",
      "Epoch 606/1000, Training Loss: 29.38948, Validation Loss: 3.88619\n",
      "Epoch 607/1000, Training Loss: 29.38511, Validation Loss: 3.88593\n",
      "Epoch 608/1000, Training Loss: 29.38065, Validation Loss: 3.88568\n",
      "Epoch 609/1000, Training Loss: 29.37637, Validation Loss: 3.88535\n",
      "Epoch 610/1000, Training Loss: 29.37187, Validation Loss: 3.88508\n",
      "Epoch 611/1000, Training Loss: 29.36750, Validation Loss: 3.88480\n",
      "Epoch 612/1000, Training Loss: 29.36307, Validation Loss: 3.88454\n",
      "Epoch 613/1000, Training Loss: 29.35870, Validation Loss: 3.88427\n",
      "Epoch 614/1000, Training Loss: 29.35428, Validation Loss: 3.88403\n",
      "Epoch 615/1000, Training Loss: 29.35000, Validation Loss: 3.88377\n",
      "Epoch 616/1000, Training Loss: 29.34550, Validation Loss: 3.88352\n",
      "Epoch 617/1000, Training Loss: 29.34113, Validation Loss: 3.88324\n",
      "Epoch 618/1000, Training Loss: 29.33674, Validation Loss: 3.88299\n",
      "Epoch 619/1000, Training Loss: 29.33232, Validation Loss: 3.88274\n",
      "Epoch 620/1000, Training Loss: 29.32794, Validation Loss: 3.88249\n",
      "Epoch 621/1000, Training Loss: 29.32364, Validation Loss: 3.88228\n",
      "Epoch 622/1000, Training Loss: 29.31930, Validation Loss: 3.88203\n",
      "Epoch 623/1000, Training Loss: 29.31484, Validation Loss: 3.88178\n",
      "Epoch 624/1000, Training Loss: 29.31047, Validation Loss: 3.88153\n",
      "Epoch 625/1000, Training Loss: 29.30607, Validation Loss: 3.88128\n",
      "Epoch 626/1000, Training Loss: 29.30175, Validation Loss: 3.88105\n",
      "Epoch 627/1000, Training Loss: 29.29735, Validation Loss: 3.88081\n",
      "Epoch 628/1000, Training Loss: 29.29303, Validation Loss: 3.88053\n",
      "Epoch 629/1000, Training Loss: 29.28867, Validation Loss: 3.88029\n",
      "Epoch 630/1000, Training Loss: 29.28428, Validation Loss: 3.88005\n",
      "Epoch 631/1000, Training Loss: 29.27999, Validation Loss: 3.87980\n",
      "Epoch 632/1000, Training Loss: 29.27562, Validation Loss: 3.87954\n",
      "Epoch 633/1000, Training Loss: 29.27130, Validation Loss: 3.87930\n",
      "Epoch 634/1000, Training Loss: 29.26702, Validation Loss: 3.87908\n",
      "Epoch 635/1000, Training Loss: 29.26274, Validation Loss: 3.87882\n",
      "Epoch 636/1000, Training Loss: 29.25830, Validation Loss: 3.87856\n",
      "Epoch 637/1000, Training Loss: 29.25402, Validation Loss: 3.87833\n",
      "Epoch 638/1000, Training Loss: 29.24963, Validation Loss: 3.87807\n",
      "Epoch 639/1000, Training Loss: 29.24526, Validation Loss: 3.87781\n",
      "Epoch 640/1000, Training Loss: 29.24093, Validation Loss: 3.87756\n",
      "Epoch 641/1000, Training Loss: 29.23669, Validation Loss: 3.87734\n",
      "Epoch 642/1000, Training Loss: 29.23222, Validation Loss: 3.87706\n",
      "Epoch 643/1000, Training Loss: 29.22784, Validation Loss: 3.87679\n",
      "Epoch 644/1000, Training Loss: 29.22350, Validation Loss: 3.87653\n",
      "Epoch 645/1000, Training Loss: 29.21911, Validation Loss: 3.87627\n",
      "Epoch 646/1000, Training Loss: 29.21472, Validation Loss: 3.87602\n",
      "Epoch 647/1000, Training Loss: 29.21031, Validation Loss: 3.87577\n",
      "Epoch 648/1000, Training Loss: 29.20596, Validation Loss: 3.87551\n",
      "Epoch 649/1000, Training Loss: 29.20153, Validation Loss: 3.87526\n",
      "Epoch 650/1000, Training Loss: 29.19710, Validation Loss: 3.87503\n",
      "Epoch 651/1000, Training Loss: 29.19267, Validation Loss: 3.87477\n",
      "Epoch 652/1000, Training Loss: 29.18825, Validation Loss: 3.87453\n",
      "Epoch 653/1000, Training Loss: 29.18388, Validation Loss: 3.87424\n",
      "Epoch 654/1000, Training Loss: 29.17954, Validation Loss: 3.87403\n",
      "Epoch 655/1000, Training Loss: 29.17524, Validation Loss: 3.87376\n",
      "Epoch 656/1000, Training Loss: 29.17084, Validation Loss: 3.87350\n",
      "Epoch 657/1000, Training Loss: 29.16650, Validation Loss: 3.87323\n",
      "Epoch 658/1000, Training Loss: 29.16217, Validation Loss: 3.87301\n",
      "Epoch 659/1000, Training Loss: 29.15776, Validation Loss: 3.87278\n",
      "Epoch 660/1000, Training Loss: 29.15350, Validation Loss: 3.87252\n",
      "Epoch 661/1000, Training Loss: 29.14906, Validation Loss: 3.87229\n",
      "Epoch 662/1000, Training Loss: 29.14469, Validation Loss: 3.87205\n",
      "Epoch 663/1000, Training Loss: 29.14029, Validation Loss: 3.87180\n",
      "Epoch 664/1000, Training Loss: 29.13594, Validation Loss: 3.87152\n",
      "Epoch 665/1000, Training Loss: 29.13157, Validation Loss: 3.87130\n",
      "Epoch 666/1000, Training Loss: 29.12717, Validation Loss: 3.87109\n",
      "Epoch 667/1000, Training Loss: 29.12281, Validation Loss: 3.87086\n",
      "Epoch 668/1000, Training Loss: 29.11842, Validation Loss: 3.87062\n",
      "Epoch 669/1000, Training Loss: 29.11405, Validation Loss: 3.87037\n",
      "Epoch 670/1000, Training Loss: 29.10969, Validation Loss: 3.87018\n",
      "Epoch 671/1000, Training Loss: 29.10549, Validation Loss: 3.86998\n",
      "Epoch 672/1000, Training Loss: 29.10100, Validation Loss: 3.86975\n",
      "Epoch 673/1000, Training Loss: 29.09662, Validation Loss: 3.86952\n",
      "Epoch 674/1000, Training Loss: 29.09225, Validation Loss: 3.86930\n",
      "Epoch 675/1000, Training Loss: 29.08789, Validation Loss: 3.86909\n",
      "Epoch 676/1000, Training Loss: 29.08348, Validation Loss: 3.86890\n",
      "Epoch 677/1000, Training Loss: 29.07911, Validation Loss: 3.86871\n",
      "Epoch 678/1000, Training Loss: 29.07469, Validation Loss: 3.86851\n",
      "Epoch 679/1000, Training Loss: 29.07031, Validation Loss: 3.86831\n",
      "Epoch 680/1000, Training Loss: 29.06593, Validation Loss: 3.86815\n",
      "Epoch 681/1000, Training Loss: 29.06161, Validation Loss: 3.86796\n",
      "Epoch 682/1000, Training Loss: 29.05713, Validation Loss: 3.86778\n",
      "Epoch 683/1000, Training Loss: 29.05280, Validation Loss: 3.86758\n",
      "Epoch 684/1000, Training Loss: 29.04838, Validation Loss: 3.86742\n",
      "Epoch 685/1000, Training Loss: 29.04403, Validation Loss: 3.86724\n",
      "Epoch 686/1000, Training Loss: 29.03962, Validation Loss: 3.86704\n",
      "Epoch 687/1000, Training Loss: 29.03518, Validation Loss: 3.86685\n",
      "Epoch 688/1000, Training Loss: 29.03085, Validation Loss: 3.86668\n",
      "Epoch 689/1000, Training Loss: 29.02643, Validation Loss: 3.86655\n",
      "Epoch 690/1000, Training Loss: 29.02203, Validation Loss: 3.86636\n",
      "Epoch 691/1000, Training Loss: 29.01757, Validation Loss: 3.86620\n",
      "Epoch 692/1000, Training Loss: 29.01315, Validation Loss: 3.86603\n",
      "Epoch 693/1000, Training Loss: 29.00860, Validation Loss: 3.86587\n",
      "Epoch 694/1000, Training Loss: 29.00430, Validation Loss: 3.86571\n",
      "Epoch 695/1000, Training Loss: 28.99988, Validation Loss: 3.86558\n",
      "Epoch 696/1000, Training Loss: 28.99530, Validation Loss: 3.86541\n",
      "Epoch 697/1000, Training Loss: 28.99096, Validation Loss: 3.86528\n",
      "Epoch 698/1000, Training Loss: 28.98645, Validation Loss: 3.86515\n",
      "Epoch 699/1000, Training Loss: 28.98205, Validation Loss: 3.86497\n",
      "Epoch 700/1000, Training Loss: 28.97754, Validation Loss: 3.86489\n",
      "Epoch 701/1000, Training Loss: 28.97312, Validation Loss: 3.86472\n",
      "Epoch 702/1000, Training Loss: 28.96860, Validation Loss: 3.86458\n",
      "Epoch 703/1000, Training Loss: 28.96415, Validation Loss: 3.86442\n",
      "Epoch 704/1000, Training Loss: 28.95973, Validation Loss: 3.86422\n",
      "Epoch 705/1000, Training Loss: 28.95528, Validation Loss: 3.86406\n",
      "Epoch 706/1000, Training Loss: 28.95100, Validation Loss: 3.86394\n",
      "Epoch 707/1000, Training Loss: 28.94657, Validation Loss: 3.86375\n",
      "Epoch 708/1000, Training Loss: 28.94214, Validation Loss: 3.86360\n",
      "Epoch 709/1000, Training Loss: 28.93787, Validation Loss: 3.86343\n",
      "Epoch 710/1000, Training Loss: 28.93352, Validation Loss: 3.86326\n",
      "Epoch 711/1000, Training Loss: 28.92916, Validation Loss: 3.86314\n",
      "Epoch 712/1000, Training Loss: 28.92482, Validation Loss: 3.86297\n",
      "Epoch 713/1000, Training Loss: 28.92041, Validation Loss: 3.86282\n",
      "Epoch 714/1000, Training Loss: 28.91600, Validation Loss: 3.86265\n",
      "Epoch 715/1000, Training Loss: 28.91172, Validation Loss: 3.86250\n",
      "Epoch 716/1000, Training Loss: 28.90731, Validation Loss: 3.86231\n",
      "Epoch 717/1000, Training Loss: 28.90299, Validation Loss: 3.86213\n",
      "Epoch 718/1000, Training Loss: 28.89863, Validation Loss: 3.86198\n",
      "Epoch 719/1000, Training Loss: 28.89430, Validation Loss: 3.86181\n",
      "Epoch 720/1000, Training Loss: 28.89001, Validation Loss: 3.86171\n",
      "Epoch 721/1000, Training Loss: 28.88571, Validation Loss: 3.86153\n",
      "Epoch 722/1000, Training Loss: 28.88145, Validation Loss: 3.86137\n",
      "Epoch 723/1000, Training Loss: 28.87714, Validation Loss: 3.86124\n",
      "Epoch 724/1000, Training Loss: 28.87284, Validation Loss: 3.86105\n",
      "Epoch 725/1000, Training Loss: 28.86853, Validation Loss: 3.86093\n",
      "Epoch 726/1000, Training Loss: 28.86420, Validation Loss: 3.86075\n",
      "Epoch 727/1000, Training Loss: 28.85984, Validation Loss: 3.86061\n",
      "Epoch 728/1000, Training Loss: 28.85560, Validation Loss: 3.86048\n",
      "Epoch 729/1000, Training Loss: 28.85125, Validation Loss: 3.86034\n",
      "Epoch 730/1000, Training Loss: 28.84700, Validation Loss: 3.86022\n",
      "Epoch 731/1000, Training Loss: 28.84273, Validation Loss: 3.86005\n",
      "Epoch 732/1000, Training Loss: 28.83849, Validation Loss: 3.85996\n",
      "Epoch 733/1000, Training Loss: 28.83423, Validation Loss: 3.85981\n",
      "Epoch 734/1000, Training Loss: 28.82994, Validation Loss: 3.85967\n",
      "Epoch 735/1000, Training Loss: 28.82566, Validation Loss: 3.85953\n",
      "Epoch 736/1000, Training Loss: 28.82138, Validation Loss: 3.85937\n",
      "Epoch 737/1000, Training Loss: 28.81705, Validation Loss: 3.85922\n",
      "Epoch 738/1000, Training Loss: 28.81279, Validation Loss: 3.85909\n",
      "Epoch 739/1000, Training Loss: 28.80851, Validation Loss: 3.85893\n",
      "Epoch 740/1000, Training Loss: 28.80426, Validation Loss: 3.85877\n",
      "Epoch 741/1000, Training Loss: 28.80003, Validation Loss: 3.85859\n",
      "Epoch 742/1000, Training Loss: 28.79572, Validation Loss: 3.85840\n",
      "Epoch 743/1000, Training Loss: 28.79149, Validation Loss: 3.85823\n",
      "Epoch 744/1000, Training Loss: 28.78716, Validation Loss: 3.85804\n",
      "Epoch 745/1000, Training Loss: 28.78267, Validation Loss: 3.85784\n",
      "Epoch 746/1000, Training Loss: 28.77825, Validation Loss: 3.85767\n",
      "Epoch 747/1000, Training Loss: 28.77348, Validation Loss: 3.85744\n",
      "Epoch 748/1000, Training Loss: 28.76791, Validation Loss: 3.85722\n",
      "Epoch 749/1000, Training Loss: 28.75870, Validation Loss: 3.85695\n",
      "Epoch 750/1000, Training Loss: 28.75297, Validation Loss: 3.85673\n",
      "Epoch 751/1000, Training Loss: 28.74874, Validation Loss: 3.85656\n",
      "Epoch 752/1000, Training Loss: 28.74461, Validation Loss: 3.85639\n",
      "Epoch 753/1000, Training Loss: 28.74035, Validation Loss: 3.85620\n",
      "Epoch 754/1000, Training Loss: 28.73615, Validation Loss: 3.85602\n",
      "Epoch 755/1000, Training Loss: 28.73197, Validation Loss: 3.85584\n",
      "Epoch 756/1000, Training Loss: 28.72779, Validation Loss: 3.85568\n",
      "Epoch 757/1000, Training Loss: 28.72356, Validation Loss: 3.85551\n",
      "Epoch 758/1000, Training Loss: 28.71928, Validation Loss: 3.85534\n",
      "Epoch 759/1000, Training Loss: 28.71514, Validation Loss: 3.85518\n",
      "Epoch 760/1000, Training Loss: 28.71091, Validation Loss: 3.85499\n",
      "Epoch 761/1000, Training Loss: 28.70670, Validation Loss: 3.85485\n",
      "Epoch 762/1000, Training Loss: 28.70251, Validation Loss: 3.85467\n",
      "Epoch 763/1000, Training Loss: 28.69829, Validation Loss: 3.85451\n",
      "Epoch 764/1000, Training Loss: 28.69408, Validation Loss: 3.85434\n",
      "Epoch 765/1000, Training Loss: 28.68986, Validation Loss: 3.85418\n",
      "Epoch 766/1000, Training Loss: 28.68558, Validation Loss: 3.85402\n",
      "Epoch 767/1000, Training Loss: 28.68141, Validation Loss: 3.85386\n",
      "Epoch 768/1000, Training Loss: 28.67717, Validation Loss: 3.85369\n",
      "Epoch 769/1000, Training Loss: 28.67291, Validation Loss: 3.85354\n",
      "Epoch 770/1000, Training Loss: 28.66876, Validation Loss: 3.85338\n",
      "Epoch 771/1000, Training Loss: 28.66457, Validation Loss: 3.85326\n",
      "Epoch 772/1000, Training Loss: 28.66039, Validation Loss: 3.85310\n",
      "Epoch 773/1000, Training Loss: 28.65614, Validation Loss: 3.85294\n",
      "Epoch 774/1000, Training Loss: 28.65199, Validation Loss: 3.85278\n",
      "Epoch 775/1000, Training Loss: 28.64775, Validation Loss: 3.85264\n",
      "Epoch 776/1000, Training Loss: 28.64360, Validation Loss: 3.85248\n",
      "Epoch 777/1000, Training Loss: 28.63935, Validation Loss: 3.85233\n",
      "Epoch 778/1000, Training Loss: 28.63522, Validation Loss: 3.85218\n",
      "Epoch 779/1000, Training Loss: 28.63100, Validation Loss: 3.85204\n",
      "Epoch 780/1000, Training Loss: 28.62676, Validation Loss: 3.85189\n",
      "Epoch 781/1000, Training Loss: 28.62265, Validation Loss: 3.85173\n",
      "Epoch 782/1000, Training Loss: 28.61842, Validation Loss: 3.85157\n",
      "Epoch 783/1000, Training Loss: 28.61412, Validation Loss: 3.85140\n",
      "Epoch 784/1000, Training Loss: 28.61002, Validation Loss: 3.85127\n",
      "Epoch 785/1000, Training Loss: 28.60581, Validation Loss: 3.85109\n",
      "Epoch 786/1000, Training Loss: 28.60165, Validation Loss: 3.85092\n",
      "Epoch 787/1000, Training Loss: 28.59740, Validation Loss: 3.85077\n",
      "Epoch 788/1000, Training Loss: 28.59326, Validation Loss: 3.85057\n",
      "Epoch 789/1000, Training Loss: 28.58905, Validation Loss: 3.85048\n",
      "Epoch 790/1000, Training Loss: 28.58476, Validation Loss: 3.85031\n",
      "Epoch 791/1000, Training Loss: 28.58060, Validation Loss: 3.85014\n",
      "Epoch 792/1000, Training Loss: 28.57635, Validation Loss: 3.84998\n",
      "Epoch 793/1000, Training Loss: 28.57210, Validation Loss: 3.84982\n",
      "Epoch 794/1000, Training Loss: 28.56794, Validation Loss: 3.84969\n",
      "Epoch 795/1000, Training Loss: 28.56370, Validation Loss: 3.84950\n",
      "Epoch 796/1000, Training Loss: 28.55955, Validation Loss: 3.84934\n",
      "Epoch 797/1000, Training Loss: 28.55533, Validation Loss: 3.84917\n",
      "Epoch 798/1000, Training Loss: 28.55110, Validation Loss: 3.84904\n",
      "Epoch 799/1000, Training Loss: 28.54690, Validation Loss: 3.84888\n",
      "Epoch 800/1000, Training Loss: 28.54269, Validation Loss: 3.84874\n",
      "Epoch 801/1000, Training Loss: 28.53863, Validation Loss: 3.84857\n",
      "Epoch 802/1000, Training Loss: 28.53442, Validation Loss: 3.84839\n",
      "Epoch 803/1000, Training Loss: 28.53024, Validation Loss: 3.84827\n",
      "Epoch 804/1000, Training Loss: 28.52598, Validation Loss: 3.84808\n",
      "Epoch 805/1000, Training Loss: 28.52183, Validation Loss: 3.84795\n",
      "Epoch 806/1000, Training Loss: 28.51766, Validation Loss: 3.84776\n",
      "Epoch 807/1000, Training Loss: 28.51335, Validation Loss: 3.84766\n",
      "Epoch 808/1000, Training Loss: 28.50936, Validation Loss: 3.84750\n",
      "Epoch 809/1000, Training Loss: 28.50505, Validation Loss: 3.84732\n",
      "Epoch 810/1000, Training Loss: 28.50087, Validation Loss: 3.84716\n",
      "Epoch 811/1000, Training Loss: 28.49670, Validation Loss: 3.84699\n",
      "Epoch 812/1000, Training Loss: 28.49251, Validation Loss: 3.84684\n",
      "Epoch 813/1000, Training Loss: 28.48834, Validation Loss: 3.84667\n",
      "Epoch 814/1000, Training Loss: 28.48410, Validation Loss: 3.84652\n",
      "Epoch 815/1000, Training Loss: 28.47986, Validation Loss: 3.84636\n",
      "Epoch 816/1000, Training Loss: 28.47575, Validation Loss: 3.84620\n",
      "Epoch 817/1000, Training Loss: 28.47146, Validation Loss: 3.84603\n",
      "Epoch 818/1000, Training Loss: 28.46724, Validation Loss: 3.84584\n",
      "Epoch 819/1000, Training Loss: 28.46299, Validation Loss: 3.84569\n",
      "Epoch 820/1000, Training Loss: 28.45889, Validation Loss: 3.84550\n",
      "Epoch 821/1000, Training Loss: 28.45457, Validation Loss: 3.84533\n",
      "Epoch 822/1000, Training Loss: 28.45030, Validation Loss: 3.84515\n",
      "Epoch 823/1000, Training Loss: 28.44609, Validation Loss: 3.84500\n",
      "Epoch 824/1000, Training Loss: 28.44188, Validation Loss: 3.84482\n",
      "Epoch 825/1000, Training Loss: 28.43759, Validation Loss: 3.84464\n",
      "Epoch 826/1000, Training Loss: 28.43343, Validation Loss: 3.84448\n",
      "Epoch 827/1000, Training Loss: 28.42907, Validation Loss: 3.84431\n",
      "Epoch 828/1000, Training Loss: 28.42493, Validation Loss: 3.84411\n",
      "Epoch 829/1000, Training Loss: 28.42070, Validation Loss: 3.84396\n",
      "Epoch 830/1000, Training Loss: 28.41652, Validation Loss: 3.84380\n",
      "Epoch 831/1000, Training Loss: 28.41229, Validation Loss: 3.84360\n",
      "Epoch 832/1000, Training Loss: 28.40798, Validation Loss: 3.84344\n",
      "Epoch 833/1000, Training Loss: 28.40371, Validation Loss: 3.84328\n",
      "Epoch 834/1000, Training Loss: 28.39955, Validation Loss: 3.84311\n",
      "Epoch 835/1000, Training Loss: 28.39520, Validation Loss: 3.84293\n",
      "Epoch 836/1000, Training Loss: 28.39103, Validation Loss: 3.84280\n",
      "Epoch 837/1000, Training Loss: 28.38674, Validation Loss: 3.84260\n",
      "Epoch 838/1000, Training Loss: 28.38249, Validation Loss: 3.84244\n",
      "Epoch 839/1000, Training Loss: 28.37825, Validation Loss: 3.84231\n",
      "Epoch 840/1000, Training Loss: 28.37408, Validation Loss: 3.84212\n",
      "Epoch 841/1000, Training Loss: 28.36984, Validation Loss: 3.84199\n",
      "Epoch 842/1000, Training Loss: 28.36548, Validation Loss: 3.84182\n",
      "Epoch 843/1000, Training Loss: 28.36122, Validation Loss: 3.84165\n",
      "Epoch 844/1000, Training Loss: 28.35704, Validation Loss: 3.84151\n",
      "Epoch 845/1000, Training Loss: 28.35283, Validation Loss: 3.84131\n",
      "Epoch 846/1000, Training Loss: 28.34865, Validation Loss: 3.84119\n",
      "Epoch 847/1000, Training Loss: 28.34434, Validation Loss: 3.84101\n",
      "Epoch 848/1000, Training Loss: 28.34024, Validation Loss: 3.84090\n",
      "Epoch 849/1000, Training Loss: 28.33602, Validation Loss: 3.84072\n",
      "Epoch 850/1000, Training Loss: 28.33178, Validation Loss: 3.84055\n",
      "Epoch 851/1000, Training Loss: 28.32767, Validation Loss: 3.84038\n",
      "Epoch 852/1000, Training Loss: 28.32341, Validation Loss: 3.84022\n",
      "Epoch 853/1000, Training Loss: 28.31933, Validation Loss: 3.84008\n",
      "Epoch 854/1000, Training Loss: 28.31507, Validation Loss: 3.83990\n",
      "Epoch 855/1000, Training Loss: 28.31100, Validation Loss: 3.83970\n",
      "Epoch 856/1000, Training Loss: 28.30685, Validation Loss: 3.83961\n",
      "Epoch 857/1000, Training Loss: 28.30265, Validation Loss: 3.83945\n",
      "Epoch 858/1000, Training Loss: 28.29868, Validation Loss: 3.83928\n",
      "Epoch 859/1000, Training Loss: 28.29451, Validation Loss: 3.83912\n",
      "Epoch 860/1000, Training Loss: 28.29032, Validation Loss: 3.83897\n",
      "Epoch 861/1000, Training Loss: 28.28623, Validation Loss: 3.83879\n",
      "Epoch 862/1000, Training Loss: 28.28208, Validation Loss: 3.83861\n",
      "Epoch 863/1000, Training Loss: 28.27796, Validation Loss: 3.83846\n",
      "Epoch 864/1000, Training Loss: 28.27383, Validation Loss: 3.83832\n",
      "Epoch 865/1000, Training Loss: 28.26970, Validation Loss: 3.83818\n",
      "Epoch 866/1000, Training Loss: 28.26555, Validation Loss: 3.83802\n",
      "Epoch 867/1000, Training Loss: 28.26156, Validation Loss: 3.83783\n",
      "Epoch 868/1000, Training Loss: 28.25743, Validation Loss: 3.83768\n",
      "Epoch 869/1000, Training Loss: 28.25331, Validation Loss: 3.83750\n",
      "Epoch 870/1000, Training Loss: 28.24922, Validation Loss: 3.83735\n",
      "Epoch 871/1000, Training Loss: 28.24504, Validation Loss: 3.83720\n",
      "Epoch 872/1000, Training Loss: 28.24107, Validation Loss: 3.83706\n",
      "Epoch 873/1000, Training Loss: 28.23688, Validation Loss: 3.83687\n",
      "Epoch 874/1000, Training Loss: 28.23280, Validation Loss: 3.83675\n",
      "Epoch 875/1000, Training Loss: 28.22873, Validation Loss: 3.83663\n",
      "Epoch 876/1000, Training Loss: 28.22467, Validation Loss: 3.83645\n",
      "Epoch 877/1000, Training Loss: 28.22056, Validation Loss: 3.83632\n",
      "Epoch 878/1000, Training Loss: 28.21645, Validation Loss: 3.83614\n",
      "Epoch 879/1000, Training Loss: 28.21225, Validation Loss: 3.83598\n",
      "Epoch 880/1000, Training Loss: 28.20823, Validation Loss: 3.83580\n",
      "Epoch 881/1000, Training Loss: 28.20403, Validation Loss: 3.83566\n",
      "Epoch 882/1000, Training Loss: 28.19995, Validation Loss: 3.83552\n",
      "Epoch 883/1000, Training Loss: 28.19586, Validation Loss: 3.83534\n",
      "Epoch 884/1000, Training Loss: 28.19171, Validation Loss: 3.83520\n",
      "Epoch 885/1000, Training Loss: 28.18780, Validation Loss: 3.83506\n",
      "Epoch 886/1000, Training Loss: 28.18353, Validation Loss: 3.83486\n",
      "Epoch 887/1000, Training Loss: 28.17933, Validation Loss: 3.83471\n",
      "Epoch 888/1000, Training Loss: 28.17532, Validation Loss: 3.83455\n",
      "Epoch 889/1000, Training Loss: 28.17106, Validation Loss: 3.83439\n",
      "Epoch 890/1000, Training Loss: 28.16697, Validation Loss: 3.83429\n",
      "Epoch 891/1000, Training Loss: 28.16282, Validation Loss: 3.83411\n",
      "Epoch 892/1000, Training Loss: 28.15863, Validation Loss: 3.83396\n",
      "Epoch 893/1000, Training Loss: 28.15461, Validation Loss: 3.83384\n",
      "Epoch 894/1000, Training Loss: 28.15043, Validation Loss: 3.83365\n",
      "Epoch 895/1000, Training Loss: 28.14633, Validation Loss: 3.83351\n",
      "Epoch 896/1000, Training Loss: 28.14210, Validation Loss: 3.83337\n",
      "Epoch 897/1000, Training Loss: 28.13792, Validation Loss: 3.83321\n",
      "Epoch 898/1000, Training Loss: 28.13375, Validation Loss: 3.83310\n",
      "Epoch 899/1000, Training Loss: 28.12959, Validation Loss: 3.83296\n",
      "Epoch 900/1000, Training Loss: 28.12538, Validation Loss: 3.83281\n",
      "Epoch 901/1000, Training Loss: 28.12123, Validation Loss: 3.83270\n",
      "Epoch 902/1000, Training Loss: 28.11710, Validation Loss: 3.83255\n",
      "Epoch 903/1000, Training Loss: 28.11297, Validation Loss: 3.83243\n",
      "Epoch 904/1000, Training Loss: 28.10888, Validation Loss: 3.83225\n",
      "Epoch 905/1000, Training Loss: 28.10462, Validation Loss: 3.83210\n",
      "Epoch 906/1000, Training Loss: 28.10053, Validation Loss: 3.83195\n",
      "Epoch 907/1000, Training Loss: 28.09633, Validation Loss: 3.83180\n",
      "Epoch 908/1000, Training Loss: 28.09219, Validation Loss: 3.83160\n",
      "Epoch 909/1000, Training Loss: 28.08807, Validation Loss: 3.83149\n",
      "Epoch 910/1000, Training Loss: 28.08382, Validation Loss: 3.83133\n",
      "Epoch 911/1000, Training Loss: 28.07976, Validation Loss: 3.83122\n",
      "Epoch 912/1000, Training Loss: 28.07567, Validation Loss: 3.83103\n",
      "Epoch 913/1000, Training Loss: 28.07144, Validation Loss: 3.83088\n",
      "Epoch 914/1000, Training Loss: 28.06726, Validation Loss: 3.83074\n",
      "Epoch 915/1000, Training Loss: 28.06301, Validation Loss: 3.83057\n",
      "Epoch 916/1000, Training Loss: 28.05882, Validation Loss: 3.83044\n",
      "Epoch 917/1000, Training Loss: 28.05467, Validation Loss: 3.83033\n",
      "Epoch 918/1000, Training Loss: 28.05046, Validation Loss: 3.83012\n",
      "Epoch 919/1000, Training Loss: 28.04620, Validation Loss: 3.82999\n",
      "Epoch 920/1000, Training Loss: 28.04220, Validation Loss: 3.82985\n",
      "Epoch 921/1000, Training Loss: 28.03789, Validation Loss: 3.82972\n",
      "Epoch 922/1000, Training Loss: 28.03359, Validation Loss: 3.82951\n",
      "Epoch 923/1000, Training Loss: 28.02937, Validation Loss: 3.82938\n",
      "Epoch 924/1000, Training Loss: 28.02514, Validation Loss: 3.82926\n",
      "Epoch 925/1000, Training Loss: 28.02091, Validation Loss: 3.82904\n",
      "Epoch 926/1000, Training Loss: 28.01661, Validation Loss: 3.82894\n",
      "Epoch 927/1000, Training Loss: 28.01236, Validation Loss: 3.82878\n",
      "Epoch 928/1000, Training Loss: 28.00821, Validation Loss: 3.82868\n",
      "Epoch 929/1000, Training Loss: 28.00402, Validation Loss: 3.82851\n",
      "Epoch 930/1000, Training Loss: 27.99971, Validation Loss: 3.82838\n",
      "Epoch 931/1000, Training Loss: 27.99534, Validation Loss: 3.82826\n",
      "Epoch 932/1000, Training Loss: 27.99105, Validation Loss: 3.82813\n",
      "Epoch 933/1000, Training Loss: 27.98683, Validation Loss: 3.82801\n",
      "Epoch 934/1000, Training Loss: 27.98251, Validation Loss: 3.82782\n",
      "Epoch 935/1000, Training Loss: 27.97824, Validation Loss: 3.82774\n",
      "Epoch 936/1000, Training Loss: 27.97398, Validation Loss: 3.82764\n",
      "Epoch 937/1000, Training Loss: 27.96981, Validation Loss: 3.82748\n",
      "Epoch 938/1000, Training Loss: 27.96544, Validation Loss: 3.82735\n",
      "Epoch 939/1000, Training Loss: 27.96119, Validation Loss: 3.82721\n",
      "Epoch 940/1000, Training Loss: 27.95681, Validation Loss: 3.82706\n",
      "Epoch 941/1000, Training Loss: 27.95262, Validation Loss: 3.82690\n",
      "Epoch 942/1000, Training Loss: 27.94835, Validation Loss: 3.82679\n",
      "Epoch 943/1000, Training Loss: 27.94407, Validation Loss: 3.82667\n",
      "Epoch 944/1000, Training Loss: 27.93995, Validation Loss: 3.82653\n",
      "Epoch 945/1000, Training Loss: 27.93563, Validation Loss: 3.82637\n",
      "Epoch 946/1000, Training Loss: 27.93130, Validation Loss: 3.82622\n",
      "Epoch 947/1000, Training Loss: 27.92707, Validation Loss: 3.82605\n",
      "Epoch 948/1000, Training Loss: 27.92275, Validation Loss: 3.82589\n",
      "Epoch 949/1000, Training Loss: 27.91857, Validation Loss: 3.82578\n",
      "Epoch 950/1000, Training Loss: 27.91427, Validation Loss: 3.82560\n",
      "Epoch 951/1000, Training Loss: 27.91012, Validation Loss: 3.82550\n",
      "Epoch 952/1000, Training Loss: 27.90604, Validation Loss: 3.82532\n",
      "Epoch 953/1000, Training Loss: 27.90164, Validation Loss: 3.82517\n",
      "Epoch 954/1000, Training Loss: 27.89740, Validation Loss: 3.82502\n",
      "Epoch 955/1000, Training Loss: 27.89310, Validation Loss: 3.82488\n",
      "Epoch 956/1000, Training Loss: 27.88891, Validation Loss: 3.82474\n",
      "Epoch 957/1000, Training Loss: 27.88456, Validation Loss: 3.82463\n",
      "Epoch 958/1000, Training Loss: 27.88040, Validation Loss: 3.82445\n",
      "Epoch 959/1000, Training Loss: 27.87621, Validation Loss: 3.82438\n",
      "Epoch 960/1000, Training Loss: 27.87200, Validation Loss: 3.82421\n",
      "Epoch 961/1000, Training Loss: 27.86779, Validation Loss: 3.82404\n",
      "Epoch 962/1000, Training Loss: 27.86344, Validation Loss: 3.82389\n",
      "Epoch 963/1000, Training Loss: 27.85927, Validation Loss: 3.82376\n",
      "Epoch 964/1000, Training Loss: 27.85497, Validation Loss: 3.82364\n",
      "Epoch 965/1000, Training Loss: 27.85074, Validation Loss: 3.82348\n",
      "Epoch 966/1000, Training Loss: 27.84659, Validation Loss: 3.82340\n",
      "Epoch 967/1000, Training Loss: 27.84230, Validation Loss: 3.82320\n",
      "Epoch 968/1000, Training Loss: 27.83807, Validation Loss: 3.82304\n",
      "Epoch 969/1000, Training Loss: 27.83387, Validation Loss: 3.82296\n",
      "Epoch 970/1000, Training Loss: 27.82958, Validation Loss: 3.82276\n",
      "Epoch 971/1000, Training Loss: 27.82538, Validation Loss: 3.82260\n",
      "Epoch 972/1000, Training Loss: 27.82115, Validation Loss: 3.82245\n",
      "Epoch 973/1000, Training Loss: 27.81691, Validation Loss: 3.82230\n",
      "Epoch 974/1000, Training Loss: 27.81276, Validation Loss: 3.82218\n",
      "Epoch 975/1000, Training Loss: 27.80849, Validation Loss: 3.82198\n",
      "Epoch 976/1000, Training Loss: 27.80433, Validation Loss: 3.82188\n",
      "Epoch 977/1000, Training Loss: 27.80005, Validation Loss: 3.82173\n",
      "Epoch 978/1000, Training Loss: 27.79578, Validation Loss: 3.82154\n",
      "Epoch 979/1000, Training Loss: 27.79156, Validation Loss: 3.82145\n",
      "Epoch 980/1000, Training Loss: 27.78725, Validation Loss: 3.82130\n",
      "Epoch 981/1000, Training Loss: 27.78292, Validation Loss: 3.82117\n",
      "Epoch 982/1000, Training Loss: 27.77874, Validation Loss: 3.82107\n",
      "Epoch 983/1000, Training Loss: 27.77446, Validation Loss: 3.82097\n",
      "Epoch 984/1000, Training Loss: 27.77022, Validation Loss: 3.82082\n",
      "Epoch 985/1000, Training Loss: 27.76570, Validation Loss: 3.82072\n",
      "Epoch 986/1000, Training Loss: 27.76144, Validation Loss: 3.82060\n",
      "Epoch 987/1000, Training Loss: 27.75709, Validation Loss: 3.82052\n",
      "Epoch 988/1000, Training Loss: 27.75274, Validation Loss: 3.82036\n",
      "Epoch 989/1000, Training Loss: 27.74836, Validation Loss: 3.82033\n",
      "Epoch 990/1000, Training Loss: 27.74410, Validation Loss: 3.82020\n",
      "Epoch 991/1000, Training Loss: 27.73969, Validation Loss: 3.82009\n",
      "Epoch 992/1000, Training Loss: 27.73540, Validation Loss: 3.82001\n",
      "Epoch 993/1000, Training Loss: 27.73117, Validation Loss: 3.81988\n",
      "Epoch 994/1000, Training Loss: 27.72666, Validation Loss: 3.81982\n",
      "Epoch 995/1000, Training Loss: 27.72224, Validation Loss: 3.81970\n",
      "Epoch 996/1000, Training Loss: 27.71789, Validation Loss: 3.81960\n",
      "Epoch 997/1000, Training Loss: 27.71348, Validation Loss: 3.81950\n",
      "Epoch 998/1000, Training Loss: 27.70901, Validation Loss: 3.81942\n",
      "Epoch 999/1000, Training Loss: 27.70476, Validation Loss: 3.81934\n",
      "Epoch 1000/1000, Training Loss: 27.70033, Validation Loss: 3.81924\n",
      "Training took: 147.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_with_normalization_base_2 = BaseModelWithNoamalization().to(device)\n",
    "summary(model_with_normalization_base_2, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.ASGD(model_with_normalization_base_2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_with_normalization_base_2=[]\n",
    "val_loss_list_with_normalization_base_2=[]\n",
    "train_accuracy_list_with_normalization_base_2=[]\n",
    "val_accuracy_list_with_normalization_base_2=[]\n",
    "test_accuracy_list_with_normalization_base_2=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_with_normalization_base_2.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_with_normalization_base_2(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_with_normalization_base_2.append(train_accuracy)\n",
    "    train_loss_list_with_normalization_base_2.append(train_loss)\n",
    "\n",
    "    model_with_normalization_base_2.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_with_normalization_base_2(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_with_normalization_base_2.append(val_accuracy)\n",
    "    val_loss_list_with_normalization_base_2.append(val_loss)\n",
    "\n",
    "    test_predictions_normalization_base_2 = model_with_normalization_base_2(X_test_tensor).view(-1)\n",
    "    test_predictions_rounded_normalization_base_2 = torch.round(test_predictions_normalization_base_2)\n",
    "    test_predictions_rounded_numpy_normalization_base_2 = test_predictions_rounded_normalization_base_2.cpu().detach().numpy()\n",
    "    y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "    accuracy_normalization_base_2 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_normalization_base_2)\n",
    "    test_accuracy_list_with_normalization_base_2.append(accuracy_normalization_base_2)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4STs9T2hyijc",
    "outputId": "1dbded88-5dd7-48fe-a23a-7ce13a70fd5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base model with normalization: 0.7662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_with_normalization_base_2.eval()\n",
    "test_predictions_normalization_base_2 = model_with_normalization_base_2(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_normalization_base_2 = torch.round(test_predictions_normalization_base_2)\n",
    "\n",
    "test_predictions_rounded_numpy_normalization_base_2 = test_predictions_rounded_normalization_base_2.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_normalization_base_2 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_normalization_base_2)\n",
    "\n",
    "print(f\"Accuracy for base model with normalization: {accuracy_normalization_base_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVJSS6rb3BkN",
    "outputId": "af32db8e-1c2b-4690-ffa3-fada3500372c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model with Normalization Test Loss: 0.47139\n"
     ]
    }
   ],
   "source": [
    "model_with_normalization_base_2.eval()\n",
    "test_loss=0\n",
    "with torch.no_grad():\n",
    "    test_outputs_normalization_base_2 = model_with_normalization_base_2(X_test_tensor)\n",
    "    test_loss_normalization_base_2 = loss_function(test_outputs_normalization_base_2, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(f\"Base Model with Normalization Test Loss: {test_loss_normalization_base_2.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "8bzKGief77Dp",
    "outputId": "933e6c3b-d8d6-41e7-9f36-589831b3237b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.plot(epochs_range, test_accuracy_list_optimizer_asgd_1, label='Base Model Testing Accuracy')\n",
    "plt.plot(epochs_range, test_accuracy_list_with_normalization_base_2, label='Base Model with Normalization Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Ip59iIIwzwvh",
    "outputId": "de044aec-f0f3-4a6d-d76d-dba708e601f3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.plot(epochs_range, train_loss_list_with_normalization_base_2, label='Base Model with Normalization Training Loss')\n",
    "plt.plot(epochs_range, val_loss_list_with_normalization_base_2, label='Base Model with Normalization Validation Loss')\n",
    "plt.plot(epochs_range, [test_loss_normalization_base_2.cpu().item()]*epochs, label='Base Model with Normalization Final Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLUfk_qL_V7F"
   },
   "source": [
    "Adding learning rate scheduler in Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "ZMa98PD53P1H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "class BaseModelWithLRS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModelWithLRS, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZrtqDU1_gp6",
    "outputId": "0084cfa5-0ac5-4480-a85c-e58ad44236ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 40.95439, Validation Loss: 6.10323\n",
      "Epoch 2/1000, Training Loss: 40.93694, Validation Loss: 6.10029\n",
      "Epoch 3/1000, Training Loss: 40.91957, Validation Loss: 6.09736\n",
      "Epoch 4/1000, Training Loss: 40.90229, Validation Loss: 6.09445\n",
      "Epoch 5/1000, Training Loss: 40.88509, Validation Loss: 6.09155\n",
      "Epoch 6/1000, Training Loss: 40.86797, Validation Loss: 6.08866\n",
      "Epoch 7/1000, Training Loss: 40.85093, Validation Loss: 6.08578\n",
      "Epoch 8/1000, Training Loss: 40.83394, Validation Loss: 6.08291\n",
      "Epoch 9/1000, Training Loss: 40.81703, Validation Loss: 6.08006\n",
      "Epoch 10/1000, Training Loss: 40.80020, Validation Loss: 6.07722\n",
      "Epoch 11/1000, Training Loss: 40.78344, Validation Loss: 6.07439\n",
      "Epoch 12/1000, Training Loss: 40.76676, Validation Loss: 6.07158\n",
      "Epoch 13/1000, Training Loss: 40.75015, Validation Loss: 6.06877\n",
      "Epoch 14/1000, Training Loss: 40.73362, Validation Loss: 6.06598\n",
      "Epoch 15/1000, Training Loss: 40.71716, Validation Loss: 6.06320\n",
      "Epoch 16/1000, Training Loss: 40.70077, Validation Loss: 6.06043\n",
      "Epoch 17/1000, Training Loss: 40.68446, Validation Loss: 6.05768\n",
      "Epoch 18/1000, Training Loss: 40.66822, Validation Loss: 6.05493\n",
      "Epoch 19/1000, Training Loss: 40.65206, Validation Loss: 6.05220\n",
      "Epoch 20/1000, Training Loss: 40.63598, Validation Loss: 6.04947\n",
      "Epoch 21/1000, Training Loss: 40.61999, Validation Loss: 6.04676\n",
      "Epoch 22/1000, Training Loss: 40.60405, Validation Loss: 6.04406\n",
      "Epoch 23/1000, Training Loss: 40.58819, Validation Loss: 6.04136\n",
      "Epoch 24/1000, Training Loss: 40.57237, Validation Loss: 6.03868\n",
      "Epoch 25/1000, Training Loss: 40.55662, Validation Loss: 6.03601\n",
      "Epoch 26/1000, Training Loss: 40.54094, Validation Loss: 6.03334\n",
      "Epoch 27/1000, Training Loss: 40.52531, Validation Loss: 6.03069\n",
      "Epoch 28/1000, Training Loss: 40.50975, Validation Loss: 6.02805\n",
      "Epoch 29/1000, Training Loss: 40.49426, Validation Loss: 6.02541\n",
      "Epoch 30/1000, Training Loss: 40.47883, Validation Loss: 6.02279\n",
      "Epoch 31/1000, Training Loss: 40.46346, Validation Loss: 6.02017\n",
      "Epoch 32/1000, Training Loss: 40.44815, Validation Loss: 6.01756\n",
      "Epoch 33/1000, Training Loss: 40.43292, Validation Loss: 6.01497\n",
      "Epoch 34/1000, Training Loss: 40.41775, Validation Loss: 6.01239\n",
      "Epoch 35/1000, Training Loss: 40.40265, Validation Loss: 6.00981\n",
      "Epoch 36/1000, Training Loss: 40.38762, Validation Loss: 6.00725\n",
      "Epoch 37/1000, Training Loss: 40.37264, Validation Loss: 6.00470\n",
      "Epoch 38/1000, Training Loss: 40.35774, Validation Loss: 6.00216\n",
      "Epoch 39/1000, Training Loss: 40.34291, Validation Loss: 5.99963\n",
      "Epoch 40/1000, Training Loss: 40.32813, Validation Loss: 5.99711\n",
      "Epoch 41/1000, Training Loss: 40.31340, Validation Loss: 5.99459\n",
      "Epoch 42/1000, Training Loss: 40.29873, Validation Loss: 5.99209\n",
      "Epoch 43/1000, Training Loss: 40.28411, Validation Loss: 5.98959\n",
      "Epoch 44/1000, Training Loss: 40.26955, Validation Loss: 5.98710\n",
      "Epoch 45/1000, Training Loss: 40.25505, Validation Loss: 5.98462\n",
      "Epoch 46/1000, Training Loss: 40.24059, Validation Loss: 5.98215\n",
      "Epoch 47/1000, Training Loss: 40.22618, Validation Loss: 5.97969\n",
      "Epoch 48/1000, Training Loss: 40.21182, Validation Loss: 5.97723\n",
      "Epoch 49/1000, Training Loss: 40.19750, Validation Loss: 5.97478\n",
      "Epoch 50/1000, Training Loss: 40.18323, Validation Loss: 5.97234\n",
      "Epoch 51/1000, Training Loss: 40.16901, Validation Loss: 5.96991\n",
      "Epoch 52/1000, Training Loss: 40.15483, Validation Loss: 5.96749\n",
      "Epoch 53/1000, Training Loss: 40.14071, Validation Loss: 5.96508\n",
      "Epoch 54/1000, Training Loss: 40.12664, Validation Loss: 5.96267\n",
      "Epoch 55/1000, Training Loss: 40.11262, Validation Loss: 5.96027\n",
      "Epoch 56/1000, Training Loss: 40.09866, Validation Loss: 5.95788\n",
      "Epoch 57/1000, Training Loss: 40.08474, Validation Loss: 5.95550\n",
      "Epoch 58/1000, Training Loss: 40.07086, Validation Loss: 5.95312\n",
      "Epoch 59/1000, Training Loss: 40.05703, Validation Loss: 5.95075\n",
      "Epoch 60/1000, Training Loss: 40.04324, Validation Loss: 5.94838\n",
      "Epoch 61/1000, Training Loss: 40.02949, Validation Loss: 5.94603\n",
      "Epoch 62/1000, Training Loss: 40.01577, Validation Loss: 5.94368\n",
      "Epoch 63/1000, Training Loss: 40.00210, Validation Loss: 5.94133\n",
      "Epoch 64/1000, Training Loss: 39.98848, Validation Loss: 5.93900\n",
      "Epoch 65/1000, Training Loss: 39.97489, Validation Loss: 5.93667\n",
      "Epoch 66/1000, Training Loss: 39.96133, Validation Loss: 5.93434\n",
      "Epoch 67/1000, Training Loss: 39.94782, Validation Loss: 5.93202\n",
      "Epoch 68/1000, Training Loss: 39.93438, Validation Loss: 5.92972\n",
      "Epoch 69/1000, Training Loss: 39.92099, Validation Loss: 5.92741\n",
      "Epoch 70/1000, Training Loss: 39.90765, Validation Loss: 5.92512\n",
      "Epoch 71/1000, Training Loss: 39.89436, Validation Loss: 5.92283\n",
      "Epoch 72/1000, Training Loss: 39.88112, Validation Loss: 5.92055\n",
      "Epoch 73/1000, Training Loss: 39.86791, Validation Loss: 5.91828\n",
      "Epoch 74/1000, Training Loss: 39.85475, Validation Loss: 5.91602\n",
      "Epoch 75/1000, Training Loss: 39.84163, Validation Loss: 5.91376\n",
      "Epoch 76/1000, Training Loss: 39.82855, Validation Loss: 5.91151\n",
      "Epoch 77/1000, Training Loss: 39.81552, Validation Loss: 5.90926\n",
      "Epoch 78/1000, Training Loss: 39.80252, Validation Loss: 5.90702\n",
      "Epoch 79/1000, Training Loss: 39.78956, Validation Loss: 5.90479\n",
      "Epoch 80/1000, Training Loss: 39.77663, Validation Loss: 5.90256\n",
      "Epoch 81/1000, Training Loss: 39.76374, Validation Loss: 5.90033\n",
      "Epoch 82/1000, Training Loss: 39.75088, Validation Loss: 5.89811\n",
      "Epoch 83/1000, Training Loss: 39.73805, Validation Loss: 5.89590\n",
      "Epoch 84/1000, Training Loss: 39.72526, Validation Loss: 5.89370\n",
      "Epoch 85/1000, Training Loss: 39.71250, Validation Loss: 5.89149\n",
      "Epoch 86/1000, Training Loss: 39.69977, Validation Loss: 5.88930\n",
      "Epoch 87/1000, Training Loss: 39.68708, Validation Loss: 5.88711\n",
      "Epoch 88/1000, Training Loss: 39.67443, Validation Loss: 5.88493\n",
      "Epoch 89/1000, Training Loss: 39.66182, Validation Loss: 5.88275\n",
      "Epoch 90/1000, Training Loss: 39.64923, Validation Loss: 5.88059\n",
      "Epoch 91/1000, Training Loss: 39.63668, Validation Loss: 5.87842\n",
      "Epoch 92/1000, Training Loss: 39.62417, Validation Loss: 5.87627\n",
      "Epoch 93/1000, Training Loss: 39.61170, Validation Loss: 5.87411\n",
      "Epoch 94/1000, Training Loss: 39.59927, Validation Loss: 5.87197\n",
      "Epoch 95/1000, Training Loss: 39.58687, Validation Loss: 5.86983\n",
      "Epoch 96/1000, Training Loss: 39.57449, Validation Loss: 5.86769\n",
      "Epoch 97/1000, Training Loss: 39.56214, Validation Loss: 5.86556\n",
      "Epoch 98/1000, Training Loss: 39.54983, Validation Loss: 5.86343\n",
      "Epoch 99/1000, Training Loss: 39.53754, Validation Loss: 5.86131\n",
      "Epoch 100/1000, Training Loss: 39.52529, Validation Loss: 5.85919\n",
      "Epoch 101/1000, Training Loss: 39.51307, Validation Loss: 5.85708\n",
      "Epoch 102/1000, Training Loss: 39.50089, Validation Loss: 5.85497\n",
      "Epoch 103/1000, Training Loss: 39.48873, Validation Loss: 5.85287\n",
      "Epoch 104/1000, Training Loss: 39.47659, Validation Loss: 5.85077\n",
      "Epoch 105/1000, Training Loss: 39.46447, Validation Loss: 5.84867\n",
      "Epoch 106/1000, Training Loss: 39.45237, Validation Loss: 5.84658\n",
      "Epoch 107/1000, Training Loss: 39.44030, Validation Loss: 5.84449\n",
      "Epoch 108/1000, Training Loss: 39.42825, Validation Loss: 5.84241\n",
      "Epoch 109/1000, Training Loss: 39.41623, Validation Loss: 5.84033\n",
      "Epoch 110/1000, Training Loss: 39.40423, Validation Loss: 5.83825\n",
      "Epoch 111/1000, Training Loss: 39.39227, Validation Loss: 5.83618\n",
      "Epoch 112/1000, Training Loss: 39.38032, Validation Loss: 5.83412\n",
      "Epoch 113/1000, Training Loss: 39.36841, Validation Loss: 5.83205\n",
      "Epoch 114/1000, Training Loss: 39.35652, Validation Loss: 5.83000\n",
      "Epoch 115/1000, Training Loss: 39.34466, Validation Loss: 5.82794\n",
      "Epoch 116/1000, Training Loss: 39.33282, Validation Loss: 5.82590\n",
      "Epoch 117/1000, Training Loss: 39.32101, Validation Loss: 5.82385\n",
      "Epoch 118/1000, Training Loss: 39.30922, Validation Loss: 5.82181\n",
      "Epoch 119/1000, Training Loss: 39.29745, Validation Loss: 5.81977\n",
      "Epoch 120/1000, Training Loss: 39.28569, Validation Loss: 5.81773\n",
      "Epoch 121/1000, Training Loss: 39.27396, Validation Loss: 5.81570\n",
      "Epoch 122/1000, Training Loss: 39.26226, Validation Loss: 5.81367\n",
      "Epoch 123/1000, Training Loss: 39.25059, Validation Loss: 5.81165\n",
      "Epoch 124/1000, Training Loss: 39.23894, Validation Loss: 5.80962\n",
      "Epoch 125/1000, Training Loss: 39.22731, Validation Loss: 5.80760\n",
      "Epoch 126/1000, Training Loss: 39.21571, Validation Loss: 5.80559\n",
      "Epoch 127/1000, Training Loss: 39.20413, Validation Loss: 5.80358\n",
      "Epoch 128/1000, Training Loss: 39.19257, Validation Loss: 5.80157\n",
      "Epoch 129/1000, Training Loss: 39.18104, Validation Loss: 5.79957\n",
      "Epoch 130/1000, Training Loss: 39.16953, Validation Loss: 5.79757\n",
      "Epoch 131/1000, Training Loss: 39.15804, Validation Loss: 5.79558\n",
      "Epoch 132/1000, Training Loss: 39.14658, Validation Loss: 5.79359\n",
      "Epoch 133/1000, Training Loss: 39.13514, Validation Loss: 5.79161\n",
      "Epoch 134/1000, Training Loss: 39.12371, Validation Loss: 5.78963\n",
      "Epoch 135/1000, Training Loss: 39.11231, Validation Loss: 5.78765\n",
      "Epoch 136/1000, Training Loss: 39.10094, Validation Loss: 5.78568\n",
      "Epoch 137/1000, Training Loss: 39.08958, Validation Loss: 5.78371\n",
      "Epoch 138/1000, Training Loss: 39.07824, Validation Loss: 5.78174\n",
      "Epoch 139/1000, Training Loss: 39.06693, Validation Loss: 5.77977\n",
      "Epoch 140/1000, Training Loss: 39.05563, Validation Loss: 5.77781\n",
      "Epoch 141/1000, Training Loss: 39.04436, Validation Loss: 5.77585\n",
      "Epoch 142/1000, Training Loss: 39.03310, Validation Loss: 5.77389\n",
      "Epoch 143/1000, Training Loss: 39.02186, Validation Loss: 5.77193\n",
      "Epoch 144/1000, Training Loss: 39.01063, Validation Loss: 5.76998\n",
      "Epoch 145/1000, Training Loss: 38.99942, Validation Loss: 5.76803\n",
      "Epoch 146/1000, Training Loss: 38.98822, Validation Loss: 5.76608\n",
      "Epoch 147/1000, Training Loss: 38.97704, Validation Loss: 5.76414\n",
      "Epoch 148/1000, Training Loss: 38.96588, Validation Loss: 5.76219\n",
      "Epoch 149/1000, Training Loss: 38.95473, Validation Loss: 5.76025\n",
      "Epoch 150/1000, Training Loss: 38.94361, Validation Loss: 5.75831\n",
      "Epoch 151/1000, Training Loss: 38.93251, Validation Loss: 5.75637\n",
      "Epoch 152/1000, Training Loss: 38.92142, Validation Loss: 5.75444\n",
      "Epoch 153/1000, Training Loss: 38.91036, Validation Loss: 5.75251\n",
      "Epoch 154/1000, Training Loss: 38.89931, Validation Loss: 5.75058\n",
      "Epoch 155/1000, Training Loss: 38.88828, Validation Loss: 5.74865\n",
      "Epoch 156/1000, Training Loss: 38.87726, Validation Loss: 5.74673\n",
      "Epoch 157/1000, Training Loss: 38.86626, Validation Loss: 5.74481\n",
      "Epoch 158/1000, Training Loss: 38.85529, Validation Loss: 5.74290\n",
      "Epoch 159/1000, Training Loss: 38.84433, Validation Loss: 5.74098\n",
      "Epoch 160/1000, Training Loss: 38.83338, Validation Loss: 5.73907\n",
      "Epoch 161/1000, Training Loss: 38.82243, Validation Loss: 5.73716\n",
      "Epoch 162/1000, Training Loss: 38.81150, Validation Loss: 5.73526\n",
      "Epoch 163/1000, Training Loss: 38.80058, Validation Loss: 5.73335\n",
      "Epoch 164/1000, Training Loss: 38.78969, Validation Loss: 5.73145\n",
      "Epoch 165/1000, Training Loss: 38.77881, Validation Loss: 5.72956\n",
      "Epoch 166/1000, Training Loss: 38.76795, Validation Loss: 5.72766\n",
      "Epoch 167/1000, Training Loss: 38.75711, Validation Loss: 5.72577\n",
      "Epoch 168/1000, Training Loss: 38.74628, Validation Loss: 5.72388\n",
      "Epoch 169/1000, Training Loss: 38.73548, Validation Loss: 5.72200\n",
      "Epoch 170/1000, Training Loss: 38.72470, Validation Loss: 5.72012\n",
      "Epoch 171/1000, Training Loss: 38.71394, Validation Loss: 5.71824\n",
      "Epoch 172/1000, Training Loss: 38.70320, Validation Loss: 5.71636\n",
      "Epoch 173/1000, Training Loss: 38.69247, Validation Loss: 5.71448\n",
      "Epoch 174/1000, Training Loss: 38.68175, Validation Loss: 5.71261\n",
      "Epoch 175/1000, Training Loss: 38.67104, Validation Loss: 5.71074\n",
      "Epoch 176/1000, Training Loss: 38.66034, Validation Loss: 5.70886\n",
      "Epoch 177/1000, Training Loss: 38.64965, Validation Loss: 5.70700\n",
      "Epoch 178/1000, Training Loss: 38.63897, Validation Loss: 5.70513\n",
      "Epoch 179/1000, Training Loss: 38.62830, Validation Loss: 5.70326\n",
      "Epoch 180/1000, Training Loss: 38.61765, Validation Loss: 5.70140\n",
      "Epoch 181/1000, Training Loss: 38.60701, Validation Loss: 5.69954\n",
      "Epoch 182/1000, Training Loss: 38.59639, Validation Loss: 5.69768\n",
      "Epoch 183/1000, Training Loss: 38.58578, Validation Loss: 5.69582\n",
      "Epoch 184/1000, Training Loss: 38.57518, Validation Loss: 5.69397\n",
      "Epoch 185/1000, Training Loss: 38.56459, Validation Loss: 5.69212\n",
      "Epoch 186/1000, Training Loss: 38.55401, Validation Loss: 5.69027\n",
      "Epoch 187/1000, Training Loss: 38.54344, Validation Loss: 5.68843\n",
      "Epoch 188/1000, Training Loss: 38.53289, Validation Loss: 5.68658\n",
      "Epoch 189/1000, Training Loss: 38.52234, Validation Loss: 5.68474\n",
      "Epoch 190/1000, Training Loss: 38.51180, Validation Loss: 5.68289\n",
      "Epoch 191/1000, Training Loss: 38.50127, Validation Loss: 5.68105\n",
      "Epoch 192/1000, Training Loss: 38.49075, Validation Loss: 5.67921\n",
      "Epoch 193/1000, Training Loss: 38.48025, Validation Loss: 5.67737\n",
      "Epoch 194/1000, Training Loss: 38.46975, Validation Loss: 5.67553\n",
      "Epoch 195/1000, Training Loss: 38.45927, Validation Loss: 5.67369\n",
      "Epoch 196/1000, Training Loss: 38.44879, Validation Loss: 5.67185\n",
      "Epoch 197/1000, Training Loss: 38.43832, Validation Loss: 5.67001\n",
      "Epoch 198/1000, Training Loss: 38.42786, Validation Loss: 5.66817\n",
      "Epoch 199/1000, Training Loss: 38.41740, Validation Loss: 5.66633\n",
      "Epoch 200/1000, Training Loss: 38.40694, Validation Loss: 5.66449\n",
      "Epoch 201/1000, Training Loss: 38.39650, Validation Loss: 5.66266\n",
      "Epoch 202/1000, Training Loss: 38.38606, Validation Loss: 5.66083\n",
      "Epoch 203/1000, Training Loss: 38.37564, Validation Loss: 5.65900\n",
      "Epoch 204/1000, Training Loss: 38.36523, Validation Loss: 5.65718\n",
      "Epoch 205/1000, Training Loss: 38.35483, Validation Loss: 5.65535\n",
      "Epoch 206/1000, Training Loss: 38.34444, Validation Loss: 5.65353\n",
      "Epoch 207/1000, Training Loss: 38.33406, Validation Loss: 5.65170\n",
      "Epoch 208/1000, Training Loss: 38.32369, Validation Loss: 5.64988\n",
      "Epoch 209/1000, Training Loss: 38.31333, Validation Loss: 5.64806\n",
      "Epoch 210/1000, Training Loss: 38.30297, Validation Loss: 5.64624\n",
      "Epoch 211/1000, Training Loss: 38.29262, Validation Loss: 5.64442\n",
      "Epoch 212/1000, Training Loss: 38.28227, Validation Loss: 5.64260\n",
      "Epoch 213/1000, Training Loss: 38.27194, Validation Loss: 5.64079\n",
      "Epoch 214/1000, Training Loss: 38.26162, Validation Loss: 5.63897\n",
      "Epoch 215/1000, Training Loss: 38.25132, Validation Loss: 5.63715\n",
      "Epoch 216/1000, Training Loss: 38.24101, Validation Loss: 5.63534\n",
      "Epoch 217/1000, Training Loss: 38.23071, Validation Loss: 5.63353\n",
      "Epoch 218/1000, Training Loss: 38.22042, Validation Loss: 5.63172\n",
      "Epoch 219/1000, Training Loss: 38.21015, Validation Loss: 5.62991\n",
      "Epoch 220/1000, Training Loss: 38.19988, Validation Loss: 5.62810\n",
      "Epoch 221/1000, Training Loss: 38.18962, Validation Loss: 5.62629\n",
      "Epoch 222/1000, Training Loss: 38.17936, Validation Loss: 5.62448\n",
      "Epoch 223/1000, Training Loss: 38.16911, Validation Loss: 5.62268\n",
      "Epoch 224/1000, Training Loss: 38.15887, Validation Loss: 5.62087\n",
      "Epoch 225/1000, Training Loss: 38.14863, Validation Loss: 5.61906\n",
      "Epoch 226/1000, Training Loss: 38.13840, Validation Loss: 5.61726\n",
      "Epoch 227/1000, Training Loss: 38.12818, Validation Loss: 5.61545\n",
      "Epoch 228/1000, Training Loss: 38.11795, Validation Loss: 5.61365\n",
      "Epoch 229/1000, Training Loss: 38.10773, Validation Loss: 5.61185\n",
      "Epoch 230/1000, Training Loss: 38.09753, Validation Loss: 5.61005\n",
      "Epoch 231/1000, Training Loss: 38.08732, Validation Loss: 5.60825\n",
      "Epoch 232/1000, Training Loss: 38.07712, Validation Loss: 5.60645\n",
      "Epoch 233/1000, Training Loss: 38.06692, Validation Loss: 5.60465\n",
      "Epoch 234/1000, Training Loss: 38.05673, Validation Loss: 5.60285\n",
      "Epoch 235/1000, Training Loss: 38.04654, Validation Loss: 5.60105\n",
      "Epoch 236/1000, Training Loss: 38.03636, Validation Loss: 5.59925\n",
      "Epoch 237/1000, Training Loss: 38.02619, Validation Loss: 5.59745\n",
      "Epoch 238/1000, Training Loss: 38.01602, Validation Loss: 5.59565\n",
      "Epoch 239/1000, Training Loss: 38.00587, Validation Loss: 5.59385\n",
      "Epoch 240/1000, Training Loss: 37.99572, Validation Loss: 5.59205\n",
      "Epoch 241/1000, Training Loss: 37.98557, Validation Loss: 5.59026\n",
      "Epoch 242/1000, Training Loss: 37.97544, Validation Loss: 5.58846\n",
      "Epoch 243/1000, Training Loss: 37.96531, Validation Loss: 5.58667\n",
      "Epoch 244/1000, Training Loss: 37.95519, Validation Loss: 5.58487\n",
      "Epoch 245/1000, Training Loss: 37.94507, Validation Loss: 5.58308\n",
      "Epoch 246/1000, Training Loss: 37.93496, Validation Loss: 5.58129\n",
      "Epoch 247/1000, Training Loss: 37.92486, Validation Loss: 5.57949\n",
      "Epoch 248/1000, Training Loss: 37.91476, Validation Loss: 5.57770\n",
      "Epoch 249/1000, Training Loss: 37.90468, Validation Loss: 5.57591\n",
      "Epoch 250/1000, Training Loss: 37.89459, Validation Loss: 5.57412\n",
      "Epoch 251/1000, Training Loss: 37.88452, Validation Loss: 5.57233\n",
      "Epoch 252/1000, Training Loss: 37.87444, Validation Loss: 5.57054\n",
      "Epoch 253/1000, Training Loss: 37.86436, Validation Loss: 5.56875\n",
      "Epoch 254/1000, Training Loss: 37.85429, Validation Loss: 5.56696\n",
      "Epoch 255/1000, Training Loss: 37.84422, Validation Loss: 5.56517\n",
      "Epoch 256/1000, Training Loss: 37.83416, Validation Loss: 5.56338\n",
      "Epoch 257/1000, Training Loss: 37.82411, Validation Loss: 5.56159\n",
      "Epoch 258/1000, Training Loss: 37.81405, Validation Loss: 5.55980\n",
      "Epoch 259/1000, Training Loss: 37.80401, Validation Loss: 5.55801\n",
      "Epoch 260/1000, Training Loss: 37.79397, Validation Loss: 5.55622\n",
      "Epoch 261/1000, Training Loss: 37.78393, Validation Loss: 5.55443\n",
      "Epoch 262/1000, Training Loss: 37.77389, Validation Loss: 5.55265\n",
      "Epoch 263/1000, Training Loss: 37.76385, Validation Loss: 5.55086\n",
      "Epoch 264/1000, Training Loss: 37.75381, Validation Loss: 5.54907\n",
      "Epoch 265/1000, Training Loss: 37.74378, Validation Loss: 5.54728\n",
      "Epoch 266/1000, Training Loss: 37.73375, Validation Loss: 5.54549\n",
      "Epoch 267/1000, Training Loss: 37.72371, Validation Loss: 5.54370\n",
      "Epoch 268/1000, Training Loss: 37.71367, Validation Loss: 5.54191\n",
      "Epoch 269/1000, Training Loss: 37.70363, Validation Loss: 5.54012\n",
      "Epoch 270/1000, Training Loss: 37.69360, Validation Loss: 5.53833\n",
      "Epoch 271/1000, Training Loss: 37.68356, Validation Loss: 5.53654\n",
      "Epoch 272/1000, Training Loss: 37.67352, Validation Loss: 5.53475\n",
      "Epoch 273/1000, Training Loss: 37.66348, Validation Loss: 5.53296\n",
      "Epoch 274/1000, Training Loss: 37.65345, Validation Loss: 5.53117\n",
      "Epoch 275/1000, Training Loss: 37.64341, Validation Loss: 5.52938\n",
      "Epoch 276/1000, Training Loss: 37.63338, Validation Loss: 5.52759\n",
      "Epoch 277/1000, Training Loss: 37.62334, Validation Loss: 5.52580\n",
      "Epoch 278/1000, Training Loss: 37.61330, Validation Loss: 5.52401\n",
      "Epoch 279/1000, Training Loss: 37.60327, Validation Loss: 5.52222\n",
      "Epoch 280/1000, Training Loss: 37.59326, Validation Loss: 5.52043\n",
      "Epoch 281/1000, Training Loss: 37.58324, Validation Loss: 5.51864\n",
      "Epoch 282/1000, Training Loss: 37.57324, Validation Loss: 5.51685\n",
      "Epoch 283/1000, Training Loss: 37.56323, Validation Loss: 5.51506\n",
      "Epoch 284/1000, Training Loss: 37.55323, Validation Loss: 5.51327\n",
      "Epoch 285/1000, Training Loss: 37.54324, Validation Loss: 5.51148\n",
      "Epoch 286/1000, Training Loss: 37.53325, Validation Loss: 5.50969\n",
      "Epoch 287/1000, Training Loss: 37.52327, Validation Loss: 5.50790\n",
      "Epoch 288/1000, Training Loss: 37.51328, Validation Loss: 5.50611\n",
      "Epoch 289/1000, Training Loss: 37.50330, Validation Loss: 5.50432\n",
      "Epoch 290/1000, Training Loss: 37.49332, Validation Loss: 5.50253\n",
      "Epoch 291/1000, Training Loss: 37.48334, Validation Loss: 5.50074\n",
      "Epoch 292/1000, Training Loss: 37.47336, Validation Loss: 5.49895\n",
      "Epoch 293/1000, Training Loss: 37.46338, Validation Loss: 5.49716\n",
      "Epoch 294/1000, Training Loss: 37.45340, Validation Loss: 5.49537\n",
      "Epoch 295/1000, Training Loss: 37.44343, Validation Loss: 5.49358\n",
      "Epoch 296/1000, Training Loss: 37.43345, Validation Loss: 5.49179\n",
      "Epoch 297/1000, Training Loss: 37.42348, Validation Loss: 5.49000\n",
      "Epoch 298/1000, Training Loss: 37.41351, Validation Loss: 5.48821\n",
      "Epoch 299/1000, Training Loss: 37.40354, Validation Loss: 5.48642\n",
      "Epoch 300/1000, Training Loss: 37.39357, Validation Loss: 5.48463\n",
      "Epoch 301/1000, Training Loss: 37.38360, Validation Loss: 5.48284\n",
      "Epoch 302/1000, Training Loss: 37.37363, Validation Loss: 5.48105\n",
      "Epoch 303/1000, Training Loss: 37.36367, Validation Loss: 5.47926\n",
      "Epoch 304/1000, Training Loss: 37.35370, Validation Loss: 5.47746\n",
      "Epoch 305/1000, Training Loss: 37.34373, Validation Loss: 5.47567\n",
      "Epoch 306/1000, Training Loss: 37.33376, Validation Loss: 5.47388\n",
      "Epoch 307/1000, Training Loss: 37.32379, Validation Loss: 5.47208\n",
      "Epoch 308/1000, Training Loss: 37.31382, Validation Loss: 5.47029\n",
      "Epoch 309/1000, Training Loss: 37.30385, Validation Loss: 5.46849\n",
      "Epoch 310/1000, Training Loss: 37.29389, Validation Loss: 5.46670\n",
      "Epoch 311/1000, Training Loss: 37.28393, Validation Loss: 5.46490\n",
      "Epoch 312/1000, Training Loss: 37.27397, Validation Loss: 5.46311\n",
      "Epoch 313/1000, Training Loss: 37.26400, Validation Loss: 5.46131\n",
      "Epoch 314/1000, Training Loss: 37.25404, Validation Loss: 5.45952\n",
      "Epoch 315/1000, Training Loss: 37.24408, Validation Loss: 5.45772\n",
      "Epoch 316/1000, Training Loss: 37.23412, Validation Loss: 5.45592\n",
      "Epoch 317/1000, Training Loss: 37.22415, Validation Loss: 5.45412\n",
      "Epoch 318/1000, Training Loss: 37.21418, Validation Loss: 5.45233\n",
      "Epoch 319/1000, Training Loss: 37.20422, Validation Loss: 5.45053\n",
      "Epoch 320/1000, Training Loss: 37.19427, Validation Loss: 5.44873\n",
      "Epoch 321/1000, Training Loss: 37.18431, Validation Loss: 5.44693\n",
      "Epoch 322/1000, Training Loss: 37.17434, Validation Loss: 5.44513\n",
      "Epoch 323/1000, Training Loss: 37.16438, Validation Loss: 5.44333\n",
      "Epoch 324/1000, Training Loss: 37.15441, Validation Loss: 5.44153\n",
      "Epoch 325/1000, Training Loss: 37.14445, Validation Loss: 5.43973\n",
      "Epoch 326/1000, Training Loss: 37.13448, Validation Loss: 5.43793\n",
      "Epoch 327/1000, Training Loss: 37.12451, Validation Loss: 5.43613\n",
      "Epoch 328/1000, Training Loss: 37.11454, Validation Loss: 5.43433\n",
      "Epoch 329/1000, Training Loss: 37.10457, Validation Loss: 5.43253\n",
      "Epoch 330/1000, Training Loss: 37.09459, Validation Loss: 5.43072\n",
      "Epoch 331/1000, Training Loss: 37.08461, Validation Loss: 5.42892\n",
      "Epoch 332/1000, Training Loss: 37.07462, Validation Loss: 5.42711\n",
      "Epoch 333/1000, Training Loss: 37.06463, Validation Loss: 5.42530\n",
      "Epoch 334/1000, Training Loss: 37.05464, Validation Loss: 5.42349\n",
      "Epoch 335/1000, Training Loss: 37.04466, Validation Loss: 5.42167\n",
      "Epoch 336/1000, Training Loss: 37.03467, Validation Loss: 5.41986\n",
      "Epoch 337/1000, Training Loss: 37.02468, Validation Loss: 5.41804\n",
      "Epoch 338/1000, Training Loss: 37.01468, Validation Loss: 5.41623\n",
      "Epoch 339/1000, Training Loss: 37.00469, Validation Loss: 5.41441\n",
      "Epoch 340/1000, Training Loss: 36.99469, Validation Loss: 5.41259\n",
      "Epoch 341/1000, Training Loss: 36.98469, Validation Loss: 5.41077\n",
      "Epoch 342/1000, Training Loss: 36.97469, Validation Loss: 5.40896\n",
      "Epoch 343/1000, Training Loss: 36.96469, Validation Loss: 5.40713\n",
      "Epoch 344/1000, Training Loss: 36.95469, Validation Loss: 5.40531\n",
      "Epoch 345/1000, Training Loss: 36.94469, Validation Loss: 5.40349\n",
      "Epoch 346/1000, Training Loss: 36.93469, Validation Loss: 5.40167\n",
      "Epoch 347/1000, Training Loss: 36.92468, Validation Loss: 5.39985\n",
      "Epoch 348/1000, Training Loss: 36.91468, Validation Loss: 5.39803\n",
      "Epoch 349/1000, Training Loss: 36.90468, Validation Loss: 5.39621\n",
      "Epoch 350/1000, Training Loss: 36.89467, Validation Loss: 5.39438\n",
      "Epoch 351/1000, Training Loss: 36.88467, Validation Loss: 5.39256\n",
      "Epoch 352/1000, Training Loss: 36.87466, Validation Loss: 5.39074\n",
      "Epoch 353/1000, Training Loss: 36.86464, Validation Loss: 5.38891\n",
      "Epoch 354/1000, Training Loss: 36.85463, Validation Loss: 5.38709\n",
      "Epoch 355/1000, Training Loss: 36.84461, Validation Loss: 5.38526\n",
      "Epoch 356/1000, Training Loss: 36.83459, Validation Loss: 5.38342\n",
      "Epoch 357/1000, Training Loss: 36.82457, Validation Loss: 5.38159\n",
      "Epoch 358/1000, Training Loss: 36.81455, Validation Loss: 5.37976\n",
      "Epoch 359/1000, Training Loss: 36.80451, Validation Loss: 5.37792\n",
      "Epoch 360/1000, Training Loss: 36.79447, Validation Loss: 5.37609\n",
      "Epoch 361/1000, Training Loss: 36.78444, Validation Loss: 5.37425\n",
      "Epoch 362/1000, Training Loss: 36.77440, Validation Loss: 5.37241\n",
      "Epoch 363/1000, Training Loss: 36.76436, Validation Loss: 5.37058\n",
      "Epoch 364/1000, Training Loss: 36.75432, Validation Loss: 5.36874\n",
      "Epoch 365/1000, Training Loss: 36.74428, Validation Loss: 5.36690\n",
      "Epoch 366/1000, Training Loss: 36.73424, Validation Loss: 5.36506\n",
      "Epoch 367/1000, Training Loss: 36.72420, Validation Loss: 5.36323\n",
      "Epoch 368/1000, Training Loss: 36.71415, Validation Loss: 5.36139\n",
      "Epoch 369/1000, Training Loss: 36.70410, Validation Loss: 5.35955\n",
      "Epoch 370/1000, Training Loss: 36.69405, Validation Loss: 5.35771\n",
      "Epoch 371/1000, Training Loss: 36.68399, Validation Loss: 5.35587\n",
      "Epoch 372/1000, Training Loss: 36.67393, Validation Loss: 5.35403\n",
      "Epoch 373/1000, Training Loss: 36.66386, Validation Loss: 5.35219\n",
      "Epoch 374/1000, Training Loss: 36.65380, Validation Loss: 5.35035\n",
      "Epoch 375/1000, Training Loss: 36.64373, Validation Loss: 5.34851\n",
      "Epoch 376/1000, Training Loss: 36.63366, Validation Loss: 5.34667\n",
      "Epoch 377/1000, Training Loss: 36.62359, Validation Loss: 5.34482\n",
      "Epoch 378/1000, Training Loss: 36.61352, Validation Loss: 5.34298\n",
      "Epoch 379/1000, Training Loss: 36.60345, Validation Loss: 5.34114\n",
      "Epoch 380/1000, Training Loss: 36.59338, Validation Loss: 5.33929\n",
      "Epoch 381/1000, Training Loss: 36.58330, Validation Loss: 5.33745\n",
      "Epoch 382/1000, Training Loss: 36.57322, Validation Loss: 5.33560\n",
      "Epoch 383/1000, Training Loss: 36.56313, Validation Loss: 5.33376\n",
      "Epoch 384/1000, Training Loss: 36.55305, Validation Loss: 5.33192\n",
      "Epoch 385/1000, Training Loss: 36.54296, Validation Loss: 5.33007\n",
      "Epoch 386/1000, Training Loss: 36.53287, Validation Loss: 5.32823\n",
      "Epoch 387/1000, Training Loss: 36.52277, Validation Loss: 5.32638\n",
      "Epoch 388/1000, Training Loss: 36.51267, Validation Loss: 5.32453\n",
      "Epoch 389/1000, Training Loss: 36.50257, Validation Loss: 5.32268\n",
      "Epoch 390/1000, Training Loss: 36.49248, Validation Loss: 5.32083\n",
      "Epoch 391/1000, Training Loss: 36.48238, Validation Loss: 5.31898\n",
      "Epoch 392/1000, Training Loss: 36.47227, Validation Loss: 5.31713\n",
      "Epoch 393/1000, Training Loss: 36.46217, Validation Loss: 5.31527\n",
      "Epoch 394/1000, Training Loss: 36.45207, Validation Loss: 5.31342\n",
      "Epoch 395/1000, Training Loss: 36.44195, Validation Loss: 5.31156\n",
      "Epoch 396/1000, Training Loss: 36.43184, Validation Loss: 5.30970\n",
      "Epoch 397/1000, Training Loss: 36.42171, Validation Loss: 5.30784\n",
      "Epoch 398/1000, Training Loss: 36.41159, Validation Loss: 5.30598\n",
      "Epoch 399/1000, Training Loss: 36.40146, Validation Loss: 5.30412\n",
      "Epoch 400/1000, Training Loss: 36.39132, Validation Loss: 5.30226\n",
      "Epoch 401/1000, Training Loss: 36.38119, Validation Loss: 5.30040\n",
      "Epoch 402/1000, Training Loss: 36.37105, Validation Loss: 5.29853\n",
      "Epoch 403/1000, Training Loss: 36.36090, Validation Loss: 5.29667\n",
      "Epoch 404/1000, Training Loss: 36.35075, Validation Loss: 5.29481\n",
      "Epoch 405/1000, Training Loss: 36.34059, Validation Loss: 5.29294\n",
      "Epoch 406/1000, Training Loss: 36.33043, Validation Loss: 5.29107\n",
      "Epoch 407/1000, Training Loss: 36.32026, Validation Loss: 5.28921\n",
      "Epoch 408/1000, Training Loss: 36.31010, Validation Loss: 5.28734\n",
      "Epoch 409/1000, Training Loss: 36.29992, Validation Loss: 5.28547\n",
      "Epoch 410/1000, Training Loss: 36.28975, Validation Loss: 5.28360\n",
      "Epoch 411/1000, Training Loss: 36.27959, Validation Loss: 5.28173\n",
      "Epoch 412/1000, Training Loss: 36.26943, Validation Loss: 5.27986\n",
      "Epoch 413/1000, Training Loss: 36.25927, Validation Loss: 5.27799\n",
      "Epoch 414/1000, Training Loss: 36.24912, Validation Loss: 5.27613\n",
      "Epoch 415/1000, Training Loss: 36.23896, Validation Loss: 5.27426\n",
      "Epoch 416/1000, Training Loss: 36.22879, Validation Loss: 5.27239\n",
      "Epoch 417/1000, Training Loss: 36.21862, Validation Loss: 5.27052\n",
      "Epoch 418/1000, Training Loss: 36.20845, Validation Loss: 5.26865\n",
      "Epoch 419/1000, Training Loss: 36.19826, Validation Loss: 5.26677\n",
      "Epoch 420/1000, Training Loss: 36.18807, Validation Loss: 5.26490\n",
      "Epoch 421/1000, Training Loss: 36.17788, Validation Loss: 5.26302\n",
      "Epoch 422/1000, Training Loss: 36.16768, Validation Loss: 5.26115\n",
      "Epoch 423/1000, Training Loss: 36.15746, Validation Loss: 5.25927\n",
      "Epoch 424/1000, Training Loss: 36.14725, Validation Loss: 5.25739\n",
      "Epoch 425/1000, Training Loss: 36.13703, Validation Loss: 5.25551\n",
      "Epoch 426/1000, Training Loss: 36.12681, Validation Loss: 5.25364\n",
      "Epoch 427/1000, Training Loss: 36.11660, Validation Loss: 5.25176\n",
      "Epoch 428/1000, Training Loss: 36.10639, Validation Loss: 5.24988\n",
      "Epoch 429/1000, Training Loss: 36.09619, Validation Loss: 5.24801\n",
      "Epoch 430/1000, Training Loss: 36.08599, Validation Loss: 5.24613\n",
      "Epoch 431/1000, Training Loss: 36.07578, Validation Loss: 5.24424\n",
      "Epoch 432/1000, Training Loss: 36.06558, Validation Loss: 5.24236\n",
      "Epoch 433/1000, Training Loss: 36.05537, Validation Loss: 5.24047\n",
      "Epoch 434/1000, Training Loss: 36.04517, Validation Loss: 5.23859\n",
      "Epoch 435/1000, Training Loss: 36.03496, Validation Loss: 5.23670\n",
      "Epoch 436/1000, Training Loss: 36.02476, Validation Loss: 5.23482\n",
      "Epoch 437/1000, Training Loss: 36.01454, Validation Loss: 5.23293\n",
      "Epoch 438/1000, Training Loss: 36.00432, Validation Loss: 5.23104\n",
      "Epoch 439/1000, Training Loss: 35.99410, Validation Loss: 5.22916\n",
      "Epoch 440/1000, Training Loss: 35.98388, Validation Loss: 5.22727\n",
      "Epoch 441/1000, Training Loss: 35.97365, Validation Loss: 5.22538\n",
      "Epoch 442/1000, Training Loss: 35.96342, Validation Loss: 5.22349\n",
      "Epoch 443/1000, Training Loss: 35.95318, Validation Loss: 5.22161\n",
      "Epoch 444/1000, Training Loss: 35.94293, Validation Loss: 5.21972\n",
      "Epoch 445/1000, Training Loss: 35.93268, Validation Loss: 5.21783\n",
      "Epoch 446/1000, Training Loss: 35.92243, Validation Loss: 5.21594\n",
      "Epoch 447/1000, Training Loss: 35.91218, Validation Loss: 5.21405\n",
      "Epoch 448/1000, Training Loss: 35.90192, Validation Loss: 5.21216\n",
      "Epoch 449/1000, Training Loss: 35.89166, Validation Loss: 5.21027\n",
      "Epoch 450/1000, Training Loss: 35.88140, Validation Loss: 5.20837\n",
      "Epoch 451/1000, Training Loss: 35.87114, Validation Loss: 5.20648\n",
      "Epoch 452/1000, Training Loss: 35.86088, Validation Loss: 5.20459\n",
      "Epoch 453/1000, Training Loss: 35.85062, Validation Loss: 5.20269\n",
      "Epoch 454/1000, Training Loss: 35.84036, Validation Loss: 5.20080\n",
      "Epoch 455/1000, Training Loss: 35.83010, Validation Loss: 5.19890\n",
      "Epoch 456/1000, Training Loss: 35.81983, Validation Loss: 5.19700\n",
      "Epoch 457/1000, Training Loss: 35.80957, Validation Loss: 5.19511\n",
      "Epoch 458/1000, Training Loss: 35.79931, Validation Loss: 5.19321\n",
      "Epoch 459/1000, Training Loss: 35.78905, Validation Loss: 5.19132\n",
      "Epoch 460/1000, Training Loss: 35.77879, Validation Loss: 5.18942\n",
      "Epoch 461/1000, Training Loss: 35.76852, Validation Loss: 5.18752\n",
      "Epoch 462/1000, Training Loss: 35.75824, Validation Loss: 5.18562\n",
      "Epoch 463/1000, Training Loss: 35.74796, Validation Loss: 5.18372\n",
      "Epoch 464/1000, Training Loss: 35.73767, Validation Loss: 5.18181\n",
      "Epoch 465/1000, Training Loss: 35.72737, Validation Loss: 5.17991\n",
      "Epoch 466/1000, Training Loss: 35.71705, Validation Loss: 5.17801\n",
      "Epoch 467/1000, Training Loss: 35.70673, Validation Loss: 5.17610\n",
      "Epoch 468/1000, Training Loss: 35.69641, Validation Loss: 5.17419\n",
      "Epoch 469/1000, Training Loss: 35.68609, Validation Loss: 5.17228\n",
      "Epoch 470/1000, Training Loss: 35.67577, Validation Loss: 5.17037\n",
      "Epoch 471/1000, Training Loss: 35.66545, Validation Loss: 5.16846\n",
      "Epoch 472/1000, Training Loss: 35.65512, Validation Loss: 5.16655\n",
      "Epoch 473/1000, Training Loss: 35.64479, Validation Loss: 5.16463\n",
      "Epoch 474/1000, Training Loss: 35.63445, Validation Loss: 5.16272\n",
      "Epoch 475/1000, Training Loss: 35.62411, Validation Loss: 5.16080\n",
      "Epoch 476/1000, Training Loss: 35.61377, Validation Loss: 5.15889\n",
      "Epoch 477/1000, Training Loss: 35.60342, Validation Loss: 5.15697\n",
      "Epoch 478/1000, Training Loss: 35.59307, Validation Loss: 5.15506\n",
      "Epoch 479/1000, Training Loss: 35.58272, Validation Loss: 5.15314\n",
      "Epoch 480/1000, Training Loss: 35.57237, Validation Loss: 5.15123\n",
      "Epoch 481/1000, Training Loss: 35.56202, Validation Loss: 5.14931\n",
      "Epoch 482/1000, Training Loss: 35.55167, Validation Loss: 5.14739\n",
      "Epoch 483/1000, Training Loss: 35.54132, Validation Loss: 5.14547\n",
      "Epoch 484/1000, Training Loss: 35.53096, Validation Loss: 5.14355\n",
      "Epoch 485/1000, Training Loss: 35.52059, Validation Loss: 5.14163\n",
      "Epoch 486/1000, Training Loss: 35.51022, Validation Loss: 5.13972\n",
      "Epoch 487/1000, Training Loss: 35.49985, Validation Loss: 5.13780\n",
      "Epoch 488/1000, Training Loss: 35.48948, Validation Loss: 5.13588\n",
      "Epoch 489/1000, Training Loss: 35.47910, Validation Loss: 5.13396\n",
      "Epoch 490/1000, Training Loss: 35.46872, Validation Loss: 5.13204\n",
      "Epoch 491/1000, Training Loss: 35.45834, Validation Loss: 5.13011\n",
      "Epoch 492/1000, Training Loss: 35.44795, Validation Loss: 5.12819\n",
      "Epoch 493/1000, Training Loss: 35.43756, Validation Loss: 5.12627\n",
      "Epoch 494/1000, Training Loss: 35.42718, Validation Loss: 5.12435\n",
      "Epoch 495/1000, Training Loss: 35.41679, Validation Loss: 5.12242\n",
      "Epoch 496/1000, Training Loss: 35.40640, Validation Loss: 5.12050\n",
      "Epoch 497/1000, Training Loss: 35.39602, Validation Loss: 5.11858\n",
      "Epoch 498/1000, Training Loss: 35.38563, Validation Loss: 5.11666\n",
      "Epoch 499/1000, Training Loss: 35.37525, Validation Loss: 5.11474\n",
      "Epoch 500/1000, Training Loss: 35.36487, Validation Loss: 5.11282\n",
      "Epoch 501/1000, Training Loss: 35.35449, Validation Loss: 5.11090\n",
      "Epoch 502/1000, Training Loss: 35.34411, Validation Loss: 5.10898\n",
      "Epoch 503/1000, Training Loss: 35.33372, Validation Loss: 5.10706\n",
      "Epoch 504/1000, Training Loss: 35.32333, Validation Loss: 5.10514\n",
      "Epoch 505/1000, Training Loss: 35.31294, Validation Loss: 5.10321\n",
      "Epoch 506/1000, Training Loss: 35.30254, Validation Loss: 5.10129\n",
      "Epoch 507/1000, Training Loss: 35.29213, Validation Loss: 5.09935\n",
      "Epoch 508/1000, Training Loss: 35.28172, Validation Loss: 5.09742\n",
      "Epoch 509/1000, Training Loss: 35.27131, Validation Loss: 5.09549\n",
      "Epoch 510/1000, Training Loss: 35.26089, Validation Loss: 5.09355\n",
      "Epoch 511/1000, Training Loss: 35.25046, Validation Loss: 5.09162\n",
      "Epoch 512/1000, Training Loss: 35.24004, Validation Loss: 5.08968\n",
      "Epoch 513/1000, Training Loss: 35.22962, Validation Loss: 5.08775\n",
      "Epoch 514/1000, Training Loss: 35.21918, Validation Loss: 5.08581\n",
      "Epoch 515/1000, Training Loss: 35.20874, Validation Loss: 5.08387\n",
      "Epoch 516/1000, Training Loss: 35.19829, Validation Loss: 5.08193\n",
      "Epoch 517/1000, Training Loss: 35.18784, Validation Loss: 5.07999\n",
      "Epoch 518/1000, Training Loss: 35.17739, Validation Loss: 5.07805\n",
      "Epoch 519/1000, Training Loss: 35.16693, Validation Loss: 5.07611\n",
      "Epoch 520/1000, Training Loss: 35.15648, Validation Loss: 5.07417\n",
      "Epoch 521/1000, Training Loss: 35.14602, Validation Loss: 5.07223\n",
      "Epoch 522/1000, Training Loss: 35.13556, Validation Loss: 5.07028\n",
      "Epoch 523/1000, Training Loss: 35.12509, Validation Loss: 5.06834\n",
      "Epoch 524/1000, Training Loss: 35.11463, Validation Loss: 5.06640\n",
      "Epoch 525/1000, Training Loss: 35.10416, Validation Loss: 5.06445\n",
      "Epoch 526/1000, Training Loss: 35.09369, Validation Loss: 5.06250\n",
      "Epoch 527/1000, Training Loss: 35.08321, Validation Loss: 5.06055\n",
      "Epoch 528/1000, Training Loss: 35.07274, Validation Loss: 5.05860\n",
      "Epoch 529/1000, Training Loss: 35.06227, Validation Loss: 5.05665\n",
      "Epoch 530/1000, Training Loss: 35.05181, Validation Loss: 5.05470\n",
      "Epoch 531/1000, Training Loss: 35.04134, Validation Loss: 5.05275\n",
      "Epoch 532/1000, Training Loss: 35.03087, Validation Loss: 5.05080\n",
      "Epoch 533/1000, Training Loss: 35.02040, Validation Loss: 5.04885\n",
      "Epoch 534/1000, Training Loss: 35.00994, Validation Loss: 5.04689\n",
      "Epoch 535/1000, Training Loss: 34.99948, Validation Loss: 5.04494\n",
      "Epoch 536/1000, Training Loss: 34.98902, Validation Loss: 5.04299\n",
      "Epoch 537/1000, Training Loss: 34.97857, Validation Loss: 5.04104\n",
      "Epoch 538/1000, Training Loss: 34.96811, Validation Loss: 5.03908\n",
      "Epoch 539/1000, Training Loss: 34.95765, Validation Loss: 5.03713\n",
      "Epoch 540/1000, Training Loss: 34.94718, Validation Loss: 5.03518\n",
      "Epoch 541/1000, Training Loss: 34.93672, Validation Loss: 5.03322\n",
      "Epoch 542/1000, Training Loss: 34.92625, Validation Loss: 5.03127\n",
      "Epoch 543/1000, Training Loss: 34.91578, Validation Loss: 5.02931\n",
      "Epoch 544/1000, Training Loss: 34.90532, Validation Loss: 5.02736\n",
      "Epoch 545/1000, Training Loss: 34.89485, Validation Loss: 5.02540\n",
      "Epoch 546/1000, Training Loss: 34.88440, Validation Loss: 5.02344\n",
      "Epoch 547/1000, Training Loss: 34.87394, Validation Loss: 5.02149\n",
      "Epoch 548/1000, Training Loss: 34.86348, Validation Loss: 5.01953\n",
      "Epoch 549/1000, Training Loss: 34.85302, Validation Loss: 5.01757\n",
      "Epoch 550/1000, Training Loss: 34.84256, Validation Loss: 5.01561\n",
      "Epoch 551/1000, Training Loss: 34.83210, Validation Loss: 5.01365\n",
      "Epoch 552/1000, Training Loss: 34.82165, Validation Loss: 5.01169\n",
      "Epoch 553/1000, Training Loss: 34.81120, Validation Loss: 5.00974\n",
      "Epoch 554/1000, Training Loss: 34.80075, Validation Loss: 5.00778\n",
      "Epoch 555/1000, Training Loss: 34.79030, Validation Loss: 5.00582\n",
      "Epoch 556/1000, Training Loss: 34.77986, Validation Loss: 5.00387\n",
      "Epoch 557/1000, Training Loss: 34.76941, Validation Loss: 5.00191\n",
      "Epoch 558/1000, Training Loss: 34.75896, Validation Loss: 4.99996\n",
      "Epoch 559/1000, Training Loss: 34.74850, Validation Loss: 4.99800\n",
      "Epoch 560/1000, Training Loss: 34.73805, Validation Loss: 4.99604\n",
      "Epoch 561/1000, Training Loss: 34.72759, Validation Loss: 4.99409\n",
      "Epoch 562/1000, Training Loss: 34.71714, Validation Loss: 4.99213\n",
      "Epoch 563/1000, Training Loss: 34.70669, Validation Loss: 4.99017\n",
      "Epoch 564/1000, Training Loss: 34.69624, Validation Loss: 4.98822\n",
      "Epoch 565/1000, Training Loss: 34.68579, Validation Loss: 4.98626\n",
      "Epoch 566/1000, Training Loss: 34.67535, Validation Loss: 4.98430\n",
      "Epoch 567/1000, Training Loss: 34.66490, Validation Loss: 4.98235\n",
      "Epoch 568/1000, Training Loss: 34.65446, Validation Loss: 4.98039\n",
      "Epoch 569/1000, Training Loss: 34.64401, Validation Loss: 4.97843\n",
      "Epoch 570/1000, Training Loss: 34.63357, Validation Loss: 4.97648\n",
      "Epoch 571/1000, Training Loss: 34.62312, Validation Loss: 4.97453\n",
      "Epoch 572/1000, Training Loss: 34.61268, Validation Loss: 4.97257\n",
      "Epoch 573/1000, Training Loss: 34.60224, Validation Loss: 4.97062\n",
      "Epoch 574/1000, Training Loss: 34.59180, Validation Loss: 4.96866\n",
      "Epoch 575/1000, Training Loss: 34.58137, Validation Loss: 4.96671\n",
      "Epoch 576/1000, Training Loss: 34.57092, Validation Loss: 4.96476\n",
      "Epoch 577/1000, Training Loss: 34.56048, Validation Loss: 4.96280\n",
      "Epoch 578/1000, Training Loss: 34.55004, Validation Loss: 4.96085\n",
      "Epoch 579/1000, Training Loss: 34.53960, Validation Loss: 4.95889\n",
      "Epoch 580/1000, Training Loss: 34.52916, Validation Loss: 4.95693\n",
      "Epoch 581/1000, Training Loss: 34.51872, Validation Loss: 4.95497\n",
      "Epoch 582/1000, Training Loss: 34.50828, Validation Loss: 4.95301\n",
      "Epoch 583/1000, Training Loss: 34.49785, Validation Loss: 4.95106\n",
      "Epoch 584/1000, Training Loss: 34.48742, Validation Loss: 4.94910\n",
      "Epoch 585/1000, Training Loss: 34.47699, Validation Loss: 4.94715\n",
      "Epoch 586/1000, Training Loss: 34.46656, Validation Loss: 4.94520\n",
      "Epoch 587/1000, Training Loss: 34.45613, Validation Loss: 4.94325\n",
      "Epoch 588/1000, Training Loss: 34.44571, Validation Loss: 4.94130\n",
      "Epoch 589/1000, Training Loss: 34.43528, Validation Loss: 4.93934\n",
      "Epoch 590/1000, Training Loss: 34.42486, Validation Loss: 4.93739\n",
      "Epoch 591/1000, Training Loss: 34.41444, Validation Loss: 4.93544\n",
      "Epoch 592/1000, Training Loss: 34.40402, Validation Loss: 4.93348\n",
      "Epoch 593/1000, Training Loss: 34.39360, Validation Loss: 4.93152\n",
      "Epoch 594/1000, Training Loss: 34.38319, Validation Loss: 4.92957\n",
      "Epoch 595/1000, Training Loss: 34.37278, Validation Loss: 4.92762\n",
      "Epoch 596/1000, Training Loss: 34.36237, Validation Loss: 4.92566\n",
      "Epoch 597/1000, Training Loss: 34.35196, Validation Loss: 4.92371\n",
      "Epoch 598/1000, Training Loss: 34.34157, Validation Loss: 4.92176\n",
      "Epoch 599/1000, Training Loss: 34.33118, Validation Loss: 4.91981\n",
      "Epoch 600/1000, Training Loss: 34.32079, Validation Loss: 4.91785\n",
      "Epoch 601/1000, Training Loss: 34.31040, Validation Loss: 4.91590\n",
      "Epoch 602/1000, Training Loss: 34.30001, Validation Loss: 4.91395\n",
      "Epoch 603/1000, Training Loss: 34.28963, Validation Loss: 4.91200\n",
      "Epoch 604/1000, Training Loss: 34.27925, Validation Loss: 4.91004\n",
      "Epoch 605/1000, Training Loss: 34.26887, Validation Loss: 4.90808\n",
      "Epoch 606/1000, Training Loss: 34.25848, Validation Loss: 4.90612\n",
      "Epoch 607/1000, Training Loss: 34.24810, Validation Loss: 4.90416\n",
      "Epoch 608/1000, Training Loss: 34.23772, Validation Loss: 4.90220\n",
      "Epoch 609/1000, Training Loss: 34.22735, Validation Loss: 4.90024\n",
      "Epoch 610/1000, Training Loss: 34.21699, Validation Loss: 4.89828\n",
      "Epoch 611/1000, Training Loss: 34.20662, Validation Loss: 4.89632\n",
      "Epoch 612/1000, Training Loss: 34.19626, Validation Loss: 4.89436\n",
      "Epoch 613/1000, Training Loss: 34.18590, Validation Loss: 4.89240\n",
      "Epoch 614/1000, Training Loss: 34.17555, Validation Loss: 4.89045\n",
      "Epoch 615/1000, Training Loss: 34.16520, Validation Loss: 4.88849\n",
      "Epoch 616/1000, Training Loss: 34.15485, Validation Loss: 4.88654\n",
      "Epoch 617/1000, Training Loss: 34.14450, Validation Loss: 4.88458\n",
      "Epoch 618/1000, Training Loss: 34.13416, Validation Loss: 4.88262\n",
      "Epoch 619/1000, Training Loss: 34.12382, Validation Loss: 4.88067\n",
      "Epoch 620/1000, Training Loss: 34.11349, Validation Loss: 4.87871\n",
      "Epoch 621/1000, Training Loss: 34.10316, Validation Loss: 4.87676\n",
      "Epoch 622/1000, Training Loss: 34.09283, Validation Loss: 4.87480\n",
      "Epoch 623/1000, Training Loss: 34.08251, Validation Loss: 4.87285\n",
      "Epoch 624/1000, Training Loss: 34.07219, Validation Loss: 4.87089\n",
      "Epoch 625/1000, Training Loss: 34.06186, Validation Loss: 4.86893\n",
      "Epoch 626/1000, Training Loss: 34.05154, Validation Loss: 4.86698\n",
      "Epoch 627/1000, Training Loss: 34.04122, Validation Loss: 4.86503\n",
      "Epoch 628/1000, Training Loss: 34.03090, Validation Loss: 4.86307\n",
      "Epoch 629/1000, Training Loss: 34.02057, Validation Loss: 4.86111\n",
      "Epoch 630/1000, Training Loss: 34.01025, Validation Loss: 4.85916\n",
      "Epoch 631/1000, Training Loss: 33.99994, Validation Loss: 4.85721\n",
      "Epoch 632/1000, Training Loss: 33.98963, Validation Loss: 4.85526\n",
      "Epoch 633/1000, Training Loss: 33.97932, Validation Loss: 4.85331\n",
      "Epoch 634/1000, Training Loss: 33.96901, Validation Loss: 4.85135\n",
      "Epoch 635/1000, Training Loss: 33.95871, Validation Loss: 4.84940\n",
      "Epoch 636/1000, Training Loss: 33.94842, Validation Loss: 4.84744\n",
      "Epoch 637/1000, Training Loss: 33.93814, Validation Loss: 4.84549\n",
      "Epoch 638/1000, Training Loss: 33.92787, Validation Loss: 4.84354\n",
      "Epoch 639/1000, Training Loss: 33.91761, Validation Loss: 4.84158\n",
      "Epoch 640/1000, Training Loss: 33.90734, Validation Loss: 4.83963\n",
      "Epoch 641/1000, Training Loss: 33.89708, Validation Loss: 4.83768\n",
      "Epoch 642/1000, Training Loss: 33.88683, Validation Loss: 4.83573\n",
      "Epoch 643/1000, Training Loss: 33.87658, Validation Loss: 4.83378\n",
      "Epoch 644/1000, Training Loss: 33.86634, Validation Loss: 4.83183\n",
      "Epoch 645/1000, Training Loss: 33.85609, Validation Loss: 4.82988\n",
      "Epoch 646/1000, Training Loss: 33.84585, Validation Loss: 4.82793\n",
      "Epoch 647/1000, Training Loss: 33.83562, Validation Loss: 4.82598\n",
      "Epoch 648/1000, Training Loss: 33.82541, Validation Loss: 4.82403\n",
      "Epoch 649/1000, Training Loss: 33.81518, Validation Loss: 4.82208\n",
      "Epoch 650/1000, Training Loss: 33.80496, Validation Loss: 4.82013\n",
      "Epoch 651/1000, Training Loss: 33.79475, Validation Loss: 4.81818\n",
      "Epoch 652/1000, Training Loss: 33.78454, Validation Loss: 4.81623\n",
      "Epoch 653/1000, Training Loss: 33.77433, Validation Loss: 4.81428\n",
      "Epoch 654/1000, Training Loss: 33.76413, Validation Loss: 4.81233\n",
      "Epoch 655/1000, Training Loss: 33.75394, Validation Loss: 4.81038\n",
      "Epoch 656/1000, Training Loss: 33.74375, Validation Loss: 4.80843\n",
      "Epoch 657/1000, Training Loss: 33.73357, Validation Loss: 4.80649\n",
      "Epoch 658/1000, Training Loss: 33.72340, Validation Loss: 4.80454\n",
      "Epoch 659/1000, Training Loss: 33.71322, Validation Loss: 4.80260\n",
      "Epoch 660/1000, Training Loss: 33.70305, Validation Loss: 4.80065\n",
      "Epoch 661/1000, Training Loss: 33.69289, Validation Loss: 4.79871\n",
      "Epoch 662/1000, Training Loss: 33.68273, Validation Loss: 4.79676\n",
      "Epoch 663/1000, Training Loss: 33.67258, Validation Loss: 4.79482\n",
      "Epoch 664/1000, Training Loss: 33.66243, Validation Loss: 4.79288\n",
      "Epoch 665/1000, Training Loss: 33.65229, Validation Loss: 4.79093\n",
      "Epoch 666/1000, Training Loss: 33.64216, Validation Loss: 4.78899\n",
      "Epoch 667/1000, Training Loss: 33.63204, Validation Loss: 4.78705\n",
      "Epoch 668/1000, Training Loss: 33.62192, Validation Loss: 4.78511\n",
      "Epoch 669/1000, Training Loss: 33.61182, Validation Loss: 4.78318\n",
      "Epoch 670/1000, Training Loss: 33.60172, Validation Loss: 4.78124\n",
      "Epoch 671/1000, Training Loss: 33.59163, Validation Loss: 4.77930\n",
      "Epoch 672/1000, Training Loss: 33.58154, Validation Loss: 4.77737\n",
      "Epoch 673/1000, Training Loss: 33.57146, Validation Loss: 4.77544\n",
      "Epoch 674/1000, Training Loss: 33.56138, Validation Loss: 4.77350\n",
      "Epoch 675/1000, Training Loss: 33.55131, Validation Loss: 4.77157\n",
      "Epoch 676/1000, Training Loss: 33.54125, Validation Loss: 4.76964\n",
      "Epoch 677/1000, Training Loss: 33.53119, Validation Loss: 4.76771\n",
      "Epoch 678/1000, Training Loss: 33.52113, Validation Loss: 4.76578\n",
      "Epoch 679/1000, Training Loss: 33.51108, Validation Loss: 4.76385\n",
      "Epoch 680/1000, Training Loss: 33.50104, Validation Loss: 4.76192\n",
      "Epoch 681/1000, Training Loss: 33.49100, Validation Loss: 4.75999\n",
      "Epoch 682/1000, Training Loss: 33.48098, Validation Loss: 4.75807\n",
      "Epoch 683/1000, Training Loss: 33.47096, Validation Loss: 4.75614\n",
      "Epoch 684/1000, Training Loss: 33.46095, Validation Loss: 4.75422\n",
      "Epoch 685/1000, Training Loss: 33.45094, Validation Loss: 4.75230\n",
      "Epoch 686/1000, Training Loss: 33.44094, Validation Loss: 4.75037\n",
      "Epoch 687/1000, Training Loss: 33.43095, Validation Loss: 4.74845\n",
      "Epoch 688/1000, Training Loss: 33.42096, Validation Loss: 4.74653\n",
      "Epoch 689/1000, Training Loss: 33.41097, Validation Loss: 4.74461\n",
      "Epoch 690/1000, Training Loss: 33.40100, Validation Loss: 4.74269\n",
      "Epoch 691/1000, Training Loss: 33.39103, Validation Loss: 4.74078\n",
      "Epoch 692/1000, Training Loss: 33.38108, Validation Loss: 4.73886\n",
      "Epoch 693/1000, Training Loss: 33.37113, Validation Loss: 4.73694\n",
      "Epoch 694/1000, Training Loss: 33.36120, Validation Loss: 4.73503\n",
      "Epoch 695/1000, Training Loss: 33.35128, Validation Loss: 4.73312\n",
      "Epoch 696/1000, Training Loss: 33.34137, Validation Loss: 4.73121\n",
      "Epoch 697/1000, Training Loss: 33.33146, Validation Loss: 4.72931\n",
      "Epoch 698/1000, Training Loss: 33.32158, Validation Loss: 4.72740\n",
      "Epoch 699/1000, Training Loss: 33.31169, Validation Loss: 4.72549\n",
      "Epoch 700/1000, Training Loss: 33.30182, Validation Loss: 4.72359\n",
      "Epoch 701/1000, Training Loss: 33.29195, Validation Loss: 4.72168\n",
      "Epoch 702/1000, Training Loss: 33.28209, Validation Loss: 4.71977\n",
      "Epoch 703/1000, Training Loss: 33.27223, Validation Loss: 4.71787\n",
      "Epoch 704/1000, Training Loss: 33.26238, Validation Loss: 4.71596\n",
      "Epoch 705/1000, Training Loss: 33.25253, Validation Loss: 4.71406\n",
      "Epoch 706/1000, Training Loss: 33.24268, Validation Loss: 4.71215\n",
      "Epoch 707/1000, Training Loss: 33.23284, Validation Loss: 4.71025\n",
      "Epoch 708/1000, Training Loss: 33.22299, Validation Loss: 4.70834\n",
      "Epoch 709/1000, Training Loss: 33.21315, Validation Loss: 4.70644\n",
      "Epoch 710/1000, Training Loss: 33.20332, Validation Loss: 4.70454\n",
      "Epoch 711/1000, Training Loss: 33.19350, Validation Loss: 4.70264\n",
      "Epoch 712/1000, Training Loss: 33.18368, Validation Loss: 4.70074\n",
      "Epoch 713/1000, Training Loss: 33.17388, Validation Loss: 4.69884\n",
      "Epoch 714/1000, Training Loss: 33.16409, Validation Loss: 4.69694\n",
      "Epoch 715/1000, Training Loss: 33.15431, Validation Loss: 4.69504\n",
      "Epoch 716/1000, Training Loss: 33.14454, Validation Loss: 4.69315\n",
      "Epoch 717/1000, Training Loss: 33.13478, Validation Loss: 4.69125\n",
      "Epoch 718/1000, Training Loss: 33.12504, Validation Loss: 4.68936\n",
      "Epoch 719/1000, Training Loss: 33.11532, Validation Loss: 4.68747\n",
      "Epoch 720/1000, Training Loss: 33.10562, Validation Loss: 4.68558\n",
      "Epoch 721/1000, Training Loss: 33.09593, Validation Loss: 4.68370\n",
      "Epoch 722/1000, Training Loss: 33.08625, Validation Loss: 4.68181\n",
      "Epoch 723/1000, Training Loss: 33.07656, Validation Loss: 4.67993\n",
      "Epoch 724/1000, Training Loss: 33.06688, Validation Loss: 4.67804\n",
      "Epoch 725/1000, Training Loss: 33.05721, Validation Loss: 4.67616\n",
      "Epoch 726/1000, Training Loss: 33.04755, Validation Loss: 4.67427\n",
      "Epoch 727/1000, Training Loss: 33.03790, Validation Loss: 4.67239\n",
      "Epoch 728/1000, Training Loss: 33.02826, Validation Loss: 4.67051\n",
      "Epoch 729/1000, Training Loss: 33.01862, Validation Loss: 4.66863\n",
      "Epoch 730/1000, Training Loss: 33.00900, Validation Loss: 4.66675\n",
      "Epoch 731/1000, Training Loss: 32.99940, Validation Loss: 4.66487\n",
      "Epoch 732/1000, Training Loss: 32.98980, Validation Loss: 4.66299\n",
      "Epoch 733/1000, Training Loss: 32.98020, Validation Loss: 4.66112\n",
      "Epoch 734/1000, Training Loss: 32.97062, Validation Loss: 4.65925\n",
      "Epoch 735/1000, Training Loss: 32.96105, Validation Loss: 4.65738\n",
      "Epoch 736/1000, Training Loss: 32.95149, Validation Loss: 4.65552\n",
      "Epoch 737/1000, Training Loss: 32.94194, Validation Loss: 4.65365\n",
      "Epoch 738/1000, Training Loss: 32.93241, Validation Loss: 4.65179\n",
      "Epoch 739/1000, Training Loss: 32.92289, Validation Loss: 4.64992\n",
      "Epoch 740/1000, Training Loss: 32.91338, Validation Loss: 4.64806\n",
      "Epoch 741/1000, Training Loss: 32.90389, Validation Loss: 4.64620\n",
      "Epoch 742/1000, Training Loss: 32.89440, Validation Loss: 4.64435\n",
      "Epoch 743/1000, Training Loss: 32.88491, Validation Loss: 4.64249\n",
      "Epoch 744/1000, Training Loss: 32.87543, Validation Loss: 4.64064\n",
      "Epoch 745/1000, Training Loss: 32.86595, Validation Loss: 4.63879\n",
      "Epoch 746/1000, Training Loss: 32.85649, Validation Loss: 4.63694\n",
      "Epoch 747/1000, Training Loss: 32.84703, Validation Loss: 4.63509\n",
      "Epoch 748/1000, Training Loss: 32.83759, Validation Loss: 4.63323\n",
      "Epoch 749/1000, Training Loss: 32.82816, Validation Loss: 4.63138\n",
      "Epoch 750/1000, Training Loss: 32.81871, Validation Loss: 4.62953\n",
      "Epoch 751/1000, Training Loss: 32.80927, Validation Loss: 4.62768\n",
      "Epoch 752/1000, Training Loss: 32.79985, Validation Loss: 4.62583\n",
      "Epoch 753/1000, Training Loss: 32.79043, Validation Loss: 4.62399\n",
      "Epoch 754/1000, Training Loss: 32.78103, Validation Loss: 4.62214\n",
      "Epoch 755/1000, Training Loss: 32.77163, Validation Loss: 4.62030\n",
      "Epoch 756/1000, Training Loss: 32.76224, Validation Loss: 4.61846\n",
      "Epoch 757/1000, Training Loss: 32.75285, Validation Loss: 4.61663\n",
      "Epoch 758/1000, Training Loss: 32.74348, Validation Loss: 4.61479\n",
      "Epoch 759/1000, Training Loss: 32.73412, Validation Loss: 4.61296\n",
      "Epoch 760/1000, Training Loss: 32.72477, Validation Loss: 4.61113\n",
      "Epoch 761/1000, Training Loss: 32.71542, Validation Loss: 4.60930\n",
      "Epoch 762/1000, Training Loss: 32.70609, Validation Loss: 4.60747\n",
      "Epoch 763/1000, Training Loss: 32.69676, Validation Loss: 4.60564\n",
      "Epoch 764/1000, Training Loss: 32.68744, Validation Loss: 4.60381\n",
      "Epoch 765/1000, Training Loss: 32.67814, Validation Loss: 4.60198\n",
      "Epoch 766/1000, Training Loss: 32.66884, Validation Loss: 4.60015\n",
      "Epoch 767/1000, Training Loss: 32.65955, Validation Loss: 4.59833\n",
      "Epoch 768/1000, Training Loss: 32.65026, Validation Loss: 4.59650\n",
      "Epoch 769/1000, Training Loss: 32.64099, Validation Loss: 4.59468\n",
      "Epoch 770/1000, Training Loss: 32.63173, Validation Loss: 4.59286\n",
      "Epoch 771/1000, Training Loss: 32.62248, Validation Loss: 4.59104\n",
      "Epoch 772/1000, Training Loss: 32.61324, Validation Loss: 4.58922\n",
      "Epoch 773/1000, Training Loss: 32.60401, Validation Loss: 4.58740\n",
      "Epoch 774/1000, Training Loss: 32.59478, Validation Loss: 4.58558\n",
      "Epoch 775/1000, Training Loss: 32.58556, Validation Loss: 4.58377\n",
      "Epoch 776/1000, Training Loss: 32.57634, Validation Loss: 4.58195\n",
      "Epoch 777/1000, Training Loss: 32.56714, Validation Loss: 4.58014\n",
      "Epoch 778/1000, Training Loss: 32.55796, Validation Loss: 4.57833\n",
      "Epoch 779/1000, Training Loss: 32.54880, Validation Loss: 4.57652\n",
      "Epoch 780/1000, Training Loss: 32.53965, Validation Loss: 4.57471\n",
      "Epoch 781/1000, Training Loss: 32.53051, Validation Loss: 4.57291\n",
      "Epoch 782/1000, Training Loss: 32.52138, Validation Loss: 4.57110\n",
      "Epoch 783/1000, Training Loss: 32.51226, Validation Loss: 4.56930\n",
      "Epoch 784/1000, Training Loss: 32.50315, Validation Loss: 4.56750\n",
      "Epoch 785/1000, Training Loss: 32.49405, Validation Loss: 4.56570\n",
      "Epoch 786/1000, Training Loss: 32.48496, Validation Loss: 4.56390\n",
      "Epoch 787/1000, Training Loss: 32.47587, Validation Loss: 4.56211\n",
      "Epoch 788/1000, Training Loss: 32.46679, Validation Loss: 4.56031\n",
      "Epoch 789/1000, Training Loss: 32.45772, Validation Loss: 4.55852\n",
      "Epoch 790/1000, Training Loss: 32.44867, Validation Loss: 4.55673\n",
      "Epoch 791/1000, Training Loss: 32.43962, Validation Loss: 4.55494\n",
      "Epoch 792/1000, Training Loss: 32.43058, Validation Loss: 4.55315\n",
      "Epoch 793/1000, Training Loss: 32.42155, Validation Loss: 4.55137\n",
      "Epoch 794/1000, Training Loss: 32.41254, Validation Loss: 4.54959\n",
      "Epoch 795/1000, Training Loss: 32.40354, Validation Loss: 4.54781\n",
      "Epoch 796/1000, Training Loss: 32.39455, Validation Loss: 4.54603\n",
      "Epoch 797/1000, Training Loss: 32.38557, Validation Loss: 4.54426\n",
      "Epoch 798/1000, Training Loss: 32.37661, Validation Loss: 4.54248\n",
      "Epoch 799/1000, Training Loss: 32.36765, Validation Loss: 4.54071\n",
      "Epoch 800/1000, Training Loss: 32.35871, Validation Loss: 4.53894\n",
      "Epoch 801/1000, Training Loss: 32.34978, Validation Loss: 4.53717\n",
      "Epoch 802/1000, Training Loss: 32.34087, Validation Loss: 4.53540\n",
      "Epoch 803/1000, Training Loss: 32.33197, Validation Loss: 4.53363\n",
      "Epoch 804/1000, Training Loss: 32.32309, Validation Loss: 4.53187\n",
      "Epoch 805/1000, Training Loss: 32.31423, Validation Loss: 4.53011\n",
      "Epoch 806/1000, Training Loss: 32.30537, Validation Loss: 4.52835\n",
      "Epoch 807/1000, Training Loss: 32.29653, Validation Loss: 4.52659\n",
      "Epoch 808/1000, Training Loss: 32.28770, Validation Loss: 4.52483\n",
      "Epoch 809/1000, Training Loss: 32.27889, Validation Loss: 4.52308\n",
      "Epoch 810/1000, Training Loss: 32.27008, Validation Loss: 4.52132\n",
      "Epoch 811/1000, Training Loss: 32.26129, Validation Loss: 4.51958\n",
      "Epoch 812/1000, Training Loss: 32.25249, Validation Loss: 4.51783\n",
      "Epoch 813/1000, Training Loss: 32.24371, Validation Loss: 4.51609\n",
      "Epoch 814/1000, Training Loss: 32.23494, Validation Loss: 4.51435\n",
      "Epoch 815/1000, Training Loss: 32.22618, Validation Loss: 4.51261\n",
      "Epoch 816/1000, Training Loss: 32.21743, Validation Loss: 4.51087\n",
      "Epoch 817/1000, Training Loss: 32.20868, Validation Loss: 4.50914\n",
      "Epoch 818/1000, Training Loss: 32.19996, Validation Loss: 4.50740\n",
      "Epoch 819/1000, Training Loss: 32.19124, Validation Loss: 4.50567\n",
      "Epoch 820/1000, Training Loss: 32.18253, Validation Loss: 4.50394\n",
      "Epoch 821/1000, Training Loss: 32.17384, Validation Loss: 4.50222\n",
      "Epoch 822/1000, Training Loss: 32.16516, Validation Loss: 4.50049\n",
      "Epoch 823/1000, Training Loss: 32.15648, Validation Loss: 4.49877\n",
      "Epoch 824/1000, Training Loss: 32.14780, Validation Loss: 4.49704\n",
      "Epoch 825/1000, Training Loss: 32.13913, Validation Loss: 4.49532\n",
      "Epoch 826/1000, Training Loss: 32.13048, Validation Loss: 4.49360\n",
      "Epoch 827/1000, Training Loss: 32.12184, Validation Loss: 4.49189\n",
      "Epoch 828/1000, Training Loss: 32.11321, Validation Loss: 4.49017\n",
      "Epoch 829/1000, Training Loss: 32.10459, Validation Loss: 4.48846\n",
      "Epoch 830/1000, Training Loss: 32.09598, Validation Loss: 4.48676\n",
      "Epoch 831/1000, Training Loss: 32.08739, Validation Loss: 4.48505\n",
      "Epoch 832/1000, Training Loss: 32.07880, Validation Loss: 4.48335\n",
      "Epoch 833/1000, Training Loss: 32.07023, Validation Loss: 4.48164\n",
      "Epoch 834/1000, Training Loss: 32.06166, Validation Loss: 4.47994\n",
      "Epoch 835/1000, Training Loss: 32.05311, Validation Loss: 4.47824\n",
      "Epoch 836/1000, Training Loss: 32.04456, Validation Loss: 4.47654\n",
      "Epoch 837/1000, Training Loss: 32.03602, Validation Loss: 4.47485\n",
      "Epoch 838/1000, Training Loss: 32.02750, Validation Loss: 4.47315\n",
      "Epoch 839/1000, Training Loss: 32.01899, Validation Loss: 4.47146\n",
      "Epoch 840/1000, Training Loss: 32.01050, Validation Loss: 4.46977\n",
      "Epoch 841/1000, Training Loss: 32.00202, Validation Loss: 4.46809\n",
      "Epoch 842/1000, Training Loss: 31.99355, Validation Loss: 4.46640\n",
      "Epoch 843/1000, Training Loss: 31.98509, Validation Loss: 4.46472\n",
      "Epoch 844/1000, Training Loss: 31.97663, Validation Loss: 4.46305\n",
      "Epoch 845/1000, Training Loss: 31.96819, Validation Loss: 4.46137\n",
      "Epoch 846/1000, Training Loss: 31.95976, Validation Loss: 4.45970\n",
      "Epoch 847/1000, Training Loss: 31.95135, Validation Loss: 4.45802\n",
      "Epoch 848/1000, Training Loss: 31.94294, Validation Loss: 4.45635\n",
      "Epoch 849/1000, Training Loss: 31.93455, Validation Loss: 4.45469\n",
      "Epoch 850/1000, Training Loss: 31.92618, Validation Loss: 4.45302\n",
      "Epoch 851/1000, Training Loss: 31.91782, Validation Loss: 4.45136\n",
      "Epoch 852/1000, Training Loss: 31.90947, Validation Loss: 4.44970\n",
      "Epoch 853/1000, Training Loss: 31.90114, Validation Loss: 4.44804\n",
      "Epoch 854/1000, Training Loss: 31.89282, Validation Loss: 4.44639\n",
      "Epoch 855/1000, Training Loss: 31.88452, Validation Loss: 4.44473\n",
      "Epoch 856/1000, Training Loss: 31.87624, Validation Loss: 4.44308\n",
      "Epoch 857/1000, Training Loss: 31.86797, Validation Loss: 4.44143\n",
      "Epoch 858/1000, Training Loss: 31.85972, Validation Loss: 4.43978\n",
      "Epoch 859/1000, Training Loss: 31.85148, Validation Loss: 4.43813\n",
      "Epoch 860/1000, Training Loss: 31.84325, Validation Loss: 4.43649\n",
      "Epoch 861/1000, Training Loss: 31.83504, Validation Loss: 4.43485\n",
      "Epoch 862/1000, Training Loss: 31.82686, Validation Loss: 4.43321\n",
      "Epoch 863/1000, Training Loss: 31.81868, Validation Loss: 4.43157\n",
      "Epoch 864/1000, Training Loss: 31.81052, Validation Loss: 4.42994\n",
      "Epoch 865/1000, Training Loss: 31.80238, Validation Loss: 4.42830\n",
      "Epoch 866/1000, Training Loss: 31.79424, Validation Loss: 4.42667\n",
      "Epoch 867/1000, Training Loss: 31.78612, Validation Loss: 4.42504\n",
      "Epoch 868/1000, Training Loss: 31.77802, Validation Loss: 4.42342\n",
      "Epoch 869/1000, Training Loss: 31.76992, Validation Loss: 4.42179\n",
      "Epoch 870/1000, Training Loss: 31.76182, Validation Loss: 4.42017\n",
      "Epoch 871/1000, Training Loss: 31.75374, Validation Loss: 4.41855\n",
      "Epoch 872/1000, Training Loss: 31.74567, Validation Loss: 4.41693\n",
      "Epoch 873/1000, Training Loss: 31.73761, Validation Loss: 4.41532\n",
      "Epoch 874/1000, Training Loss: 31.72957, Validation Loss: 4.41371\n",
      "Epoch 875/1000, Training Loss: 31.72154, Validation Loss: 4.41209\n",
      "Epoch 876/1000, Training Loss: 31.71352, Validation Loss: 4.41049\n",
      "Epoch 877/1000, Training Loss: 31.70552, Validation Loss: 4.40888\n",
      "Epoch 878/1000, Training Loss: 31.69753, Validation Loss: 4.40728\n",
      "Epoch 879/1000, Training Loss: 31.68956, Validation Loss: 4.40567\n",
      "Epoch 880/1000, Training Loss: 31.68160, Validation Loss: 4.40407\n",
      "Epoch 881/1000, Training Loss: 31.67365, Validation Loss: 4.40247\n",
      "Epoch 882/1000, Training Loss: 31.66573, Validation Loss: 4.40088\n",
      "Epoch 883/1000, Training Loss: 31.65781, Validation Loss: 4.39929\n",
      "Epoch 884/1000, Training Loss: 31.64991, Validation Loss: 4.39769\n",
      "Epoch 885/1000, Training Loss: 31.64202, Validation Loss: 4.39611\n",
      "Epoch 886/1000, Training Loss: 31.63414, Validation Loss: 4.39452\n",
      "Epoch 887/1000, Training Loss: 31.62628, Validation Loss: 4.39294\n",
      "Epoch 888/1000, Training Loss: 31.61843, Validation Loss: 4.39136\n",
      "Epoch 889/1000, Training Loss: 31.61060, Validation Loss: 4.38978\n",
      "Epoch 890/1000, Training Loss: 31.60278, Validation Loss: 4.38820\n",
      "Epoch 891/1000, Training Loss: 31.59497, Validation Loss: 4.38663\n",
      "Epoch 892/1000, Training Loss: 31.58717, Validation Loss: 4.38506\n",
      "Epoch 893/1000, Training Loss: 31.57939, Validation Loss: 4.38349\n",
      "Epoch 894/1000, Training Loss: 31.57162, Validation Loss: 4.38192\n",
      "Epoch 895/1000, Training Loss: 31.56386, Validation Loss: 4.38035\n",
      "Epoch 896/1000, Training Loss: 31.55612, Validation Loss: 4.37879\n",
      "Epoch 897/1000, Training Loss: 31.54841, Validation Loss: 4.37723\n",
      "Epoch 898/1000, Training Loss: 31.54071, Validation Loss: 4.37567\n",
      "Epoch 899/1000, Training Loss: 31.53302, Validation Loss: 4.37412\n",
      "Epoch 900/1000, Training Loss: 31.52535, Validation Loss: 4.37257\n",
      "Epoch 901/1000, Training Loss: 31.51770, Validation Loss: 4.37102\n",
      "Epoch 902/1000, Training Loss: 31.51007, Validation Loss: 4.36948\n",
      "Epoch 903/1000, Training Loss: 31.50245, Validation Loss: 4.36794\n",
      "Epoch 904/1000, Training Loss: 31.49485, Validation Loss: 4.36640\n",
      "Epoch 905/1000, Training Loss: 31.48726, Validation Loss: 4.36487\n",
      "Epoch 906/1000, Training Loss: 31.47969, Validation Loss: 4.36334\n",
      "Epoch 907/1000, Training Loss: 31.47213, Validation Loss: 4.36181\n",
      "Epoch 908/1000, Training Loss: 31.46459, Validation Loss: 4.36028\n",
      "Epoch 909/1000, Training Loss: 31.45707, Validation Loss: 4.35876\n",
      "Epoch 910/1000, Training Loss: 31.44957, Validation Loss: 4.35723\n",
      "Epoch 911/1000, Training Loss: 31.44208, Validation Loss: 4.35571\n",
      "Epoch 912/1000, Training Loss: 31.43460, Validation Loss: 4.35420\n",
      "Epoch 913/1000, Training Loss: 31.42713, Validation Loss: 4.35268\n",
      "Epoch 914/1000, Training Loss: 31.41968, Validation Loss: 4.35117\n",
      "Epoch 915/1000, Training Loss: 31.41225, Validation Loss: 4.34965\n",
      "Epoch 916/1000, Training Loss: 31.40482, Validation Loss: 4.34814\n",
      "Epoch 917/1000, Training Loss: 31.39741, Validation Loss: 4.34664\n",
      "Epoch 918/1000, Training Loss: 31.39002, Validation Loss: 4.34513\n",
      "Epoch 919/1000, Training Loss: 31.38264, Validation Loss: 4.34362\n",
      "Epoch 920/1000, Training Loss: 31.37527, Validation Loss: 4.34212\n",
      "Epoch 921/1000, Training Loss: 31.36792, Validation Loss: 4.34062\n",
      "Epoch 922/1000, Training Loss: 31.36058, Validation Loss: 4.33913\n",
      "Epoch 923/1000, Training Loss: 31.35326, Validation Loss: 4.33763\n",
      "Epoch 924/1000, Training Loss: 31.34595, Validation Loss: 4.33614\n",
      "Epoch 925/1000, Training Loss: 31.33865, Validation Loss: 4.33465\n",
      "Epoch 926/1000, Training Loss: 31.33137, Validation Loss: 4.33316\n",
      "Epoch 927/1000, Training Loss: 31.32411, Validation Loss: 4.33168\n",
      "Epoch 928/1000, Training Loss: 31.31685, Validation Loss: 4.33020\n",
      "Epoch 929/1000, Training Loss: 31.30962, Validation Loss: 4.32872\n",
      "Epoch 930/1000, Training Loss: 31.30240, Validation Loss: 4.32724\n",
      "Epoch 931/1000, Training Loss: 31.29519, Validation Loss: 4.32576\n",
      "Epoch 932/1000, Training Loss: 31.28799, Validation Loss: 4.32429\n",
      "Epoch 933/1000, Training Loss: 31.28082, Validation Loss: 4.32282\n",
      "Epoch 934/1000, Training Loss: 31.27366, Validation Loss: 4.32135\n",
      "Epoch 935/1000, Training Loss: 31.26652, Validation Loss: 4.31989\n",
      "Epoch 936/1000, Training Loss: 31.25940, Validation Loss: 4.31843\n",
      "Epoch 937/1000, Training Loss: 31.25230, Validation Loss: 4.31696\n",
      "Epoch 938/1000, Training Loss: 31.24520, Validation Loss: 4.31550\n",
      "Epoch 939/1000, Training Loss: 31.23812, Validation Loss: 4.31405\n",
      "Epoch 940/1000, Training Loss: 31.23105, Validation Loss: 4.31259\n",
      "Epoch 941/1000, Training Loss: 31.22399, Validation Loss: 4.31113\n",
      "Epoch 942/1000, Training Loss: 31.21695, Validation Loss: 4.30968\n",
      "Epoch 943/1000, Training Loss: 31.20992, Validation Loss: 4.30823\n",
      "Epoch 944/1000, Training Loss: 31.20290, Validation Loss: 4.30679\n",
      "Epoch 945/1000, Training Loss: 31.19589, Validation Loss: 4.30534\n",
      "Epoch 946/1000, Training Loss: 31.18890, Validation Loss: 4.30390\n",
      "Epoch 947/1000, Training Loss: 31.18193, Validation Loss: 4.30246\n",
      "Epoch 948/1000, Training Loss: 31.17497, Validation Loss: 4.30102\n",
      "Epoch 949/1000, Training Loss: 31.16803, Validation Loss: 4.29958\n",
      "Epoch 950/1000, Training Loss: 31.16110, Validation Loss: 4.29815\n",
      "Epoch 951/1000, Training Loss: 31.15419, Validation Loss: 4.29672\n",
      "Epoch 952/1000, Training Loss: 31.14729, Validation Loss: 4.29529\n",
      "Epoch 953/1000, Training Loss: 31.14041, Validation Loss: 4.29386\n",
      "Epoch 954/1000, Training Loss: 31.13355, Validation Loss: 4.29244\n",
      "Epoch 955/1000, Training Loss: 31.12670, Validation Loss: 4.29102\n",
      "Epoch 956/1000, Training Loss: 31.11987, Validation Loss: 4.28960\n",
      "Epoch 957/1000, Training Loss: 31.11305, Validation Loss: 4.28818\n",
      "Epoch 958/1000, Training Loss: 31.10625, Validation Loss: 4.28677\n",
      "Epoch 959/1000, Training Loss: 31.09946, Validation Loss: 4.28537\n",
      "Epoch 960/1000, Training Loss: 31.09268, Validation Loss: 4.28396\n",
      "Epoch 961/1000, Training Loss: 31.08592, Validation Loss: 4.28256\n",
      "Epoch 962/1000, Training Loss: 31.07917, Validation Loss: 4.28117\n",
      "Epoch 963/1000, Training Loss: 31.07244, Validation Loss: 4.27977\n",
      "Epoch 964/1000, Training Loss: 31.06573, Validation Loss: 4.27838\n",
      "Epoch 965/1000, Training Loss: 31.05902, Validation Loss: 4.27699\n",
      "Epoch 966/1000, Training Loss: 31.05233, Validation Loss: 4.27561\n",
      "Epoch 967/1000, Training Loss: 31.04566, Validation Loss: 4.27423\n",
      "Epoch 968/1000, Training Loss: 31.03900, Validation Loss: 4.27284\n",
      "Epoch 969/1000, Training Loss: 31.03235, Validation Loss: 4.27147\n",
      "Epoch 970/1000, Training Loss: 31.02572, Validation Loss: 4.27009\n",
      "Epoch 971/1000, Training Loss: 31.01911, Validation Loss: 4.26872\n",
      "Epoch 972/1000, Training Loss: 31.01250, Validation Loss: 4.26735\n",
      "Epoch 973/1000, Training Loss: 31.00592, Validation Loss: 4.26598\n",
      "Epoch 974/1000, Training Loss: 30.99934, Validation Loss: 4.26461\n",
      "Epoch 975/1000, Training Loss: 30.99278, Validation Loss: 4.26325\n",
      "Epoch 976/1000, Training Loss: 30.98624, Validation Loss: 4.26189\n",
      "Epoch 977/1000, Training Loss: 30.97970, Validation Loss: 4.26053\n",
      "Epoch 978/1000, Training Loss: 30.97318, Validation Loss: 4.25917\n",
      "Epoch 979/1000, Training Loss: 30.96666, Validation Loss: 4.25781\n",
      "Epoch 980/1000, Training Loss: 30.96015, Validation Loss: 4.25646\n",
      "Epoch 981/1000, Training Loss: 30.95366, Validation Loss: 4.25511\n",
      "Epoch 982/1000, Training Loss: 30.94719, Validation Loss: 4.25376\n",
      "Epoch 983/1000, Training Loss: 30.94073, Validation Loss: 4.25242\n",
      "Epoch 984/1000, Training Loss: 30.93428, Validation Loss: 4.25108\n",
      "Epoch 985/1000, Training Loss: 30.92785, Validation Loss: 4.24974\n",
      "Epoch 986/1000, Training Loss: 30.92142, Validation Loss: 4.24840\n",
      "Epoch 987/1000, Training Loss: 30.91502, Validation Loss: 4.24707\n",
      "Epoch 988/1000, Training Loss: 30.90862, Validation Loss: 4.24573\n",
      "Epoch 989/1000, Training Loss: 30.90224, Validation Loss: 4.24441\n",
      "Epoch 990/1000, Training Loss: 30.89586, Validation Loss: 4.24308\n",
      "Epoch 991/1000, Training Loss: 30.88950, Validation Loss: 4.24175\n",
      "Epoch 992/1000, Training Loss: 30.88315, Validation Loss: 4.24043\n",
      "Epoch 993/1000, Training Loss: 30.87681, Validation Loss: 4.23911\n",
      "Epoch 994/1000, Training Loss: 30.87048, Validation Loss: 4.23780\n",
      "Epoch 995/1000, Training Loss: 30.86417, Validation Loss: 4.23648\n",
      "Epoch 996/1000, Training Loss: 30.85787, Validation Loss: 4.23517\n",
      "Epoch 997/1000, Training Loss: 30.85159, Validation Loss: 4.23386\n",
      "Epoch 998/1000, Training Loss: 30.84532, Validation Loss: 4.23255\n",
      "Epoch 999/1000, Training Loss: 30.83906, Validation Loss: 4.23125\n",
      "Epoch 1000/1000, Training Loss: 30.83282, Validation Loss: 4.22995\n",
      "Training took: 120.13 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_with_lrs_base_3 = BaseModelWithLRS().to(device)\n",
    "summary(model_with_lrs_base_3, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.ASGD(model_with_lrs_base_3.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_with_lrs_base_3=[]\n",
    "val_loss_list_with_lrs_base_3=[]\n",
    "train_accuracy_list_with_lrs_base_3=[]\n",
    "val_accuracy_list_with_lrs_base_3=[]\n",
    "test_accuracy_list_with_lrs_base_3=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_with_lrs_base_3.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_with_lrs_base_3(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_with_lrs_base_3.append(train_accuracy)\n",
    "    train_loss_list_with_lrs_base_3.append(train_loss)\n",
    "\n",
    "    model_with_lrs_base_3.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_with_lrs_base_3(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_with_lrs_base_3.append(val_accuracy)\n",
    "    val_loss_list_with_lrs_base_3.append(val_loss)\n",
    "\n",
    "    test_predictions_lrs_base_3 = model_with_lrs_base_3(X_test_tensor).view(-1)\n",
    "    test_predictions_rounded_lrs_base_3 = torch.round(test_predictions_lrs_base_3)\n",
    "    test_predictions_rounded_numpy_lrs_base_3 = test_predictions_rounded_lrs_base_3.cpu().detach().numpy()\n",
    "    y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "    accuracy_lrs_base_3 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_lrs_base_3)\n",
    "    test_accuracy_list_with_lrs_base_3.append(accuracy_lrs_base_3)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrCX_43pApsf",
    "outputId": "e8036e02-18d0-4a31-dfa7-fa15a2c4613d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base model with LRS: 0.7792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_with_lrs_base_3.eval()\n",
    "test_predictions_lrs_base_3 = model_with_lrs_base_3(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_lrs_base_3 = torch.round(test_predictions_lrs_base_3)\n",
    "\n",
    "test_predictions_rounded_numpy_lrs_base_3 = test_predictions_rounded_lrs_base_3.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_lrs_base_3 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_lrs_base_3)\n",
    "\n",
    "print(f\"Accuracy for base model with LRS: {accuracy_lrs_base_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "V_NN7IWJBmgB",
    "outputId": "b5ada7d9-875b-49da-9992-1fff7e0c103c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.plot(epochs_range, test_accuracy_list_optimizer_asgd_1, label='Base Model Testing Accuracy')\n",
    "plt.plot(epochs_range, test_accuracy_list_with_lrs_base_3, label='Base Model with LRS Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQxI_ZeHIxnU"
   },
   "source": [
    "Adding Early Stopping in Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "dLCaoeY5CS2A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "class BaseModelWithEarlyStopping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModelWithEarlyStopping, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7xKDPKPFoOG",
    "outputId": "650b8600-b763-427f-c429-36430c270a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val diff is:  6.281123220920563\n",
      "Epoch 1/1000, Training Loss: 42.46593, Validation Loss: 6.28112\n",
      "val diff is:  0.005314290523529053\n",
      "Epoch 2/1000, Training Loss: 42.43246, Validation Loss: 6.27581\n",
      "val diff is:  0.0052874088287353516\n",
      "Epoch 3/1000, Training Loss: 42.39920, Validation Loss: 6.27052\n",
      "val diff is:  0.005263268947601318\n",
      "Epoch 4/1000, Training Loss: 42.36610, Validation Loss: 6.26526\n",
      "val diff is:  0.005235254764556885\n",
      "Epoch 5/1000, Training Loss: 42.33318, Validation Loss: 6.26002\n",
      "val diff is:  0.005209088325500488\n",
      "Epoch 6/1000, Training Loss: 42.30043, Validation Loss: 6.25481\n",
      "val diff is:  0.005187511444091797\n",
      "Epoch 7/1000, Training Loss: 42.26784, Validation Loss: 6.24963\n",
      "val diff is:  0.005163908004760742\n",
      "Epoch 8/1000, Training Loss: 42.23541, Validation Loss: 6.24446\n",
      "val diff is:  0.005144238471984863\n",
      "Epoch 9/1000, Training Loss: 42.20313, Validation Loss: 6.23932\n",
      "val diff is:  0.0051223039627075195\n",
      "Epoch 10/1000, Training Loss: 42.17098, Validation Loss: 6.23420\n",
      "val diff is:  0.005098581314086914\n",
      "Epoch 11/1000, Training Loss: 42.13899, Validation Loss: 6.22910\n",
      "val diff is:  0.005075275897979736\n",
      "Epoch 12/1000, Training Loss: 42.10715, Validation Loss: 6.22402\n",
      "val diff is:  0.005053043365478516\n",
      "Epoch 13/1000, Training Loss: 42.07546, Validation Loss: 6.21897\n",
      "val diff is:  0.005029916763305664\n",
      "Epoch 14/1000, Training Loss: 42.04392, Validation Loss: 6.21394\n",
      "val diff is:  0.00500720739364624\n",
      "Epoch 15/1000, Training Loss: 42.01253, Validation Loss: 6.20893\n",
      "val diff is:  0.00498586893081665\n",
      "Epoch 16/1000, Training Loss: 41.98128, Validation Loss: 6.20395\n",
      "val diff is:  0.004967749118804932\n",
      "Epoch 17/1000, Training Loss: 41.95018, Validation Loss: 6.19898\n",
      "val diff is:  0.004944503307342529\n",
      "Epoch 18/1000, Training Loss: 41.91922, Validation Loss: 6.19403\n",
      "val diff is:  0.0049217939376831055\n",
      "Epoch 19/1000, Training Loss: 41.88842, Validation Loss: 6.18911\n",
      "val diff is:  0.004900872707366943\n",
      "Epoch 20/1000, Training Loss: 41.85777, Validation Loss: 6.18421\n",
      "val diff is:  0.004880547523498535\n",
      "Epoch 21/1000, Training Loss: 41.82725, Validation Loss: 6.17933\n",
      "val diff is:  0.004860281944274902\n",
      "Epoch 22/1000, Training Loss: 41.79686, Validation Loss: 6.17447\n",
      "val diff is:  0.00483393669128418\n",
      "Epoch 23/1000, Training Loss: 41.76662, Validation Loss: 6.16964\n",
      "val diff is:  0.0048136115074157715\n",
      "Epoch 24/1000, Training Loss: 41.73651, Validation Loss: 6.16482\n",
      "val diff is:  0.004792451858520508\n",
      "Epoch 25/1000, Training Loss: 41.70653, Validation Loss: 6.16003\n",
      "val diff is:  0.0047713518142700195\n",
      "Epoch 26/1000, Training Loss: 41.67669, Validation Loss: 6.15526\n",
      "val diff is:  0.004751145839691162\n",
      "Epoch 27/1000, Training Loss: 41.64700, Validation Loss: 6.15051\n",
      "val diff is:  0.004731297492980957\n",
      "Epoch 28/1000, Training Loss: 41.61743, Validation Loss: 6.14578\n",
      "val diff is:  0.0047106146812438965\n",
      "Epoch 29/1000, Training Loss: 41.58799, Validation Loss: 6.14107\n",
      "val diff is:  0.004692137241363525\n",
      "Epoch 30/1000, Training Loss: 41.55868, Validation Loss: 6.13637\n",
      "val diff is:  0.004673004150390625\n",
      "Epoch 31/1000, Training Loss: 41.52950, Validation Loss: 6.13170\n",
      "val diff is:  0.0046533942222595215\n",
      "Epoch 32/1000, Training Loss: 41.50043, Validation Loss: 6.12705\n",
      "val diff is:  0.004631161689758301\n",
      "Epoch 33/1000, Training Loss: 41.47150, Validation Loss: 6.12242\n",
      "val diff is:  0.004612565040588379\n",
      "Epoch 34/1000, Training Loss: 41.44269, Validation Loss: 6.11780\n",
      "val diff is:  0.004594743251800537\n",
      "Epoch 35/1000, Training Loss: 41.41400, Validation Loss: 6.11321\n",
      "val diff is:  0.00457531213760376\n",
      "Epoch 36/1000, Training Loss: 41.38543, Validation Loss: 6.10863\n",
      "val diff is:  0.0045555830001831055\n",
      "Epoch 37/1000, Training Loss: 41.35698, Validation Loss: 6.10408\n",
      "val diff is:  0.004539906978607178\n",
      "Epoch 38/1000, Training Loss: 41.32865, Validation Loss: 6.09954\n",
      "val diff is:  0.004521667957305908\n",
      "Epoch 39/1000, Training Loss: 41.30044, Validation Loss: 6.09502\n",
      "val diff is:  0.004503726959228516\n",
      "Epoch 40/1000, Training Loss: 41.27236, Validation Loss: 6.09051\n",
      "val diff is:  0.004484951496124268\n",
      "Epoch 41/1000, Training Loss: 41.24441, Validation Loss: 6.08603\n",
      "val diff is:  0.004467129707336426\n",
      "Epoch 42/1000, Training Loss: 41.21657, Validation Loss: 6.08156\n",
      "val diff is:  0.004446208477020264\n",
      "Epoch 43/1000, Training Loss: 41.18886, Validation Loss: 6.07711\n",
      "val diff is:  0.00442957878112793\n",
      "Epoch 44/1000, Training Loss: 41.16126, Validation Loss: 6.07268\n",
      "val diff is:  0.004412293434143066\n",
      "Epoch 45/1000, Training Loss: 41.13377, Validation Loss: 6.06827\n",
      "val diff is:  0.004397034645080566\n",
      "Epoch 46/1000, Training Loss: 41.10639, Validation Loss: 6.06388\n",
      "val diff is:  0.004380285739898682\n",
      "Epoch 47/1000, Training Loss: 41.07911, Validation Loss: 6.05950\n",
      "val diff is:  0.0043642520904541016\n",
      "Epoch 48/1000, Training Loss: 41.05193, Validation Loss: 6.05513\n",
      "val diff is:  0.004345953464508057\n",
      "Epoch 49/1000, Training Loss: 41.02487, Validation Loss: 6.05079\n",
      "val diff is:  0.004329979419708252\n",
      "Epoch 50/1000, Training Loss: 40.99792, Validation Loss: 6.04646\n",
      "val diff is:  0.004314839839935303\n",
      "Epoch 51/1000, Training Loss: 40.97108, Validation Loss: 6.04214\n",
      "val diff is:  0.004297912120819092\n",
      "Epoch 52/1000, Training Loss: 40.94436, Validation Loss: 6.03784\n",
      "val diff is:  0.004281342029571533\n",
      "Epoch 53/1000, Training Loss: 40.91774, Validation Loss: 6.03356\n",
      "val diff is:  0.004267275333404541\n",
      "Epoch 54/1000, Training Loss: 40.89124, Validation Loss: 6.02929\n",
      "val diff is:  0.004251599311828613\n",
      "Epoch 55/1000, Training Loss: 40.86483, Validation Loss: 6.02504\n",
      "val diff is:  0.004235625267028809\n",
      "Epoch 56/1000, Training Loss: 40.83852, Validation Loss: 6.02081\n",
      "val diff is:  0.004216194152832031\n",
      "Epoch 57/1000, Training Loss: 40.81232, Validation Loss: 6.01659\n",
      "val diff is:  0.004201054573059082\n",
      "Epoch 58/1000, Training Loss: 40.78621, Validation Loss: 6.01239\n",
      "val diff is:  0.004183411598205566\n",
      "Epoch 59/1000, Training Loss: 40.76020, Validation Loss: 6.00821\n",
      "val diff is:  0.004169285297393799\n",
      "Epoch 60/1000, Training Loss: 40.73429, Validation Loss: 6.00404\n",
      "val diff is:  0.0041519999504089355\n",
      "Epoch 61/1000, Training Loss: 40.70848, Validation Loss: 5.99988\n",
      "val diff is:  0.004137277603149414\n",
      "Epoch 62/1000, Training Loss: 40.68276, Validation Loss: 5.99575\n",
      "val diff is:  0.0041228532791137695\n",
      "Epoch 63/1000, Training Loss: 40.65714, Validation Loss: 5.99162\n",
      "val diff is:  0.004108786582946777\n",
      "Epoch 64/1000, Training Loss: 40.63162, Validation Loss: 5.98752\n",
      "val diff is:  0.0040937066078186035\n",
      "Epoch 65/1000, Training Loss: 40.60619, Validation Loss: 5.98342\n",
      "val diff is:  0.00408017635345459\n",
      "Epoch 66/1000, Training Loss: 40.58087, Validation Loss: 5.97934\n",
      "val diff is:  0.004067540168762207\n",
      "Epoch 67/1000, Training Loss: 40.55564, Validation Loss: 5.97527\n",
      "val diff is:  0.004055976867675781\n",
      "Epoch 68/1000, Training Loss: 40.53051, Validation Loss: 5.97122\n",
      "val diff is:  0.004041314125061035\n",
      "Epoch 69/1000, Training Loss: 40.50547, Validation Loss: 5.96718\n",
      "val diff is:  0.004025161266326904\n",
      "Epoch 70/1000, Training Loss: 40.48052, Validation Loss: 5.96315\n",
      "val diff is:  0.004012346267700195\n",
      "Epoch 71/1000, Training Loss: 40.45565, Validation Loss: 5.95914\n",
      "val diff is:  0.003999233245849609\n",
      "Epoch 72/1000, Training Loss: 40.43088, Validation Loss: 5.95514\n",
      "val diff is:  0.003988444805145264\n",
      "Epoch 73/1000, Training Loss: 40.40620, Validation Loss: 5.95115\n",
      "val diff is:  0.003975033760070801\n",
      "Epoch 74/1000, Training Loss: 40.38160, Validation Loss: 5.94718\n",
      "val diff is:  0.003961741924285889\n",
      "Epoch 75/1000, Training Loss: 40.35709, Validation Loss: 5.94321\n",
      "val diff is:  0.003947138786315918\n",
      "Epoch 76/1000, Training Loss: 40.33266, Validation Loss: 5.93927\n",
      "val diff is:  0.003933906555175781\n",
      "Epoch 77/1000, Training Loss: 40.30832, Validation Loss: 5.93533\n",
      "val diff is:  0.003920793533325195\n",
      "Epoch 78/1000, Training Loss: 40.28407, Validation Loss: 5.93141\n",
      "val diff is:  0.003908216953277588\n",
      "Epoch 79/1000, Training Loss: 40.25990, Validation Loss: 5.92750\n",
      "val diff is:  0.003893911838531494\n",
      "Epoch 80/1000, Training Loss: 40.23581, Validation Loss: 5.92361\n",
      "val diff is:  0.0038803815841674805\n",
      "Epoch 81/1000, Training Loss: 40.21181, Validation Loss: 5.91973\n",
      "val diff is:  0.003865957260131836\n",
      "Epoch 82/1000, Training Loss: 40.18791, Validation Loss: 5.91586\n",
      "val diff is:  0.0038535594940185547\n",
      "Epoch 83/1000, Training Loss: 40.16409, Validation Loss: 5.91201\n",
      "val diff is:  0.0038407444953918457\n",
      "Epoch 84/1000, Training Loss: 40.14035, Validation Loss: 5.90817\n",
      "val diff is:  0.0038291215896606445\n",
      "Epoch 85/1000, Training Loss: 40.11669, Validation Loss: 5.90434\n",
      "val diff is:  0.003818035125732422\n",
      "Epoch 86/1000, Training Loss: 40.09310, Validation Loss: 5.90052\n",
      "val diff is:  0.0038077831268310547\n",
      "Epoch 87/1000, Training Loss: 40.06959, Validation Loss: 5.89672\n",
      "val diff is:  0.003798067569732666\n",
      "Epoch 88/1000, Training Loss: 40.04616, Validation Loss: 5.89292\n",
      "val diff is:  0.0037837624549865723\n",
      "Epoch 89/1000, Training Loss: 40.02282, Validation Loss: 5.88913\n",
      "val diff is:  0.0037689805030822754\n",
      "Epoch 90/1000, Training Loss: 39.99955, Validation Loss: 5.88536\n",
      "val diff is:  0.0037580132484436035\n",
      "Epoch 91/1000, Training Loss: 39.97636, Validation Loss: 5.88161\n",
      "val diff is:  0.0037478208541870117\n",
      "Epoch 92/1000, Training Loss: 39.95325, Validation Loss: 5.87786\n",
      "val diff is:  0.0037345290184020996\n",
      "Epoch 93/1000, Training Loss: 39.93021, Validation Loss: 5.87412\n",
      "val diff is:  0.003721296787261963\n",
      "Epoch 94/1000, Training Loss: 39.90726, Validation Loss: 5.87040\n",
      "val diff is:  0.003709733486175537\n",
      "Epoch 95/1000, Training Loss: 39.88439, Validation Loss: 5.86669\n",
      "val diff is:  0.003696620464324951\n",
      "Epoch 96/1000, Training Loss: 39.86161, Validation Loss: 5.86300\n",
      "val diff is:  0.0036856532096862793\n",
      "Epoch 97/1000, Training Loss: 39.83890, Validation Loss: 5.85931\n",
      "val diff is:  0.003674328327178955\n",
      "Epoch 98/1000, Training Loss: 39.81626, Validation Loss: 5.85564\n",
      "val diff is:  0.0036624670028686523\n",
      "Epoch 99/1000, Training Loss: 39.79370, Validation Loss: 5.85197\n",
      "val diff is:  0.003651738166809082\n",
      "Epoch 100/1000, Training Loss: 39.77121, Validation Loss: 5.84832\n",
      "val diff is:  0.003641843795776367\n",
      "Epoch 101/1000, Training Loss: 39.74881, Validation Loss: 5.84468\n",
      "val diff is:  0.00363236665725708\n",
      "Epoch 102/1000, Training Loss: 39.72648, Validation Loss: 5.84105\n",
      "val diff is:  0.0036221742630004883\n",
      "Epoch 103/1000, Training Loss: 39.70422, Validation Loss: 5.83743\n",
      "val diff is:  0.003610670566558838\n",
      "Epoch 104/1000, Training Loss: 39.68203, Validation Loss: 5.83382\n",
      "val diff is:  0.003600001335144043\n",
      "Epoch 105/1000, Training Loss: 39.65992, Validation Loss: 5.83022\n",
      "val diff is:  0.0035892128944396973\n",
      "Epoch 106/1000, Training Loss: 39.63787, Validation Loss: 5.82663\n",
      "val diff is:  0.0035776495933532715\n",
      "Epoch 107/1000, Training Loss: 39.61590, Validation Loss: 5.82305\n",
      "val diff is:  0.003566145896911621\n",
      "Epoch 108/1000, Training Loss: 39.59400, Validation Loss: 5.81948\n",
      "val diff is:  0.003553628921508789\n",
      "Epoch 109/1000, Training Loss: 39.57218, Validation Loss: 5.81593\n",
      "val diff is:  0.0035415291786193848\n",
      "Epoch 110/1000, Training Loss: 39.55042, Validation Loss: 5.81239\n",
      "val diff is:  0.003531038761138916\n",
      "Epoch 111/1000, Training Loss: 39.52873, Validation Loss: 5.80886\n",
      "val diff is:  0.003521561622619629\n",
      "Epoch 112/1000, Training Loss: 39.50710, Validation Loss: 5.80533\n",
      "val diff is:  0.003511488437652588\n",
      "Epoch 113/1000, Training Loss: 39.48554, Validation Loss: 5.80182\n",
      "val diff is:  0.003500699996948242\n",
      "Epoch 114/1000, Training Loss: 39.46403, Validation Loss: 5.79832\n",
      "val diff is:  0.003490149974822998\n",
      "Epoch 115/1000, Training Loss: 39.44259, Validation Loss: 5.79483\n",
      "val diff is:  0.003482222557067871\n",
      "Epoch 116/1000, Training Loss: 39.42121, Validation Loss: 5.79135\n",
      "val diff is:  0.0034737586975097656\n",
      "Epoch 117/1000, Training Loss: 39.39988, Validation Loss: 5.78788\n",
      "val diff is:  0.003464937210083008\n",
      "Epoch 118/1000, Training Loss: 39.37861, Validation Loss: 5.78441\n",
      "val diff is:  0.0034559965133666992\n",
      "Epoch 119/1000, Training Loss: 39.35740, Validation Loss: 5.78096\n",
      "val diff is:  0.003446519374847412\n",
      "Epoch 120/1000, Training Loss: 39.33625, Validation Loss: 5.77751\n",
      "val diff is:  0.0034381747245788574\n",
      "Epoch 121/1000, Training Loss: 39.31514, Validation Loss: 5.77407\n",
      "val diff is:  0.0034288763999938965\n",
      "Epoch 122/1000, Training Loss: 39.29409, Validation Loss: 5.77064\n",
      "val diff is:  0.0034195780754089355\n",
      "Epoch 123/1000, Training Loss: 39.27310, Validation Loss: 5.76722\n",
      "val diff is:  0.003413259983062744\n",
      "Epoch 124/1000, Training Loss: 39.25216, Validation Loss: 5.76381\n",
      "val diff is:  0.003404676914215088\n",
      "Epoch 125/1000, Training Loss: 39.23129, Validation Loss: 5.76040\n",
      "val diff is:  0.0033953189849853516\n",
      "Epoch 126/1000, Training Loss: 39.21047, Validation Loss: 5.75701\n",
      "val diff is:  0.003387629985809326\n",
      "Epoch 127/1000, Training Loss: 39.18970, Validation Loss: 5.75362\n",
      "val diff is:  0.0033797025680541992\n",
      "Epoch 128/1000, Training Loss: 39.16900, Validation Loss: 5.75024\n",
      "val diff is:  0.003369748592376709\n",
      "Epoch 129/1000, Training Loss: 39.14835, Validation Loss: 5.74687\n",
      "val diff is:  0.003361046314239502\n",
      "Epoch 130/1000, Training Loss: 39.12775, Validation Loss: 5.74351\n",
      "val diff is:  0.0033533573150634766\n",
      "Epoch 131/1000, Training Loss: 39.10722, Validation Loss: 5.74016\n",
      "val diff is:  0.0033439993858337402\n",
      "Epoch 132/1000, Training Loss: 39.08674, Validation Loss: 5.73681\n",
      "val diff is:  0.003334939479827881\n",
      "Epoch 133/1000, Training Loss: 39.06633, Validation Loss: 5.73348\n",
      "val diff is:  0.0033248066902160645\n",
      "Epoch 134/1000, Training Loss: 39.04599, Validation Loss: 5.73015\n",
      "val diff is:  0.0033113956451416016\n",
      "Epoch 135/1000, Training Loss: 39.02570, Validation Loss: 5.72684\n",
      "val diff is:  0.0033031105995178223\n",
      "Epoch 136/1000, Training Loss: 39.00546, Validation Loss: 5.72354\n",
      "val diff is:  0.003292977809906006\n",
      "Epoch 137/1000, Training Loss: 38.98528, Validation Loss: 5.72025\n",
      "val diff is:  0.0032837986946105957\n",
      "Epoch 138/1000, Training Loss: 38.96515, Validation Loss: 5.71696\n",
      "val diff is:  0.003275930881500244\n",
      "Epoch 139/1000, Training Loss: 38.94508, Validation Loss: 5.71369\n",
      "val diff is:  0.003266572952270508\n",
      "Epoch 140/1000, Training Loss: 38.92507, Validation Loss: 5.71042\n",
      "val diff is:  0.003256499767303467\n",
      "Epoch 141/1000, Training Loss: 38.90514, Validation Loss: 5.70716\n",
      "val diff is:  0.00324857234954834\n",
      "Epoch 142/1000, Training Loss: 38.88527, Validation Loss: 5.70391\n",
      "val diff is:  0.003242015838623047\n",
      "Epoch 143/1000, Training Loss: 38.86545, Validation Loss: 5.70067\n",
      "val diff is:  0.0032344460487365723\n",
      "Epoch 144/1000, Training Loss: 38.84569, Validation Loss: 5.69744\n",
      "val diff is:  0.0032271742820739746\n",
      "Epoch 145/1000, Training Loss: 38.82597, Validation Loss: 5.69421\n",
      "val diff is:  0.0032209157943725586\n",
      "Epoch 146/1000, Training Loss: 38.80629, Validation Loss: 5.69099\n",
      "val diff is:  0.0032115578651428223\n",
      "Epoch 147/1000, Training Loss: 38.78665, Validation Loss: 5.68778\n",
      "val diff is:  0.0032036900520324707\n",
      "Epoch 148/1000, Training Loss: 38.76706, Validation Loss: 5.68457\n",
      "val diff is:  0.0031957030296325684\n",
      "Epoch 149/1000, Training Loss: 38.74752, Validation Loss: 5.68138\n",
      "val diff is:  0.003189682960510254\n",
      "Epoch 150/1000, Training Loss: 38.72802, Validation Loss: 5.67819\n",
      "val diff is:  0.003183722496032715\n",
      "Epoch 151/1000, Training Loss: 38.70856, Validation Loss: 5.67501\n",
      "val diff is:  0.0031792521476745605\n",
      "Epoch 152/1000, Training Loss: 38.68915, Validation Loss: 5.67183\n",
      "val diff is:  0.0031708478927612305\n",
      "Epoch 153/1000, Training Loss: 38.66978, Validation Loss: 5.66866\n",
      "val diff is:  0.003163158893585205\n",
      "Epoch 154/1000, Training Loss: 38.65046, Validation Loss: 5.66549\n",
      "val diff is:  0.0031537413597106934\n",
      "Epoch 155/1000, Training Loss: 38.63121, Validation Loss: 5.66234\n",
      "val diff is:  0.0031462907791137695\n",
      "Epoch 156/1000, Training Loss: 38.61201, Validation Loss: 5.65919\n",
      "val diff is:  0.0031385421752929688\n",
      "Epoch 157/1000, Training Loss: 38.59285, Validation Loss: 5.65605\n",
      "val diff is:  0.003129899501800537\n",
      "Epoch 158/1000, Training Loss: 38.57374, Validation Loss: 5.65292\n",
      "val diff is:  0.0031210780143737793\n",
      "Epoch 159/1000, Training Loss: 38.55468, Validation Loss: 5.64980\n",
      "val diff is:  0.003114163875579834\n",
      "Epoch 160/1000, Training Loss: 38.53567, Validation Loss: 5.64669\n",
      "val diff is:  0.003107309341430664\n",
      "Epoch 161/1000, Training Loss: 38.51670, Validation Loss: 5.64358\n",
      "val diff is:  0.0031012892723083496\n",
      "Epoch 162/1000, Training Loss: 38.49777, Validation Loss: 5.64048\n",
      "val diff is:  0.003094315528869629\n",
      "Epoch 163/1000, Training Loss: 38.47889, Validation Loss: 5.63739\n",
      "val diff is:  0.003089308738708496\n",
      "Epoch 164/1000, Training Loss: 38.46005, Validation Loss: 5.63430\n",
      "val diff is:  0.003082573413848877\n",
      "Epoch 165/1000, Training Loss: 38.44126, Validation Loss: 5.63121\n",
      "val diff is:  0.003075122833251953\n",
      "Epoch 166/1000, Training Loss: 38.42253, Validation Loss: 5.62814\n",
      "val diff is:  0.0030701756477355957\n",
      "Epoch 167/1000, Training Loss: 38.40383, Validation Loss: 5.62507\n",
      "val diff is:  0.003065049648284912\n",
      "Epoch 168/1000, Training Loss: 38.38518, Validation Loss: 5.62200\n",
      "val diff is:  0.0030561089515686035\n",
      "Epoch 169/1000, Training Loss: 38.36657, Validation Loss: 5.61895\n",
      "val diff is:  0.0030471086502075195\n",
      "Epoch 170/1000, Training Loss: 38.34801, Validation Loss: 5.61590\n",
      "val diff is:  0.0030396580696105957\n",
      "Epoch 171/1000, Training Loss: 38.32951, Validation Loss: 5.61286\n",
      "val diff is:  0.0030332207679748535\n",
      "Epoch 172/1000, Training Loss: 38.31107, Validation Loss: 5.60983\n",
      "val diff is:  0.0030258893966674805\n",
      "Epoch 173/1000, Training Loss: 38.29267, Validation Loss: 5.60680\n",
      "val diff is:  0.003020346164703369\n",
      "Epoch 174/1000, Training Loss: 38.27432, Validation Loss: 5.60378\n",
      "val diff is:  0.003014206886291504\n",
      "Epoch 175/1000, Training Loss: 38.25602, Validation Loss: 5.60077\n",
      "val diff is:  0.003006458282470703\n",
      "Epoch 176/1000, Training Loss: 38.23776, Validation Loss: 5.59776\n",
      "val diff is:  0.003000795841217041\n",
      "Epoch 177/1000, Training Loss: 38.21956, Validation Loss: 5.59476\n",
      "val diff is:  0.0029952526092529297\n",
      "Epoch 178/1000, Training Loss: 38.20138, Validation Loss: 5.59176\n",
      "val diff is:  0.0029889345169067383\n",
      "Epoch 179/1000, Training Loss: 38.18325, Validation Loss: 5.58878\n",
      "val diff is:  0.0029829740524291992\n",
      "Epoch 180/1000, Training Loss: 38.16515, Validation Loss: 5.58579\n",
      "val diff is:  0.002977311611175537\n",
      "Epoch 181/1000, Training Loss: 38.14710, Validation Loss: 5.58282\n",
      "val diff is:  0.002969026565551758\n",
      "Epoch 182/1000, Training Loss: 38.12908, Validation Loss: 5.57985\n",
      "val diff is:  0.002961456775665283\n",
      "Epoch 183/1000, Training Loss: 38.11112, Validation Loss: 5.57688\n",
      "val diff is:  0.00295412540435791\n",
      "Epoch 184/1000, Training Loss: 38.09321, Validation Loss: 5.57393\n",
      "val diff is:  0.0029491186141967773\n",
      "Epoch 185/1000, Training Loss: 38.07532, Validation Loss: 5.57098\n",
      "val diff is:  0.002943694591522217\n",
      "Epoch 186/1000, Training Loss: 38.05748, Validation Loss: 5.56804\n",
      "val diff is:  0.002937138080596924\n",
      "Epoch 187/1000, Training Loss: 38.03967, Validation Loss: 5.56510\n",
      "val diff is:  0.002930581569671631\n",
      "Epoch 188/1000, Training Loss: 38.02190, Validation Loss: 5.56217\n",
      "val diff is:  0.0029244422912597656\n",
      "Epoch 189/1000, Training Loss: 38.00416, Validation Loss: 5.55925\n",
      "val diff is:  0.0029199719429016113\n",
      "Epoch 190/1000, Training Loss: 37.98648, Validation Loss: 5.55633\n",
      "val diff is:  0.0029148459434509277\n",
      "Epoch 191/1000, Training Loss: 37.96883, Validation Loss: 5.55341\n",
      "val diff is:  0.002907276153564453\n",
      "Epoch 192/1000, Training Loss: 37.95122, Validation Loss: 5.55050\n",
      "val diff is:  0.002901136875152588\n",
      "Epoch 193/1000, Training Loss: 37.93365, Validation Loss: 5.54760\n",
      "val diff is:  0.0028946995735168457\n",
      "Epoch 194/1000, Training Loss: 37.91613, Validation Loss: 5.54471\n",
      "val diff is:  0.0028887391090393066\n",
      "Epoch 195/1000, Training Loss: 37.89865, Validation Loss: 5.54182\n",
      "val diff is:  0.002883613109588623\n",
      "Epoch 196/1000, Training Loss: 37.88120, Validation Loss: 5.53894\n",
      "val diff is:  0.002876877784729004\n",
      "Epoch 197/1000, Training Loss: 37.86380, Validation Loss: 5.53606\n",
      "val diff is:  0.0028708577156066895\n",
      "Epoch 198/1000, Training Loss: 37.84644, Validation Loss: 5.53319\n",
      "val diff is:  0.0028650760650634766\n",
      "Epoch 199/1000, Training Loss: 37.82913, Validation Loss: 5.53032\n",
      "val diff is:  0.0028591156005859375\n",
      "Epoch 200/1000, Training Loss: 37.81185, Validation Loss: 5.52746\n",
      "val diff is:  0.002852916717529297\n",
      "Epoch 201/1000, Training Loss: 37.79461, Validation Loss: 5.52461\n",
      "val diff is:  0.0028479695320129395\n",
      "Epoch 202/1000, Training Loss: 37.77740, Validation Loss: 5.52176\n",
      "val diff is:  0.0028433799743652344\n",
      "Epoch 203/1000, Training Loss: 37.76023, Validation Loss: 5.51892\n",
      "val diff is:  0.002838432788848877\n",
      "Epoch 204/1000, Training Loss: 37.74308, Validation Loss: 5.51608\n",
      "val diff is:  0.002833843231201172\n",
      "Epoch 205/1000, Training Loss: 37.72598, Validation Loss: 5.51325\n",
      "val diff is:  0.002830326557159424\n",
      "Epoch 206/1000, Training Loss: 37.70891, Validation Loss: 5.51042\n",
      "val diff is:  0.002826690673828125\n",
      "Epoch 207/1000, Training Loss: 37.69186, Validation Loss: 5.50759\n",
      "val diff is:  0.0028203725814819336\n",
      "Epoch 208/1000, Training Loss: 37.67486, Validation Loss: 5.50477\n",
      "val diff is:  0.0028162002563476562\n",
      "Epoch 209/1000, Training Loss: 37.65789, Validation Loss: 5.50195\n",
      "val diff is:  0.002810537815093994\n",
      "Epoch 210/1000, Training Loss: 37.64096, Validation Loss: 5.49914\n",
      "val diff is:  0.0028069019317626953\n",
      "Epoch 211/1000, Training Loss: 37.62406, Validation Loss: 5.49634\n",
      "val diff is:  0.002801358699798584\n",
      "Epoch 212/1000, Training Loss: 37.60719, Validation Loss: 5.49353\n",
      "val diff is:  0.002795398235321045\n",
      "Epoch 213/1000, Training Loss: 37.59036, Validation Loss: 5.49074\n",
      "val diff is:  0.002790689468383789\n",
      "Epoch 214/1000, Training Loss: 37.57357, Validation Loss: 5.48795\n",
      "val diff is:  0.0027858614921569824\n",
      "Epoch 215/1000, Training Loss: 37.55680, Validation Loss: 5.48516\n",
      "val diff is:  0.0027799606323242188\n",
      "Epoch 216/1000, Training Loss: 37.54008, Validation Loss: 5.48238\n",
      "val diff is:  0.0027725696563720703\n",
      "Epoch 217/1000, Training Loss: 37.52338, Validation Loss: 5.47961\n",
      "val diff is:  0.002764582633972168\n",
      "Epoch 218/1000, Training Loss: 37.50672, Validation Loss: 5.47685\n",
      "val diff is:  0.0027600526809692383\n",
      "Epoch 219/1000, Training Loss: 37.49009, Validation Loss: 5.47409\n",
      "val diff is:  0.0027550458908081055\n",
      "Epoch 220/1000, Training Loss: 37.47350, Validation Loss: 5.47133\n",
      "val diff is:  0.002750575542449951\n",
      "Epoch 221/1000, Training Loss: 37.45693, Validation Loss: 5.46858\n",
      "val diff is:  0.002747654914855957\n",
      "Epoch 222/1000, Training Loss: 37.44040, Validation Loss: 5.46583\n",
      "val diff is:  0.002742767333984375\n",
      "Epoch 223/1000, Training Loss: 37.42391, Validation Loss: 5.46309\n",
      "val diff is:  0.002737760543823242\n",
      "Epoch 224/1000, Training Loss: 37.40745, Validation Loss: 5.46035\n",
      "val diff is:  0.002734243869781494\n",
      "Epoch 225/1000, Training Loss: 37.39101, Validation Loss: 5.45762\n",
      "val diff is:  0.0027300119400024414\n",
      "Epoch 226/1000, Training Loss: 37.37460, Validation Loss: 5.45489\n",
      "val diff is:  0.002729177474975586\n",
      "Epoch 227/1000, Training Loss: 37.35821, Validation Loss: 5.45216\n",
      "val diff is:  0.0027263760566711426\n",
      "Epoch 228/1000, Training Loss: 37.34185, Validation Loss: 5.44943\n",
      "val diff is:  0.0027214884757995605\n",
      "Epoch 229/1000, Training Loss: 37.32552, Validation Loss: 5.44671\n",
      "val diff is:  0.002715766429901123\n",
      "Epoch 230/1000, Training Loss: 37.30923, Validation Loss: 5.44399\n",
      "val diff is:  0.002711057662963867\n",
      "Epoch 231/1000, Training Loss: 37.29297, Validation Loss: 5.44128\n",
      "val diff is:  0.0027103424072265625\n",
      "Epoch 232/1000, Training Loss: 37.27672, Validation Loss: 5.43857\n",
      "val diff is:  0.002705812454223633\n",
      "Epoch 233/1000, Training Loss: 37.26050, Validation Loss: 5.43587\n",
      "val diff is:  0.0027022957801818848\n",
      "Epoch 234/1000, Training Loss: 37.24431, Validation Loss: 5.43317\n",
      "val diff is:  0.002698183059692383\n",
      "Epoch 235/1000, Training Loss: 37.22814, Validation Loss: 5.43047\n",
      "val diff is:  0.00269317626953125\n",
      "Epoch 236/1000, Training Loss: 37.21200, Validation Loss: 5.42777\n",
      "val diff is:  0.002688586711883545\n",
      "Epoch 237/1000, Training Loss: 37.19589, Validation Loss: 5.42509\n",
      "val diff is:  0.002684652805328369\n",
      "Epoch 238/1000, Training Loss: 37.17980, Validation Loss: 5.42240\n",
      "val diff is:  0.0026823878288269043\n",
      "Epoch 239/1000, Training Loss: 37.16373, Validation Loss: 5.41972\n",
      "val diff is:  0.002678513526916504\n",
      "Epoch 240/1000, Training Loss: 37.14770, Validation Loss: 5.41704\n",
      "val diff is:  0.002673208713531494\n",
      "Epoch 241/1000, Training Loss: 37.13170, Validation Loss: 5.41437\n",
      "val diff is:  0.0026674270629882812\n",
      "Epoch 242/1000, Training Loss: 37.11573, Validation Loss: 5.41170\n",
      "val diff is:  0.0026634931564331055\n",
      "Epoch 243/1000, Training Loss: 37.09978, Validation Loss: 5.40904\n",
      "val diff is:  0.0026587247848510742\n",
      "Epoch 244/1000, Training Loss: 37.08387, Validation Loss: 5.40638\n",
      "val diff is:  0.0026547908782958984\n",
      "Epoch 245/1000, Training Loss: 37.06798, Validation Loss: 5.40372\n",
      "val diff is:  0.0026491880416870117\n",
      "Epoch 246/1000, Training Loss: 37.05213, Validation Loss: 5.40107\n",
      "val diff is:  0.002643406391143799\n",
      "Epoch 247/1000, Training Loss: 37.03631, Validation Loss: 5.39843\n",
      "val diff is:  0.00264132022857666\n",
      "Epoch 248/1000, Training Loss: 37.02053, Validation Loss: 5.39579\n",
      "val diff is:  0.00263822078704834\n",
      "Epoch 249/1000, Training Loss: 37.00477, Validation Loss: 5.39315\n",
      "val diff is:  0.0026343464851379395\n",
      "Epoch 250/1000, Training Loss: 36.98904, Validation Loss: 5.39052\n",
      "val diff is:  0.002628624439239502\n",
      "Epoch 251/1000, Training Loss: 36.97334, Validation Loss: 5.38789\n",
      "val diff is:  0.0026251673698425293\n",
      "Epoch 252/1000, Training Loss: 36.95767, Validation Loss: 5.38526\n",
      "val diff is:  0.002623438835144043\n",
      "Epoch 253/1000, Training Loss: 36.94203, Validation Loss: 5.38264\n",
      "val diff is:  0.0026189684867858887\n",
      "Epoch 254/1000, Training Loss: 36.92641, Validation Loss: 5.38002\n",
      "val diff is:  0.002615630626678467\n",
      "Epoch 255/1000, Training Loss: 36.91082, Validation Loss: 5.37740\n",
      "val diff is:  0.0026113390922546387\n",
      "Epoch 256/1000, Training Loss: 36.89525, Validation Loss: 5.37479\n",
      "val diff is:  0.0026071667671203613\n",
      "Epoch 257/1000, Training Loss: 36.87971, Validation Loss: 5.37219\n",
      "val diff is:  0.0026033520698547363\n",
      "Epoch 258/1000, Training Loss: 36.86419, Validation Loss: 5.36958\n",
      "val diff is:  0.0025995373725891113\n",
      "Epoch 259/1000, Training Loss: 36.84871, Validation Loss: 5.36698\n",
      "val diff is:  0.002595245838165283\n",
      "Epoch 260/1000, Training Loss: 36.83325, Validation Loss: 5.36439\n",
      "val diff is:  0.0025902986526489258\n",
      "Epoch 261/1000, Training Loss: 36.81782, Validation Loss: 5.36180\n",
      "val diff is:  0.0025846362113952637\n",
      "Epoch 262/1000, Training Loss: 36.80242, Validation Loss: 5.35921\n",
      "val diff is:  0.0025810599327087402\n",
      "Epoch 263/1000, Training Loss: 36.78706, Validation Loss: 5.35663\n",
      "val diff is:  0.002576172351837158\n",
      "Epoch 264/1000, Training Loss: 36.77172, Validation Loss: 5.35405\n",
      "val diff is:  0.00257265567779541\n",
      "Epoch 265/1000, Training Loss: 36.75640, Validation Loss: 5.35148\n",
      "val diff is:  0.002568960189819336\n",
      "Epoch 266/1000, Training Loss: 36.74111, Validation Loss: 5.34891\n",
      "val diff is:  0.0025655031204223633\n",
      "Epoch 267/1000, Training Loss: 36.72584, Validation Loss: 5.34635\n",
      "val diff is:  0.0025611519813537598\n",
      "Epoch 268/1000, Training Loss: 36.71059, Validation Loss: 5.34379\n",
      "val diff is:  0.0025573372840881348\n",
      "Epoch 269/1000, Training Loss: 36.69537, Validation Loss: 5.34123\n",
      "val diff is:  0.002553701400756836\n",
      "Epoch 270/1000, Training Loss: 36.68018, Validation Loss: 5.33868\n",
      "val diff is:  0.0025490522384643555\n",
      "Epoch 271/1000, Training Loss: 36.66502, Validation Loss: 5.33613\n",
      "val diff is:  0.0025454163551330566\n",
      "Epoch 272/1000, Training Loss: 36.64990, Validation Loss: 5.33358\n",
      "val diff is:  0.0025435686111450195\n",
      "Epoch 273/1000, Training Loss: 36.63479, Validation Loss: 5.33104\n",
      "val diff is:  0.002542555332183838\n",
      "Epoch 274/1000, Training Loss: 36.61972, Validation Loss: 5.32850\n",
      "val diff is:  0.0025415420532226562\n",
      "Epoch 275/1000, Training Loss: 36.60467, Validation Loss: 5.32595\n",
      "val diff is:  0.0025382637977600098\n",
      "Epoch 276/1000, Training Loss: 36.58964, Validation Loss: 5.32342\n",
      "val diff is:  0.002534627914428711\n",
      "Epoch 277/1000, Training Loss: 36.57463, Validation Loss: 5.32088\n",
      "val diff is:  0.002530813217163086\n",
      "Epoch 278/1000, Training Loss: 36.55965, Validation Loss: 5.31835\n",
      "val diff is:  0.002527594566345215\n",
      "Epoch 279/1000, Training Loss: 36.54470, Validation Loss: 5.31582\n",
      "val diff is:  0.00252455472946167\n",
      "Epoch 280/1000, Training Loss: 36.52977, Validation Loss: 5.31330\n",
      "val diff is:  0.0025212764739990234\n",
      "Epoch 281/1000, Training Loss: 36.51486, Validation Loss: 5.31078\n",
      "val diff is:  0.0025171637535095215\n",
      "Epoch 282/1000, Training Loss: 36.49999, Validation Loss: 5.30826\n",
      "val diff is:  0.0025138258934020996\n",
      "Epoch 283/1000, Training Loss: 36.48514, Validation Loss: 5.30575\n",
      "val diff is:  0.002512037754058838\n",
      "Epoch 284/1000, Training Loss: 36.47031, Validation Loss: 5.30323\n",
      "val diff is:  0.0025080442428588867\n",
      "Epoch 285/1000, Training Loss: 36.45550, Validation Loss: 5.30073\n",
      "val diff is:  0.002504885196685791\n",
      "Epoch 286/1000, Training Loss: 36.44071, Validation Loss: 5.29822\n",
      "val diff is:  0.00250089168548584\n",
      "Epoch 287/1000, Training Loss: 36.42595, Validation Loss: 5.29572\n",
      "val diff is:  0.0024982094764709473\n",
      "Epoch 288/1000, Training Loss: 36.41122, Validation Loss: 5.29322\n",
      "val diff is:  0.0024944543838500977\n",
      "Epoch 289/1000, Training Loss: 36.39651, Validation Loss: 5.29073\n",
      "val diff is:  0.002492249011993408\n",
      "Epoch 290/1000, Training Loss: 36.38183, Validation Loss: 5.28823\n",
      "val diff is:  0.002488553524017334\n",
      "Epoch 291/1000, Training Loss: 36.36718, Validation Loss: 5.28575\n",
      "val diff is:  0.002484619617462158\n",
      "Epoch 292/1000, Training Loss: 36.35254, Validation Loss: 5.28326\n",
      "val diff is:  0.0024805665016174316\n",
      "Epoch 293/1000, Training Loss: 36.33794, Validation Loss: 5.28078\n",
      "val diff is:  0.002476811408996582\n",
      "Epoch 294/1000, Training Loss: 36.32335, Validation Loss: 5.27830\n",
      "val diff is:  0.0024764537811279297\n",
      "Epoch 295/1000, Training Loss: 36.30878, Validation Loss: 5.27583\n",
      "val diff is:  0.0024750232696533203\n",
      "Epoch 296/1000, Training Loss: 36.29423, Validation Loss: 5.27335\n",
      "val diff is:  0.002472817897796631\n",
      "Epoch 297/1000, Training Loss: 36.27969, Validation Loss: 5.27088\n",
      "val diff is:  0.0024687647819519043\n",
      "Epoch 298/1000, Training Loss: 36.26518, Validation Loss: 5.26841\n",
      "val diff is:  0.002465486526489258\n",
      "Epoch 299/1000, Training Loss: 36.25069, Validation Loss: 5.26595\n",
      "val diff is:  0.0024631619453430176\n",
      "Epoch 300/1000, Training Loss: 36.23622, Validation Loss: 5.26348\n",
      "val diff is:  0.002460002899169922\n",
      "Epoch 301/1000, Training Loss: 36.22176, Validation Loss: 5.26102\n",
      "val diff is:  0.0024562478065490723\n",
      "Epoch 302/1000, Training Loss: 36.20732, Validation Loss: 5.25857\n",
      "val diff is:  0.0024518966674804688\n",
      "Epoch 303/1000, Training Loss: 36.19291, Validation Loss: 5.25611\n",
      "val diff is:  0.0024495720863342285\n",
      "Epoch 304/1000, Training Loss: 36.17851, Validation Loss: 5.25366\n",
      "val diff is:  0.002446413040161133\n",
      "Epoch 305/1000, Training Loss: 36.16412, Validation Loss: 5.25122\n",
      "val diff is:  0.0024456381797790527\n",
      "Epoch 306/1000, Training Loss: 36.14976, Validation Loss: 5.24877\n",
      "val diff is:  0.002442479133605957\n",
      "Epoch 307/1000, Training Loss: 36.13541, Validation Loss: 5.24633\n",
      "val diff is:  0.0024376511573791504\n",
      "Epoch 308/1000, Training Loss: 36.12109, Validation Loss: 5.24389\n",
      "val diff is:  0.00243455171585083\n",
      "Epoch 309/1000, Training Loss: 36.10678, Validation Loss: 5.24146\n",
      "val diff is:  0.0024309754371643066\n",
      "Epoch 310/1000, Training Loss: 36.09249, Validation Loss: 5.23903\n",
      "val diff is:  0.0024248361587524414\n",
      "Epoch 311/1000, Training Loss: 36.07822, Validation Loss: 5.23660\n",
      "val diff is:  0.002420961856842041\n",
      "Epoch 312/1000, Training Loss: 36.06398, Validation Loss: 5.23418\n",
      "val diff is:  0.0024187564849853516\n",
      "Epoch 313/1000, Training Loss: 36.04975, Validation Loss: 5.23176\n",
      "val diff is:  0.0024149417877197266\n",
      "Epoch 314/1000, Training Loss: 36.03552, Validation Loss: 5.22935\n",
      "val diff is:  0.002411186695098877\n",
      "Epoch 315/1000, Training Loss: 36.02132, Validation Loss: 5.22694\n",
      "val diff is:  0.0024111270904541016\n",
      "Epoch 316/1000, Training Loss: 36.00713, Validation Loss: 5.22453\n",
      "val diff is:  0.002408325672149658\n",
      "Epoch 317/1000, Training Loss: 35.99296, Validation Loss: 5.22212\n",
      "val diff is:  0.002405822277069092\n",
      "Epoch 318/1000, Training Loss: 35.97882, Validation Loss: 5.21971\n",
      "val diff is:  0.00240403413772583\n",
      "Epoch 319/1000, Training Loss: 35.96470, Validation Loss: 5.21731\n",
      "val diff is:  0.0023999810218811035\n",
      "Epoch 320/1000, Training Loss: 35.95060, Validation Loss: 5.21491\n",
      "val diff is:  0.0023975372314453125\n",
      "Epoch 321/1000, Training Loss: 35.93652, Validation Loss: 5.21251\n",
      "val diff is:  0.0023932456970214844\n",
      "Epoch 322/1000, Training Loss: 35.92246, Validation Loss: 5.21012\n",
      "val diff is:  0.0023911595344543457\n",
      "Epoch 323/1000, Training Loss: 35.90843, Validation Loss: 5.20773\n",
      "val diff is:  0.002389073371887207\n",
      "Epoch 324/1000, Training Loss: 35.89441, Validation Loss: 5.20534\n",
      "val diff is:  0.002385258674621582\n",
      "Epoch 325/1000, Training Loss: 35.88041, Validation Loss: 5.20295\n",
      "val diff is:  0.0023822784423828125\n",
      "Epoch 326/1000, Training Loss: 35.86643, Validation Loss: 5.20057\n",
      "val diff is:  0.0023784637451171875\n",
      "Epoch 327/1000, Training Loss: 35.85248, Validation Loss: 5.19819\n",
      "val diff is:  0.0023748278617858887\n",
      "Epoch 328/1000, Training Loss: 35.83856, Validation Loss: 5.19582\n",
      "val diff is:  0.0023726224899291992\n",
      "Epoch 329/1000, Training Loss: 35.82466, Validation Loss: 5.19344\n",
      "val diff is:  0.0023698806762695312\n",
      "Epoch 330/1000, Training Loss: 35.81078, Validation Loss: 5.19107\n",
      "val diff is:  0.0023682713508605957\n",
      "Epoch 331/1000, Training Loss: 35.79691, Validation Loss: 5.18870\n",
      "val diff is:  0.002365410327911377\n",
      "Epoch 332/1000, Training Loss: 35.78305, Validation Loss: 5.18634\n",
      "val diff is:  0.0023624300956726074\n",
      "Epoch 333/1000, Training Loss: 35.76922, Validation Loss: 5.18398\n",
      "val diff is:  0.0023592114448547363\n",
      "Epoch 334/1000, Training Loss: 35.75540, Validation Loss: 5.18162\n",
      "val diff is:  0.0023589134216308594\n",
      "Epoch 335/1000, Training Loss: 35.74160, Validation Loss: 5.17926\n",
      "val diff is:  0.002358555793762207\n",
      "Epoch 336/1000, Training Loss: 35.72782, Validation Loss: 5.17690\n",
      "val diff is:  0.0023571252822875977\n",
      "Epoch 337/1000, Training Loss: 35.71402, Validation Loss: 5.17454\n",
      "val diff is:  0.002354145050048828\n",
      "Epoch 338/1000, Training Loss: 35.70024, Validation Loss: 5.17219\n",
      "val diff is:  0.0023502707481384277\n",
      "Epoch 339/1000, Training Loss: 35.68648, Validation Loss: 5.16984\n",
      "val diff is:  0.0023464858531951904\n",
      "Epoch 340/1000, Training Loss: 35.67276, Validation Loss: 5.16749\n",
      "val diff is:  0.0023429691791534424\n",
      "Epoch 341/1000, Training Loss: 35.65906, Validation Loss: 5.16515\n",
      "val diff is:  0.002338886260986328\n",
      "Epoch 342/1000, Training Loss: 35.64539, Validation Loss: 5.16281\n",
      "val diff is:  0.0023370087146759033\n",
      "Epoch 343/1000, Training Loss: 35.63173, Validation Loss: 5.16047\n",
      "val diff is:  0.002334088087081909\n",
      "Epoch 344/1000, Training Loss: 35.61809, Validation Loss: 5.15814\n",
      "val diff is:  0.0023332536220550537\n",
      "Epoch 345/1000, Training Loss: 35.60446, Validation Loss: 5.15581\n",
      "val diff is:  0.0023312270641326904\n",
      "Epoch 346/1000, Training Loss: 35.59086, Validation Loss: 5.15347\n",
      "val diff is:  0.0023280680179595947\n",
      "Epoch 347/1000, Training Loss: 35.57728, Validation Loss: 5.15115\n",
      "val diff is:  0.0023250579833984375\n",
      "Epoch 348/1000, Training Loss: 35.56372, Validation Loss: 5.14882\n",
      "val diff is:  0.002322763204574585\n",
      "Epoch 349/1000, Training Loss: 35.55018, Validation Loss: 5.14650\n",
      "val diff is:  0.0023207366466522217\n",
      "Epoch 350/1000, Training Loss: 35.53665, Validation Loss: 5.14418\n",
      "val diff is:  0.0023171305656433105\n",
      "Epoch 351/1000, Training Loss: 35.52314, Validation Loss: 5.14186\n",
      "val diff is:  0.0023145973682403564\n",
      "Epoch 352/1000, Training Loss: 35.50964, Validation Loss: 5.13955\n",
      "val diff is:  0.00231286883354187\n",
      "Epoch 353/1000, Training Loss: 35.49616, Validation Loss: 5.13723\n",
      "val diff is:  0.002309650182723999\n",
      "Epoch 354/1000, Training Loss: 35.48269, Validation Loss: 5.13492\n",
      "val diff is:  0.002308070659637451\n",
      "Epoch 355/1000, Training Loss: 35.46923, Validation Loss: 5.13262\n",
      "val diff is:  0.00230562686920166\n",
      "Epoch 356/1000, Training Loss: 35.45580, Validation Loss: 5.13031\n",
      "val diff is:  0.002303004264831543\n",
      "Epoch 357/1000, Training Loss: 35.44238, Validation Loss: 5.12801\n",
      "val diff is:  0.0023050904273986816\n",
      "Epoch 358/1000, Training Loss: 35.42898, Validation Loss: 5.12570\n",
      "val diff is:  0.00230330228805542\n",
      "Epoch 359/1000, Training Loss: 35.41559, Validation Loss: 5.12340\n",
      "val diff is:  0.002304643392562866\n",
      "Epoch 360/1000, Training Loss: 35.40221, Validation Loss: 5.12109\n",
      "val diff is:  0.0023031234741210938\n",
      "Epoch 361/1000, Training Loss: 35.38886, Validation Loss: 5.11879\n",
      "val diff is:  0.002300351858139038\n",
      "Epoch 362/1000, Training Loss: 35.37552, Validation Loss: 5.11649\n",
      "val diff is:  0.002299070358276367\n",
      "Epoch 363/1000, Training Loss: 35.36221, Validation Loss: 5.11419\n",
      "val diff is:  0.0022971034049987793\n",
      "Epoch 364/1000, Training Loss: 35.34891, Validation Loss: 5.11189\n",
      "val diff is:  0.0022915005683898926\n",
      "Epoch 365/1000, Training Loss: 35.33563, Validation Loss: 5.10960\n",
      "val diff is:  0.0022885501384735107\n",
      "Epoch 366/1000, Training Loss: 35.32236, Validation Loss: 5.10731\n",
      "val diff is:  0.002285093069076538\n",
      "Epoch 367/1000, Training Loss: 35.30912, Validation Loss: 5.10503\n",
      "val diff is:  0.0022831857204437256\n",
      "Epoch 368/1000, Training Loss: 35.29589, Validation Loss: 5.10275\n",
      "val diff is:  0.00228196382522583\n",
      "Epoch 369/1000, Training Loss: 35.28268, Validation Loss: 5.10046\n",
      "val diff is:  0.0022789835929870605\n",
      "Epoch 370/1000, Training Loss: 35.26948, Validation Loss: 5.09818\n",
      "val diff is:  0.0022746622562408447\n",
      "Epoch 371/1000, Training Loss: 35.25631, Validation Loss: 5.09591\n",
      "val diff is:  0.0022728443145751953\n",
      "Epoch 372/1000, Training Loss: 35.24316, Validation Loss: 5.09364\n",
      "val diff is:  0.0022733211517333984\n",
      "Epoch 373/1000, Training Loss: 35.23003, Validation Loss: 5.09136\n",
      "val diff is:  0.002271980047225952\n",
      "Epoch 374/1000, Training Loss: 35.21692, Validation Loss: 5.08909\n",
      "val diff is:  0.0022695958614349365\n",
      "Epoch 375/1000, Training Loss: 35.20381, Validation Loss: 5.08682\n",
      "val diff is:  0.0022675395011901855\n",
      "Epoch 376/1000, Training Loss: 35.19070, Validation Loss: 5.08455\n",
      "val diff is:  0.0022699832916259766\n",
      "Epoch 377/1000, Training Loss: 35.17762, Validation Loss: 5.08228\n",
      "val diff is:  0.002267897129058838\n",
      "Epoch 378/1000, Training Loss: 35.16455, Validation Loss: 5.08002\n",
      "val diff is:  0.0022671520709991455\n",
      "Epoch 379/1000, Training Loss: 35.15150, Validation Loss: 5.07775\n",
      "val diff is:  0.0022664666175842285\n",
      "Epoch 380/1000, Training Loss: 35.13846, Validation Loss: 5.07548\n",
      "val diff is:  0.0022661983966827393\n",
      "Epoch 381/1000, Training Loss: 35.12543, Validation Loss: 5.07322\n",
      "val diff is:  0.0022643208503723145\n",
      "Epoch 382/1000, Training Loss: 35.11242, Validation Loss: 5.07095\n",
      "val diff is:  0.002261340618133545\n",
      "Epoch 383/1000, Training Loss: 35.09944, Validation Loss: 5.06869\n",
      "val diff is:  0.00225985050201416\n",
      "Epoch 384/1000, Training Loss: 35.08646, Validation Loss: 5.06643\n",
      "val diff is:  0.0022584497928619385\n",
      "Epoch 385/1000, Training Loss: 35.07351, Validation Loss: 5.06417\n",
      "val diff is:  0.0022555291652679443\n",
      "Epoch 386/1000, Training Loss: 35.06057, Validation Loss: 5.06192\n",
      "val diff is:  0.0022524893283843994\n",
      "Epoch 387/1000, Training Loss: 35.04765, Validation Loss: 5.05967\n",
      "val diff is:  0.002249389886856079\n",
      "Epoch 388/1000, Training Loss: 35.03475, Validation Loss: 5.05742\n",
      "val diff is:  0.0022478699684143066\n",
      "Epoch 389/1000, Training Loss: 35.02186, Validation Loss: 5.05517\n",
      "val diff is:  0.0022455155849456787\n",
      "Epoch 390/1000, Training Loss: 35.00898, Validation Loss: 5.05292\n",
      "val diff is:  0.0022434592247009277\n",
      "Epoch 391/1000, Training Loss: 34.99611, Validation Loss: 5.05068\n",
      "val diff is:  0.002240777015686035\n",
      "Epoch 392/1000, Training Loss: 34.98326, Validation Loss: 5.04844\n",
      "val diff is:  0.0022369325160980225\n",
      "Epoch 393/1000, Training Loss: 34.97043, Validation Loss: 5.04620\n",
      "val diff is:  0.0022350549697875977\n",
      "Epoch 394/1000, Training Loss: 34.95760, Validation Loss: 5.04397\n",
      "val diff is:  0.002233445644378662\n",
      "Epoch 395/1000, Training Loss: 34.94478, Validation Loss: 5.04173\n",
      "val diff is:  0.002232849597930908\n",
      "Epoch 396/1000, Training Loss: 34.93197, Validation Loss: 5.03950\n",
      "val diff is:  0.002229928970336914\n",
      "Epoch 397/1000, Training Loss: 34.91916, Validation Loss: 5.03727\n",
      "val diff is:  0.002227604389190674\n",
      "Epoch 398/1000, Training Loss: 34.90637, Validation Loss: 5.03504\n",
      "val diff is:  0.002225428819656372\n",
      "Epoch 399/1000, Training Loss: 34.89360, Validation Loss: 5.03282\n",
      "val diff is:  0.0022226274013519287\n",
      "Epoch 400/1000, Training Loss: 34.88085, Validation Loss: 5.03059\n",
      "val diff is:  0.00222054123878479\n",
      "Epoch 401/1000, Training Loss: 34.86812, Validation Loss: 5.02837\n",
      "val diff is:  0.002217710018157959\n",
      "Epoch 402/1000, Training Loss: 34.85540, Validation Loss: 5.02616\n",
      "val diff is:  0.0022152066230773926\n",
      "Epoch 403/1000, Training Loss: 34.84270, Validation Loss: 5.02394\n",
      "val diff is:  0.00221291184425354\n",
      "Epoch 404/1000, Training Loss: 34.83002, Validation Loss: 5.02173\n",
      "val diff is:  0.002213120460510254\n",
      "Epoch 405/1000, Training Loss: 34.81735, Validation Loss: 5.01951\n",
      "val diff is:  0.0022104978561401367\n",
      "Epoch 406/1000, Training Loss: 34.80467, Validation Loss: 5.01730\n",
      "val diff is:  0.002209603786468506\n",
      "Epoch 407/1000, Training Loss: 34.79200, Validation Loss: 5.01509\n",
      "val diff is:  0.0022086501121520996\n",
      "Epoch 408/1000, Training Loss: 34.77933, Validation Loss: 5.01289\n",
      "val diff is:  0.0022058188915252686\n",
      "Epoch 409/1000, Training Loss: 34.76668, Validation Loss: 5.01068\n",
      "val diff is:  0.0022034943103790283\n",
      "Epoch 410/1000, Training Loss: 34.75404, Validation Loss: 5.00848\n",
      "val diff is:  0.0022006630897521973\n",
      "Epoch 411/1000, Training Loss: 34.74142, Validation Loss: 5.00628\n",
      "val diff is:  0.002197444438934326\n",
      "Epoch 412/1000, Training Loss: 34.72883, Validation Loss: 5.00408\n",
      "val diff is:  0.002197742462158203\n",
      "Epoch 413/1000, Training Loss: 34.71624, Validation Loss: 5.00188\n",
      "val diff is:  0.002197176218032837\n",
      "Epoch 414/1000, Training Loss: 34.70366, Validation Loss: 4.99968\n",
      "val diff is:  0.0021986663341522217\n",
      "Epoch 415/1000, Training Loss: 34.69109, Validation Loss: 4.99749\n",
      "val diff is:  0.002196192741394043\n",
      "Epoch 416/1000, Training Loss: 34.67854, Validation Loss: 4.99529\n",
      "val diff is:  0.0021941065788269043\n",
      "Epoch 417/1000, Training Loss: 34.66599, Validation Loss: 4.99309\n",
      "val diff is:  0.0021918118000030518\n",
      "Epoch 418/1000, Training Loss: 34.65346, Validation Loss: 4.99090\n",
      "val diff is:  0.0021886229515075684\n",
      "Epoch 419/1000, Training Loss: 34.64094, Validation Loss: 4.98871\n",
      "val diff is:  0.0021860599517822266\n",
      "Epoch 420/1000, Training Loss: 34.62843, Validation Loss: 4.98653\n",
      "val diff is:  0.0021837949752807617\n",
      "Epoch 421/1000, Training Loss: 34.61595, Validation Loss: 4.98434\n",
      "val diff is:  0.0021805763244628906\n",
      "Epoch 422/1000, Training Loss: 34.60347, Validation Loss: 4.98216\n",
      "val diff is:  0.002177029848098755\n",
      "Epoch 423/1000, Training Loss: 34.59100, Validation Loss: 4.97999\n",
      "val diff is:  0.002176046371459961\n",
      "Epoch 424/1000, Training Loss: 34.57856, Validation Loss: 4.97781\n",
      "val diff is:  0.0021736323833465576\n",
      "Epoch 425/1000, Training Loss: 34.56613, Validation Loss: 4.97564\n",
      "val diff is:  0.0021727681159973145\n",
      "Epoch 426/1000, Training Loss: 34.55370, Validation Loss: 4.97346\n",
      "val diff is:  0.002170860767364502\n",
      "Epoch 427/1000, Training Loss: 34.54129, Validation Loss: 4.97129\n",
      "val diff is:  0.002169013023376465\n",
      "Epoch 428/1000, Training Loss: 34.52890, Validation Loss: 4.96912\n",
      "val diff is:  0.0021674931049346924\n",
      "Epoch 429/1000, Training Loss: 34.51652, Validation Loss: 4.96696\n",
      "val diff is:  0.0021646320819854736\n",
      "Epoch 430/1000, Training Loss: 34.50416, Validation Loss: 4.96479\n",
      "val diff is:  0.0021643340587615967\n",
      "Epoch 431/1000, Training Loss: 34.49182, Validation Loss: 4.96263\n",
      "val diff is:  0.00216066837310791\n",
      "Epoch 432/1000, Training Loss: 34.47949, Validation Loss: 4.96047\n",
      "val diff is:  0.00215834379196167\n",
      "Epoch 433/1000, Training Loss: 34.46719, Validation Loss: 4.95831\n",
      "val diff is:  0.0021573007106781006\n",
      "Epoch 434/1000, Training Loss: 34.45490, Validation Loss: 4.95615\n",
      "val diff is:  0.0021545886993408203\n",
      "Epoch 435/1000, Training Loss: 34.44263, Validation Loss: 4.95400\n",
      "val diff is:  0.002152681350708008\n",
      "Epoch 436/1000, Training Loss: 34.43038, Validation Loss: 4.95184\n",
      "val diff is:  0.0021510422229766846\n",
      "Epoch 437/1000, Training Loss: 34.41814, Validation Loss: 4.94969\n",
      "val diff is:  0.002150297164916992\n",
      "Epoch 438/1000, Training Loss: 34.40592, Validation Loss: 4.94754\n",
      "val diff is:  0.0021477043628692627\n",
      "Epoch 439/1000, Training Loss: 34.39372, Validation Loss: 4.94540\n",
      "val diff is:  0.0021456480026245117\n",
      "Epoch 440/1000, Training Loss: 34.38153, Validation Loss: 4.94325\n",
      "val diff is:  0.0021431148052215576\n",
      "Epoch 441/1000, Training Loss: 34.36936, Validation Loss: 4.94111\n",
      "val diff is:  0.0021424293518066406\n",
      "Epoch 442/1000, Training Loss: 34.35721, Validation Loss: 4.93896\n",
      "val diff is:  0.002142488956451416\n",
      "Epoch 443/1000, Training Loss: 34.34507, Validation Loss: 4.93682\n",
      "val diff is:  0.002139449119567871\n",
      "Epoch 444/1000, Training Loss: 34.33296, Validation Loss: 4.93468\n",
      "val diff is:  0.0021361708641052246\n",
      "Epoch 445/1000, Training Loss: 34.32087, Validation Loss: 4.93255\n",
      "val diff is:  0.0021333694458007812\n",
      "Epoch 446/1000, Training Loss: 34.30881, Validation Loss: 4.93041\n",
      "val diff is:  0.0021309852600097656\n",
      "Epoch 447/1000, Training Loss: 34.29677, Validation Loss: 4.92828\n",
      "val diff is:  0.0021286606788635254\n",
      "Epoch 448/1000, Training Loss: 34.28474, Validation Loss: 4.92615\n",
      "val diff is:  0.0021257996559143066\n",
      "Epoch 449/1000, Training Loss: 34.27272, Validation Loss: 4.92403\n",
      "val diff is:  0.0021236836910247803\n",
      "Epoch 450/1000, Training Loss: 34.26072, Validation Loss: 4.92190\n",
      "val diff is:  0.0021224915981292725\n",
      "Epoch 451/1000, Training Loss: 34.24873, Validation Loss: 4.91978\n",
      "val diff is:  0.0021179020404815674\n",
      "Epoch 452/1000, Training Loss: 34.23677, Validation Loss: 4.91766\n",
      "val diff is:  0.0021166205406188965\n",
      "Epoch 453/1000, Training Loss: 34.22482, Validation Loss: 4.91555\n",
      "val diff is:  0.0021142661571502686\n",
      "Epoch 454/1000, Training Loss: 34.21289, Validation Loss: 4.91343\n",
      "val diff is:  0.002113938331604004\n",
      "Epoch 455/1000, Training Loss: 34.20094, Validation Loss: 4.91132\n",
      "val diff is:  0.002111107110977173\n",
      "Epoch 456/1000, Training Loss: 34.18901, Validation Loss: 4.90921\n",
      "val diff is:  0.0021094977855682373\n",
      "Epoch 457/1000, Training Loss: 34.17710, Validation Loss: 4.90710\n",
      "val diff is:  0.0021078288555145264\n",
      "Epoch 458/1000, Training Loss: 34.16520, Validation Loss: 4.90499\n",
      "val diff is:  0.0021072030067443848\n",
      "Epoch 459/1000, Training Loss: 34.15330, Validation Loss: 4.90288\n",
      "val diff is:  0.002108335494995117\n",
      "Epoch 460/1000, Training Loss: 34.14142, Validation Loss: 4.90077\n",
      "val diff is:  0.0021076500415802\n",
      "Epoch 461/1000, Training Loss: 34.12956, Validation Loss: 4.89867\n",
      "val diff is:  0.00210648775100708\n",
      "Epoch 462/1000, Training Loss: 34.11770, Validation Loss: 4.89656\n",
      "val diff is:  0.0021049678325653076\n",
      "Epoch 463/1000, Training Loss: 34.10586, Validation Loss: 4.89446\n",
      "val diff is:  0.0021041929721832275\n",
      "Epoch 464/1000, Training Loss: 34.09404, Validation Loss: 4.89235\n",
      "val diff is:  0.0021036267280578613\n",
      "Epoch 465/1000, Training Loss: 34.08224, Validation Loss: 4.89025\n",
      "val diff is:  0.0021021664142608643\n",
      "Epoch 466/1000, Training Loss: 34.07044, Validation Loss: 4.88815\n",
      "val diff is:  0.002099066972732544\n",
      "Epoch 467/1000, Training Loss: 34.05867, Validation Loss: 4.88605\n",
      "val diff is:  0.0020972490310668945\n",
      "Epoch 468/1000, Training Loss: 34.04691, Validation Loss: 4.88395\n",
      "val diff is:  0.0020951032638549805\n",
      "Epoch 469/1000, Training Loss: 34.03516, Validation Loss: 4.88185\n",
      "val diff is:  0.0020924806594848633\n",
      "Epoch 470/1000, Training Loss: 34.02342, Validation Loss: 4.87976\n",
      "val diff is:  0.0020909905433654785\n",
      "Epoch 471/1000, Training Loss: 34.01170, Validation Loss: 4.87767\n",
      "val diff is:  0.0020884275436401367\n",
      "Epoch 472/1000, Training Loss: 34.00000, Validation Loss: 4.87558\n",
      "val diff is:  0.002088487148284912\n",
      "Epoch 473/1000, Training Loss: 33.98830, Validation Loss: 4.87349\n",
      "val diff is:  0.0020874738693237305\n",
      "Epoch 474/1000, Training Loss: 33.97662, Validation Loss: 4.87141\n",
      "val diff is:  0.0020854175090789795\n",
      "Epoch 475/1000, Training Loss: 33.96495, Validation Loss: 4.86932\n",
      "val diff is:  0.0020837783813476562\n",
      "Epoch 476/1000, Training Loss: 33.95329, Validation Loss: 4.86724\n",
      "val diff is:  0.0020824074745178223\n",
      "Epoch 477/1000, Training Loss: 33.94164, Validation Loss: 4.86515\n",
      "val diff is:  0.002081573009490967\n",
      "Epoch 478/1000, Training Loss: 33.93001, Validation Loss: 4.86307\n",
      "val diff is:  0.0020786821842193604\n",
      "Epoch 479/1000, Training Loss: 33.91839, Validation Loss: 4.86099\n",
      "val diff is:  0.0020762383937835693\n",
      "Epoch 480/1000, Training Loss: 33.90678, Validation Loss: 4.85892\n",
      "val diff is:  0.0020742714405059814\n",
      "Epoch 481/1000, Training Loss: 33.89517, Validation Loss: 4.85684\n",
      "val diff is:  0.002072542905807495\n",
      "Epoch 482/1000, Training Loss: 33.88358, Validation Loss: 4.85477\n",
      "val diff is:  0.002070218324661255\n",
      "Epoch 483/1000, Training Loss: 33.87200, Validation Loss: 4.85270\n",
      "val diff is:  0.0020663440227508545\n",
      "Epoch 484/1000, Training Loss: 33.86045, Validation Loss: 4.85063\n",
      "val diff is:  0.0020650625228881836\n",
      "Epoch 485/1000, Training Loss: 33.84891, Validation Loss: 4.84857\n",
      "val diff is:  0.0020614564418792725\n",
      "Epoch 486/1000, Training Loss: 33.83737, Validation Loss: 4.84651\n",
      "val diff is:  0.002059042453765869\n",
      "Epoch 487/1000, Training Loss: 33.82585, Validation Loss: 4.84445\n",
      "val diff is:  0.0020560026168823242\n",
      "Epoch 488/1000, Training Loss: 33.81433, Validation Loss: 4.84239\n",
      "val diff is:  0.0020537376403808594\n",
      "Epoch 489/1000, Training Loss: 33.80283, Validation Loss: 4.84034\n",
      "val diff is:  0.0020524263381958008\n",
      "Epoch 490/1000, Training Loss: 33.79134, Validation Loss: 4.83829\n",
      "val diff is:  0.002050250768661499\n",
      "Epoch 491/1000, Training Loss: 33.77988, Validation Loss: 4.83624\n",
      "val diff is:  0.0020488202571868896\n",
      "Epoch 492/1000, Training Loss: 33.76843, Validation Loss: 4.83419\n",
      "val diff is:  0.0020483732223510742\n",
      "Epoch 493/1000, Training Loss: 33.75699, Validation Loss: 4.83214\n",
      "val diff is:  0.0020433664321899414\n",
      "Epoch 494/1000, Training Loss: 33.74557, Validation Loss: 4.83010\n",
      "val diff is:  0.0020401477813720703\n",
      "Epoch 495/1000, Training Loss: 33.73418, Validation Loss: 4.82806\n",
      "val diff is:  0.0020370781421661377\n",
      "Epoch 496/1000, Training Loss: 33.72281, Validation Loss: 4.82602\n",
      "val diff is:  0.002033740282058716\n",
      "Epoch 497/1000, Training Loss: 33.71147, Validation Loss: 4.82399\n",
      "val diff is:  0.002031773328781128\n",
      "Epoch 498/1000, Training Loss: 33.70015, Validation Loss: 4.82195\n",
      "val diff is:  0.0020296573638916016\n",
      "Epoch 499/1000, Training Loss: 33.68884, Validation Loss: 4.81992\n",
      "val diff is:  0.002027064561843872\n",
      "Epoch 500/1000, Training Loss: 33.67754, Validation Loss: 4.81790\n",
      "val diff is:  0.0020245909690856934\n",
      "Epoch 501/1000, Training Loss: 33.66627, Validation Loss: 4.81587\n",
      "val diff is:  0.002023458480834961\n",
      "Epoch 502/1000, Training Loss: 33.65501, Validation Loss: 4.81385\n",
      "val diff is:  0.002024531364440918\n",
      "Epoch 503/1000, Training Loss: 33.64376, Validation Loss: 4.81182\n",
      "val diff is:  0.0020231902599334717\n",
      "Epoch 504/1000, Training Loss: 33.63251, Validation Loss: 4.80980\n",
      "val diff is:  0.0020241737365722656\n",
      "Epoch 505/1000, Training Loss: 33.62127, Validation Loss: 4.80778\n",
      "val diff is:  0.00202372670173645\n",
      "Epoch 506/1000, Training Loss: 33.61005, Validation Loss: 4.80575\n",
      "val diff is:  0.002021491527557373\n",
      "Epoch 507/1000, Training Loss: 33.59884, Validation Loss: 4.80373\n",
      "val diff is:  0.0020174384117126465\n",
      "Epoch 508/1000, Training Loss: 33.58765, Validation Loss: 4.80171\n",
      "val diff is:  0.0020157694816589355\n",
      "Epoch 509/1000, Training Loss: 33.57648, Validation Loss: 4.79970\n",
      "val diff is:  0.00201377272605896\n",
      "Epoch 510/1000, Training Loss: 33.56531, Validation Loss: 4.79768\n",
      "val diff is:  0.0020108819007873535\n",
      "Epoch 511/1000, Training Loss: 33.55416, Validation Loss: 4.79567\n",
      "val diff is:  0.0020086467266082764\n",
      "Epoch 512/1000, Training Loss: 33.54302, Validation Loss: 4.79366\n",
      "val diff is:  0.002006351947784424\n",
      "Epoch 513/1000, Training Loss: 33.53190, Validation Loss: 4.79166\n",
      "val diff is:  0.0020022988319396973\n",
      "Epoch 514/1000, Training Loss: 33.52079, Validation Loss: 4.78966\n",
      "val diff is:  0.00200045108795166\n",
      "Epoch 515/1000, Training Loss: 33.50969, Validation Loss: 4.78766\n",
      "val diff is:  0.0019985437393188477\n",
      "Epoch 516/1000, Training Loss: 33.49860, Validation Loss: 4.78566\n",
      "Training took: 69.70 seconds\n"
     ]
    }
   ],
   "source": [
    "def should_we_stop_training(val_diff_list):\n",
    "  if val_diff_list[-1]<=.002:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_with_early_stop_base_4 = BaseModelWithEarlyStopping().to(device)\n",
    "summary(model_with_early_stop_base_4, input_size=(32, 7))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.ASGD(model_with_early_stop_base_4.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_train_validate_tensor, y_train_validate_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list_with_early_stop_base_4=[]\n",
    "val_loss_list_with_early_stop_base_4=[]\n",
    "train_accuracy_list_with_early_stop_base_4=[]\n",
    "val_accuracy_list_with_early_stop_base_4=[]\n",
    "test_accuracy_list_with_early_stop_base_4=[]\n",
    "\n",
    "last_val_loss=0\n",
    "val_diff_list=[]\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model_with_early_stop_base_4.train()\n",
    "    train_loss = 0.0\n",
    "    train_epoch_predictions = []\n",
    "    train_epoch_actuals = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_with_early_stop_base_4(inputs)\n",
    "        train_binary_outputs = torch.round(outputs).cpu().detach().numpy()\n",
    "        train_actuals = labels.cpu().numpy()\n",
    "        train_epoch_predictions.extend(train_binary_outputs)\n",
    "        train_epoch_actuals.extend(train_actuals)\n",
    "        loss = loss_function(outputs, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = accuracy_score(train_epoch_actuals, train_epoch_predictions)\n",
    "    train_accuracy_list_with_early_stop_base_4.append(train_accuracy)\n",
    "    train_loss_list_with_early_stop_base_4.append(train_loss)\n",
    "\n",
    "    model_with_early_stop_base_4.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_predictions = []\n",
    "    val_epoch_actuals = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model_with_early_stop_base_4(val_inputs)\n",
    "            val_binary_outputs = torch.round(val_outputs).cpu().detach().numpy()\n",
    "            val_actuals = val_labels.cpu().numpy()\n",
    "            val_epoch_predictions.extend(val_binary_outputs)\n",
    "            val_epoch_actuals.extend(val_actuals)\n",
    "            loss = loss_function(val_outputs, val_labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = accuracy_score(val_epoch_actuals, val_epoch_predictions)\n",
    "    val_accuracy_list_with_early_stop_base_4.append(val_accuracy)\n",
    "    val_loss_list_with_early_stop_base_4.append(val_loss)\n",
    "\n",
    "    val_diff_list.append(abs(val_loss-last_val_loss))\n",
    "    last_val_loss=val_loss\n",
    "    print(\"val diff is: \", val_diff_list[-1])\n",
    "\n",
    "    test_predictions_early_stop_base_4 = model_with_early_stop_base_4(X_test_tensor).view(-1)\n",
    "    test_predictions_rounded_early_stop_base_4 = torch.round(test_predictions_early_stop_base_4)\n",
    "    test_predictions_rounded_numpy_early_stop_base_4 = test_predictions_rounded_early_stop_base_4.cpu().detach().numpy()\n",
    "    y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "    accuracy_early_stop_base_4 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_early_stop_base_4)\n",
    "    test_accuracy_list_with_early_stop_base_4.append(accuracy_early_stop_base_4)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}\".format())\n",
    "    if should_we_stop_training(val_diff_list):\n",
    "        break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training took: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvIngS42H_2_",
    "outputId": "35cd226c-67ea-44c2-ad6e-e588e02afbe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base model with Early Stopping: 0.7792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_with_early_stop_base_4.eval()\n",
    "test_predictions_early_stop_base_4 = model_with_early_stop_base_4(X_test_tensor).view(-1)\n",
    "test_predictions_rounded_early_stop_base_4 = torch.round(test_predictions_early_stop_base_4)\n",
    "\n",
    "test_predictions_rounded_numpy_early_stop_base_4 = test_predictions_rounded_early_stop_base_4.cpu().detach().numpy()\n",
    "y_test_numpy = y_test_tensor.cpu().numpy()\n",
    "\n",
    "accuracy_early_stop_base_4 = accuracy_score(y_test_numpy, test_predictions_rounded_numpy_early_stop_base_4)\n",
    "\n",
    "print(f\"Accuracy for base model with Early Stopping: {accuracy_early_stop_base_4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Xcgs3lIFI8RG",
    "outputId": "8e4d6661-1065-4349-f92e-9af7a635461f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "while len(test_accuracy_list_with_early_stop_base_4) < len(epochs_range):\n",
    "    test_accuracy_list_with_early_stop_base_4.append(test_accuracy_list_with_early_stop_base_4[-1])\n",
    "\n",
    "plt.plot(epochs_range, test_accuracy_list_optimizer_asgd_1, label='Base Model Testing Accuracy')\n",
    "plt.plot(epochs_range, test_accuracy_list_with_early_stop_base_4, label='Base Model with Early Stopping Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op1tAyP9JNht"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
